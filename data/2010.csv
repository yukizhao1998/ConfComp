Issue Type,Issue key,Issue id,Summary,Description,Assignee,Reporter,Priority,Status,Resolution,Created,Updated
Bug,ZOOKEEPER-637,12444803,Trunk build is failing,"The trunk build is failing when Hudson runs it. The problem seems to be that ivy-init is executed only once, but its definitions (in particular ivy:settings) do not persist, and the failure occurs when we run ivy-retrieve a second time, which requires the definition of ivy:settings.

It seems that the problem occur with ant 1.7.0, but not with 1.7.1, so it could be an ant issue. ",fpj,fpj,Major,Closed,Fixed,06/Jan/10 16:26,26/Mar/10 17:33
Bug,ZOOKEEPER-642,12445368,"""exceeded deadline by N ms"" floods logs","More important zookeeper warnings are drown out by the following several times per minute:

2010-01-12 17:39:57,227:22317(0x4147eb90):ZOO_WARN@zookeeper_interest@1335: Exceeded deadline by 13ms

Perhaps this is an issue with the way virtualized systems manage gettimeofday results?

Maybe the current 10ms threshold could be pushed up a bit.  I notice that 95% of the messages are below 50ms.

Is there an obvious configuration change that I can make to fix this?

config file below:

# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
dataDir=/mnt/zookeeper
# the port at which the clients will connect
clientPort=2181

server.1=hbase.1:2888:3888
server.2=hbase.2:2888:3888
server.3=hbase.3:2888:3888
server.4=hbase.4:2888:3888
server.5=hbase.5:2888:3888
",marccelani,dale6john,Major,Resolved,Fixed,13/Jan/10 01:52,11/May/12 11:00
Bug,ZOOKEEPER-644,12445541,Nightly build failed on hudson.,"the nighthly build has been failing. http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/664/. The problem seems to be 
{code}
BUILD FAILED
java.lang.NoClassDefFoundError: org/apache/ivy/ant/IvyMakePom$Mapping

Total time: 15 minutes 14 seconds
{code}",phunt,mahadev,Major,Closed,Fixed,14/Jan/10 18:43,26/Mar/10 17:25
Bug,ZOOKEEPER-647,12445692,hudson failure in testLeaderShutdown,"http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/666/testReport/org.apache.zookeeper.test/QuorumTest/testLeaderShutdown/

junit.framework.AssertionFailedError: QP failed to shutdown in 30 seconds
	at org.apache.zookeeper.test.QuorumBase.shutdown(QuorumBase.java:293)
	at org.apache.zookeeper.test.QuorumBase.shutdownServers(QuorumBase.java:281)
	at org.apache.zookeeper.test.QuorumBase.tearDown(QuorumBase.java:266)
	at org.apache.zookeeper.test.QuorumTest.tearDown(QuorumTest.java:55)

Flavio, can you triage this one?
",fpj,phunt,Critical,Closed,Fixed,15/Jan/10 22:33,26/Mar/10 17:25
Bug,ZOOKEEPER-653,12446348,hudson failure in LETest,"http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/675/testReport/org.apache.zookeeper.test/LETest/testLE/

junit.framework.AssertionFailedError: Threads didn't join
	at org.apache.zookeeper.test.LETest.testLE(LETest.java:116)
",phunt,phunt,Major,Resolved,Fixed,22/Jan/10 18:07,29/Mar/12 11:40
Bug,ZOOKEEPER-656,12446452,SledgeHammer test - thread.run() deprecated ,Thread.run() used instead of Thread.start() . ,kaykay.unique,kaykay.unique,Major,Closed,Fixed,24/Jan/10 01:27,26/Mar/10 17:25
Bug,ZOOKEEPER-663,12455085,hudson failure in ZKDatabaseCorruptionTest,"http://hudson.zones.apache.org/hudson/job/ZooKeeper-trunk/686/

java.lang.RuntimeException: Unable to run quorum server 
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:380)
	at org.apache.zookeeper.test.ZkDatabaseCorruptionTest.testCorruption(ZkDatabaseCorruptionTest.java:99)
Caused by: java.io.IOException: Invalid magic number 0 != 1514884167
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:455)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:471)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:438)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:519)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:145)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:193)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:377)",mahadev,phunt,Critical,Closed,Fixed,02/Feb/10 17:30,26/Mar/10 17:25
Bug,ZOOKEEPER-667,12456060,java client doesn't allow ipv6 numeric connect string,"The java client doesn't handle ipv6 numeric addresses as they are colon (:) delmited. After splitting the host/port on : we look for the port as the second entry in the array rather than the last entry in the array.
",phunt,phunt,Critical,Closed,Fixed,11/Feb/10 19:26,20/Jun/12 10:39
Bug,ZOOKEEPER-668,12456134,Close method in LedgerInputStream doesn't do anything,I think we should remove the close call in LedgerInputStream. ,fpj,fpj,Major,Closed,Fixed,12/Feb/10 12:12,26/Mar/10 17:25
Bug,ZOOKEEPER-669,12456182,watchedevent tostring should clearly output the state/type/path,the current tostring method is broken,phunt,phunt,Critical,Closed,Fixed,12/Feb/10 20:46,26/Mar/10 17:25
Bug,ZOOKEEPER-670,12456204,zkpython leading to segfault on zookeeper server restart,"Zookeeper client using zkpython segfaults on zookeeper server restart. It is reliably reproducible using the attached script zk.py.
I'm able to stop segfault using the attached patch voyager.patch, but zkpython seems to have deeper issue on its use of watcher_dispatch - on zookeeper server restart, I see up to 6 invocation of watcher_dispatch while my script is simply sleeping in the main thread. This can't be right.",henryr,voyager,Critical,Resolved,Fixed,12/Feb/10 23:21,25/Apr/14 01:44
Bug,ZOOKEEPER-673,12456466,Fix observer documentation regarding leader election,We just need to remove the first two paragraphs of Section 2.,fpj,fpj,Major,Closed,Fixed,16/Feb/10 19:03,26/Mar/10 17:25
Bug,ZOOKEEPER-677,12457054,c client doesn't allow ipv6 numeric connect string,The c client doesn't handle ipv6 numeric addresses as they are colon : delmited. After splitting the host/port on : we look for the port as the second entry in the array rather than the last entry in the array.,mahadev,phunt,Critical,Closed,Fixed,22/Feb/10 18:20,26/Mar/10 17:25
Bug,ZOOKEEPER-681,12457411,Minor doc issue re unset maxClientCnxns,"Just a small issue, the doc says that ""Setting this to 0 or omitting it entirely removes the limit on concurrent connections."", but we ran without this setting, and saw: WARN  [NIOServerCxn.Factory:2181:NIOServerCnxn$Factory@226] - Too many connections from /10.76.251.190 - max is 10

Bug in doc possibly?

",phunt,vegardh,Blocker,Closed,Fixed,25/Feb/10 11:25,26/Mar/10 17:25
Bug,ZOOKEEPER-682,12457414,"Event is not processed when the watcher is set to watch ""/"" if chrooted","After the event notification response from server is received, the client will convert the server path to the client path if chrooted by:

event.setPath(serverPath.substring(chrootPath.length());

If chrootPath and serverPath are the same, then the event's path will be set to a null string.

But the key of the watcher's map is ""/"", not a null string, so the watcher will not get notified at all.",lunastorm,lunastorm,Blocker,Closed,Fixed,25/Feb/10 12:01,26/Mar/10 17:25
Bug,ZOOKEEPER-683,12457699,LogFormatter fails to parse transactional log files,"LogFormatter fails to parse txn log files - seems the tool was never updated to handle FileHeader.

It would be good to update the docs on txn log file to include detail on the file format.",phunt,phunt,Blocker,Closed,Fixed,01/Mar/10 07:04,26/Mar/10 17:25
Bug,ZOOKEEPER-684,12457960,Race in LENonTerminateTest,"testNonTermination failed during a Hudson run for ZOOKEEPER-59. After inspecting the output, it looks like server is electing 2 as a leader and leaving. Given that 2 is just a mock server, server 0 remains alone in leader election.
",henryr,fpj,Critical,Closed,Fixed,03/Mar/10 10:18,26/Mar/10 17:25
Bug,ZOOKEEPER-687,12458181,LENonterminatetest fails on some machines.,"LENonterminateTest fails with the following error:

{noformat}
2010-03-04 20:26:32,347 - INFO  [Thread-0:LeaderElection@155] - Server address: 0.0.0.0/0.0.0.0:11223
2010-03-04 20:26:32,348 - WARN  [Thread-0:LeaderElection@195] - Ignoring exception while looking for leader
java.io.IOException: Network is unreachable
	at java.net.PlainDatagramSocketImpl.send(Native Method)
	at java.net.DatagramSocket.send(DatagramSocket.java:612)
	at org.apache.zookeeper.server.quorum.LeaderElection.lookForLeader(LeaderElection.java:169)
	at org.apache.zookeeper.test.LENonTerminateTest$LEThread.run(LENonTerminateTest.java:83)
{noformat}
",mahadev,mahadev,Major,Closed,Fixed,04/Mar/10 22:12,26/Mar/10 17:25
Bug,ZOOKEEPER-688,12458264,explain session expiration better in the docs & faq,"We are not clear enough (and the diagram we do have seems misleading) on _when_ session expirations are generated. In particular the fact that you only get expirations when the client is connected to the cluster, not when disconnected.

we need to detail:

1) when do you get expiration
2) what is the sequence of events that the watcher sees, from disco state, to getting the expiration (say the expiration happens when the client is disco, what do you see in the watcher while you are getting reconnected)
3) we need to give some examples of how to test this. We should be explicit that ""pulling the network cable"" on the client will not show expiration since the cliient will not be reconnected.
",phunt,phunt,Critical,Closed,Fixed,05/Mar/10 17:10,26/Mar/10 17:25
Bug,ZOOKEEPER-689,12458487,"release build broken - ivysettings.xml not copied during ""package""","ivysettings.xml was added in 3.3.0 but it is not copied into the release artifact via ant ""package"" target of build.xml",phunt,phunt,Blocker,Closed,Fixed,09/Mar/10 01:23,26/Mar/10 17:25
Bug,ZOOKEEPER-691,12458578,Interface changed for NIOServer.Factory,"BookKeeper starts a ZooKeeper server and needs to create an NIOServer.Factory, but the constructor changed.",breed,breed,Major,Closed,Fixed,09/Mar/10 20:40,26/Mar/10 17:25
Bug,ZOOKEEPER-693,12458585,TestObserver stuck in tight notification loop in FLE,"See http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/Zookeeper-Patch-h7.grid.sp2.yahoo.net/77/testReport/junit/org.apache.zookeeper.test/ObserverTest/testObserver/

     [exec]     [junit] 2010-03-04 00:23:37,803 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11229:FastLeaderElection@683] - Notification time out: 3200
     [exec]     [junit] 2010-03-04 00:23:37,804 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11229:FastLeaderElection@689] - Notification: 2, 0, 2, 3, LOOKING, LOOKING, 1

ad infinitum. ",fpj,henryr,Critical,Closed,Fixed,09/Mar/10 21:40,26/Mar/10 17:25
Bug,ZOOKEEPER-696,12458616,"NPE in the hudson logs, seems nioservercnxn closed twice","seeing the following on the console for http://hudson.zones.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/729/

looks like the cnxn is closed twice? (the second time 'sock' is null). perhaps it's due to client closing and sending session term, then closing socket, server sees the read return -1, so closes cnxn, then sees the session close request (which was queued)?

    [junit] 2010-03-10 03:15:53,205 - INFO  [main:NIOServerCnxn@1232] - Closed socket connection for client /127.0.0.1:41285 which had sessionid 0x127461233fc0000
    [junit] 2010-03-10 03:15:53,206 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn$Factory@269] - Ignoring unexpected runtime exception
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.zookeeper.server.NIOServerCnxn.close(NIOServerCnxn.java:1232)
    [junit] 	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:594)
    [junit] 	at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:259)
",phunt,phunt,Blocker,Closed,Fixed,10/Mar/10 05:41,26/Mar/10 17:25
Bug,ZOOKEEPER-698,12458983,intermittent JMX test failures due to not verifying QuorumPeer shutdown ,"in some cases the tests are failing with JMX errors. From the logs I can see that QP was shutdown, however it did not exit it's thread until some time much later. This is causing interference with subsequent tests, causing the test to fail.

I have a patch that attempts to verify that the QP was shutdown (by joining the thread). It turns out that tests based on QuorumBase do this check (join) however some of the other tests do not. I believe this will address the issue.",phunt,phunt,Critical,Closed,Fixed,12/Mar/10 22:22,26/Mar/10 17:25
Bug,ZOOKEEPER-706,12459410,large numbers of watches can cause session re-establishment to fail,"If a client sets a large number of watches the ""set watches"" operation during session re-establishment can fail.

for example:
 WARN  [NIOServerCxn.Factory:22801:NIOServerCnxn@417] - Exception causing close of session 0xe727001201a4ee7c due to java.io.IOException: Len error 4348380

in this case the client was a web monitoring app and had set both data and child watches on > 32k znodes.

there are two issues I see here we need to fix:

1) handle this case properly (split up the set watches into multiple calls I guess...)
2) the session should have expired after the ""timeout"". however we seem to consider any message from the client as re-setting the expiration on the server side. Probably we should only consider messages from the client that are sent during an established session, otherwise we can see this situation where the session is not established however the session is not expired either. Perhaps we should create another JIRA for this particular issue.


",cthunes,phunt,Critical,Closed,Fixed,17/Mar/10 18:08,10/Dec/22 02:21
Bug,ZOOKEEPER-708,12459481,zkpython failing due to undefined symbol deallocate_String_vector,"ant test in zkpython is failing. I think this is due to mahadev's changes to remove unnecessary exports from the client lib.

     [exec] ImportError: /home/phunt/dev/workspace/gitzk/build/contrib/zkpython/lib.linux-x86_64-2.6/zookeeper.so: undefined symbol: deallocate_String_vector

Mahadev can you take a look?
",mahadev,phunt,Blocker,Closed,Fixed,18/Mar/10 05:34,26/Mar/10 17:25
Bug,ZOOKEEPER-709,12459483,bookkeeper build failing with missing factory,"ant test in bookkeeper results in

compile-test:
    [javac] Compiling 10 source files to /home/phunt/dev/workspace/gitzk/build/contrib/bookkeeper/test
    [javac] /home/phunt/dev/workspace/gitzk/src/contrib/bookkeeper/test/org/apache/bookkeeper/test/BaseTestCase.java:91: cannot find symbol
    [javac] symbol  : constructor Factory(java.lang.Integer)
    [javac] location: class org.apache.zookeeper.server.NIOServerCnxn.Factory
    [javac]         serverFactory = new NIOServerCnxn.Factory(ZooKeeperDefaultPort);
    [javac]                         ^
    [javac] 1 error

Flavio can you take a look at this one? (patch)
",phunt,phunt,Blocker,Closed,Fixed,18/Mar/10 05:57,26/Mar/10 17:25
Bug,ZOOKEEPER-710,12459507,permanent ZSESSIONMOVED error after client app reconnects to zookeeper cluster,"Originally problem was described on Users mailing list starting with this [post|http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-user/201003.mbox/<3b910d891003160743k38e2e7c9y830b182d88396d55@mail.gmail.com>].
Below I restate it in more organized form.

We occasionally (few times a day) observe that our client application disconnects from Zookeeper cluster.
Application is written in C++ and we are using libzookeeper_mt library. In version 3.2.2.

The disconnects we are observing are probably related to some problems with our network infrastructure - we are observing periods with great packet loss between machines in our DC. 

Sometimes after client application (i.e. zookeeper library) reconnects to zookeeper cluster we are observing that all subsequent requests return ZSESSIONMOVED error. Restarting client app helps - we always pass 0 as clientid to zookeeper_init function so old session is not reused.

On 16-03-2010 we observed few occurences of problem. Example ones:
- 22:08; client IP 10.1.112.60 (app1); sessionID 0x22767e1c9630000
- 14:21; client IP 10.1.112.61 (app2); sessionID 0x324dcc1ba580085

I attach logs of cluster and application nodes (only stuff concerining zookeeper):
- [^zookeeper-node1.log.2010-03-16.gz] - logs of zookeepr cluster node 1 10.1.112.62
- [^zookeeper-node2.log.2010-03-16.gz] - logs of zookeepr cluster node 2 10.1.112.63
- [^zookeeper-node3.log.2010-03-16.gz] - logs of zookeepr cluster node 3 10.1.112.64
- [^app1.log.2010-03-16.gz] - application logs of app1 10.1.112.60
- [^app2.log.2010-03-16.gz] - application logs of app2 10.1.112.61

I also made some analysis of case at 22:08:
- Network glitch which resulted in problem occurred at about 22:08.
- From what I see since 17:48 node2 was the leader and it did not
change later yesterday.
- Client was connected to node2 since 17:50
- At around 22:09 client tried to connect to every node (1,2,3).
Connections to node1 and node3 were closed
 with exception ""Exception causing close of session 0x22767e1c9630000
due to java.io.IOException: Read error"".
 Connection to node2 stood alive.
- All subsequent operations were refused with ZSESSIONMOVED error.
Error visible both on client and on server side.

",phunt,losipiuk,Blocker,Closed,Fixed,18/Mar/10 11:39,26/Mar/10 17:25
Bug,ZOOKEEPER-718,12459672,the fatjar is missing libraries,"when we moved to ivy, we didn't update the fatjar build.xml to grab libraries out of the new location that ivy uses for downloaded libraries.",breed,breed,Major,Closed,Fixed,19/Mar/10 16:09,26/Mar/10 17:25
Bug,ZOOKEEPER-719,12459682,Add throttling to BookKeeper client,Add throttling to client to control the rate of operations to bookies. ,fpj,fpj,Major,Closed,Fixed,19/Mar/10 17:35,23/Nov/11 19:22
Bug,ZOOKEEPER-720,12459786,Use zookeeper-{version}-sources.jar instead of zookeeper-{version}-src.jar to publish sources in the Maven repository,"The artifact with the sources to be published in the Maven repository should be named ${artifactId}-${version}-sources.jar not ${artifactId}-${version}-src.jar.

See also: http://maven.apache.org/guides/mini/guide-central-repository-upload.html and ZOOKEEPER-224",castagna,castagna,Trivial,Closed,Fixed,21/Mar/10 08:04,23/Nov/11 19:22
Bug,ZOOKEEPER-722,12459860,"zkServer.sh uses sh's builtin echo on BSD, behaves incorrectly.","zkServer.sh output the PID of the zookeeper process with:
echo -n $! > ""$ZOOPIDFILE""

This uses -n which sh's builtin echo does not support. From echo's manpage.
<snip>
     Some shells may provide a builtin echo command which is similar or identical to this utility.  Most notably, the builtin echo in sh(1) does not accept
     the -n option.  Consult the builtin(1) manual page.
</snip>

This means that echo -n PID > ZOOPIDFILE will mean the contents of ZOOPIDFILE will be ""-n PID"". This stops zkServer.sh stop from working correctly.",ikelly,ikelly,Minor,Closed,Fixed,22/Mar/10 14:49,23/Nov/11 19:22
Bug,ZOOKEEPER-731,12460402,"Zookeeper#delete  , #create - async versions miss a verb in the javadoc ","    /**
     * The Asynchronous version of delete. ""The request doesn't  *missing* actually until
     * the asynchronous callback is called.""
     */
    public void delete(final String path, int version, VoidCallback cb, Object ctx) .. 


Also some information in the javadoc about how to instantiate the callback objects / context would be useful . 
",thkoch,kaykay.unique,Minor,Closed,Fixed,26/Mar/10 23:12,23/Nov/11 19:22
Bug,ZOOKEEPER-732,12460641,Improper translation of error into Python exception,"Apparently errors returned by the C library are not being correctly converted into a Python exception in some cases: 

>>> zookeeper.get_children(0, ""/"", None)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
SystemError: error return without exception set
",voyager,niemeyer,Minor,Closed,Fixed,29/Mar/10 22:48,13/Mar/14 18:17
Bug,ZOOKEEPER-734,12461314,QuorumPeerTestBase.java and ZooKeeperServerMainTest.java do not handle windows path correctly,"While runniing ""ant test-core-java"" QuorumPeerTestBase.java and ZooKeeperServerMainTest.java fail. The problem seems to be in ZookeeperserverMainTest.java:MainThread():66 and in QuorumPeerBaseTest.java:MainThread:76.

FileWriter.write() writes windows path to the conf file. Java does not like windows path. Therefore, the test complains that it cannot find myid and fails. 

Solution - convert windows path to UNIX path. This worked for me on windows.  Diffs are attached below. Solution not tested on Linux since for some reason build is failing (due to problems not related to this change).


vmc-floorb-dhcp116-114:/opt/zksrc/zookeeper-3.3.0/src/java/test/org/apache/zookeeper/server # svn diff
Index: ZooKeeperServerMainTest.java
===================================================================
--- ZooKeeperServerMainTest.java	(revision 931240)
+++ ZooKeeperServerMainTest.java	(working copy)
@@ -61,7 +61,8 @@
             if (!dataDir.mkdir()) {
                 throw new IOException(""unable to mkdir "" + dataDir);
             }
-            fwriter.write(""dataDir="" + dataDir.toString() + ""\n"");
+            String data = dataDir.toString().replace('\\', '/');
+            fwriter.write(""dataDir="" + data + ""\n"");
 
             fwriter.write(""clientPort="" + clientPort + ""\n"");
             fwriter.flush();
Index: quorum/QuorumPeerTestBase.java
===================================================================
--- quorum/QuorumPeerTestBase.java	(revision 931240)
+++ quorum/QuorumPeerTestBase.java	(working copy)
@@ -73,7 +73,8 @@
             if (!dataDir.mkdir()) {
                 throw new IOException(""Unable to mkdir "" + dataDir);
             }
-            fwriter.write(""dataDir="" + dataDir.toString() + ""\n"");
+            String data = dataDir.toString().replace('\\', '/');
+            fwriter.write(""dataDir="" + data + ""\n"");
 
             fwriter.write(""clientPort="" + clientPort + ""\n"");
             fwriter.write(quorumCfgSection + ""\n"");",vishalmlst,vishalmlst,Major,Closed,Fixed,06/Apr/10 22:43,23/Nov/11 19:22
Bug,ZOOKEEPER-735,12461462,cppunit test testipv6 assumes that the machine is ipv6 enabled.,The test should be fixed so that it runs only if ipv6 is enabled and does not run if ipv6 is not enabled.,mahadev,mahadev,Major,Closed,Fixed,07/Apr/10 21:26,23/Nov/11 19:22
Bug,ZOOKEEPER-737,12461726,some 4 letter words may fail with netcat (nc),"nc closes the write channel as soon as it's sent it's information, for example ""echo stat|nc localhost 2181""
in general this is fine, however the server code will close the socket as soon as it receives notice that nc has
closed it's write channel. if not all the 4 letter word result has been written back to the client yet, this will cause
some or all of the result to be lost - ie the client will not see the full result. this was introduced in 3.3.0 as part
of a change to reduce blocking of the selector by long running 4letter words.

here's an example of the logs from the server during this

echo -n stat | nc localhost 2181
2010-04-09 21:55:36,124 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn$Factory@251] - Accepted socket connection from /127.0.0.1:42179
2010-04-09 21:55:36,124 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@968] - Processing stat command from /127.0.0.1:42179
2010-04-09 21:55:36,125 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@606] - EndOfStreamException: Unable to read additional data from client sessionid 0x0, likely client has closed socket
2010-04-09 21:55:36,125 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1286] - Closed socket connection for client /127.0.0.1:42179 (no session established for client)
[phunt@gsbl90850 zookeeper-3.3.0]$ 2010-04-09 21:55:36,126 - ERROR [Thread-15:NIOServerCnxn@422] - Unexpected Exception: 
java.nio.channels.CancelledKeyException
	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
	at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)
	at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:395)
	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.checkFlush(NIOServerCnxn.java:907)
	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.flush(NIOServerCnxn.java:945)
	at java.io.BufferedWriter.flush(BufferedWriter.java:236)
	at java.io.PrintWriter.flush(PrintWriter.java:276)
	at org.apache.zookeeper.server.NIOServerCnxn$2.run(NIOServerCnxn.java:1089)
2010-04-09 21:55:36,126 - ERROR [Thread-15:NIOServerCnxn$Factory$1@82] - Thread Thread[Thread-15,5,main] died
java.nio.channels.CancelledKeyException
	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
	at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:64)
	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.wakeup(NIOServerCnxn.java:927)
	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.checkFlush(NIOServerCnxn.java:909)
	at org.apache.zookeeper.server.NIOServerCnxn$SendBufferWriter.flush(NIOServerCnxn.java:945)
	at java.io.BufferedWriter.flush(BufferedWriter.java:236)
	at java.io.PrintWriter.flush(PrintWriter.java:276)
	at org.apache.zookeeper.server.NIOServerCnxn$2.run(NIOServerCnxn.java:1089)
",mahadev,phunt,Blocker,Closed,Fixed,10/Apr/10 21:13,29/Dec/11 23:08
Bug,ZOOKEEPER-738,12461728,zookeeper.jute.h fails to compile with -pedantic ,"/home/y/include/zookeeper/zookeeper.jute.h:96: error: extra semicolon
/home/y/include/zookeeper/zookeeper.jute.h:158: error: extra semicolon
/home/y/include/zookeeper/zookeeper.jute.h:288: error: extra semicolon

the code generator needs to be updated to not output a naked semi
",jhatala,phunt,Major,Closed,Fixed,10/Apr/10 21:17,23/Nov/11 19:22
Bug,ZOOKEEPER-740,12461931,zkpython leading to segfault on zookeeper,"The program that we are implementing uses the python binding for zookeeper but sometimes it crash with segfault; here is the bt from gdb:

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0xad244b70 (LWP 28216)]
0x080611d5 in PyObject_Call (func=0x862fab0, arg=0x8837194, kw=0x0)
    at ../Objects/abstract.c:2488
2488    ../Objects/abstract.c: No such file or directory.
        in ../Objects/abstract.c
(gdb) bt
#0  0x080611d5 in PyObject_Call (func=0x862fab0, arg=0x8837194, kw=0x0)
    at ../Objects/abstract.c:2488
#1  0x080d6ef2 in PyEval_CallObjectWithKeywords (func=0x862fab0,
    arg=0x8837194, kw=0x0) at ../Python/ceval.c:3575
#2  0x080612a0 in PyObject_CallObject (o=0x862fab0, a=0x8837194)
    at ../Objects/abstract.c:2480
#3  0x0047af42 in watcher_dispatch (zzh=0x86174e0, type=-1, state=1,
    path=0x86337c8 """", context=0x8588660) at src/c/zookeeper.c:314
#4  0x00496559 in do_foreach_watcher (zh=0x86174e0, type=-1, state=1,
    path=0x86337c8 """", list=0xa5354140) at src/zk_hashtable.c:275
#5  deliverWatchers (zh=0x86174e0, type=-1, state=1, path=0x86337c8 """",
    list=0xa5354140) at src/zk_hashtable.c:317
#6  0x0048ae3c in process_completions (zh=0x86174e0) at src/zookeeper.c:1766
#7  0x0049706b in do_completion (v=0x86174e0) at src/mt_adaptor.c:333
#8  0x0013380e in start_thread () from /lib/tls/i686/cmov/libpthread.so.0
#9  0x002578de in clone () from /lib/tls/i686/cmov/libc.so.6",henryr,ciccio_79,Major,Resolved,Fixed,13/Apr/10 08:08,25/Apr/14 01:45
Bug,ZOOKEEPER-741,12462056,root level create on REST proxy fails,"Create /foo using the REST proxy fails.

Also upgrade to the latest Jersey/Grizzly while we are at it (fixes for func/security)",phunt,phunt,Critical,Closed,Fixed,14/Apr/10 17:12,23/Nov/11 19:22
Bug,ZOOKEEPER-742,12462196,Deallocatng None on writes,"On write operations, getting:

Fatal Python error: deallocating None
Aborted

This error happens on write operations only.  Here's the backtrace:

Fatal Python error: deallocating None

Program received signal SIGABRT, Aborted.
0x000000383fc30215 in raise () from /lib64/libc.so.6
(gdb) bt
#0  0x000000383fc30215 in raise () from /lib64/libc.so.6
#1  0x000000383fc31cc0 in abort () from /lib64/libc.so.6
#2  0x00002adbd0be8189 in Py_FatalError () from /usr/lib64/libpython2.4.so.1.0
#3  0x00002adbd0bc7493 in PyEval_EvalFrame () from /usr/lib64/libpython2.4.so.1.0
#4  0x00002adbd0bcab66 in PyEval_EvalFrame () from /usr/lib64/libpython2.4.so.1.0
#5  0x00002adbd0bcbfe5 in PyEval_EvalCodeEx () from /usr/lib64/libpython2.4.so.1.0
#6  0x00002adbd0bcc032 in PyEval_EvalCode () from /usr/lib64/libpython2.4.so.1.0
#7  0x00002adbd0be8729 in ?? () from /usr/lib64/libpython2.4.so.1.0
#8  0x00002adbd0be9bd8 in PyRun_SimpleFileExFlags () from /usr/lib64/libpython2.4.so.1.0
#9  0x00002adbd0bf000d in Py_Main () from /usr/lib64/libpython2.4.so.1.0
#10 0x000000383fc1d974 in __libc_start_main () from /lib64/libc.so.6
#11 0x0000000000400629 in _start ()
",henryr,jfray,Major,Closed,Fixed,15/Apr/10 23:21,23/Nov/11 19:22
Bug,ZOOKEEPER-746,12462711,learner outputs session id to log in dec (should be hex),"usability issue, should be in hex:

2010-04-21 11:31:13,827 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11354:Learner@95] - Revalidating client: 83353578391797760
",phunt,phunt,Minor,Closed,Fixed,21/Apr/10 22:10,23/Nov/11 19:22
Bug,ZOOKEEPER-749,12462783,OSGi metadata not included in binary only jar,"See this JIRA/comment for background:
https://issues.apache.org/jira/browse/ZOOKEEPER-425?focusedCommentId=12859697&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12859697

basically the issue is that OSGi metadata is included in the legacy jar (zookeeper-<version>.jar) but not in the binary only
jar (zookeeper-<version>-bin.jar) which is eventually deployed to the maven repo.
",phunt,phunt,Critical,Closed,Fixed,22/Apr/10 16:21,23/Nov/11 19:22
Bug,ZOOKEEPER-750,12462787,"move maven artifacts into ""dist-maven"" subdir of the release (package target)","The maven artifacts are currently (3.3.0) put into the toplevel of the release. This causes confusion
amonst new users (ie ""which jar do I use?""). Also the naming of the bin jar is wrong for maven (to put
onto the maven repo it must be named without the -bin) which adds extra burden for the release
manager. Putting into a subdir fixes this and makes it explicit what's being deployed to maven repo.
",phunt,phunt,Major,Closed,Fixed,22/Apr/10 16:45,23/Nov/11 19:22
Bug,ZOOKEEPER-753,12462988,update log4j dependency from 1.2.15 to 1.2.16 in branch 3.4,"http://repo2.maven.org/maven2/org/apache/hadoop/zookeeper/3.3.0/zookeeper-3.3.0.pom 

The pom contains log4j dependency as itself. 

  <dependency> 
      <groupId>log4j</groupId> 
      <artifactId>log4j</artifactId> 
      <version>1.2.15</version> 
      <scope>compile</scope> 
    </dependency> 

This is broken without an exclusion list, since the pending dependencies of javax.mail. etc. are not necessary for the most part. 

Please fix this along with 3.3.1 and republish new dependencies , since at its current state , it is usable by some projects (to host in central , say). 

Correct dependency for log4j: 


<dependency> 
      <groupId>log4j</groupId> 
      <artifactId>log4j</artifactId> 
      <version>1.2.15</version> 
      <scope>compile</scope> 
      <exclusions> 
        <exclusion> 
          <groupId>javax.mail</groupId> 
          <artifactId>mail</artifactId> 
        </exclusion> 
        <exclusion> 
          <groupId>javax.jms</groupId> 
          <artifactId>jms</artifactId> 
        </exclusion> 
        <exclusion> 
          <groupId>com.sun.jdmk</groupId> 
          <artifactId>jmxtools</artifactId> 
        </exclusion> 
        <exclusion> 
          <groupId>com.sun.jmx</groupId> 
          <artifactId>jmxri</artifactId> 
        </exclusion> 
      </exclusions> 
    </dependency> 
",busbey,kaykay.unique,Major,Closed,Fixed,26/Apr/10 06:31,13/Mar/14 18:16
Bug,ZOOKEEPER-758,12463281,zkpython segfaults on invalid acl with missing key,"Currently when setting an acl, there is a minimal parse to ensure that its a list of dicts, however if one of the dicts is missing a required key, the subsequent usage doesn't check for it, and will segfault.. for example using an acl of [{""schema"":id, ""id"":world, permissions:PERM_ALL}] will segfault if used, because the scheme key is missing (its been purposefully typo'd to schema in example). 

I've expanded the check_acl macro to include verifying that all keys are present and added some unit tests against trunk in the attachments.",kapilt,kapilt,Major,Closed,Fixed,29/Apr/10 01:28,23/Nov/11 19:22
Bug,ZOOKEEPER-763,12463663,Deadlock on close w/ zkpython / c client,"deadlocks occur if we attempt to close a handle while there are any outstanding async requests (aget, acreate, etc). Normally on close both the io thread terminates and the completion thread are terminated and joined, however w\ith outstanding async requests, the completion thread won't be in a joinable state, and we effectively hang when the main thread does the join.

afaics ideal behavior would be on close of a handle, to effectively clear out any remaining callbacks and let the completion thread terminate.

i've tried adding some bookkeeping to within a python client to guard against closing while there is an outstanding async completion request, but its an imperfect solution since even after the python callback is executed there is still a window for deadlock before the completion thread finishes the callback.

a simple example to reproduce the deadlock is attached.",henryr,kapilt,Major,Closed,Fixed,04/May/10 13:09,23/Nov/11 19:22
Bug,ZOOKEEPER-764,12463709,Observer elected leader due to inconsistent voting view,"In ZOOKEEPER-690, we noticed that an observer was being elected, and Henry proposed a patch to fix the issue. However, it seems that the patch does not solve the issue one user (Alan Cabrera) has observed. Given that we would like to fix this issue, and to work separately with Alan to determine the problem with his setup, I'm creating this jira and re-posting Henry's patch.",henryr,fpj,Major,Closed,Fixed,04/May/10 21:41,23/Nov/11 19:22
Bug,ZOOKEEPER-766,12463808,forrest recipes docs don't mention the lock/queue recipe implementations available in the release,Update the forrest recipes docs to point to the recipe implementations (where available).,phunt,phunt,Minor,Closed,Fixed,05/May/10 19:04,23/Nov/11 19:22
Bug,ZOOKEEPER-769,12463907,Leader can treat observers as quorum members,"In short: it seems leader can treat observers as quorum members.

Steps to repro:

1. Server configuration: 3 voters, 2 observers (attached).
2. Bring up 2 voters and one observer. It's enough for quorum.
3. Shut down the one from the quorum who is the follower.

As I understand, expected result is that leader will start a new election round so that to regain quorum.
But the real situation is that it just says goodbye to that follower, and is still operable. (When I'm shutting down 3rd one -- observer -- leader starts trying to regain a quorum).

(Expectedly, if on step 3 we shut down the leader, not the follower, remaining follower starta new leader election, as it should be).",dorserg,dorserg,Major,Closed,Fixed,06/May/10 18:01,23/Nov/11 19:22
Bug,ZOOKEEPER-772,12464142,zkpython segfaults when watcher from async get children is invoked.,"When utilizing the zkpython async get children api with a watch, i consistently get segfaults when the watcher is invoked to process events. ",henryr,kapilt,Major,Closed,Fixed,10/May/10 14:42,23/Nov/11 19:22
Bug,ZOOKEEPER-774,12464391,Recipes tests are slightly outdated: they do not compile against JUnit 4.8,As title,dorserg,dorserg,Minor,Closed,Fixed,12/May/10 21:31,23/Nov/11 19:22
Bug,ZOOKEEPER-782,12465807,Incorrect C API documentation for Watches," The C API Doxygen documentation states:

"" .... If the client is ever disconnected from the service, even if the
  disconnection is temporary, the watches of the client will be removed from
  the service, so a client must treat a disconnect notification as an implicit
  trigger of all outstanding watches.""

This is incorrect as of v.3. Watches are only lost and need to be re-registered when a session times out. When a normal disconnection occurs watches are reset automatically on reconnection.

The documentation in zookeeper.h needs to be updated to correct this explanation.",mahadev,wrightd,Trivial,Closed,Fixed,31/May/10 20:47,23/Nov/11 19:22
Bug,ZOOKEEPER-783,12465900,committedLog in ZKDatabase is not properly synchronized,"ZKDatabase.getCommittedLog() returns a reference to the LinkedList<Proposal> committedLog in ZKDatabase. This is then iterated over by at least one caller. 

I have seen a bug that causes a NPE in LinkedList.clear on committedLog, which I am pretty sure is due to the lack of synchronization. This bug has not been apparent in normal ZK operation, but in code that I have that starts and stops a ZK server in process repeatedly (clear() is called from ZooKeeperServerMain.shutdown()). 

It's better style to defensively copy the list in getCommittedLog, and to synchronize on the list in ZKDatabase.clear.

",henryr,henryr,Critical,Closed,Fixed,01/Jun/10 19:22,23/Nov/11 19:22
Bug,ZOOKEEPER-785,12466014, Zookeeper 3.3.1 shouldn't infinite loop if someone creates a server.0 line,"The following config causes an infinite loop

[zoo.cfg]
tickTime=2000
dataDir=/var/zookeeper/
clientPort=2181
initLimit=10
syncLimit=5
server.0=localhost:2888:3888

Output:

2010-06-01 16:20:32,471 - INFO [main:QuorumPeerMain@119] - Starting quorum peer
2010-06-01 16:20:32,489 - INFO [main:NIOServerCnxn$Factory@143] - binding to port 0.0.0.0/0.0.0.0:2181
2010-06-01 16:20:32,504 - INFO [main:QuorumPeer@818] - tickTime set to 2000
2010-06-01 16:20:32,504 - INFO [main:QuorumPeer@829] - minSessionTimeout set to -1
2010-06-01 16:20:32,505 - INFO [main:QuorumPeer@840] - maxSessionTimeout set to -1
2010-06-01 16:20:32,505 - INFO [main:QuorumPeer@855] - initLimit set to 10
2010-06-01 16:20:32,526 - INFO [main:FileSnap@82] - Reading snapshot /var/zookeeper/version-2/snapshot.c
2010-06-01 16:20:32,547 - INFO [Thread-1:QuorumCnxManager$Listener@436] - My election bind port: 3888
2010-06-01 16:20:32,554 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@620] - LOOKING
2010-06-01 16:20:32,556 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@649] - New election. My id = 0, Proposed zxid = 12
2010-06-01 16:20:32,558 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@689] - Notification: 0, 12, 1, 0, LOOKING, LOOKING, 0
2010-06-01 16:20:32,560 - WARN [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@623] - Unexpected exception
java.lang.NullPointerException
at org.apache.zookeeper.server.quorum.FastLeaderElection.totalOrderPredicate(FastLeaderElection.java:496)
at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:709)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:621)
2010-06-01 16:20:32,560 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@620] - LOOKING
2010-06-01 16:20:32,560 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@649] - New election. My id = 0, Proposed zxid = 12
2010-06-01 16:20:32,561 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@689] - Notification: 0, 12, 2, 0, LOOKING, LOOKING, 0
2010-06-01 16:20:32,561 - WARN [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@623] - Unexpected exception
java.lang.NullPointerException
at org.apache.zookeeper.server.quorum.FastLeaderElection.totalOrderPredicate(FastLeaderElection.java:496)
at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:709)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:621)
2010-06-01 16:20:32,561 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@620] - LOOKING
2010-06-01 16:20:32,562 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@649] - New election. My id = 0, Proposed zxid = 12
2010-06-01 16:20:32,562 - INFO [QuorumPeer:/0:0:0:0:0:0:0:0:2181:FastLeaderElection@689] - Notification: 0, 12, 3, 0, LOOKING, LOOKING, 0
2010-06-01 16:20:32,562 - WARN [QuorumPeer:/0:0:0:0:0:0:0:0:2181:QuorumPeer@623] - Unexpected exception
java.lang.NullPointerException


Things like HBase require that the zookeeper servers be listed in the zoo.cfg. This is a bug on their part, but zookeeper shouldn't null pointer in a loop though.
",phunt,posix4e,Major,Closed,Fixed,02/Jun/10 22:51,23/Nov/11 19:22
Bug,ZOOKEEPER-786,12466224,Exception in ZooKeeper.toString,"When trying to call ZooKeeper.toString during client disconnections, an exception can be generated:


[04/06/10 15:39:57.744] ERROR Error while calling watcher 
java.lang.Error: java.net.SocketException: Socket operation on non-socket
	at sun.nio.ch.Net.localAddress(Net.java:128)
	at sun.nio.ch.SocketChannelImpl.localAddress(SocketChannelImpl.java:430)
	at sun.nio.ch.SocketAdaptor.getLocalAddress(SocketAdaptor.java:147)
	at java.net.Socket.getLocalSocketAddress(Socket.java:717)
	at org.apache.zookeeper.ClientCnxn.getLocalSocketAddress(ClientCnxn.java:227)
	at org.apache.zookeeper.ClientCnxn.toString(ClientCnxn.java:183)
	at java.lang.String.valueOf(String.java:2826)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
	at org.apache.zookeeper.ZooKeeper.toString(ZooKeeper.java:1486)
	at java.util.Formatter$FormatSpecifier.printString(Formatter.java:2794)
	at java.util.Formatter$FormatSpecifier.print(Formatter.java:2677)
	at java.util.Formatter.format(Formatter.java:2433)
	at java.util.Formatter.format(Formatter.java:2367)
	at java.lang.String.format(String.java:2769)
	at com.echonest.cluster.ZooContainer.process(ZooContainer.java:544)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:488)
Caused by: java.net.SocketException: Socket operation on non-socket
	at sun.nio.ch.Net.localInetAddress(Native Method)
	at sun.nio.ch.Net.localAddress(Net.java:125)
	... 15 more
",thkoch,eelstretching,Minor,Resolved,Fixed,04/Jun/10 21:25,17/Oct/11 01:00
Bug,ZOOKEEPER-787,12466668,groupId in deployed pom is wrong,"The pom deployed to repo1.maven.org has the project declared like this:

<groupId>org.apache.zookeeper</groupId>
<artifactId>zookeeper</artifactId>
<packaging>jar</packaging>
<version>3.3.1</version>

But it is deployed here: http://repo2.maven.org/maven2/org/apache/hadoop/zookeeper/3.3.1

So either the groupId needs to change or the location it is deployed to needs to be changed because having them different results in bad behavior.  If you specify the correct groupId in your own pom/ivy files you can't even download zookeeper because it's not where your pom says it is and if you use the ""incorrect"" groupId then you can download zookeeper but then ivy complains about:

[error] :: problems summary ::
[error] :::: ERRORS
[error] 		public: bad organisation found in http://repo1.maven.org/maven2/org/apache/hadoop/zookeeper/3.3.1/zookeeper-3.3.1.pom: expected='org.apache.hadoop' found='org.apache.zookeeper'
",,rhavyn,Blocker,Closed,Fixed,10/Jun/10 16:46,23/Nov/11 19:22
Bug,ZOOKEEPER-790,12467595,Last processed zxid set prematurely while establishing leadership,"The leader code is setting the last processed zxid to the first of the new epoch even before connecting to a quorum of followers. Because the leader code sets this value before connecting to a quorum of followers (Leader.java:281) and the follower code throws an IOException (Follower.java:73) if the leader epoch is smaller, we have that when the false leader drops leadership and becomes a follower, it finds a smaller epoch and kills itself.",fpj,fpj,Blocker,Closed,Fixed,22/Jun/10 16:47,23/Nov/11 19:22
Bug,ZOOKEEPER-792,12467836,zkpython memory leak,"We recently upgraded zookeeper from 3.2.1 to 3.3.1, now we are seeing less client deadlock on session expiration, which is a definite plus!

Unfortunately we are seeing memory leak that requires our zk clients to be restarted every half-day. Valgrind result:

==8804== 25 (12 direct, 13 indirect) bytes in 1 blocks are definitely lost in loss record 255 of 670
==8804==    at 0x4021C42: calloc (vg_replace_malloc.c:418)
==8804==    by 0x5047B42: parse_acls (zookeeper.c:369)
==8804==    by 0x5047EF6: pyzoo_create (zookeeper.c:1009)
==8804==    by 0x40786CC: PyCFunction_Call (in /usr/lib/libpython2.4.so.1.0)
==8804==    by 0x40B31DC: PyEval_EvalFrame (in /usr/lib/libpython2.4.so.1.0)
==8804==    by 0x40B4485: PyEval_EvalCodeEx (in /usr/lib/libpython2.4.so.1.0)
",voyager,voyager,Major,Closed,Fixed,24/Jun/10 21:36,23/Nov/11 19:22
Bug,ZOOKEEPER-794,12467962,Callbacks are not invoked when the client is closed,"I noticed that ZooKeeper has different behaviors when calling synchronous or asynchronous actions on a closed ZooKeeper client.
Actually a synchronous call will throw a ""session expired"" exception while an asynchronous call will do nothing. No exception, no callback invocation.

Actually, even if the EventThread receives the Packet with the session expired err code, the packet is never processed since the thread has been killed by the ventOfDeath. So the call back is not invoked.

",alexismidon,alexismidon,Blocker,Closed,Fixed,26/Jun/10 02:47,23/Nov/11 19:22
Bug,ZOOKEEPER-795,12468037,"eventThread isn't shutdown after a connection ""session expired"" event coming","Hi,

I notice a problem with the eventThread located in ClientCnxn.java file.
The eventThread isn't shutdown after a connection ""session expired"" event coming (i.e. never receive EventOfDeath).

When a session timeout occurs and the session is marked as expired, the connexion is fully closed (socket, SendThread...) expect for the eventThread.
As a result, if i create a new zookeeper object and connect through it, I got a zombi thread which will never be kill (as for the previous zookeeper object, the state is already close, calling close again don't do anything).

So everytime I will create a new zookeeper connection after a expired session, I will have a one more zombi EventThread.

How to reproduce :
- Start a zookeeper client connection in debug mode
- Pause the jvm enough time to the expired event occur
- Watch for example with jvisualvm the list of threads, the sendThread is succesfully killed, but the EventThread go to wait state for a infinity of time
- if you reopen a new zookeeper connection, and do again the previous steps, another EventThread will be present in infinite wait state




",dorserg,mbarcikowski,Blocker,Closed,Fixed,28/Jun/10 10:12,13/Jun/13 18:28
Bug,ZOOKEEPER-796,12468097,zkServer.sh should support an external PIDFILE variable,So currently the pid file has to be tied to the datadirectory when starting zkServer.sh. It would be good to be able to break them up.,posix4e,posix4e,Major,Closed,Fixed,28/Jun/10 22:02,23/Nov/11 19:22
Bug,ZOOKEEPER-800,12468212,zoo_add_auth returns ZOK if zookeeper handle is in ZOO_CLOSED_STATE,"This happened when I called zoo_add_auth() immediately after zookeeper_init(). It took me a while to figure out that authentication actually failed since zoo_add_auth() returned ZOK. It should return ZINVALIDSTATE instead. 

--Michi",michim,michim,Minor,Closed,Fixed,29/Jun/10 23:26,23/Nov/11 19:22
Bug,ZOOKEEPER-804,12468586,"c unit tests failing due to ""assertion cptr failed""","I'm seeing this frequently:

     [exec] Zookeeper_simpleSystem::testPing : elapsed 18006 : OK
     [exec] Zookeeper_simpleSystem::testAcl : elapsed 1022 : OK
     [exec] Zookeeper_simpleSystem::testChroot : elapsed 3145 : OK
     [exec] Zookeeper_simpleSystem::testAuth ZooKeeper server started : elapsed 25687 : OK
     [exec] zktest-mt: /home/phunt/dev/workspace/gitzk/src/c/src/zookeeper.c:1952: zookeeper_process: Assertion `cptr' failed.
     [exec] make: *** [run-check] Aborted
     [exec] Zookeeper_simpleSystem::testHangingClient

Mahadev can you take a look?
",michim,phunt,Critical,Closed,Fixed,05/Jul/10 20:35,23/Nov/11 19:22
Bug,ZOOKEEPER-805,12468677,four letter words fail with latest ubuntu nc.openbsd,"In both 3.3 branch and trunk ""echo stat|nc localhost 2181"" fails against the ZK server on Ubuntu Lucid Lynx.

I noticed this after upgrading to lucid lynx - which is now shipping openbsd nc as the default:

OpenBSD netcat (Debian patchlevel 1.89-3ubuntu2)

vs nc traditional

[v1.10-38]

which works fine. Not sure if this is a bug in us or nc.openbsd, but it's currently not working for me. Ugh.
",,phunt,Critical,Resolved,Fixed,06/Jul/10 23:48,30/Apr/14 20:19
Bug,ZOOKEEPER-814,12469226,monitoring scripts are missing apache license headers,"Andrei, I just realized that src/contrib/monitoring files are missing apache license headers.  Please add them (in particular any script files like python, see similar files in svn for examples - in some cases like README it's not strictly necessary.) 

You can run the RAT tool to verify (see build.xml or http://incubator.apache.org/rat/)
",savu.andrei,phunt,Blocker,Closed,Fixed,14/Jul/10 06:59,23/Nov/11 19:22
Bug,ZOOKEEPER-820,12469466,"update c unit tests to ensure ""zombie"" java server processes don't cause failure","When the c unit tests are run sometimes the server doesn't shutdown at the end of the test, this causes subsequent tests (hudson esp) to fail.

1) we should try harder to make the server shut down at the end of the test, I suspect this is related to test failing/cleanup
2) before the tests are run we should see if the old server is still running and try to shut it down
",michim,phunt,Critical,Closed,Fixed,16/Jul/10 16:00,23/Nov/11 19:22
Bug,ZOOKEEPER-822,12469627,Leader election taking a long time  to complete,"Created a 3 node cluster.

1 Fail the ZK leader
2. Let leader election finish. Restart the leader and let it join the 
3. Repeat 

After a few rounds leader election takes anywhere 25- 60 seconds to finish. Note- we didn't have any ZK clients and no new znodes were created.

zoo.cfg is shown below:

#Mon Jul 19 12:15:10 UTC 2010
server.1=192.168.4.12\:2888\:3888
server.0=192.168.4.11\:2888\:3888
clientPort=2181
dataDir=/var/zookeeper
syncLimit=2
server.2=192.168.4.13\:2888\:3888
initLimit=5
tickTime=2000

I have attached logs from two nodes that took a long time to form the cluster after failing the leader. The leader was down anyways so logs from that node shouldn't matter.
Look for ""START HERE"". Logs after that point should be of our interest.",vishalmlst,vishalmlst,Blocker,Closed,Fixed,19/Jul/10 15:51,23/Nov/11 19:22
Bug,ZOOKEEPER-831,12470839,BookKeeper: Throttling improved for reads,"Reads and writes in BookKeeper are asymmetric: a write request writes one entry, whereas a read request may read multiple requests. The current implementation of throttling only counts the number of read requests instead of counting the number of entries being read. Consequently, a few read requests reading a large number of entries each will spawn a large number of read-entry requests. ",fpj,fpj,Major,Closed,Fixed,04/Aug/10 21:25,02/May/13 02:29
Bug,ZOOKEEPER-833,12470999,Attachments in the wiki do not work (so no presentations),"This is apparently a known issue:

http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-user/201005.mbox/%3C562709E0-0516-481F-87AD-2039A564E5BD@yahoo-inc.com%3E

None of the attachments on the Presentations page in the wiki work (nor does the link to the screenshot on the performance page).
",thkoch,brucem,Major,Resolved,Fixed,06/Aug/10 19:17,05/Sep/11 19:55
Bug,ZOOKEEPER-844,12471434,handle auth failure in java client,"ClientCnxn.java currently has the following code:
  if (replyHdr.getXid() == -4) {
                // -2 is the xid for AuthPacket
                // TODO: process AuthPacket here
                if (LOG.isDebugEnabled()) {
                    LOG.debug(""Got auth sessionid:0x""
                            + Long.toHexString(sessionId));
                }
                return;
            }

Auth failures appear to cause the server to disconnect but the client never gets a proper state change or notification that auth has failed, which makes handling this scenario very difficult as it causes the client to go into a loop of sending bad auth, getting disconnected, trying to reconnect, sending bad auth again, over and over. 
",fournc,fournc,Major,Closed,Fixed,12/Aug/10 16:05,23/Nov/11 19:22
Bug,ZOOKEEPER-846,12471442,zookeeper client doesn't shut down cleanly on the close call,"Using HBase 0.20.6 (with HBASE-2473) we encountered a situation where Regionserver
process was shutting down and seemed to hang.

Here is the bottom of region server log:
http://pastebin.com/YYawJ4jA

zookeeper-3.2.2 is used.

Here is relevant portion from jstack - I attempted to attach jstack twice in my email to dev@hbase.apache.org but failed:

""DestroyJavaVM"" prio=10 tid=0x00002aabb849c800 nid=0x6c60 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""regionserver/10.32.42.245:60020"" prio=10 tid=0x00002aabb84ce000 nid=0x6c81 in Object.wait() [0x0000000043755000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab76633c0> (a org.apache.zookeeper.ClientCnxn$Packet)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1099)
        - locked <0x00002aaab76633c0> (a org.apache.zookeeper.ClientCnxn$Packet)
        at org.apache.zookeeper.ClientCnxn.close(ClientCnxn.java:1077)
        at org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:505)
        - locked <0x00002aaabf5e0c30> (a org.apache.zookeeper.ZooKeeper)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.close(ZooKeeperWrapper.java:681)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:654)
        at java.lang.Thread.run(Thread.java:619)

""main-EventThread"" daemon prio=10 tid=0x0000000043474000 nid=0x6c80 waiting on condition [0x00000000413f3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaabf6e9150> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:414)
",phunt,ted_yu,Blocker,Closed,Fixed,12/Aug/10 17:19,23/Nov/11 19:22
Bug,ZOOKEEPER-854,12472035,BookKeeper does not compile due to changes in the ZooKeeper code,BookKeeper does not compile due to changes in the NIOServerCnxn class of ZooKeeper.,fpj,fpj,Major,Closed,Fixed,19/Aug/10 20:04,23/Nov/11 19:22
Bug,ZOOKEEPER-855,12472604,clientPortBindAddress should be clientPortAddress,The server documentation states that the configuration parameter for binding to a specific ip address is clientPortBindAddress.  The code believes the parameter is clientPortAddress.  The documentation for 3.3.X versions needs changed to reflect the correct parameter .  This parameter was added in ZOOKEEPER-635.,jaredc,jaredc,Trivial,Closed,Fixed,26/Aug/10 14:49,23/Nov/11 19:22
Bug,ZOOKEEPER-861,12473165,Missing the test SSL certificate used for running junit tests.,The Hedwig code checked into Apache is missing a test SSL certificate file used for running the server junit tests.  We need this file otherwise the tests that use this (e.g. TestHedwigHub) will fail.,erwin.tam,erwin.tam,Minor,Closed,Fixed,02/Sep/10 17:40,23/Nov/11 19:22
Bug,ZOOKEEPER-867,12473427,ClientTest is failing on hudson - fd cleanup,client cleanup test is failing on hudson. fd count is off.,phunt,phunt,Blocker,Closed,Fixed,07/Sep/10 07:29,23/Nov/11 19:22
Bug,ZOOKEEPER-870,12474130,Zookeeper trunk build broken.,the zookeeper current trunk build is broken mostly due to some netty changes. This is causing a huge backlog of PA's and other impediments to the review process. For now I plan to disable the test and fix them as part of 3.4 later.,mahadev,mahadev,Major,Closed,Fixed,14/Sep/10 22:16,23/Nov/11 19:22
Bug,ZOOKEEPER-874,12474397,FileTxnSnapLog.restore does not call listener,"FileTxnSnapLog.restore() does not call listener passed as parameter. The result is that the commitLogs list is empty. When a follower connects to the leader, the leader is forced to send a snapshot to the follower instead of a couple of requests and commits.",dioog,dioog,Trivial,Closed,Fixed,17/Sep/10 16:01,02/May/13 02:29
Bug,ZOOKEEPER-876,12474695,Unnecessary snapshot transfers between new leader and followers,"When starting a new leadership, unnecessary snapshot transfers happen between new leader and followers. This is so because of multiple small bugs. 

1) the comparison of zxids is done based on a new proposal, instead of the last logged zxid. (LearnerHandler.java ~ 297)
2) if follower is one zxid behind, the check of the interval of committed logs excludes the follower. (LearnerHandler.java ~ 277)
3) the bug reported in ZOOKEEPER-874 (commitLogs are empty after recover).",dioog,dioog,Minor,Resolved,Fixed,21/Sep/10 13:37,01/Jul/13 17:28
Bug,ZOOKEEPER-877,12474807,zkpython does not work with python3.1,"as written in the contrib/zkpython/README file:


""Python >= 2.6 is required. We have tested against 2.6. We have not tested against 3.x.""

this is probably more a 'new feature' request than a bug; anyway compiling the pythn module and calling it returns an error at load time:


python3.1
Python 3.1.2 (r312:79147, May  8 2010, 16:36:46) 
[GCC 4.4.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import zookeeper
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: /usr/local/lib/python3.1/dist-packages/zookeeper.so: undefined symbol: PyString_AsString



are there any plan to support Python3.X?

I also tried to write a 3.1 ctypes wrapper but the C API seems in fact to be written in C++, so python ctypes cannot be used.",enmand,tuxracer69,Major,Closed,Fixed,22/Sep/10 10:44,13/Mar/14 18:16
Bug,ZOOKEEPER-880,12475256,QuorumCnxManager$SendWorker grows without bounds,"We're seeing an issue where one server in the ensemble has a steady growing number of QuorumCnxManager$SendWorker threads up to a point where the OS runs out of native threads, and at the same time we see a lot of exceptions in the logs.  This is on 3.2.2 and our config looks like:

{noformat}
tickTime=3000
dataDir=/somewhere_thats_not_tmp
clientPort=2181
initLimit=10
syncLimit=5
server.0=sv4borg9:2888:3888
server.1=sv4borg10:2888:3888
server.2=sv4borg11:2888:3888
server.3=sv4borg12:2888:3888
server.4=sv4borg13:2888:3888
{noformat}

The issue is on the first server. I'm going to attach threads dumps and logs in moment.",vishalmlst,jdcryans,Blocker,Closed,Fixed,27/Sep/10 23:40,23/Nov/11 19:22
Bug,ZOOKEEPER-881,12475363,ZooKeeperServer.loadData loads database twice,"zkDb.loadDataBase() is called twice at the beginning of loadData().  It shouldn't have any negative affects, but is unnecessary.   A patch should be trivial.",jaredc,jaredc,Trivial,Closed,Fixed,28/Sep/10 23:41,23/Nov/11 19:21
Bug,ZOOKEEPER-882,12475365,Startup loads last transaction from snapshot,"On startup, the server first loads the latest snapshot, and then loads from the log starting at the last transaction in the snapshot.  It should begin from one past that last transaction in the log.  I will attach a possible patch.",jaredc,jaredc,Minor,Closed,Fixed,28/Sep/10 23:46,23/Nov/11 19:22
Bug,ZOOKEEPER-884,12475587,Remove LedgerSequence references from BookKeeper documentation and comments in tests ,"We no longer use LedgerSequence, so we need to remove references in documentation and comments sprinkled throughout the code.",fpj,fpj,Major,Closed,Fixed,01/Oct/10 09:11,23/Nov/11 19:22
Bug,ZOOKEEPER-886,12475889,"Hedwig Server stays in ""disconnected"" state when connection to ZK dies but gets reconnected","The Hedwig Server is connected to ZooKeeper.  In the ZkTopicManager, it registers a watcher so that if it ever gets disconnected from ZK, it will temporarily fail all incoming requests since the Hedwig server does not know for sure if it is still the master for the topics.  When the ZK client gets reconnected, the logic currently is wrong and it does not unset the suspended flag.  Thus once it gets disconnected, it will stay in the suspended state forever, thereby making the Hedwig server hub dead.",erwin.tam,erwin.tam,Major,Resolved,Fixed,05/Oct/10 20:18,12/Oct/10 10:52
Bug,ZOOKEEPER-888,12476669,c-client / zkpython: Double free corruption on node watcher,"the c-client / zkpython wrapper invokes already freed watcher callback

steps to reproduce:
  0. start a zookeper server on your machine
  1. run the attached python script
  2. suspend the zookeeper server process (e.g. using `pkill -STOP -f org.apache.zookeeper.server.quorum.QuorumPeerMain` )
  3. wait until the connection and the node observer fired with a session event
  4. resume the zookeeper server process  (e.g. using `pkill -CONT -f org.apache.zookeeper.server.quorum.QuorumPeerMain` )

-> the client tries to dispatch the node observer function again, but it was already freed -> double free corruption",mr_luk,mr_luk,Critical,Closed,Fixed,06/Oct/10 13:26,23/Nov/11 19:22
Bug,ZOOKEEPER-889,12476737,pyzoo_aget_children crashes due to incorrect watcher context,"The pyzoo_aget_children function passes the completion callback (""pyw"") in place of the watcher callback (""get_pyw""). Since it is a one-shot callback, it is deallocated after the completion callback fires, causing a crash when the watcher callback should be invoked.
",,austin,Critical,Resolved,Fixed,07/Oct/10 04:16,07/Oct/10 04:19
Bug,ZOOKEEPER-893,12476980,ZooKeeper high cpu usage when invalid requests,"When ZooKeeper receives certain illegally formed messages on the internal communication port (:4181 by default), it's possible for ZooKeeper to enter an infinite loop which causes 100% cpu usage. It's related to ZOOKEEPER-427, but that patch does not resolve all issues.

from: src/java/main/org/apache/zookeeper/server/quorum/QuorumCnxManager.java 

the two affected parts:
===========
int length = msgLength.getInt();                                                        
if(length <= 0) {                                                                       
    throw new IOException(""Invalid packet length:"" + length);                           
} 
===========


===========
while (message.hasRemaining()) {                                                    
    temp_numbytes = channel.read(message);                                          
    if(temp_numbytes < 0) {                                                         
        throw new IOException(""Channel eof before end"");                            
    }                                                                               
    numbytes += temp_numbytes;                                                      
} 
===========

how to replicate this bug:

perform an nmap portscan against your zookeeper server: ""nmap -sV -n your.ip.here -p4181""
wait for a while untill you see some messages in the logfile and then you will see 100% cpu usage. It does not recover from this situation. With my patch, it does not occur anymore",tt,tt,Critical,Closed,Fixed,11/Oct/10 07:15,23/Nov/11 19:22
Bug,ZOOKEEPER-895,12477324,ClientCnxn.authInfo must be thread safe,"authInfo can be accessed concurrently by different Threads, as exercised in 
org.apache.zookeeper.test.ACLTest

The two concurrent access points in this case were (presumably):
org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:805) and
org.apache.zookeeper.ClientCnxn.addAuthInfo(ClientCnxn.java:1121)

The line numbers refer to the latest patch in ZOOKEEPER-823.

The exception that pointed to this issue:
    [junit] 2010-10-13 09:35:55,113 [myid:] - WARN  [main-SendThread(localhost:11221):ClientCnxn$SendThread@713] - Session 0x0 for server localhost/127.0.0.1:11221, unexpected error, closing socket connection and attempting reconnect
    [junit] java.util.ConcurrentModificationException
    [junit] 	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
    [junit] 	at java.util.AbstractList$Itr.next(AbstractList.java:343)
    [junit] 	at org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:805)
    [junit] 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:247)
    [junit] 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:694)

Proposed solution: Use a thread save list for authInfo",,thkoch,Major,Resolved,Fixed,14/Oct/10 07:25,19/Nov/10 17:40
Bug,ZOOKEEPER-897,12477390,C Client seg faults during close,"We observed a crash while closing our c client.  It was in the do_io() thread that was processing as during the close() call.

#0  queue_buffer (list=0x6bd4f8, b=0x0, add_to_front=0) at src/zookeeper.c:969
#1  0x000000000046234e in check_events (zh=0x6bd480, events=<value optimized out>) at src/zookeeper.c:1687
#2  0x0000000000462d74 in zookeeper_process (zh=0x6bd480, events=2) at src/zookeeper.c:1971
#3  0x0000000000469c34 in do_io (v=0x6bd480) at src/mt_adaptor.c:311
#4  0x00007ffff7bc59ca in start_thread () from /lib/libpthread.so.0
#5  0x00007ffff6f706fd in clone () from /lib/libc.so.6
#6  0x0000000000000000 in ?? ()

We tracked down the sequence of events, and the cause is that input_buffer is being freed from a thread other than the do_io thread that relies on it:

1. do_io() call check_events()
2. if(events&ZOOKEEPER_READ) branch executes
3. if (rc > 0) branch executes
4. if (zh->input_buffer != &zh->primer_buffer) branch executes
.....in the meantime......
     5. zookeeper_close() called
     6. if (inc_ref_counter(zh,0)!=0) branch executes
     7. cleanup_bufs() is called
     8. input_buffer is freed at the end
..... back to check_events().........
9. queue_events() is called on a NULL buffer.

I believe the patch is to only call free_completions() in zookeeper_close() and not cleanup_bufs().  The original reason cleanup_bufs() was added was to call any outstanding synhcronous completions, so only free_completions (which is guarded) is needed.  I will submit a patch for review with this change.",jaredc,jaredc,Major,Closed,Fixed,14/Oct/10 19:26,23/Nov/11 19:22
Bug,ZOOKEEPER-898,12477392,C Client might not cleanup correctly during close,"I was looking through the c-client code and noticed a situation where a counter can be incorrectly incremented and a small memory leak can occur.

In zookeeper.c : add_completion(), if close_requested is true, then the completion will not be queued.  But at the end, outstanding_sync is still incremented and free() never called on the newly allocated completion_list_t.  

I will submit for review a diff that I believe corrects this issue.",jaredc,jaredc,Trivial,Closed,Fixed,14/Oct/10 19:35,23/Nov/11 19:22
Bug,ZOOKEEPER-902,12477655,"Fix findbug issue in trunk ""Malicious code vulnerability""","https://hudson.apache.org/hudson/view/ZooKeeper/job/ZooKeeper-trunk/970/artifact/trunk/findbugs/zookeeper-findbugs-report.html#Warnings_MALICIOUS_CODE

Malicious code vulnerability Warnings

Code	Warning
MS	org.apache.zookeeper.server.quorum.LeaderElection.epochGen isn't final but should be",fpj,phunt,Minor,Closed,Fixed,18/Oct/10 17:41,23/Nov/11 19:21
Bug,ZOOKEEPER-904,12477777,super digest is not actually acting as a full superuser,"The documentation states:
New in 3.2:  Enables a ZooKeeper ensemble administrator to access the znode hierarchy as a ""super"" user. In particular no ACL checking occurs for a user authenticated as super.

However, if a super user does something like:
zk.setACL(""/"", Ids.READ_ACL_UNSAFE, -1);

the super user is now bound by read-only ACL. This is not what I would expect to see given the documentation. It can be fixed by moving the chec for the ""super"" authId in PrepRequestProcessor.checkACL to before the for(ACL a : acl) loop.",fournc,fournc,Major,Closed,Fixed,19/Oct/10 20:44,23/Nov/11 19:22
Bug,ZOOKEEPER-907,12477893,"Spurious ""KeeperErrorCode = Session moved"" messages","The sync request does not set the session owner in Request.

As a result, the leader keeps printing:
2010-07-01 10:55:36,733 - INFO  [ProcessThread:-1:PrepRequestProcessor@405] - Got user-level KeeperException when processing sessionid:0x298d3b1fa90000 type:sync: cxid:0x6 zxid:0xfffffffffffffffe txntype:unknown reqpath:/ Error Path:null Error:KeeperErrorCode = Session moved
",vishalmlst,vishalmlst,Blocker,Closed,Fixed,20/Oct/10 18:27,23/Nov/11 19:22
Bug,ZOOKEEPER-913,12478305,"Version parser fails to parse ""3.3.2-dev"" from build.xml.","Cannot build 3.3.1 from release tarball do to VerGen parser inability to parse ""3.3.2-dev"".

version-info:
     [java] All version-related parameters must be valid integers!
     [java] Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""2-dev""
     [java] 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
     [java] 	at java.lang.Integer.parseInt(Integer.java:481)
     [java] 	at java.lang.Integer.parseInt(Integer.java:514)
     [java] 	at org.apache.zookeeper.version.util.VerGen.main(VerGen.java:131)
     [java] Java Result: 1
",phunt,anthonyu,Critical,Closed,Fixed,26/Oct/10 06:50,23/Nov/11 19:22
Bug,ZOOKEEPER-916,12478968,Problem receiving messages from subscribed channels in c++ client ,"We see this bug with receiving messages from a subscribed channel.  This problem seems to happen with larger messages.  The flow is to first read at least 4 bytes from the socket channel. Extract the first 4 bytes to get the message size.  If we've read enough data into the buffer already, we're done so invoke the messageReadCallbackHandler passing the channel and message size.  If not, then do an async read for at least the remaining amount of bytes in the message from the socket channel.  When done, invoke the messageReadCallbackHandler.

The problem seems that when the second async read is done, the same sizeReadCallbackHandler is invoked instead of the messageReadCallbackHandler.  The result is that we then try to read the first 4 bytes again from the buffer.  This will get a random message size and screw things up.  I'm not sure if it's an incorrect use of the boost asio async_read function or we're doing the boost bind to the callback function incorrectly.


101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler system:0,512 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: size of buffer before reading message size: 512 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: size of incoming message 599, currently in buffer 508 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: Still have more data to read, 91 from channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler system:0, 91 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: size of buffer before reading message size: 599 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: size of incoming message 134287360, currently in buffer 595 channel(0x80b7a18)
101015 15:30:40.108 DEBUG hedwig.channel.cpp - DuplexChannel::sizeReadCallbackHandler: Still have more data to read, 134286765 from channel(0x80b7a18)
",ikelly,ikelly,Major,Resolved,Fixed,03/Nov/10 09:10,05/Nov/10 10:52
Bug,ZOOKEEPER-921,12479354,zkPython incorrectly checks for existence of required ACL elements,"Calling {{zookeeper.create()}} seems, under certain circumstances, to be corrupting a subsequent call to Python's {{logging}} module.

Specifically, if the node does not exist (but its parent does), I end up with a traceback like this when I try to make the logging call:

{noformat}
Traceback (most recent call last):
  File ""zktest.py"", line 21, in <module>
    logger.error(""Boom?"")
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/logging/__init__.py"", line 1046, in error
    if self.isEnabledFor(ERROR):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/logging/__init__.py"", line 1206, in isEnabledFor
    return level >= self.getEffectiveLevel()
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/logging/__init__.py"", line 1194, in getEffectiveLevel
    while logger:
TypeError: an integer is required
{noformat}

But if the node already exists, or the parent does not exist, I get the appropriate NodeExists or NoNode exceptions.

I'll be attaching a test script that can be used to reproduce this behavior.",nknight,nknight,Major,Closed,Fixed,08/Nov/10 08:51,23/Nov/11 19:22
Bug,ZOOKEEPER-930,12479938,Hedwig c++ client uses a non thread safe logging library,,ikelly,ikelly,Major,Resolved,Fixed,15/Nov/10 09:55,17/Nov/10 10:55
Bug,ZOOKEEPER-937,12480456,test -e not available on solaris /bin/sh,test -e FILENAME is not support on /bin/sh in solaris. This is used in bin/zkEnv.sh. We can substitute test -f FILENAME. Attaching a patch.,egh,egh,Major,Closed,Fixed,19/Nov/10 22:12,23/Nov/11 19:21
Bug,ZOOKEEPER-957,12493091,zkCleanup.sh doesn't do anything,"Somebody left some echo statements in the zkCleanup.sh which prevents the java commands from actually running.

Patch coming forthwith.",tdunning,tdunning,Major,Closed,Fixed,13/Dec/10 17:09,23/Nov/11 19:21
Bug,ZOOKEEPER-958,12493283,Flag to turn off autoconsume in hedwig c++ client,"Currently the hedwig cpp client will automatically send a consume message to the server when the calling client indicated that it has received the message. If the client wants to queue the messages and not acknowledge them to the server immediately, they need to block, which means interfering with any other running callbacks. ",ikelly,ikelly,Major,Closed,Fixed,15/Dec/10 09:18,23/Nov/11 19:22
Bug,ZOOKEEPER-961,12493765,Watch recovery after disconnection when connection string contains a prefix,"Let's say you're using connection string ""127.0.0.1:2182/foo"".
1) put a childrenchanged watch on relative / (that is, on absolute path /foo)
2) stop the zk server
3) start the zk server
4) at this point, the client recovers the connection, and should have put back a watch on relative path /, but instead the client puts a watch on the *absolute* path /
- if some other client adds or removes a node under /foo, nothing will happen
- if some other client adds or removes a node under /, then you will get an error from the zk client library (string operation error)",mspycher,pm47,Critical,Closed,Fixed,21/Dec/10 16:07,23/Nov/11 19:22
Bug,ZOOKEEPER-962,12493781,leader/follower coherence issue when follower is receiving a DIFF,"From mailing list:
It seems like we rely on the LearnerHandler thread startup to capture all of the missing committed
transactions in the SNAP or DIFF, but I don't see anything (especially in the DIFF case) that
is preventing us for committing more transactions before we actually start forwarding updates
to the new follower.

Let me explain using my example from ZOOKEEPER-919. Assume we have quorum already, so the
leader can be processing transactions while my follower is starting up.

I'm a follower at zxid N-5, the leader is at N. I send my FOLLOWERINFO packet to the leader
with that information. The leader gets the proposals from its committed log (time T1), then
syncs on the proposal list (LearnerHandler line 267. Why? It's a copy of the underlying proposal
list... this might be part of our problem). I check to see if the peerLastZxid is within my
max and min committed log and it is, so I'm going to send a diff. I set the zxidToSend to
be the maxCommittedLog at time T3 (we already know this is sketchy), and forward the proposals
from my copied proposal list starting at the peerLastZxid+1 up to the last proposal transaction
(as seen at time T1).

After I have queued up all those diffs to send, I tell the leader to startFowarding updates
to this follower (line 308). 

So, let's say that at time T2 I actually swap out the leader to the thread that is handling
the various request processors, and see that I got enough votes to commit zxid N+1. I commit
N+1 and so my maxCommittedLog at T3 is N+1, but this proposal is not in the list of proposals
that I got back at time T1, so I don't forward this diff to the client. Additionally, I processed
the commit and removed it from my leader's toBeApplied list. So when I call startForwarding
for this new follower, I don't see this transaction as a transaction to be forwarded. 

There's one problem. Let's also imagine, however, that I commit N+1 at time T4. The maxCommittedLog
value is consistent with the max of the diff packets I am going to send the follower. But,
I still committed N+1 and removed it from the toBeApplied list before calling startFowarding
with this follower. How does the follower get this transaction? Does it?

To put it another way, here is the thread interaction, hopefully formatted so you can read
it...

		LearnerHandlerThread					RequestProcessorThread
T1(LH):	get list of proposals (COPY)
T2(RPT):								commit N+1, remove from toBeApplied
T3(LH):	get maxCommittedLog
T4(LH):	send diffs from view at T1
T5(LH):	startForwarding


Or
T1(LH):	get list of proposals (COPY)
T2(LH):	get maxCommittedLog
T3(RPT):								commit N+1, remove from toBeApplied
T4(LH):	send diffs from view at T1
T5(LH):	startFowarding


I'm trying to figure out what, if anything, keeps the requests from being committed, removed,
and never seen by the follower before it fully starts up. 

",chl501,fournc,Critical,Closed,Fixed,21/Dec/10 18:42,23/Nov/11 19:22
Bug,ZOOKEEPER-963,12493953,Make Forrest work with JDK6,"It's possible to make Forrest work with JDK6 by disabling sitemap validation
in the forrest.properties file. See FOR-984 and PIG-1508 for more details.",cwsteinbach,cwsteinbach,Major,Closed,Fixed,23/Dec/10 08:34,23/Nov/11 19:22
Bug,MAPREDUCE-1358,12444763,Utils.OutputLogFilter incorrectly filters for _logs,"OutputLogFilter checks if the path contains _logs. This would incorrectly filter out all contents of a directory called server_logs, for example. Instead it should check for a path component exactly equal to _logs",tlipcon,tlipcon,Major,Closed,Fixed,06/Jan/10 01:48,24/Aug/10 21:19
Bug,MAPREDUCE-1365,12444994,TestTaskTrackerBlacklisting.AtestTrackerBlacklistingForJobFailures is mistyped.,The name of TestTaskTrackerBlacklisting.testTrackerBlacklistingForJobFailures got changed to TestTaskTrackerBlacklisting.AtestTrackerBlacklistingForJobFailures unintentionally in MAPREDUCE-686.,amareshwari,amareshwari,Trivial,Closed,Fixed,08/Jan/10 04:53,24/Aug/10 21:19
Bug,MAPREDUCE-1366,12445005,Tests should not timeout if TaskTracker/JobTracker crashes in MiniMRCluster,"Currently tests timeout if there is any problem bringing up JobTracker or TaskTracker in MiniMRCluster. Instead tests should fail saying JT/TT crashed.
See test timeout on MAPREDUCE-1365",,amareshwari,Major,Resolved,Fixed,08/Jan/10 08:14,29/Jul/14 22:03
Bug,MAPREDUCE-1369,12445156,JUnit tests should never depend on anything in conf,The recent change to mapred-queues.xml that causes many mapreduce tests to break unless you delete conf/mapred-queues.xml out of your build tree is bad. We need to make sure that nothing in conf is used in the unit tests. One potential solution is to copy the templates into build/test/conf and use that instead.,anatoli.fomenko,anatoli.fomenko,Blocker,Closed,Fixed,11/Jan/10 02:50,24/Aug/10 21:20
Bug,MAPREDUCE-1372,12445266,ConcurrentModificationException in JobInProgress,"We have seen the following  ConcurrentModificationException in one of our clusters
{noformat}
java.io.IOException: java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$KeyIterator.next(HashMap.java:828)
        at org.apache.hadoop.mapred.JobInProgress.findNewMapTask(JobInProgress.java:2018)
        at org.apache.hadoop.mapred.JobInProgress.obtainNewMapTask(JobInProgress.java:1077)
        at org.apache.hadoop.mapred.CapacityTaskScheduler$MapSchedulingMgr.obtainNewTask(CapacityTaskScheduler.java:796)
        at org.apache.hadoop.mapred.CapacityTaskScheduler$TaskSchedulingMgr.getTaskFromQueue(CapacityTaskScheduler.java:589)
        at org.apache.hadoop.mapred.CapacityTaskScheduler$TaskSchedulingMgr.assignTasks(CapacityTaskScheduler.java:677)
        at org.apache.hadoop.mapred.CapacityTaskScheduler$TaskSchedulingMgr.access$500(CapacityTaskScheduler.java:348)
        at org.apache.hadoop.mapred.CapacityTaskScheduler.addMapTask(CapacityTaskScheduler.java:1397)
        at org.apache.hadoop.mapred.CapacityTaskScheduler.assignTasks(CapacityTaskScheduler.java:1349)
        at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:2976)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
{noformat}
",dking,amareshwari,Blocker,Closed,Fixed,12/Jan/10 08:30,24/Aug/10 21:20
Bug,MAPREDUCE-1375,12445518,TestFileArgs fails intermittently,"TestFileArgs failed once for me with the following error
{code}
expected:<[job.jar
sidefile
tmp
]> but was:<[]>
sidefile
tmp
]> but was:<[]>
        at org.apache.hadoop.streaming.TestStreaming.checkOutput(TestStreaming.java:107)
        at org.apache.hadoop.streaming.TestStreaming.testCommandLine(TestStreaming.java:123)
{code}",tlipcon,amar_kamat,Major,Closed,Fixed,14/Jan/10 14:28,12/Dec/11 06:18
Bug,MAPREDUCE-1378,12445596,Args in job details links on jobhistory.jsp are not URL encoded,"The logFile argument in the job links on the JT jobhistory.jsp page is not properly URL encoded leading to links that result in 500 errors. I found the issue while working with the Cloudera distro which contained a plus ('+') in the path which is interpreted as a space character (%20) by Firefox. Here is the (trimmed) URL. Note the hadoop-0.20.1+152 directory which should be hadoop-0.20.1%2B152. I have created a patch against current ASF svn trunk but it is untested (although the jsp compiles to a class file ok).

A job link from http://host:50030/jobhistory.jsp:
http://host:50030/jobdetailshistory.jsp?jobid=job_201001141235_0001&logFile=file:/Users/esammer/hadoop-0.20.1+152/logs/history/done/...",esammer,esammer,Trivial,Closed,Fixed,15/Jan/10 05:04,18/Nov/10 19:50
Bug,MAPREDUCE-1394,12446192,Sqoop generates incorrect URIs in paths sent to Hive,"Hive used to require a ':8020' in HDFS URIs used with LOAD DATA statements, even though the normalized form of such a URI does not contain an explicit port number (since 8020 is the default port). Sqoop matched this by hacking the URI strings it forwarded to Hive.

Hive fixed this bug a while ago -- Sqoop should catch up.",kimballa,kimballa,Major,Resolved,Fixed,21/Jan/10 04:12,02/Jul/10 06:31
Bug,MAPREDUCE-1395,12446266,Sqoop does not check return value of Job.waitForCompletion(),"Old code depended on JobClient.runJob() throwing IOException on failure. Job.waitForCompletion can fail in that manner, or it can fail by returning false. Sqoop needs to check for this condition.",kimballa,kimballa,Major,Resolved,Fixed,21/Jan/10 22:31,02/Jul/10 06:31
Bug,MAPREDUCE-1397,12446300,NullPointerException observed during task failures,"In an environment where many jobs are killed simultaneously, NPEs are observed in the TT/JT logs when a task fails. The situation is aggravated when the taskcontroller.cfg is not configured properly. Below is the exception obtained:

{noformat}
INFO org.apache.hadoop.mapred.TaskInProgress: Error from <attempt_ID>:
java.lang.Throwable: Child Error
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:529)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:329)
        at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:315)
        at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:146)
        at org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:109)
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:502)

 {noformat}",amareshwari,rramya,Minor,Closed,Fixed,22/Jan/10 10:36,20/May/13 15:26
Bug,MAPREDUCE-1398,12446344,TaskLauncher remains stuck on tasks waiting for free nodes even if task is killed.,"Tasks could be assigned to trackers for slots that are running other tasks in a commit pending state. This is an optimization done to pipeline task assignment and launch. When the task reaches the tracker, it waits until sufficient slots become free for it. This wait is done in the TaskLauncher thread. Now, while waiting, if the task is killed externally (maybe because the job finishes, etc), the TaskLauncher is not notified of this. So, it continues to wait for the killed task to get sufficient slots. If slots do not become free for a long time, this would result in considerable delay in waking up the TaskLauncher thread. If the waiting task happens to be a high RAM task, then it is also wasteful, because by waking up, it can make way for normal tasks that can run on the available number of slots.",amareshwari,yhemanth,Major,Closed,Fixed,22/Jan/10 17:09,24/Aug/10 21:20
Bug,MAPREDUCE-1399,12446375,The archive command shows a null error message,"{noformat}
bash-3.1$ hadoop archive -archiveName foo.har -p . foo .
Exception in archives
null
{noformat}",szetszwo,szetszwo,Major,Closed,Fixed,22/Jan/10 21:52,24/Aug/10 21:20
Bug,MAPREDUCE-1400,12446379,sed in build.xml fails,MAPRED version of HADOOP-6505,aw,aw,Minor,Closed,Fixed,22/Jan/10 22:13,24/Aug/10 21:20
Bug,MAPREDUCE-1406,12446496,JobContext.MAP_COMBINE_MIN_SPILLS is misspelled,{{JobContext.MAP_COMBINE_MIN_SPILLS}} is misspelled as {{JobContext.MAP_COMBINE_MIN_SPISS}},cdouglas,cdouglas,Trivial,Closed,Fixed,25/Jan/10 04:28,24/Aug/10 21:20
Bug,MAPREDUCE-1407,12446504,"Invalid example in the documentation of org.apache.hadoop.mapreduce.{Mapper,Reducer}",Both examples are using context.collect instead of context.write,tsuna,tsuna,Trivial,Resolved,Fixed,25/Jan/10 05:53,20/Mar/10 16:55
Bug,MAPREDUCE-1408,12446505,Allow customization of job submission policies,"Currently, gridmix3 replay job submission faithfully. For evaluation purposes, it would be great if we can support other job submission policies such as sequential job submission, or stress job submission.",rksingh,rksingh,Major,Closed,Fixed,25/Jan/10 06:28,24/Aug/10 21:20
Bug,MAPREDUCE-1409,12446523,FileOutputCommitter.abortTask should not catch IOException,"FileOutputCommitter.abortTask currently catches IOException. It should be thrown out, thus making the task failed.",amareshwari,amareshwari,Major,Closed,Fixed,25/Jan/10 11:13,24/Aug/10 21:20
Bug,MAPREDUCE-1412,12446626,TestTaskTrackerBlacklisting fails sometimes,"{{TestTaskTrackerBlacklisting}} fails occasionally. The granularity of the timer is responsible; the unit test adds a day to the expiration interval to verify that the tracker is removed from the blacklist, but the tracker is not removed if the interval exactly matches 1 day.",cdouglas,cdouglas,Minor,Closed,Fixed,26/Jan/10 01:47,24/Aug/10 21:20
Bug,MAPREDUCE-1416,12446772,New JIRA components for Map/Reduce project,"We need more JIRA components for the Map/Reduce project for better tracking. Some missing ones: DistributedCache, TaskController, contrib/vaidya, contrib/mruit, contrib/dynamic-scheduler, contrib/data_join.",stevel@apache.org,vinodkv,Major,Resolved,Fixed,27/Jan/10 06:49,18/Feb/12 16:20
Bug,MAPREDUCE-1417,12446776,Forrest documentation should be updated to reflect the changes in MAPREDUCE-744,MAPREDUCE-744 introduced private/public visibility of DistributedCache files on the TaskTracker. Forrest documentation is stale and only refers to private visible files.,ravidotg,vinodkv,Major,Closed,Fixed,27/Jan/10 07:15,24/Aug/10 21:20
Bug,MAPREDUCE-1420,12446800,TestTTResourceReporting failing in trunk,"TestTTResourceReporting failing in trunk. 

The most specific issue from the logs seems to be : Error executing shell command org.apache.hadoop.util.Shell$ExitCodeException: kill: No such process 

Link :
http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-Mapreduce-trunk/217/

Attaching output in a  file.

",schen,iyappans,Major,Closed,Fixed,27/Jan/10 12:07,24/Aug/10 21:20
Bug,MAPREDUCE-1421,12446875,LinuxTaskController tests failing on trunk after the commit of MAPREDUCE-1385,"The following tests fail, in particular:
 - TestDebugScriptWithLinuxTaskController
 - TestJobExecutionAsDifferentUser
 - TestPipesAsDifferentUser
 - TestKillSubProcessesWithLinuxTaskController",amareshwari,amareshwari,Major,Closed,Fixed,28/Jan/10 03:40,24/Aug/10 21:20
Bug,MAPREDUCE-1422,12446883,Changing permissions of files/dirs under job-work-dir may be needed sothat cleaning up of job-dir in all mapred-local-directories succeeds always,"After MAPREDUCE-896, if LinuxTaskController is set in config, task-controller binary is launched for changing permissions of taskAttemptDir and taskWorkDir before cleaning up of these directories sothat cleanup will be succeeded even if user had created files/dirs under taskAttemptDir or taskWorkDir with non-writable permissions. Users can't create files/dirs under job-dir directly as we set 2570 for job-dir. But as job-work-dir has 2770 permissions and user can create files/dirs under job-work-dir with non-writable permissions, Changing permissions of files/dirs under job-work-dir may be needed sothat cleaning up of job-dir in all mapred-local-directories succeeds always.",amar_kamat,ravidotg,Major,Closed,Fixed,28/Jan/10 08:01,24/Aug/10 21:20
Bug,MAPREDUCE-1435,12454886,symlinks in cwd of the task are not handled properly after MAPREDUCE-896,"With JVM reuse, TaskRunner.setupWorkDir() lists the contents of workDir and does a fs.delete on each path listed. If the listed file is a symlink to directory, it will delete the contents of those linked directories. This would delete files from distributed cache and jars directory,if mapred.create.symlink is true.
Changing ownership/permissions of symlinks through ENABLE_TASK_FOR_CLEANUP would change ownership/permissions of underlying files.

This is observed by Karam while running streaming jobs with DistributedCache and jvm reuse.",ravidotg,amareshwari,Major,Closed,Fixed,01/Feb/10 09:34,24/Aug/10 21:20
Bug,MAPREDUCE-1443,12455108,DBInputFormat can leak connections,"The DBInputFormat creates a Connection to use when enumerating splits, but never closes it. This can leak connections to the database which are not cleaned up for a long time.",kimballa,kimballa,Major,Resolved,Fixed,02/Feb/10 20:14,04/Feb/10 17:20
Bug,MAPREDUCE-1444,12455109,Sqoop ConnManager instances can leak Statement objects,"The ConnManager API returns ResultSets to users but does not provide a mechanism to clean up the underlying Statement that generated the ResultSet. Problematically, closing the Statement will invalidate the ResultSet, so these must be cleaned up in LIFO order, putting the onus on the receiver of the ResultSet.",kimballa,kimballa,Major,Resolved,Fixed,02/Feb/10 20:17,02/May/13 02:29
Bug,MAPREDUCE-1448,12455144,[Mumak] mumak.sh does not honor --config option.,"When --config is specified, mumak.sh should put the customized conf directory in the classpath.",hong.tang,hong.tang,Major,Closed,Fixed,03/Feb/10 07:55,24/Aug/10 21:20
Bug,MAPREDUCE-1469,12455711,Sqoop should disable speculative execution in export,Concurrent writers of the same output shard may cause the database to try to insert duplicate primary keys concurrently. Not a good situation. Speculative execution should be forced off for this operation.,kimballa,kimballa,Major,Resolved,Fixed,09/Feb/10 01:33,02/Jul/10 06:32
Bug,MAPREDUCE-1474,12455860,forrest docs for archives is out of date.,The docs for archives are out of date. The new docs that were checked into hadoop common were lost because of the project split.,mahadev,mahadev,Major,Closed,Fixed,10/Feb/10 02:23,24/Aug/10 21:20
Bug,MAPREDUCE-1476,12455885,committer.needsTaskCommit should not be called for a task cleanup attempt,"Currently, Task.done() calls committer.needsTaskCommit() to know whether it needs a commit or not. This need not be called for task cleanup attempt as no commit is required for a cleanup attempt. 
Due to MAPREDUCE-1409, we saw a case where cleanup attempt went into COMMIT_PENDING state.",amareshwari,amareshwari,Major,Closed,Fixed,10/Feb/10 10:52,24/Aug/10 21:20
Bug,MAPREDUCE-1480,12455962,CombineFileRecordReader does not properly initialize child RecordReader,CombineFileRecordReader instantiates child RecordReader instances but never calls their initialize() method to give them the proper TaskAttemptContext.,kimballa,kimballa,Major,Resolved,Fixed,11/Feb/10 00:29,02/May/13 02:29
Bug,MAPREDUCE-1482,12455988,Better handling of task diagnostic information stored in the TaskInProgress,Task diagnostic information can be very large at times eating up Jobtracker's memory. There should be some way to avoid storing large error strings in JobTracker.,amar_kamat,amar_kamat,Major,Closed,Fixed,11/Feb/10 08:21,24/Aug/10 21:20
Bug,MAPREDUCE-1490,12456250,Raid client throws NullPointerException during initialization,"During instantiation and initialization, the DistributedRaidFileSystem class throws a NullPointerException.",rschmidt,rschmidt,Major,Closed,Fixed,14/Feb/10 01:35,24/Aug/10 21:20
Bug,MAPREDUCE-1494,12456312,TestJobDirCleanup verifies wrong jobcache directory,TestJobDirCleanup verifies tasktracker/jobcache directory to be empty. But localization happens in tasktracker/user/jobcache after MAPREDUCE-856.,amareshwari,amareshwari,Minor,Closed,Fixed,15/Feb/10 11:13,24/Aug/10 21:20
Bug,MAPREDUCE-1497,12456410,Suppress warning on inconsistent TaskTracker.indexCache synchronization,"Findbugs warns that TaskTracker.indexCache is incorrectly synchronized. 
It is not accessed synchronously from MapOutputServlet.doGet, TaskCleanupThread and  TaskTracker.killOverflowingTasks() method. Other callers access it synchronously.",amareshwari,amareshwari,Major,Closed,Fixed,16/Feb/10 10:09,24/Aug/10 21:20
Bug,MAPREDUCE-1505,12456720,Cluster class should create the rpc client only when needed,"It will be good to have the org.apache.hadoop.mapreduce.Cluster create the rpc client object only when needed (when a call to the jobtracker is actually required). org.apache.hadoop.mapreduce.Job constructs the Cluster object internally and in many cases the application that created the Job object really wants to look at the configuration only. It'd help to not have these connections to the jobtracker especially when Job is used in the tasks (for e.g., Pig calls mapreduce.FileInputFormat.setInputPath in the tasks and that requires a Job object to be passed).

In Hadoop 20, the Job object internally creates the JobClient object, and the same argument applies there too.
",dking,ddas,Major,Closed,Fixed,18/Feb/10 21:56,12/Dec/11 06:19
Bug,MAPREDUCE-1506,12456725,Assertion failure in TestTaskTrackerMemoryManager,"With asserts enabled, TestTaskTrackerMemoryManager sometimes fails. From what I've inspected, it's because some tasks are marked as FAILED/TIPFAILED while others are marked SUCCEEDED.

This can be reproduced by applying MAPREDUCE-1092 and then running {{ant clean test -Dtestcase=TestTaskTrackerMemoryManager}}",,kimballa,Major,Resolved,Fixed,18/Feb/10 23:16,09/Mar/15 20:17
Bug,MAPREDUCE-1508,12456731,NPE in TestMultipleLevelCaching on error cleanup path,TestMultipleLevelCaching dereferences objects in a finally block which may not have been initialized.,kimballa,kimballa,Major,Closed,Fixed,19/Feb/10 00:03,24/Aug/10 21:20
Bug,MAPREDUCE-1515,12456912,need to pass down java5 and forrest home variables,"Currently, the build script doesn't pass down the variables for java5 and forrest, so the build breaks unless they are on the command line.",almacro,omalley,Major,Closed,Fixed,20/Feb/10 22:15,27/Dec/10 20:08
Bug,MAPREDUCE-1519,12456968,RaidNode fails to create new parity file if an older version already exists,"When RaidNode tries to recreate a parity file for a source file that has been modified (recreated) recently, it crashes.

",rschmidt,rschmidt,Major,Closed,Fixed,22/Feb/10 03:02,24/Aug/10 21:20
Bug,MAPREDUCE-1520,12456971,TestMiniMRLocalFS fails on trunk,TestMiniMRLocalFS fails on trunk. I checked with both trunk revs - pre and post MAPREDUCE-1430 commit and it failed in both the cases.,amareshwari,ddas,Major,Closed,Fixed,22/Feb/10 06:11,24/Aug/10 21:20
Bug,MAPREDUCE-1522,12457080,FileInputFormat may change the file system of an input path,"org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath(Job job, Path path) uses the default FileSystem but not the FileSystem specified in the path.
{code}
//org.apache.hadoop.mapreduce.lib.input.FileInputFormat
  public static void addInputPath(Job job, 
                                  Path path) throws IOException {
    Configuration conf = job.getConfiguration();
    FileSystem fs = FileSystem.get(conf);
    path = path.makeQualified(fs); // the original FileSystem is lost.
    ...
  }
{code}
There is a similar problem in FileInputFormat.setInputPaths(..).",szetszwo,szetszwo,Blocker,Resolved,Fixed,22/Feb/10 22:00,02/May/13 02:29
Bug,MAPREDUCE-1523,12457082,Sometimes rumen trace generator fails to extract the job finish time.,We saw sometimes (not very often) that rumen may fail to extract the job finish time from Hadoop 0.20 history log.,dking,hong.tang,Major,Closed,Fixed,22/Feb/10 22:21,29/Oct/10 02:04
Bug,MAPREDUCE-1528,12457232,TokenStorage should not be static,"Currently, TokenStorage is a singleton. This doesn't work for some use cases, such as Oozie. I think that each Job should have a TokenStorage that is associated it.",jnp,omalley,Major,Closed,Fixed,23/Feb/10 22:26,12/Dec/11 06:18
Bug,MAPREDUCE-1532,12457377,Delegation token is obtained as the superuser,"When the UserGroupInformation.doAs is invoked for proxy users, the delegation token is incorrectly obtained as the real user.",ddas,ddas,Major,Closed,Fixed,25/Feb/10 02:55,12/Dec/11 06:20
Bug,MAPREDUCE-1533,12457381,Reduce or remove usage of String.format() usage in CapacityTaskScheduler.updateQSIObjects and Counters.makeEscapedString(),"When short jobs are executed in hadoop with OutOfBandHeardBeat=true, JT executes heartBeat() method heavily. This internally makes a call to CapacityTaskScheduler.updateQSIObjects(). 

CapacityTaskScheduler.updateQSIObjects(), internally calls String.format() for setting the job scheduling information. Based on the datastructure size of ""jobQueuesManager"" and ""queueInfoMap"", the number of times String.format() gets executed becomes very high. String.format() internally does pattern matching which turns to be out very heavy (This was revealed while profiling JT. Almost 57% of time was spent in CapacityScheduler.assignTasks(), out of which String.format() took 46%.

Would it be possible to do String.format() only at the time of invoking JobInProgress.getSchedulingInfo?. This might reduce the pressure on JT while processing heartbeats. ",dking,rajesh.balamohan,Major,Closed,Fixed,25/Feb/10 05:06,12/Dec/11 06:19
Bug,MAPREDUCE-1536,12457519,DataDrivenDBInputFormat does not split date columns correctly.,"The DateSplitter does not properly split a range of (min, max) dates.",kimballa,kimballa,Major,Closed,Fixed,26/Feb/10 03:51,24/Aug/10 21:20
Bug,MAPREDUCE-1537,12457520,TestDelegationTokenRenewal fails,"TestDelegationTokenRenewal does not compile in trunk.
The reason is that DelegationTokenSecretManager in hdfs requires namesystem in constructor.",jnp,jnp,Major,Closed,Fixed,26/Feb/10 03:53,24/Aug/10 21:20
Bug,MAPREDUCE-1538,12457524,TrackerDistributedCacheManager can fail because the number of subdirectories reaches system limit,"TrackerDistributedCacheManager deletes the cached files when the size goes up to a configured number.
But there is no such limit for the number of subdirectories. Therefore the number of subdirectories may grow large and exceed system limit.
This will make TT cannot create directory when getLocalCache and fails the tasks.",schen,schen,Major,Closed,Fixed,26/Feb/10 04:29,24/Aug/10 21:20
Bug,MAPREDUCE-1543,12457695,Log messages of JobACLsManager should use security logging of HADOOP-6586,{{JobACLsManager}} added in MAPREDUCE-1307 logs the successes and failures w.r.t job-level authorization in the corresponding Daemons' logs. The log messages should instead use security logging of HADOOP-6586.,vicaya,vinodkv,Major,Closed,Fixed,01/Mar/10 04:35,12/Dec/11 06:19
Bug,MAPREDUCE-1547,12457822,Build Hadoop-Mapreduce-trunk and Mapreduce-trunk-Commit  fails,"http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-Mapreduce-trunk/243/console



Build Hadoop-Mapreduce-trunk fails with this message

BUILD FAILED
/grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build.xml:1382: Execute failed: java.io.IOException: Cannot run program ""autoreconf"" (in directory ""/grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/src/c++/utils""): java.io.IOException: error=2, No such file or directory

http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-Mapreduce-trunk-Commit/251/console

Build Mapreduce-trunk-Commit fails with this message:

/bin/bash /tmp/hudson2120484997034746272.sh
bash: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk-Commit/nightly/commitBuild.sh: No such file or directory

",gkesavan,iyappans,Major,Closed,Fixed,02/Mar/10 05:14,24/Aug/10 21:20
Bug,MAPREDUCE-1558,12458047,specify correct server principal for RefreshAuthorizationPolicyProtocol and RefreshUserToGroupMappingsProtocol protocols in MRAdmin (for HADOOP-6612),,boryas,boryas,Major,Closed,Fixed,03/Mar/10 22:47,12/Dec/11 06:18
Bug,MAPREDUCE-1559,12458060,The DelegationTokenRenewal timer task should use the jobtracker's credentials to create the filesystem,"The submitJob RPC finally creates a timer task for renewing the delegation tokens of the submitting user. This timer task inherits the context of the RPC handler that runs in the context of the job submitting user, and when it tries to create a filesystem, the RPC client tries to use the user's credentials. This should instead use the JobTracker's credentials.",ddas,ddas,Major,Closed,Fixed,04/Mar/10 01:03,12/Dec/11 06:19
Bug,MAPREDUCE-1566,12458310,Need to add a mechanism to import tokens and secrets into a submitted job.,We need to include tokens and secrets into a submitted job. I propose adding a configuration attribute that when pointed at a token storage file will include the tokens and secrets from that token storage file.,jnp,omalley,Major,Closed,Fixed,05/Mar/10 23:06,12/Dec/11 06:19
Bug,MAPREDUCE-1573,12458428,TestStreamingAsDifferentUser fails if run as tt_user,"TestStreamingAsDifferentUser fails if run as tt_user. MAPREDUCE-890 didn't make the necessary changes needed for the newly added testcase in TestStreamignAsDifferentUser.

{code}
Testcase: testStreamingWithDistCache took 21.228 sec
  FAILED
Path /tmp/hadoop-gravi/mapred/local/0_0/taskTracker/gravi/distcache has the permissions drwxrws--- instead of the expected dr-xrws---
junit.framework.AssertionFailedError: Path /tmp/hadoop-gravi/mapred/local/0_0/taskTracker/gravi/distcache has the permissions drwxrws--- instead of the expected dr-xrws---   at org.apache.hadoop.mapred.TestTaskTrackerLocalization.checkFilePermissions(TestTaskTrackerLocalization.java:292)
  at org.apache.hadoop.mapred.ClusterWithLinuxTaskController.checkPermissionsOnDir(ClusterWithLinuxTaskController.java:440)
  at org.apache.hadoop.mapred.ClusterWithLinuxTaskController.checkPermissionsOnPrivateDistCache(ClusterWithLinuxTaskController.java:354)
  at org.apache.hadoop.streaming.TestStreamingAsDifferentUser$2.run(TestStreamingAsDifferentUser.java:157)
  at org.apache.hadoop.streaming.TestStreamingAsDifferentUser$2.run(TestStreamingAsDifferentUser.java:120)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:396)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:706)
  at org.apache.hadoop.streaming.TestStreamingAsDifferentUser.testStreamingWithDistCache(TestStreamingAsDifferentUser.java:120)
{code}",ravidotg,ravidotg,Major,Closed,Fixed,08/Mar/10 11:39,24/Aug/10 21:21
Bug,MAPREDUCE-1578,12458491,HadoopArchives.java should not use HarFileSystem.VERSION,"If we upgrade the protocol on HarFileSystem, HadoopArchives might generate an old archive and assign the new version number to it.

This should be fixed",rschmidt,rschmidt,Major,Closed,Fixed,09/Mar/10 02:10,24/Aug/10 21:21
Bug,MAPREDUCE-1585,12458599,Create Hadoop Archives version 2 with filenames URL-encoded,"Hadoop Archives version 1 don't cope with files that have spaces on their names.

One proposal is to URLEncode filenames inside the index file (version 2, refers to HADOOP-6591).

This task is to allow the creation of version 2 files that have file names encoded appropriately. It currently depends on HADOOP-6591",rschmidt,rschmidt,Major,Closed,Fixed,10/Mar/10 00:34,24/Aug/10 21:21
Bug,MAPREDUCE-1596,12458974,MapReduce trunk snapshot is not being published to maven,"The hadoop-core and hadoop-hdfs artifacts are pushed to maven on a regular basis (daily?), but hadoop-mapreduce has not been updated since 2/18/10. Is there something automatic in Hudson that is configured for these core and hdfs, but not mapred?

Downstream projects that try to build against Hadoop's trunk (via Ivy or Maven) cannot compile due to API inconsistency here.

",gkesavan,kimballa,Critical,Closed,Fixed,12/Mar/10 19:18,24/Aug/10 21:21
Bug,MAPREDUCE-1597,12458980,combinefileinputformat does not work with non-splittable files,"CombineFileInputFormat.getSplits() does not take into account whether a file is splittable.
This can lead to a problem for compressed text files - for example, getSplits() may return more
than 1 split depending on the size of the compressed file, all the splits recordreader will read the
complete file.

I ran into this problem while using Hive on hadoop 20.
",amareshwari,namit,Major,Closed,Fixed,12/Mar/10 21:57,27/Jul/15 11:39
Bug,MAPREDUCE-1599,12459174,MRBench reuses jobConf and credentials there in.,"MRBench reuses the jobconf and therefore credentials are re-used, but JobTracker cancels the delegation tokens therefore the test fails sometimes. 
The fix is to pass the mapreduce.job.complete.cancel.delegation.tokens=false in the jobconf so that JobTracker does not cancel the tokens.",jnp,jnp,Major,Closed,Fixed,15/Mar/10 17:30,12/Dec/11 06:18
Bug,MAPREDUCE-1602,12459326,"When the src does not exist, archive shows IndexOutOfBoundsException","{noformat}
-bash-3.1$ $H archive -archiveName foo.har -p / src-not-exists dst
IndexOutOfBoundsException in archives
Index: 0, Size: 0
{noformat}",szetszwo,szetszwo,Major,Closed,Fixed,16/Mar/10 21:51,24/Aug/10 21:21
Bug,MAPREDUCE-1604,12459352,Job acls should be documented in forrest.,Job acls introduced in MAPREDUCE-1307 should be documented in forrest.,amareshwari,amareshwari,Major,Closed,Fixed,17/Mar/10 06:02,24/Aug/10 21:21
Bug,MAPREDUCE-1606,12459364,TestJobACLs may timeout as there are no slots for launching JOB_CLEANUP task,"TestJobACLs may timeout as there are no slots for launching JOB_CLEANUP task. Because MiniMRCluster with 0 TaskTrackers is started in the test. In trunk, we can set the config property mapreduce.job.committer.setup.cleanup.needed to false sothat we don't get into this issue.",ravidotg,ravidotg,Major,Closed,Fixed,17/Mar/10 08:58,24/Aug/10 21:21
Bug,MAPREDUCE-1607,12459508,Task controller may not set permissions for a task cleanup attempt's log directory,"Task controller uses the INITIALIZE_TASK command to initialize task attempt and task log directories. For cleanup tasks, task attempt directories are named as task-attempt-id.cleanup. But log directories do not have the .cleanup suffix. The task controller is not aware of this distinction and tries to set permissions for log directories named task-attempt-id.cleanup. This is a NO-OP. Typically the task cleanup runs on the same node that ran the original task attempt as well. So, the task log directories are already properly initialized. However, the task cleanup can run on a node that has not run the original task attempt. In that case, the initialization would not happen and this could result in the cleanup task failing.",amareshwari,yhemanth,Major,Closed,Fixed,18/Mar/10 12:00,24/Aug/10 21:21
Bug,MAPREDUCE-1609,12459606,TaskTracker.localizeJob should not set permissions on job log directory recursively,"Currently TaskTracker.localizeJob sets permissions (570 with LinuxTaskController) on job log directory recursively. When the tracker restarts/reinits, if there are new tasks for the job, localizeJob would find attempt log directories for the earlier tasks. If the job has many tasks, it would spend significant time in chmod'ing.

Also, if a cleanup attempt is launched after the restart/reinit, we would hit MAPREDUCE-1607.

This problem is missed by the patch for MAPREDUCE-927. The above problem never existed before MAPREDUCE:927.",amareshwari,amareshwari,Major,Closed,Fixed,19/Mar/10 03:52,24/Aug/10 21:21
Bug,MAPREDUCE-1610,12459607,Forrest documentation should be updated to reflect the changes in MAPREDUCE-856,The directory structure under mapred-local-dir is changed in MAPREDUCE-856. This needs changes to the forrest documentation.,ravidotg,ravidotg,Major,Closed,Fixed,19/Mar/10 05:00,24/Aug/10 21:21
Bug,MAPREDUCE-1611,12459677,Refresh nodes and refresh queues doesnt work with service authorization enabled,"If service-level authorization enabled (i.e _hadoop.security.authorization_ set to *true*) for MapReduce then refreshing the node and queue fails with the following message
{noformat}
Protocol interface org.apache.hadoop.mapred.AdminOperationsProtocol is not known.
{noformat}",amar_kamat,amar_kamat,Blocker,Closed,Fixed,19/Mar/10 16:53,24/Aug/10 21:21
Bug,MAPREDUCE-1612,12459685,job conf file is not accessible from job history web page,Clicking on conf file link from job history web page is causing an NPE if history file(and the job conf file) are stored on DFS. This NPE is from jobconf_history.jsp because jobConf built from path on DFS is not having any properties.,ravidotg,ravidotg,Major,Closed,Fixed,19/Mar/10 17:47,24/Aug/10 21:21
Bug,MAPREDUCE-1615,12459725,ant test on trunk does not compile.,"ant test on trunk fails to compile with the following error:

{noformat}
    [javac] Compiling 264 source files to /home/mahadev/workspace/hadoop-commit-trunk/mapreduce/build/test/mapred/classes
    [javac] /mapreduce/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java:266: \
            getListing(java.lang.String,byte[]) in org.apache.hadoop.hdfs.protocol.ClientProtocol \
            cannot be applied to (java.lang.String)
    [javac]         client.getListing(path.toString());
    [javac]               ^
    [javac] /mapreduce/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java:279: \
            getListing(java.lang.String,byte[]) in org.apache.hadoop.hdfs.protocol.ClientProtocol \
            cannot be applied to (java.lang.String)
    [javac]         client.getListing(jobSubmitDirpath.toString());
    [javac]               ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 2 errors

BUILD FAILED
{noformat}",cdouglas,mahadev,Blocker,Closed,Fixed,19/Mar/10 23:36,24/Aug/10 21:21
Bug,MAPREDUCE-1617,12459948,TestBadRecords failed once in our test runs,"org.apache.hadoop.mapred.TestBadRecords.testBadMapRed failed with the following
exception:
java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1142)
        at org.apache.hadoop.mapred.TestBadRecords.runMapReduce(TestBadRecords.java:94)
        at org.apache.hadoop.mapred.TestBadRecords.testBadMapRed(TestBadRecords.java:211)
",vicaya,amareshwari,Major,Closed,Fixed,23/Mar/10 03:57,12/Dec/11 06:18
Bug,MAPREDUCE-1618,12459953,JobStatus.getJobAcls() and setJobAcls should have javadoc,org.apache.hadoop.mapreduce.JobStatus.getJobAcls() and setJobAcls are added in MAPREDUCE-1307. They should have javadoc.,amareshwari,amareshwari,Trivial,Closed,Fixed,23/Mar/10 06:34,24/Aug/10 21:21
Bug,MAPREDUCE-1621,12460035,Streaming's TextOutputReader.getLastOutput throws NPE if it has never read any output,"If TextOutputReader.readKeyValue() has never successfully read a line, then its bytes member will be left null. Thus when logging a task failure, PipeMapRed.getContext() can trigger an NPE when it calls outReader_.getLastOutput().",amareshwari,tlipcon,Major,Closed,Fixed,23/Mar/10 19:25,12/Dec/11 06:19
Bug,MAPREDUCE-1622,12460040,Include slf4j dependencies in binary tarball,"After MAPREDUCE-1556 and HADOOP-6486, starting \*Trackers from a binary tarball produces the following warning:
{noformat}
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
2010-03-18 01:11:32.988::INFO:  Logging to STDERR via org.mortbay.log.StdErrLog
2010-03-18 01:11:33.056::INFO:  jetty-6.1.14
{noformat}",cdouglas,cdouglas,Minor,Closed,Fixed,23/Mar/10 20:41,24/Aug/10 21:21
Bug,MAPREDUCE-1628,12457618,HarFileSystem shows incorrect replication numbers and permissions,"In the har dir, the replication # of part-0 is 3.
{noformat}
-bash-3.1$ hadoop fs -ls  ${DIR}.har
Found 3 items
-rw-------   5 tsz users       1141 2010-02-10 18:34 /user/tsz/t20.har/_index
-rw-------   5 tsz users         24 2010-02-10 18:34 /user/tsz/t20.har/_masterindex
-rw-------   3 tsz users      15052 2010-02-10 18:34 /user/tsz/t20.har/part-0
{noformat}
but the replication # of the individual har:// files is shown as 5.
{noformat}
-bash-3.1$ hadoop fs -lsr  ${HAR_FULL}/
drw-------   - tsz users          0 2010-02-10 18:34 /user/tsz/t20.har/t20
-rw-------   5 tsz users        723 2010-02-10 18:34 /user/tsz/t20.har/t20/text-00000000
-rw-------   5 tsz users        779 2010-02-10 18:34 /user/tsz/t20.har/t20/text-00000001
-rw-------   5 tsz users        818 2010-02-10 18:34 /user/tsz/t20.har/t20/text-00000002
...
{noformat}
The permission also has similar problem.  Clearly, the permission of t20.har/t20 shown above is incorrect.",szetszwo,szetszwo,Major,Closed,Fixed,27/Feb/10 00:02,24/Aug/10 21:21
Bug,MAPREDUCE-1629,12459733,"Get rid of fakeBlockLocations() on HarFileSystem, since it's not used","On HarFileSystem.java, I think function fakeBlockLocations() was left behind when Mahadev fixed HADOOP-6467.",mahadev,rschmidt,Trivial,Closed,Fixed,20/Mar/10 01:36,24/Aug/10 21:21
Bug,MAPREDUCE-1633,12460264,Queue ACLs documentation must talk about wildcards,Currently the Forrest documentation about queue ACLs is not talking about wildcards - the value that states that all users are allowed for an operation in a queue. This should be documented.,,yhemanth,Major,Resolved,Fixed,25/Mar/10 18:04,30/Jul/14 17:15
Bug,MAPREDUCE-1635,12460335,ResourceEstimator does not work after MAPREDUCE-842,"MAPREDUCE-842 changed Child's mapred.local.dir to have attemptDir as the base local directory. Also assumption is that
org.apache.hadoop.mapred.MapOutputFile always gets Child's mapred.local.dir. 
But, MapOuptutFile.getOutputFile() is called from TaskTracker's conf, which does not find the output file. Thus TaskTracker.tryToGetOutputSize() always returns -1.
",amareshwari,amareshwari,Major,Closed,Fixed,26/Mar/10 10:35,24/Aug/10 21:21
Bug,MAPREDUCE-1636,12460374,Missing counters on taskdetails.jsp,A tip counter is actually the counter of its best performing attempt. This is correctly displayed on jobtasks.jsp but is missing on the taskdetails.jsp.,,amar_kamat,Major,Resolved,Fixed,26/Mar/10 16:33,30/Jul/14 17:16
Bug,MAPREDUCE-1641,12460530,Job submission should fail if same uri is added for mapred.cache.files and mapred.cache.archives,"The behavior of mapred.cache.files and mapred.cache.archives is different during localization in the following way:

If a jar file is added to mapred.cache.files,  it will be localized under TaskTracker under a unique path. 
If a jar file is added to mapred.cache.archives, it will be localized under a unique path in a directory named the jar file name, and will be unarchived under the same directory.

If same jar file is passed for both the configurations, the behavior undefined. Thus the job submission should fail.
Currently, since distributed cache processes files before archives, the jar file will be just localized and not unarchived.",dking,amareshwari,Major,Resolved,Fixed,29/Mar/10 10:13,30/Jul/14 17:48
Bug,MAPREDUCE-1645,12460670,Task cleanup attempt details should also be logged to JobHistory,"Currently, Task cleanup attempt details are not logged to JobHistory. JobHistory should log at least where the task cleanup attempt ran.",,amareshwari,Major,Resolved,Fixed,30/Mar/10 04:53,30/Jul/14 17:20
Bug,MAPREDUCE-1657,12460790,"After task logs directory is deleted, tasklog servlet displays wrong error message about job ACLs","When task log gets deleted if from Web UI we click view task log, web page displays wrong error message -:
[
HTTP ERROR: 401

User user1 failed to view tasklogs of job job_201003241521_0001!

user1 is not authorized for performing the operation VIEW_JOB on job_201003241521_0001. VIEW_JOB Access control list
configured for this job : 

RequestURI=/tasklog
]
Even if user is having view job acls set / or user is owner of job.
",ravidotg,ravidotg,Major,Closed,Fixed,31/Mar/10 08:45,24/Aug/10 21:21
Bug,MAPREDUCE-1659,12460806,RaidNode should write temp files on /tmp and add random numbers to their names to avoid conflicts,"The RaidNode methods to raid files and recover them should write recovery and tmp files on /tmp instead of /raid.

Besides that, filenames should have a random number appended to them to avoid conflicts. This makes the code safer and avoids errors when multiple recoveries run in parallel.",rschmidt,rschmidt,Major,Closed,Fixed,31/Mar/10 12:12,24/Aug/10 21:21
Bug,MAPREDUCE-1662,12460892,TaskRunner.prepare() and close() can be removed,"TaskRunner.prepare() and close() methods call only mapOutputFile.removeAll(). The removeAll() call is a always a no-op in prepare(), because the directory is always empty during start up of the task. The removeAll() call in close() is useless, because it is followed by a attempt directory cleanup. Since the map output files are in attempt directory,  the call to close() is useless.
After MAPREDUCE-842, these calls are under TaskTracker space, passing the wrong conf. Now, the calls do not make sense at all.
I think we can remove the methods.",amareshwari,amareshwari,Major,Closed,Fixed,01/Apr/10 05:47,12/Dec/11 06:18
Bug,MAPREDUCE-1664,12460904,Job Acls affect Queue Acls,"MAPREDUCE-1307 introduced job ACLs for securing job level operations. So in current trunk, queue ACLs and job ACLs are checked(with AND for both acls) for allowing job level operations. So for doing operations like killJob, killTask and setJobPriority user should be part of both mapred.queue.{queuename}.acl-administer-jobs and in mapreduce.job.acl-modify-job. This needs to change so that users who are part of mapred.queue.{queuename}.acl-administer-jobs will be able to do killJob,killTask,setJobPriority and users part of mapreduce.job.acl-modify-job will be able to do killJob,killTask,setJobPriority.",ravidotg,ravidotg,Major,Closed,Fixed,01/Apr/10 08:57,11/Feb/16 23:12
Bug,MAPREDUCE-1668,12461023,RaidNode should only Har a directory if all its parity files have been created,"In the current code, it can happen that a directory will be Archived (Har'ed) before all its parity files have been generated since parity file generation is not atomic. We should verify if all the parity files are present before Archiving a directory.",rvadali,rschmidt,Major,Closed,Fixed,02/Apr/10 14:17,12/Dec/11 06:20
Bug,MAPREDUCE-1670,12461105,RAID should avoid policies that scan their own destination path,"Raid currently allows policies that include the destination directory into the source directory and vice-versa.
Both situations can create cycles and should be avoided.",rvadali,rschmidt,Major,Closed,Fixed,03/Apr/10 23:52,12/Dec/11 06:18
Bug,MAPREDUCE-1682,12461346,Tasks should not be scheduled after tip is killed/failed.,"We have seen the following scenario in our cluster:
A job got marked failed, because four attempts of a TIP failed. This would kill all the map and reduce tips. Then a job-cleanup attempt is launched.
The job-cleanup attempt failed because it could not report status for 10 minutes. There are 3 such job-cleanup attempts leading the job to get killed after 1/2 hour.
While waiting for the job cleanup to finish, JobTracker scheduled many tasks of the job on TaskTrackers and sent a KillTaskAction in the next heartbeat. 

This is just wasting lots of resources, we should avoid scheduling tasks of a tip once the tip is killed/failed.

",acmurthy,amareshwari,Major,Resolved,Fixed,07/Apr/10 04:37,30/Jul/14 17:31
Bug,MAPREDUCE-1683,12461358,Remove JNI calls from ClusterStatus cstr,"The {{ClusterStatus}} constructor makes two JNI calls to the {{Runtime}} to fetch memory information. {{ClusterStatus}} instances are often created inside the {{JobTracker}} to obtain other, unrelated metrics (sometimes from schedulers' inner loops). Given that this information is related to the {{JobTracker}} process and not the cluster, the metrics are also available via {{JvmMetrics}}, and the jsps can gather this information for themselves: these fields can be removed from {{ClusterStatus}}",vicaya,cdouglas,Major,Closed,Fixed,07/Apr/10 06:58,12/Dec/11 06:18
Bug,MAPREDUCE-1684,12461380,ClusterStatus can be cached in CapacityTaskScheduler.assignTasks(),"Currently,  CapacityTaskScheduler.assignTasks() calls getClusterStatus() thrice: once in assignTasks(), once in MapTaskScheduler and once in ReduceTaskScheduler. It can be cached in assignTasks() and re-used.
",knoguchi,amareshwari,Major,Closed,Fixed,07/Apr/10 10:46,15/May/13 05:16
Bug,MAPREDUCE-1686,12461544,ClassNotFoundException for custom format classes provided in libjars,"The StreamUtil::goodClassOrNull method assumes user-provided classes have package names and if not, they are part of the Hadoop Streaming package. For example, using custom InputFormat or OutputFormat classes without package names will fail with a ClassNotFound exception which is not indicative given the classes are provided in the libjars option. Admittedly, most Java packages should have a package name so this should rarely come up.

Possible resolution options:

1) modify the error message to include the actual classname that was attempted in the goodClassOrNull method
2) call the Configuration::getClassByName method first and if class not found check for default package name and try the call again
{code}
    public static Class goodClassOrNull(Configuration conf, String className, String defaultPackage) {
        Class clazz = null;
        try {
            clazz = conf.getClassByName(className);
        } catch (ClassNotFoundException cnf) {
        }
        if (clazz == null) {
            if (className.indexOf('.') == -1 && defaultPackage != null) {
                className = defaultPackage + ""."" + className;
                try {
                    clazz = conf.getClassByName(className);
                } catch (ClassNotFoundException cnf) {
                }
            }
        }
        return clazz;
    }
{code}",pburkhardt,pburkhardt,Minor,Resolved,Fixed,08/Apr/10 16:30,29/Oct/10 02:04
Bug,MAPREDUCE-1687,12461633,Stress submission policy does not always stress the cluster.,"Currently, the rough idea of stress submission policy is to continue submitting jobs until the pending map tasks reach 2x of the cluster capacity. This proves to be inadequate and we saw a large job could monopolize the whole cluster.",amar_kamat,hong.tang,Major,Resolved,Fixed,09/Apr/10 07:24,01/Apr/12 05:01
Bug,MAPREDUCE-1692,12461797,Remove TestStreamedMerge from the streaming tests,"Currently the {{TestStreamedMerge}} is never run as a part of the streaming test suite, the code paths which were exercised by the test was removed in HADOOP-1315, so it is better to remove the testcase from the code base.",amareshwari,sreekanth,Minor,Closed,Fixed,12/Apr/10 06:37,24/Aug/10 21:21
Bug,MAPREDUCE-1694,12461895,streaming documentation appears to be wrong on overriding settings w/-D,"Throughout http://hadoop.apache.org/common/docs/current/streaming.html , there are many examples that do ""hadoop jar streaming blah -Dsomething=something"".  None of these examples appear to work anymore.  Moving the ""-Dsomething=something"" to be after ""hadoop jar streaming"" works.",,aw,Major,Closed,Fixed,12/Apr/10 20:12,24/Aug/10 21:21
Bug,MAPREDUCE-1695,12461911,capacity scheduler is not included in findbugs/javadoc targets,Capacity Scheduler is not included in findbugs/javadoc targets.,hong.tang,hong.tang,Major,Closed,Fixed,13/Apr/10 00:26,24/Aug/10 21:21
Bug,MAPREDUCE-1697,12461924,Document the behavior of -file option in streaming and deprecate it in favour of generic -files option.,"The behavior of -file option in streaming is not documented anywhere.
The behavior of -file is the following :
1) All the files passed through  -file option are packaged into job.jar.
2) If -file option is used for .class or .jar files, they are unjarred on tasktracker and placed in ${mapred.local.dir}/taskTracker/jobcache/job_ID/jars/classes or /lib, respectively. Symlinks to the directories classes and lib are created from the cwd of the task, . The names of symlinks are ""classes"", ""lib"". So file names of .class or .jar files do not appear in cwd of the task. 
Paths to these files are automatically added to classpath. The tricky part is that hadoop framework can pick .class or .jar using classpath, but actual mapper script cannot. If you'd like to access these .class or .jar inside script, please do something like ""java -cp lib/*;classes/* <ClassName>"". 
3) If -file option is used for files other than .class or .jar (e.g, .txt or .pl), these files are unjarred into ${mapred.local.dir}/taskTracker/jobcache/job_ID/jars/. Symlinks to these files are created from the cwd of the task. Names of these symlinks are actually file names. 

",amareshwari,amareshwari,Major,Closed,Fixed,13/Apr/10 05:03,24/Aug/10 21:21
Bug,MAPREDUCE-1699,12462003,JobHistory shouldn't be disabled for any reason,Recently we have had issues with JobTracker silently disabling job-history and starting to keep all completed jobs in memory. This leads to OOM on the JobTracker. We should never do this.,ramach,acmurthy,Major,Resolved,Fixed,14/Apr/10 01:04,31/Oct/11 05:00
Bug,MAPREDUCE-1700,12462010,User supplied dependencies may conflict with MapReduce system JARs,"If user code has a dependency on a version of a JAR that is different to the one that happens to be used by Hadoop, then it may not work correctly. This happened with user code using a different version of Avro, as reported [here|https://issues.apache.org/jira/browse/AVRO-493?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=12852081#action_12852081].

The problem is analogous to the one that application servers have with WAR loading. Using a specialized classloader in the Child JVM is probably the way to solve this.",tomwhite,tomwhite,Major,Closed,Fixed,14/Apr/10 04:10,09/Dec/16 03:31
Bug,MAPREDUCE-1701,12462014,AccessControlException while renewing a delegation token in not correctly handled in the JobTracker,The timer task for renewing delegation token gets scheduled even when an AccessControlException is obtained. ,boryas,ddas,Major,Closed,Fixed,14/Apr/10 06:17,12/Dec/11 06:19
Bug,MAPREDUCE-1705,12462118,Archiving and Purging of parity files should handle globbed policies,Archiving (har) and purging of parity files don't work in policies whose source is a globbed path.,rschmidt,rschmidt,Major,Closed,Fixed,15/Apr/10 08:28,24/Aug/10 21:21
Bug,MAPREDUCE-1707,12462121,TaskRunner can get NPE in getting ugi from TaskTracker,"The following code in TaskRunner can get NPE in the scenario described below.
{code}
      UserGroupInformation ugi = 
        tracker.getRunningJob(t.getJobID()).getUGI();
{code}

The scenario:
Tracker got a LaunchTaskAction; Task is localized and TaskRunner is started.
Then Tracker got a KillJobAction; This would issue a kill for the task. But, kill will be a no-op because the task did not actually start; The job is removed from runningJobs. 
Then if TaskRunner calls tracker.getRunningJob(t.getJobID()), it will be null.

Instead of TaskRunner doing a back call to tasktracker to get the ugi, tracker.getRunningJob(t.getJobID()).getUGI(), ugi should be passed a parameter in the constructor of TaskRunner. 
",vinodkv,amareshwari,Major,Closed,Fixed,15/Apr/10 08:58,12/Dec/11 06:19
Bug,MAPREDUCE-1718,12462716,job conf key for the services name of DelegationToken for HFTP url is constructed incorrectly in HFTPFileSystem,"the key (build in TokenCache) is hdfs.service.host_HOSTNAME.PORT, but 
in HftpFileSystem it is sometimes built as hdfs.service.host_IP.PORT.

Fix. change it to always be IP.",boryas,boryas,Major,Closed,Fixed,22/Apr/10 00:26,12/Dec/11 06:19
Bug,MAPREDUCE-1724,12462855,JobTracker balks at empty String for locations,"If a split has locations which are """" (empty String), then the JobTracker will get upset during initialization:

2010-04-22 19:09:20,395 ERROR org.apache.hadoop.mapred.JobTracker: Job initialization failed:
java.lang.StringIndexOutOfBoundsException: String index out of range: 0
        at java.lang.String.charAt(String.java:687)
        at org.apache.hadoop.net.NetUtils.normalizeHostName(NetUtils.java:420)
        at org.apache.hadoop.net.NetUtils.normalizeHostNames(NetUtils.java:443)
        at org.apache.hadoop.net.CachedDNSToSwitchMapping.resolve(CachedDNSToSwitchMapping.java:42)
        at org.apache.hadoop.mapred.JobTracker.resolveAndAddToTopology(JobTracker.java:2411)
        at org.apache.hadoop.mapred.JobInProgress.createCache(JobInProgress.java:360)
        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:462)
        at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3183)
        at org.apache.hadoop.mapred.EagerTaskInitializationListener$InitJob.run(EagerTaskInitializationListener.java:79)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:637)

Two key points:
 * This is different from Hadoop 0.18
 * CombineFileSplit has a constructor where String[] location is not specified, and hence the location array is populated with empty Strings.",,craigm,Minor,Resolved,Fixed,23/Apr/10 10:08,30/Jul/14 18:15
Bug,MAPREDUCE-1725,12462903,Fix MapReduce API incompatibilities between 0.20 and 0.21,A few API compatibilities have crept in since 0.20 (they are being tracked in MAPREDUCE-1623). These should be fixed before 0.21 is released.,tomwhite,tomwhite,Blocker,Closed,Fixed,23/Apr/10 20:55,24/Aug/10 21:21
Bug,MAPREDUCE-1727,12462993,TestJobACLs fails after HADOOP-6686,"HADOOP-6686, an incompatbile change, removed exception class name in unwrapped exceptions thrown at the RPC client. TestJobACLs depended on this for verifying exceptions, and thus is broken now.",ravidotg,vinodkv,Major,Closed,Fixed,26/Apr/10 07:22,24/Aug/10 21:21
Bug,MAPREDUCE-1728,12463058,Oracle timezone strings do not match Java,"OracleDBRecordReader sets the session timezone based on the toString representation of the current java.util.TimeZone. This is incorrect; Oracle manages a separate database of acceptable timezone strings, whose string representations are different than the timezone representations recognized by Java.",kimballa,kimballa,Major,Closed,Fixed,26/Apr/10 23:26,24/Aug/10 21:21
Bug,MAPREDUCE-1740,12463278,NPE in getMatchingLevelForNodes when node locations are variable depth,"In getMatchingLevelForNodes, we assume that both nodes have the same ""depth"" (ie number of path components). If the user provides a topology script that assigns one node a path like /foo/bar/baz and another node a path like /foo/blah, this function will throw an NPE.

I'm not sure if there are other places where we assume that all node locations have a constant number of paths. If so we should check the output of the topology script aggressively to be sure this is the case. Otherwise I think we simply need to add && n2 != null to the while loop",ahmed.radwan,tlipcon,Major,Closed,Fixed,29/Apr/10 00:24,10/Mar/15 04:32
Bug,MAPREDUCE-1744,12463383,DistributedCache creates its own FileSytem instance when adding a file/archive to the path,"According to the contract of {{UserGroupInformation.doAs()}} the only required operations within the {{doAs()}} block are the
creation of a {{JobClient}} or getting a {{FileSystem}} .

The {{DistributedCache.add(File/Archive)ToClasspath()}} methods create a {{FileSystem}} instance outside of the {{doAs()}} block,
this {{FileSystem}} instance is not in the scope of the proxy user but of the superuser and permissions may make the method
fail.

One option is to overload the methods above to receive a filesystem.

Another option is to do obtain the {{FileSystem}} within a {{doAs()}} block, for this it would be required to have the proxy
user set in the passed configuration.

The second option seems nicer, but I don't know if the proxy user is as a property in the jobconf.",dking,dking,Major,Closed,Fixed,29/Apr/10 22:36,05/Mar/12 02:49
Bug,MAPREDUCE-1747,12463406,Remove documentation for the 'unstable' job-acls feature,"As discussed [here|https://issues.apache.org/jira/browse/MAPREDUCE-1604?focusedCommentId=12862151&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12862151] and [here|https://issues.apache.org/jira/browse/MAPREDUCE-1604?focusedCommentId=12860916&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12860916] at MAPREDUCE-1604, the job-acls feature is currently unstable. Without MAPREDUCE-1664, job-acls are practically useless because of their problematic interactions with queue-acls. Removing them for 0.21 will both relieve ourselves of these problems as well as the burden to support the backwards compatibility of the configuration options as well as the going-to-be-changed semantics of the feature. This jira is about removing the documentation from 0.21 so that the completed feature can be added in 0.22 with ease.",vinodkv,vinodkv,Blocker,Closed,Fixed,30/Apr/10 06:55,24/Aug/10 21:21
Bug,MAPREDUCE-1754,12463735,Replace mapred.persmissions.supergroup with an acl : mapreduce.cluster.administrators,"mapred.permissions.supergroup should be replaced with an acl so that it does not restrict the admins to a single group.
See more details on MAPREDUCE-1542.",amareshwari,amareshwari,Major,Closed,Fixed,05/May/10 05:42,12/Dec/11 06:19
Bug,MAPREDUCE-1780,12464122,AccessControlList.toString() is used for serialization of ACL in JobStatus.java,"HADOOP-6715 is created to fix AccessControlList.toString() for the case of WILDCARD. JobStatus.write() and readFields() assume that toString() returns the serialized String of AccessControlList object, which is not true. Once HADOOP-6715 gets fixed in COMMON, JobStatus.write() and JobStatus.readFields() should be fixed depending on the fix of HADOOP-6715.",ravidotg,ravidotg,Major,Closed,Fixed,10/May/10 09:55,12/Dec/11 06:19
Bug,MAPREDUCE-1784,12464313,IFile should check for null compressor,"IFile assumes that when it has a codec it can always get a compressor. This fails when mapred.compress.map.output is true but the native libraries are not installed, resulting in an NPE:

{code}
java.lang.NullPointerException
at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:102)
at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1198)
at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1091)
at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:359)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
at org.apache.hadoop.mapred.Child.main(Child.java:170)
{code}

Let's make IFile handle this case by logging and using non-compressed streams.l",eli,eli,Minor,Resolved,Fixed,12/May/10 02:00,07/Apr/11 15:40
Bug,MAPREDUCE-1788,12464461,o.a.h.mapreduce.Job shouldn't make a copy of the JobConf,Having o.a.h.mapreduce.Job make a copy of the passed in JobConf has several issues: any modifications done by various pieces such as InputSplit etc. are not reflected back and causes issues for frameworks built on top.,acmurthy,acmurthy,Major,Closed,Fixed,13/May/10 20:25,15/Nov/11 00:48
Bug,MAPREDUCE-1806,12465071,CombineFileInputFormat does not work with paths not on default FS,"In generating the splits in CombineFileInputFormat, the scheme and authority are stripped out. This creates problems when trying to access the files while generating the splits, as without the har:/, the file won't be accessed through the HarFileSystem.",jira.shegalov,pauly,Major,Closed,Fixed,20/May/10 21:48,17/Apr/13 21:43
Bug,MAPREDUCE-1810,12465246,0.21 build is broken,"/src/java/org/apache/hadoop/mapred/AdminOperationsProtocol.java:31: cannot find symbol
    [javac] symbol  : method value()
    [javac] location: @interface org.apache.hadoop.security.KerberosInfo
    [javac] @KerberosInfo(MRJobConfig.JOB_JOBTRACKER_ID)
",tomwhite,sharadag,Major,Closed,Fixed,24/May/10 05:19,02/May/13 02:29
Bug,MAPREDUCE-1811,12465262,Job.monitorAndPrintJob() should print status of the job at completion,"Job.monitorAndPrintJob() just prints ""Job Complete"" at the end of the job. It should print the state whether the job SUCCEEDED/FAILED/KILLED.",qwertymaniac,amareshwari,Minor,Closed,Fixed,24/May/10 10:18,15/Nov/11 00:48
Bug,MAPREDUCE-1813,12465346,NPE in PipeMapred.MRErrorThread,"Some reduce tasks fail with following NPE
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)
        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:540)
        at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:137)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)
        at org.apache.hadoop.mapred.Child.main(Child.java:159)
Caused by: java.lang.NullPointerException
       at org.apache.hadoop.streaming.PipeMapRed$MRErrorThread.setStatus(PipeMapRed.java:517)
        at org.apache.hadoop.streaming.PipeMapRed$MRErrorThread.run(PipeMapRed.java:449)
",ravidotg,amareshwari,Major,Closed,Fixed,25/May/10 08:07,12/Dec/11 06:19
Bug,MAPREDUCE-1820,12465515,"InputSampler does not create a deep copy of the key object when creating a sample, which causes problems with some formats like SequenceFile<Text,Text>","I tried to use the InputSampler on a SequenceFile<Text,Text> and found that it comes up with duplicate keys in the sample.  The problem was tracked down to the fact that the Text object returned from the reader is essentially a wrapper pointing to a byte array, which changes as the sequence file reader progresses.  There was also a bug in that the reader should be initialized before the use.  The am attaching a patch that fixes both of the issues.  --Alex K",alexvk,alexvk,Major,Closed,Fixed,26/May/10 22:30,08/May/13 21:13
Bug,MAPREDUCE-1825,12465763,jobqueue_details.jsp and FairSchedulerServelet should not call finishedMaps and finishedReduces when job is not initialized,"JobInProgress.finishedMaps() and finishedReduces() are synchronized. They are called from jobqueue_details.jsp and FairSchedulerServelet which iterates through all jobs. If any job is in initialization, these pages don't come up until the initialization finishes.

See [comment|https://issues.apache.org/jira/browse/MAPREDUCE-1354?focusedCommentId=12834139&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12834139] for more details",schen,amareshwari,Major,Closed,Fixed,31/May/10 05:51,12/Dec/11 06:19
Bug,MAPREDUCE-1834,12465942,TestSimulatorDeterministicReplay timesout on trunk,"TestSimulatorDeterministicReplay timesout on trunk.
See hudson patch build http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/216/testReport/org.apache.hadoop.mapred/TestSimulatorDeterministicReplay/testMain/",hong.tang,amareshwari,Major,Closed,Fixed,02/Jun/10 07:57,12/Dec/11 06:20
Bug,MAPREDUCE-1836,12465985,Refresh for proxy superuser config (mr part for HDFS-1096),,boryas,boryas,Major,Closed,Fixed,02/Jun/10 17:21,12/Dec/11 06:20
Bug,MAPREDUCE-1841,12466115,o.a.h.mapreduce.FileOutputCommitter doens't check for existence of ${mapred.output.dir}/_temporary,"o.a.h.mapred.FileOutputCommitter.getWorkOutputPath checks for existence of ${mapred.output.dir}/_temporary to ensure tasks launched _after_ the job-cleanup task fail early (in a vast majority of cases). This check is missing in the mapreduce libraries.

Related note: FileOutputCommitter.setupTask seems a more appropriate place for the above check...",acmurthy,acmurthy,Major,Resolved,Fixed,03/Jun/10 18:44,30/Jul/14 19:57
Bug,MAPREDUCE-1844,12466322,Tests failing with java.lang.NoClassDefFoundError,"Tests are failing with java.lang.NoClassDefFoundError (see http://pastebin.com/Y3E8iDw0). Steps to reproduce on trunk
1) Delete ~/.ivy2
2) checkout trunk
3) ant -Dtestcase=TestMRCLI run-test-mapred
",,amar_kamat,Blocker,Resolved,Fixed,07/Jun/10 08:08,25/Jun/10 20:42
Bug,MAPREDUCE-1845,12466374,FairScheduler.tasksToPeempt() can return negative number,"This method can return negative number. This will cause the preemption to under-preempt.
The bug was discovered by Joydeep.",schen,schen,Major,Closed,Fixed,07/Jun/10 18:40,24/Aug/10 21:21
Bug,MAPREDUCE-1853,12466655,MultipleOutputs does not cache TaskAttemptContext,"In MultipleOutputs there is
{code}
 private TaskAttemptContext getContext(String nameOutput) throws IOException {
    // The following trick leverages the instantiation of a record writer via
    // the job thus supporting arbitrary output formats.
    Job job = new Job(context.getConfiguration());
    job.setOutputFormatClass(getNamedOutputFormatClass(context, nameOutput));
    job.setOutputKeyClass(getNamedOutputKeyClass(context, nameOutput));
    job.setOutputValueClass(getNamedOutputValueClass(context, nameOutput));
    TaskAttemptContext taskContext = 
      new TaskAttemptContextImpl(job.getConfiguration(), 
                                 context.getTaskAttemptID());
    return taskContext;
  }
{code}

so for every reduce call it creates a new Job instance ...which creates a new LocalJobRunner.
That does not sound like a good idea.

You end up with a flood of ""jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized""

This should probably also be added to 0.22.",tcurdt,tcurdt,Critical,Closed,Fixed,10/Jun/10 14:35,23/Nov/11 06:03
Bug,MAPREDUCE-1855,12466674,refreshSuperUserGroupsConfiguration for MR should use server side configuration for the refresh (for HADOOP-6815),,boryas,boryas,Major,Resolved,Fixed,10/Jun/10 17:52,29/Oct/10 02:04
Bug,MAPREDUCE-1857,12466719,Remove unused streaming configuration from src,The configuration stream.numinputspecs is just set and not read anywhere. It can be removed.,amareshwari,amareshwari,Trivial,Resolved,Fixed,11/Jun/10 08:11,29/Oct/10 02:04
Bug,MAPREDUCE-1858,12466729,TestCopyFiles fails consistently on trunk,"
All the tests in TestCopyFiles fail. For e.g.

{code}
Testcase: testCopyFromLocalToLocal took 10.051 sec
        Caused an ERROR
File /home/vinodkv/Workspace/eclipse-workspace/apache-svn-mapreduce/trunk/build/test/data/destdat/one/one/9208695393420117603 does not exist.
java.io.FileNotFoundException: File /home/vinodkv/Workspace/eclipse-workspace/apache-svn-mapreduce/trunk/build/test/data/destdat/one/one/9208695393420117603 does not exist.
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:420)
        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:290)
        at org.apache.hadoop.tools.TestCopyFiles.checkFiles(TestCopyFiles.java:169)
        at org.apache.hadoop.tools.TestCopyFiles.checkFiles(TestCopyFiles.java:159)
        at org.apache.hadoop.tools.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:271)
{code}",,vinodkv,Major,Resolved,Fixed,11/Jun/10 11:00,13/Jun/10 04:40
Bug,MAPREDUCE-1863,12466882,[Rumen] Null failedMapAttemptCDFs in job traces generated by Rumen,All the traces generated by Rumen for jobs having failed task attempts has null value for failedMapAttemptCDFs.,amar_kamat,amar_kamat,Major,Closed,Fixed,14/Jun/10 09:23,12/Dec/11 06:19
Bug,MAPREDUCE-1864,12466965,PipeMapRed.java has uninitialized members log_ and LOGNAME ,"PipeMapRed.java has members log_ and LOGNAME, which are never initialized and they are used in code for logging in several places. 
They should be removed and PipeMapRed should use commons LogFactory and Log for logging. This would improve code maintainability.

Also, as per [comment | https://issues.apache.org/jira/browse/MAPREDUCE-1851?focusedCommentId=12878530&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12878530], stream.joblog_ configuration property can be removed.
",amareshwari,amareshwari,Major,Closed,Fixed,15/Jun/10 05:28,12/Dec/11 06:19
Bug,MAPREDUCE-1865,12466966,[Rumen] Rumen should also support jobhistory files generated using trunk,Rumen code in trunk parses and process only jobhistory files from pre-21 hadoop mapreduce clusters. It should also support jobhistory files generated using trunk.,amar_kamat,amar_kamat,Major,Closed,Fixed,15/Jun/10 06:04,12/Dec/11 06:19
Bug,MAPREDUCE-1866,12466971,Remove deprecated class org.apache.hadoop.streaming.UTF8ByteArrayUtils,"The class org.apache.hadoop.streaming.UTF8ByteArrayUtils is deprecated in favor of org.apache.hadoop.util.UTF8ByteArrayUtils in branch 0.19.
The same should be removed.",amareshwari,amareshwari,Minor,Resolved,Fixed,15/Jun/10 08:32,29/Oct/10 02:04
Bug,MAPREDUCE-1867,12466976,Remove unused methods in org.apache.hadoop.streaming.StreamUtil,There are many unused methods in org.apache.hadoop.streaming.StreamUtil. They should be removed from the class for maintainability. ,amareshwari,amareshwari,Minor,Resolved,Fixed,15/Jun/10 09:32,07/Apr/11 15:40
Bug,MAPREDUCE-1870,12467065,Harmonize MapReduce JAR library versions with Common and HDFS,MapReduce part of HADOOP-6800.,tomwhite,tomwhite,Blocker,Closed,Fixed,16/Jun/10 05:34,29/Oct/10 02:03
Bug,MAPREDUCE-1876,12467310,TaskAttemptStartedEvent.java incorrectly logs MAP_ATTEMPT_STARTED as event type for reduce tasks,"{{TaskAttemptStartedEvent}} is used to log the start time of both the map and reduce task attempts to {{JobHistory}}. Following is the implementation of _getEventType()_ method of {{TaskAttemptStartedEvent}}

{code}
/** Get the event type */
  public EventType getEventType() {
    return EventType.MAP_ATTEMPT_STARTED;
  }
{code}

",amar_kamat,amar_kamat,Major,Closed,Fixed,18/Jun/10 04:44,29/Oct/10 02:03
Bug,MAPREDUCE-1880,12467415,"""java.lang.ArithmeticException: Non-terminating decimal expansion; no exact representable decimal result."" while running ""hadoop jar hadoop-0.20.1+169.89-examples.jar pi 4 30""","If I run ""hadoop jar hadoop-0.20.1+169.89-examples.jar pi 4 30"", I get the following output:

Number of Maps  = 4
Samples per Map = 30
Wrote input for Map #0
Wrote input for Map #1
Wrote input for Map #2
Wrote input for Map #3
Starting Job
10/06/19 21:50:34 INFO mapred.FileInputFormat: Total input paths to process : 4
10/06/19 21:50:34 INFO mapred.JobClient: Running job: job_201006192101_0003
10/06/19 21:50:35 INFO mapred.JobClient:  map 0% reduce 0%
10/06/19 21:50:45 INFO mapred.JobClient:  map 50% reduce 0%
10/06/19 21:50:51 INFO mapred.JobClient:  map 100% reduce 0%
10/06/19 21:51:00 INFO mapred.JobClient:  map 100% reduce 100%
10/06/19 21:51:02 INFO mapred.JobClient: Job complete: job_201006192101_0003
10/06/19 21:51:02 INFO mapred.JobClient: Counters: 18
10/06/19 21:51:02 INFO mapred.JobClient:   Job Counters 
10/06/19 21:51:02 INFO mapred.JobClient:     Launched reduce tasks=1
10/06/19 21:51:02 INFO mapred.JobClient:     Launched map tasks=4
10/06/19 21:51:02 INFO mapred.JobClient:     Data-local map tasks=4
10/06/19 21:51:02 INFO mapred.JobClient:   FileSystemCounters
10/06/19 21:51:02 INFO mapred.JobClient:     FILE_BYTES_READ=94
10/06/19 21:51:02 INFO mapred.JobClient:     HDFS_BYTES_READ=472
10/06/19 21:51:02 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=334
10/06/19 21:51:02 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=215
10/06/19 21:51:02 INFO mapred.JobClient:   Map-Reduce Framework
10/06/19 21:51:02 INFO mapred.JobClient:     Reduce input groups=8
10/06/19 21:51:02 INFO mapred.JobClient:     Combine output records=0
10/06/19 21:51:02 INFO mapred.JobClient:     Map input records=4
10/06/19 21:51:02 INFO mapred.JobClient:     Reduce shuffle bytes=112
10/06/19 21:51:02 INFO mapred.JobClient:     Reduce output records=0
10/06/19 21:51:02 INFO mapred.JobClient:     Spilled Records=16
10/06/19 21:51:02 INFO mapred.JobClient:     Map output bytes=72
10/06/19 21:51:02 INFO mapred.JobClient:     Map input bytes=96
10/06/19 21:51:02 INFO mapred.JobClient:     Combine input records=0
10/06/19 21:51:02 INFO mapred.JobClient:     Map output records=8
10/06/19 21:51:02 INFO mapred.JobClient:     Reduce input records=8
Job Finished in 28.593 seconds
java.lang.ArithmeticException: Non-terminating decimal expansion; no exact representable decimal result.
	at java.math.BigDecimal.divide(BigDecimal.java:1603)
	at org.apache.hadoop.examples.PiEstimator.estimate(PiEstimator.java:313)
	at org.apache.hadoop.examples.PiEstimator.run(PiEstimator.java:342)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.examples.PiEstimator.main(PiEstimator.java:351)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:64)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)


""hadoop jar hadoop-0.20.1+169.89-examples.jar pi 2 10"" finishes fine",szetszwo,fossil,Minor,Closed,Fixed,19/Jun/10 20:03,24/Aug/10 21:21
Bug,MAPREDUCE-1885,12467536,Trunk compilation is broken because of FileSystem api change in HADOOP-6826,"Trunk compilation is broken because of FileSystem api change in HADOOP-6826.

Here are the error messages:

     [iajc] /home/gravi/workspace/gitMR/hadoop-mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java:277 [error] The method create(Path, FsPermission, boolean, int, short, long, Progressable) in the type FileSystem is not applicable for the arguments (Path, FsPermission, EnumSet<CreateFlag>, int, short, long, null)
     [iajc] FSDataOutputStream out = logDirFs.create(logFile,
     [iajc]                                   ^
     [iajc] /home/gravi/workspace/gitMR/hadoop-mapreduce/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java:297 [error] The method create(Path, FsPermission, boolean, int, short, long, Progressable) in the type FileSystem is not applicable for the arguments (Path, FsPermission, EnumSet<CreateFlag>, int, short, long, null)
     [iajc] jobFileOut = logDirFs.create(logDirConfPath,
     [iajc]
     [iajc]
     [iajc] 2 errors
",ravidotg,ravidotg,Major,Closed,Fixed,22/Jun/10 05:01,02/May/13 02:29
Bug,MAPREDUCE-1887,12467614,MRAsyncDiskService does not properly absolutize volume root paths,"In MRAsyncDiskService, volume names are sometimes specified as relative paths, which are not converted to absolute paths. This can cause errors of the form ""cannot delete </full/path/to/foo> since it is outside of <relative/volume/root>"" even though the actual path is inside the root. ",kimballa,kimballa,Major,Closed,Fixed,22/Jun/10 20:20,12/Dec/11 06:18
Bug,MAPREDUCE-1888,12467659,Streaming overrides user given output key and value types.,"The following code in StreamJob.java overrides user given output key and value types.
{code}
    idResolver.resolve(conf.get(StreamJobConfig.MAP_OUTPUT,
        IdentifierResolver.TEXT_ID));
    conf.setClass(StreamJobConfig.MAP_OUTPUT_READER_CLASS,
      idResolver.getOutputReaderClass(), OutputReader.class);
    job.setMapOutputKeyClass(idResolver.getOutputKeyClass());
    job.setMapOutputValueClass(idResolver.getOutputValueClass());
    
    idResolver.resolve(conf.get(StreamJobConfig.REDUCE_OUTPUT,
        IdentifierResolver.TEXT_ID));
    conf.setClass(StreamJobConfig.REDUCE_OUTPUT_READER_CLASS,
      idResolver.getOutputReaderClass(), OutputReader.class);
    job.setOutputKeyClass(idResolver.getOutputKeyClass());
    job.setOutputValueClass(idResolver.getOutputValueClass());
{code}
",ravidotg,amareshwari,Major,Closed,Fixed,23/Jun/10 04:31,12/Dec/11 06:19
Bug,MAPREDUCE-1894,12467841,DistributedRaidFileSystem.readFully() does not return,DistributedRaidFileSystem.readFully() has a while(true) loop with no return. The read(*) functions do not have this problem.,rvadali,rvadali,Major,Closed,Fixed,24/Jun/10 23:37,12/Dec/11 06:20
Bug,MAPREDUCE-1897,12467953,trunk build broken on compile-mapred-test,"...apparently.  Fresh checkout of trunk (all three hadoop-*), build.properties project.version fix, ant veryclean mvn-install of common, hdfs, and then mapreduce:

    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java:52: cannot access org.apache.hadoop.test.system.DaemonProtocol
    [javac] class file for org.apache.hadoop.test.system.DaemonProtocol not found
    [javac]   static class FakeJobTracker extends JobTracker {
    [javac]          ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java:60: non-static variable this cannot be referenced from a static context
    [javac]       this.trackers = tts;
    [javac]       ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java:60: cannot find symbol
    [javac] symbol  : variable trackers
    [javac] location: class org.apache.hadoop.mapred.FakeObjectUtilities
    [javac]       this.trackers = tts;
    [javac]           ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java:67: cannot find symbol
    [javac] symbol  : method taskTrackers()
    [javac] location: class org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobTracker
    [javac]           taskTrackers().size() - getBlacklistedTrackerCount(),
    [javac]           ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java:67: cannot find symbol
    [javac] symbol  : method getBlacklistedTrackerCount()
    [javac] location: class org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobTracker
    [javac]           taskTrackers().size() - getBlacklistedTrackerCount(),
    [javac]                                   ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java:68: cannot find symbol
    [javac] symbol  : method getBlacklistedTrackerCount()
    [javac] location: class org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobTracker
    [javac]           getBlacklistedTrackerCount(), 0, 0, 0, totalSlots/2, totalSlots/2, 
    [javac]           ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java:64: method does not override or implement a method from a supertype
    [javac]     @Override
    [javac]     ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java:73: non-static variable this cannot be referenced from a static context
    [javac]       this.totalSlots = totalSlots;
    [javac]       ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java:73: cannot find symbol
    [javac] symbol  : variable totalSlots
    [javac] location: class org.apache.hadoop.mapred.FakeObjectUtilities
    [javac]       this.totalSlots = totalSlots;
    [javac]           ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java:91: establishFirstContact(org.apache.hadoop.mapred.JobTracker,java.lang.String) in org.apache.hadoop.mapred.FakeObjectUtilities cannot be applied to (org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobTracker,java.lang.String)
    [javac]           FakeObjectUtilities.establishFirstContact(jobTracker, s);
    [javac]                              ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java:170: cannot find symbol
    [javac] symbol  : constructor MyFakeJobInProgress(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobTracker)
    [javac] location: class org.apache.hadoop.mapred.TestJobInProgress.MyFakeJobInProgress
    [javac]     MyFakeJobInProgress job1 = new MyFakeJobInProgress(conf, jobTracker);
    [javac]                                ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java:185: cannot find symbol
    [javac] symbol  : constructor MyFakeJobInProgress(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobTracker)
    [javac] location: class org.apache.hadoop.mapred.TestJobInProgress.MyFakeJobInProgress
    [javac]     MyFakeJobInProgress job2 = new MyFakeJobInProgress(conf, jobTracker);
    [javac]                                ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java:225: cannot find symbol
    [javac] symbol  : constructor MyFakeJobInProgress(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobTracker)
    [javac] location: class org.apache.hadoop.mapred.TestJobInProgress.MyFakeJobInProgress
    [javac]     MyFakeJobInProgress jip = new MyFakeJobInProgress(conf, jobTracker);
    [javac]                               ^
    [javac] /home/roelofs/grid/trunk2/hadoop-mapreduce/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java:296: cannot find symbol
    [javac] symbol  : constructor MyFakeJobInProgress(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobTracker)
    [javac] location: class org.apache.hadoop.mapred.TestJobInProgress.MyFakeJobInProgress
    [javac]     MyFakeJobInProgress jspy = spy(new MyFakeJobInProgress(conf, jobTracker));
    [javac]                                    ^
",cos,roelofs,Major,Resolved,Fixed,25/Jun/10 23:37,29/Oct/10 02:03
Bug,MAPREDUCE-1900,12468081,MapReduce daemons should close FileSystems that are not needed anymore,"Related to HADOOP-6843, this jira is to make MapReduce behave better with respect to closing FileSystems when they are not needed anymore.",kzhang,ddas,Major,Closed,Fixed,28/Jun/10 18:58,12/Dec/11 06:19
Bug,MAPREDUCE-1905,12468324,Context.setStatus() and progress() api are ignored,"TaskAttemptContext.setStatus() and progress() were overriden in TaskInputOutputContext, inbranch 0.20, to call the underlying reporter apis. But the methods are no more over-riden in TaskInputOutputContextImpl after MAPREDUCE-954.",amareshwari,amareshwari,Blocker,Closed,Fixed,01/Jul/10 05:44,12/Dec/11 06:20
Bug,MAPREDUCE-1908,12468381,DistributedRaidFileSystem does not handle ChecksumException correctly,"ChecksumException reports the offset of corruption within a block,
whereas DistributedRaidFileSystem.setAlternateLocations was expecting it
to report the offset of corruption within the file.

The best way of dealing with a missing block/corrupt block is to just
use the current seek offset in the file as the position of corruption.
",rvadali,rvadali,Major,Closed,Fixed,01/Jul/10 20:40,12/Dec/11 06:18
Bug,MAPREDUCE-1911,12468405,Fix errors in -info option in streaming,"Here are some of the findings by Karam while verifying -info option in streaming:
# We need to add ""Optional"" for -mapper, -reducer,-combiner and -file options.
# For -inputformat and -outputformat options, we should put ""Optional"" in the prefix for the sake on uniformity.
# We need to remove -cluster decription.
# -help option is not displayed in usage message.
# when displaying message for -info or -help options, we should not display ""Streaming Job Failed!""; also exit code should be 0 in case of -help/-info option.

",amareshwari,amareshwari,Major,Closed,Fixed,02/Jul/10 06:07,12/Dec/11 06:19
Bug,MAPREDUCE-1914,12468472,TrackerDistributedCacheManager never cleans its input directories,"When we localize a file into a node's cache, it's installed in a directory whose subroot is a random {{long}} .  These {{long}} s all sit in a single flat directory [per disk, per cluster node].  When the cached file is no longer needed, its reference count becomes zero in a tracking data structure.  The file then becomes eligible for deletion when the total amount of space occupied by cached files exceeds 10G [by default] or the total number of such files exceeds 10K.

However, when we delete a cached file, we don't delete the directory that contains it; this importantly includes the elements of the flat directory, which then accumulate until they reach a system limit, 32K in some cases, and then the node stops working.

We need to delete the flat directory when we delete the localized cache file it contains.",dking,dking,Major,Resolved,Fixed,02/Jul/10 23:46,30/Jul/14 21:53
Bug,MAPREDUCE-1915,12468590,IndexCache - getIndexInformation - check reduce index Out Of Bounds,"When checking if the ""reduce"" index is out of bounds the check should be 

info.mapSpillRecord.size() <= reduce

instead of:

info.mapSpillRecord.size() < reduce

Not a big bug since an Out Of Bounds is thrown downstream anyway.",priyomustafi,rvernica,Trivial,Resolved,Fixed,05/Jul/10 23:02,07/Apr/11 15:40
Bug,MAPREDUCE-1920,12468666,Job.getCounters() returns null when using a cluster,"Calling Job.getCounters() after the job has completed (successfully) returns null.
",tomwhite,kimballa,Critical,Closed,Fixed,06/Jul/10 20:07,29/Oct/10 02:03
Bug,MAPREDUCE-1925,12468807,TestRumenJobTraces fails in trunk,"TestRumenJobTraces failed with following error:
Error Message

the gold file contains more text at line 1 expected:<56> but was:<0>

Stacktrace

	at org.apache.hadoop.tools.rumen.TestRumenJobTraces.testHadoop20JHParser(TestRumenJobTraces.java:294)

Full log of the failure is available at http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/292/testReport/org.apache.hadoop.tools.rumen/TestRumenJobTraces/testHadoop20JHParser/",ravidotg,amareshwari,Major,Closed,Fixed,08/Jul/10 10:31,12/Dec/11 06:19
Bug,MAPREDUCE-1926,12468861,MapReduce distribution is missing build-utils.xml,The tarball should be able to build itself.,tomwhite,tomwhite,Blocker,Closed,Fixed,08/Jul/10 21:47,29/Oct/10 02:03
Bug,MAPREDUCE-1929,12468907,Allow artifacts to be published to the staging Apache Nexus Maven Repository,MapReduce companion issue to HADOOP-6847.,tomwhite,tomwhite,Blocker,Resolved,Fixed,09/Jul/10 10:53,07/Apr/11 15:40
Bug,MAPREDUCE-1942,12469278, 'compile-fault-inject' should never be called directly.,Similar to HDFS-1299: prevent calls to helper targets.,cos,cos,Minor,Closed,Fixed,14/Jul/10 18:35,29/Oct/10 02:03
Bug,MAPREDUCE-1958,12469873,using delegation token over hftp for long running clients (part of hdfs 1296),,boryas,boryas,Major,Closed,Fixed,22/Jul/10 00:11,12/Dec/11 06:18
Bug,MAPREDUCE-1961,12469970,[gridmix3] ConcurrentModificationException when shutting down Gridmix,"We observed the following exception occasionally at the end of the Gridmix run:

{code}
Exception in thread ""StatsCollectorThread"" java.util.ConcurrentModificationException
          at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
          at java.util.AbstractList$Itr.next(AbstractList.java:343)
          at org.apache.hadoop.mapred.gridmix.Statistics$StatCollector.updateAndNotifyClusterStatsListeners(Statistics.java:220)
          at org.apache.hadoop.mapred.gridmix.Statistics$StatCollector.run(Statistics.java:205)
{code}",hong.tang,hong.tang,Major,Closed,Fixed,23/Jul/10 00:47,12/Dec/11 06:18
Bug,MAPREDUCE-1974,12470304,FairScheduler can preempt the same task many times,"In FairScheduler.preemptTasks(), tasks are collected from JobInProgress.runningMapCache.
But tasks repeat multiple times in  JobInProgress.runningMapCache (on rack, node and cluster).
This makes FairScheduler preempt the same task many times.",schen,schen,Major,Closed,Fixed,27/Jul/10 23:36,12/Dec/11 06:19
Bug,MAPREDUCE-1975,12470316,gridmix shows unnecessary InterruptedException,"The following InterruptedException is seen when gridmix is run and it ran successfully:

10/06/24 20:43:03 INFO gridmix.ReplayJobFactory: START REPLAY @ 11331037109
10/06/24 20:43:03 ERROR gridmix.Statistics: Statistics interrupt while waiting for polling null
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObjec\
t.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObjec\
t.await(AbstractQueuedSynchronizer.java:2066)
        at org.apache.hadoop.mapred.gridmix.Statistics$StatCollector.run(Statis\
tics.java:190)
10/06/24 20:43:03 INFO gridmix.Gridmix: Exiting...",ravidotg,ravidotg,Major,Closed,Fixed,28/Jul/10 06:36,12/Dec/11 06:19
Bug,MAPREDUCE-1979,12470335,"""Output directory already exists"" error in gridmix when gridmix.output.directory is not defined","""Output directory already exists"" error is seen in gridmix when gridmix.output.directory is not defined. When gridmix.output.directory is not defined, then gridmix uses inputDir/gridmix/ as output path for gridmix run. Because gridmix is creating outputPath(in this case, inputDir/gridmix/) at the begining, the output path to generate-data-mapreduce-job(i.e. inputDir) already exists and becomes error from mapreduce.

There is need for creation of this outputPath in any case(whether user specifies the path using gridmix.output.directory OR gridmix itself considering inputDir/gridmix/ ) even though the paths are automatically created for output paths of mapreduce jobs(like mkdir -p), because gridmix needs to set 777 permissions for this outputPath sothat different users can create different output directories of different mapreduce jobs within this gridmix run.

The other case in which this problem is seen is when gridmix.output.directory is defined as a relative path. This is because in this case also, gridmix tries to create relative path under ioPath/ and thus the same issue.",ravidotg,ravidotg,Major,Closed,Fixed,28/Jul/10 11:40,12/Dec/11 06:19
Bug,MAPREDUCE-1980,12470339,TaskAttemptUnsuccessfulCompletionEvent.java incorrectly logs MAP_ATTEMPT_KILLED as event type for reduce tasks,"TaskAttemptUnsuccessfulCompletionEvent is used to log unsuccessful map and reduce task attempts to JobHistory. Following is the implementation of getEventType() method of TaskAttemptUnsuccessfulCompletionEvent

/** Get the event type */
  public EventType getEventType() {
    return EventType.MAP_ATTEMPT_KILLED;
  }

",amar_kamat,amar_kamat,Major,Closed,Fixed,28/Jul/10 13:12,07/Apr/11 15:40
Bug,MAPREDUCE-1982,12470420,[Rumen] TraceBuilder's output shows jobname as NULL for jobhistory files with valid jobnames,"{{TraceBuilder}} fails to extract configuration properties (like job-name) from the job-conf if the job-conf has the properties stored using the deprecated keys.
",ravidotg,amar_kamat,Major,Closed,Fixed,29/Jul/10 08:31,12/Dec/11 06:19
Bug,MAPREDUCE-1984,12470431,herriot TestCluster fails because exclusion is not there,"restart is part of the test case which causes ioexceptions, and this needs to be ignored. The test case should not be incorrectly failed. ",balajirg,balajirg,Major,Resolved,Fixed,29/Jul/10 10:17,29/Oct/10 02:04
Bug,MAPREDUCE-1989,12470640,Gridmix3 doesn't emit out proper mesage when user resolver is set and no user list is given,"Currently, Gridmix3 emits out the following mesage when user resolver is set and no user list is given.
""Resource null ignored"".
This is not clear/meaningful to user.",ravidotg,ravidotg,Major,Closed,Fixed,02/Aug/10 11:43,12/Dec/11 06:18
Bug,MAPREDUCE-1992,12470699,NPE in JobTracker's constructor,"On my local machine, JobTracker is *not* coming up with current trunk. Logs show the following NPE:

2010-08-03 14:01:41,449 FATAL org.apache.hadoop.mapred.JobTracker: java.lang.NullPointerException
  at org.apache.hadoop.security.UserGroupInformation.isLoginKeytabBased(UserGroupInformation.java:703)
  at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:1383)
  at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:275)
  at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:267)
  at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:262)
  at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:4236)

2010-08-03 14:01:41,449 INFO org.apache.hadoop.mapred.JobTracker: SHUTDOWN_MSG:",kzhang,ravidotg,Major,Closed,Fixed,03/Aug/10 08:30,12/Dec/11 06:18
Bug,MAPREDUCE-1993,12470786,TestTrackerDistributedCacheManagerWithLinuxTaskController fails on trunk,TestTrackerDistributedCacheManagerWithLinuxTaskController fails when run with the DefaultTaskController. ,ddas,ddas,Major,Closed,Fixed,04/Aug/10 06:09,12/Dec/11 06:19
Bug,MAPREDUCE-1994,12470844,Linux task-controller determines its own path insecurely,"The task-controller uses argv[0] to determine its own path, and then calls stat() on that. Instead it should stat(""/proc/self/exe"") directly. This is important since argv[0] can be spoofed to point to another program and thus either fool the autodetection of HADOOP_HOME or evade various permissions checks.",tlipcon,tlipcon,Major,Resolved,Fixed,04/Aug/10 22:40,30/Jul/14 23:21
Bug,MAPREDUCE-1996,12470923,API: Reducer.reduce() method detail misstatement,"method detail for Reducer.reduce() method has paragraph starting:

""Applications can use the Reporter  provided to report progress or just indicate that they are alive. In scenarios where the application takes an insignificant amount of time to process individual key/value pairs, this is crucial since the framework might assume that the task has timed-out and kill that task. ""

s/an insignificant amount of time/a significant amount of time/
",qwertymaniac,glynnbach,Trivial,Closed,Fixed,05/Aug/10 17:05,15/Nov/11 00:49
Bug,MAPREDUCE-1999,12471005,ClientProtocol incorrectly uses hdfs DelegationTokenSelector,ClientProtocol in MR incorrectly uses the DelegationTokenSelector in hdfs due to a wrong import. It should use the DelegationTokenSelector in mapreduce.,jnp,jnp,Major,Closed,Fixed,06/Aug/10 19:49,12/Dec/11 06:19
Bug,MAPREDUCE-2000,12471015,Rumen is not able to extract counters for Job history logs from Hadoop 0.20,"Rumen tries to match the end of a value string through indexOf(""\""""). It does not take into account the case when an escaped '""' in the value string. This leads to the incorrect parsing the remaining key=value properties in the same line.",hong.tang,hong.tang,Major,Closed,Fixed,06/Aug/10 23:04,12/Dec/11 06:19
Bug,MAPREDUCE-2012,12471607,Some contrib tests fail in branch 0.21 and trunk,"After the commit of MAPREDUCE-1920, some contrib tests such as TestStreamingStatus, TestStreamingTaskLog and etc. are timing out.",amareshwari,amareshwari,Blocker,Closed,Fixed,14/Aug/10 09:19,29/Oct/10 02:03
Bug,MAPREDUCE-2014,12471735,Remove task-controller from 0.21 branch,,tomwhite,tomwhite,Major,Closed,Fixed,16/Aug/10 18:33,29/Nov/11 09:20
Bug,MAPREDUCE-2018,12471859,TeraSort example fails in trunk,"Exceptions are thrown while computing splits near the end of file - typically when the number of bytes read is smaller than RECORD_LENGTH

10/08/17 22:44:17 WARN conf.Configuration: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
10/08/17 22:44:17 INFO input.FileInputFormat: Total input paths to process : 1
Spent 19ms computing base-splits.
Spent 2ms computing TeraScheduler splits.
Computing input splits took 22ms
Sampling 1 splits of 1
Got an exception while reading splits java.io.EOFException: read past eof
        at org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:267)
        at org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:181)

TeraInoutFormat I believe assumes the file sizes are exact multiples of RECORD_LENGTH

",,ramach,Major,Resolved,Fixed,17/Aug/10 22:52,30/Jul/14 23:31
Bug,MAPREDUCE-2021,12472058,CombineFileInputFormat returns duplicate  hostnames in split locations,"CombineFileInputFormat.getSplits creates splits with duplicate locations. It adds locations of the files in the split to an ArrayList; if all the files are on same location, the location is added again and again. Instead, it should add it to a Set instead of List to avoid duplicates.",amareshwari,amareshwari,Major,Closed,Fixed,20/Aug/10 04:21,02/May/13 02:30
Bug,MAPREDUCE-2022,12472065,trunk build broken,"Trunk compilation fails with following errors:
    [javac] /home/amarsri/workspace/trunk/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java:267: getListing(java.lang.String,byte[],boolean) in org.apache.hadoop.hdfs.protocol.ClientProtocol cannot be applied to (java.lang.String,byte[])
    [javac]         client.getListing(
    [javac]               ^
    [javac] /home/amarsri/workspace/trunk/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java:281: getListing(java.lang.String,byte[],boolean) in org.apache.hadoop.hdfs.protocol.ClientProtocol cannot be applied to (java.lang.String,byte[])
    [javac]         client.getListing(
    [javac]               ^

This is due to commit of HDFS-202",amareshwari,amareshwari,Blocker,Closed,Fixed,20/Aug/10 08:22,12/Dec/11 06:19
Bug,MAPREDUCE-2023,12472073,TestDFSIO read test may not read specified bytes.,TestDFSIO's read test may read less bytes than specified when reading large files.,hong.tang,hong.tang,Major,Closed,Fixed,20/Aug/10 09:24,12/Dec/11 06:19
Bug,MAPREDUCE-2029,12472429,DistributedRaidFileSystem not removed from cache on close(),"When DistributedRaidFileSystem.close() is called, it does not remove itself from the FileSystem cache, but it does close the underlying filesystem, e.g. DFS.

Because the DRFS with the closed DFS is still in the cache, calling FileSystem.get() returns a stale DRFS that throws 'filesystem closed' exceptions.",rvadali,pauly,Major,Closed,Fixed,25/Aug/10 00:32,12/Dec/11 06:19
Bug,MAPREDUCE-2031,12472452,TestTaskLauncher and TestTaskTrackerLocalization fail with NPE in trunk.,"TestTaskLauncher and TestTaskTrackerLocalization fail in trunk after the commit of MAPREDUCE-1881 with NPE:
{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.kill(TaskTracker.java:2978)
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.jobHasFinished(TaskTracker.java:2941)
        at org.apache.hadoop.mapred.TaskTracker.purgeJob(TaskTracker.java:1919)
        at org.apache.hadoop.mapred.TestTaskTrackerLocalization.verifyUserLogsRemoval(TestTaskTrackerLocalization.java:816)
        at org.apache.hadoop.mapred.TestTaskTrackerLocalization.testJobFilesRemoval(TestTaskTrackerLocalization.java:897)
{noformat}

{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.kill(TaskTracker.java:2978)
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.jobHasFinished(TaskTracker.java:2941)
        at org.apache.hadoop.mapred.TaskTracker.purgeTask(TaskTracker.java:1981)
        at org.apache.hadoop.mapred.TaskTracker.processKillTaskAction(TaskTracker.java:420)
        at org.apache.hadoop.mapred.TestTaskLauncher.testExternalKillForLaunchTask(TestTaskLauncher.java:95)
{noformat}

NPE happens because taskTracker.myInstrumentation is not initialized.",ravidotg,amareshwari,Major,Closed,Fixed,25/Aug/10 08:49,12/Dec/11 06:18
Bug,MAPREDUCE-2032,12472454,TestJobOutputCommitter fails in ant test run,"TestJobOutputCommitter fails in a ""ant test"" run with following exception :
{noformat}
Output directory /home/amarsri/mapred/build/test/data/test-job-cleanup/output-2 already exists
org.apache.hadoop.fs.FileAlreadyExistsException: Output directory /home/amarsri/mapred/build/test/data/test-job-cleanup/output-2 already exists
        at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:141)
        at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:391)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:350)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1037)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1034)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1093)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1034)
        at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testKilledJob(TestJobOutputCommitter.java:192)
        at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testDefaultCleanupAndAbort(TestJobOutputCommitter.java:232)
{noformat}
But it passes when it is run individually.",dking,amareshwari,Major,Closed,Fixed,25/Aug/10 09:18,12/Dec/11 06:19
Bug,MAPREDUCE-2054,12473597,Hierarchical queue implementation broke dynamic queue addition in Dynamic Scheduler,Queue names were returned from the queue manager as an immutable set after the hierarchical queuname feature which breaks the dynamic priority scheduler,sandholm,sandholm,Major,Closed,Fixed,08/Sep/10 16:23,12/Dec/11 06:18
Bug,MAPREDUCE-2057,12473701,"Job Tracker appears to do host access-control (mapred.hosts, mapred.hosts.exclude) based on presented name from TaskTracker","As far as I can tell, where the NameNode, in validating the dfs.hosts and dfs.hosts.exclude files uses the source IP address for the RPC connection, the JobTracker appears to use the presented hostname (set via slave.host.name or the standard hostname-search semantics) from the TaskTracker. Obviously this is a security bug as in a production environment it could allow rogue machines to present the hostname of a real TaskTracker and take over that role, but it also turns up as a configuration bug because it means that you can set up a (multi-homed, natch) environment where the same set of files work for the NameNode, but don't for the JobTracker or vice versa - with the same binding hostname for fs.default.name and mapred.job.tracker.",,mbm,Major,Resolved,Fixed,09/Sep/10 16:53,30/Jul/14 23:52
Bug,MAPREDUCE-2059,12473712,RecoveryManager attempts to add jobtracker.info,"The jobtracker is treating the file 'jobtracker.info' in the system data directory as a job to be recovered, resulting in the following:

10/09/09 18:06:02 WARN mapred.JobTracker: Failed to add the job jobtracker.info
java.lang.IllegalArgumentException: JobId string : jobtracker.info is not properly formed
        at org.apache.hadoop.mapreduce.JobID.forName(JobID.java:158)
        at org.apache.hadoop.mapred.JobID.forName(JobID.java:84)
        at org.apache.hadoop.mapred.JobTracker$RecoveryManager.addJobForRecovery(JobTracker.java:1057)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:1565)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:275)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:267)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:262)
        at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:4256)
",subrotosanyal,dadkins,Major,Closed,Fixed,09/Sep/10 19:04,12/Dec/11 06:18
Bug,MAPREDUCE-2062,12473889,speculative execution is too aggressive under certain conditions,"The function canBeSpeculated has subtle bugs that cause too much speculation in certain cases.

- it compares the current progress of the task with the last observed mean of all the tasks. if only one task is in question - then the progress rate decays as time progresses (in the absence of updates) and std-dev is zero. So a job with a single reducer or mapper is almost always speculated.
- is only a single task has reported progress - then the stddev is zero. so other tasks may be speculated aggressively.
- several tasks take a while to report progress initially. they seem to get speculated as soon as speculative-lag is over. the lag should be configurable at the minimum.
",,jsensarma,Major,Resolved,Fixed,11/Sep/10 01:21,31/Jul/14 03:53
Bug,MAPREDUCE-2067,12474106,Distinct minicluster services (e.g. NN and JT) overwrite each other's service policies,MR portion of HADOOP-6951.,atm,atm,Major,Closed,Fixed,14/Sep/10 18:47,12/Dec/11 06:19
Bug,MAPREDUCE-2074,12474326,Task should fail when symlink creation fail,"If I pass an invalid symlink as   -Dmapred.cache.files=/user/knoguchi/onerecord.txt#abc/abc

Task only reports a WARN and goes on.

{noformat} 
2010-09-16 21:38:49,782 INFO org.apache.hadoop.mapred.TaskRunner: Creating symlink: /0/tmp/mapred-local/taskTracker/knoguchi/distcache/-5031501808205559510_-128488332_1354038698/abc-nn1.def.com/user/knoguchi/onerecord.txt <- /0/tmp/mapred-local/taskTracker/knoguchi/jobcache/job_201008310107_15105/attempt_201008310107_15105_m_000000_0/work/./abc/abc
2010-09-16 21:38:49,789 WARN org.apache.hadoop.mapred.TaskRunner: Failed to create symlink: /0/tmp/mapred-local/taskTracker/knoguchi/distcache/-5031501808205559510_-128488332_1354038698/abc-nn1.def.com/user/knoguchi/onerecord.txt <- /0/tmp/mapred-local/taskTracker/knoguchi/jobcache/job_201008310107_15105/attempt_201008310107_15105_m_000000_0/work/./abc/abc
{noformat} 

I believe we should fail the task at this point.",priyomustafi,knoguchi,Minor,Closed,Fixed,16/Sep/10 21:44,15/Nov/11 00:49
Bug,MAPREDUCE-2077,12474337,Name clash in the deprecated o.a.h.util.MemoryCalculatorPlugin,"Name clash compile error in the deprecated org.apache.hadoop.util.MemoryCalculatorPlugin due to JLS3 8.4.8.3 (cf. http://bugs.sun.com/view_bug.do?bug_id=6182950)

The bug doesn't manifest in jdk 1.6 up to 20, but shows up in NetBeans 6.9+ due to its bundled (conforming) compiler. Fix is trivial: just remove the offending method in the deprecated subclass as its equivalent erasure is inherited from the parent class anyway.",vicaya,vicaya,Major,Closed,Fixed,16/Sep/10 23:00,12/Dec/11 06:19
Bug,MAPREDUCE-2078,12474355,TraceBuilder unable to generate the traces while giving the job history path by globing.,"I was trying to generate the traces for MR job histories by using TraceBuilder. However, it's unable to generate the traces while giving the job history path by globing. It throws a file not found exception even though the job history path is exists.

I have provide the job history path in the below way.

hdfs://<<clustername>>/dir1/dir2/dir3/*/*/*/*/*/*/

Exception:

java.io.FileNotFoundException: File does not exist:
hdfs://<<clustername>>/dir1/dir2/dir3/*/*/*/*/*/*
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:525)
        at org.apache.hadoop.tools.rumen.TraceBuilder$MyOptions.<init>(TraceBuilder.java:88)
        at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:183)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:121)

It's truncating the last  slash in the path.
",amar_kamat,vinaythota,Major,Closed,Fixed,17/Sep/10 05:59,12/Dec/11 06:19
Bug,MAPREDUCE-2080,12474543,Reducer JVM process hangs,"We use cdh3b2

After reducer took more than 600 seconds to report to tasktracker, tasktracker tried to kill it.
However reducer JVM process hung.

Attaching stack trace of reducer.",,ted_yu,Major,Resolved,Fixed,19/Sep/10 21:31,31/Jul/14 00:00
Bug,MAPREDUCE-2082,12474653,Race condition in writing the jobtoken password file when launching pipes jobs,"In Application.java, when jobtoken password file is written, there is a race condition because the file is written in job's work directory. The file should rather be written in the task's working directory.",jnp,jnp,Major,Closed,Fixed,21/Sep/10 02:45,12/Dec/11 06:18
Bug,MAPREDUCE-2084,12474840,Deprecated org.apache.hadoop.util package in MapReduce produces deprecations in Common classes in Eclipse,"As reported in [this thread|http://mail-archives.apache.org/mod_mbox/hadoop-mapreduce-user/201009.mbox/%3C4C9A0A08.3030901@web.de%3E] the classes in org.apache.hadoop.util from the Common JAR, like Tool, are marked as deprecated by Eclipse, even though they are not deprecated. The fix is to mark the individual classes in the MapReduce org.apache.hadoop.util class as deprecated, not the whole package.",tomwhite,tomwhite,Blocker,Closed,Fixed,22/Sep/10 16:09,12/Dec/11 06:18
Bug,MAPREDUCE-2086,12474965,CHANGES.txt does not reflect the release of version 0.21.0.,"CHANGES.txt should show the release date for 0.21.0 and include section for for 0.21.1 - Unreleased. Latest changes, that did not make into 0.21.0, should be moved under 0.21.1 section.",tomwhite,shv,Major,Resolved,Fixed,23/Sep/10 19:18,29/Oct/10 02:03
Bug,MAPREDUCE-2090,12475070,Clover build doesn't generate per-test coverage,Because of the way the structure of Hadoop's builds is done Clover can't properly detect test classes among the sources. As the result clover reports are incomplete and do not provide viral per-test coverage info.,cos,cos,Major,Resolved,Fixed,24/Sep/10 19:13,29/Oct/10 02:03
Bug,MAPREDUCE-2092,12475084,Trunk can't be compiled,"Compilation of the trunk is broken because of an attempt to call
{{ServiceAuthorizationManager.refresh}} from a static content. 
0.21 branch seems to be Ok.",,cos,Major,Resolved,Fixed,24/Sep/10 21:45,25/Sep/10 02:57
Bug,MAPREDUCE-2094,12475294,"LineRecordReader should not seek into non-splittable, compressed streams.","When implementing a custom derivative of FileInputFormat we ran into the effect that a large Gzipped input file would be processed several times. 

A near 1GiB file would be processed around 36 times in its entirety. Thus producing garbage results and taking up a lot more CPU time than needed.

It took a while to figure out and what we found is that the default implementation of the isSplittable method in [org.apache.hadoop.mapreduce.lib.input.FileInputFormat | http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/java/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.java?view=markup ] is simply ""return true;"". 

This is a very unsafe default and is in contradiction with the JavaDoc of the method which states: ""Is the given filename splitable? Usually, true, but if the file is stream compressed, it will not be. "" . The actual implementation effectively does ""Is the given filename splitable? Always true, even if the file is stream compressed using an unsplittable compression codec. ""

For our situation (where we always have Gzipped input) we took the easy way out and simply implemented an isSplittable in our class that does ""return false; ""

Now there are essentially 3 ways I can think of for fixing this (in order of what I would find preferable):
# Implement something that looks at the used compression of the file (i.e. do migrate the implementation from TextInputFormat to FileInputFormat). This would make the method do what the JavaDoc describes.
# ""Force"" developers to think about it and make this method abstract.
# Use a ""safe"" default (i.e. return false)",nielsbasjes,nielsbasjes,Major,Resolved,Fixed,28/Sep/10 09:40,30/Aug/16 01:20
Bug,MAPREDUCE-2095,12475299,Gridmix unable to run for compressed traces(.gz format).,"I was trying to run gridmix with compressed trace file.However, it throws a JsonParseException and exit.

exception details:
==================
org.codehaus.jackson.JsonParseException: Illegal character ((CTRL-CHAR, code 31)): only regular white space (\r, \n,
\t) is allowed between tokens
 at [Source: org.apache.hadoop.fs.FSDataInputStream@17ba38f; line: 1, column: 2]
        at org.codehaus.jackson.impl.JsonParserBase._constructError(JsonParserBase.java:651)
        at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:635)
        at org.codehaus.jackson.impl.JsonParserBase._throwInvalidSpace(JsonParserBase.java:596)
        at org.codehaus.jackson.impl.Utf8StreamParser._skipWSOrEnd(Utf8StreamParser.java:981)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:77)
        at org.codehaus.jackson.map.ObjectMapper._initForReading(ObjectMapper.java:688)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:624)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:275)
        at org.apache.hadoop.tools.rumen.JsonObjectMapperParser.getNext(JsonObjectMapperParser.java:84)
        at org.apache.hadoop.tools.rumen.ZombieJobProducer.getNextJob(ZombieJobProducer.java:117)
        at org.apache.hadoop.tools.rumen.ZombieJobProducer.getNextJob(ZombieJobProducer.java:29)
        at org.apache.hadoop.mapred.gridmix.JobFactory.getNextJobFiltered(JobFactory.java:174)
        at org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread.run(StressJobFactory.java:166)
10/09/23 09:43:17 ERROR gridmix.Gridmix: Error in trace
org.codehaus.jackson.JsonParseException: Illegal character ((CTRL-CHAR, code 31)): only regular white space (\r, \n,
\t) is allowed between tokens
 at [Source: org.apache.hadoop.fs.FSDataInputStream@17ba38f; line: 1, column: 2]
        at org.codehaus.jackson.impl.JsonParserBase._constructError(JsonParserBase.java:651)
        at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:635)
        at org.codehaus.jackson.impl.JsonParserBase._throwInvalidSpace(JsonParserBase.java:596)
        at org.codehaus.jackson.impl.Utf8StreamParser._skipWSOrEnd(Utf8StreamParser.java:981)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:77)
        at org.codehaus.jackson.map.ObjectMapper._initForReading(ObjectMapper.java:688)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:624)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:275)
        at org.apache.hadoop.tools.rumen.JsonObjectMapperParser.getNext(JsonObjectMapperParser.java:84)
        at org.apache.hadoop.tools.rumen.ZombieJobProducer.getNextJob(ZombieJobProducer.java:117)
        at org.apache.hadoop.tools.rumen.ZombieJobProducer.getNextJob(ZombieJobProducer.java:29)
        at org.apache.hadoop.mapred.gridmix.JobFactory.getNextJobFiltered(JobFactory.java:174)
        at org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread.run(StressJobFactory.java:166)
10/09/23 09:43:17 INFO gridmix.Gridmix: Exiting...
",ranjit,vinaythota,Major,Closed,Fixed,28/Sep/10 11:06,02/May/13 02:29
Bug,MAPREDUCE-2096,12475339,Secure local filesystem IO from symlink vulnerabilities,"This JIRA is to contribute a patch developed on the private security@ mailing list.

The vulnerability is that MR daemons occasionally open files that are located in a path where the user has write access. A malicious user may place a symlink in place of the expected file in order to cause the daemon to instead read another file on the system -- one which the attacker may not naturally be able to access. This includes delegation tokens belong to other users, log files, keytabs, etc.",tlipcon,tlipcon,Blocker,Closed,Fixed,28/Sep/10 19:46,12/Dec/11 06:18
Bug,MAPREDUCE-2099,12475545,RaidNode should recreate outdated parity HARs,"After parity files are archived into a parity HAR, a change in the source file does not cause the HAR to be recreated. Instead, individual parity files are created for the modified files but the HAR is not touched. This causes increased disk usage for parity data.

The parity HAR could be recreated if a certain percentage of files in the HAR are determined to be outdated.

",rvadali,rvadali,Major,Closed,Fixed,30/Sep/10 21:19,12/Dec/11 06:19
Bug,MAPREDUCE-2104,12475747,Rumen TraceBuilder Does Not Emit CPU/Memory Usage Details in Traces,"Via MAPREDUCE-220, we now have CPU/Memory usage information in MapReduce JobHistory files. However, Rumen's TraceBuilder
does not emit this information in the JSON traces. Without this information, GridMix3 cannot emulate CPU/Memory usage correctly.",amar_kamat,ranjit,Major,Closed,Fixed,04/Oct/10 11:04,02/May/13 02:29
Bug,MAPREDUCE-2127,12477103,mapreduce trunk builds are failing on hudson,"https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/507/console

[exec] checking for pthread.h... yes
     [exec] checking for pthread_create in -lpthread... yes
     [exec] checking for HMAC_Init in -lssl... no
     [exec] configure: error: Cannot find libssl.so
     [exec] /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/src/c++/pipes/configure: line 4250: exit: please: numeric argument required
     [exec] /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/src/c++/pipes/configure: line 4250: exit: please: numeric argument required

BUILD FAILED
/grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/build.xml:1647: exec returned: 255
",bmahe,gkesavan,Major,Closed,Fixed,12/Oct/10 06:50,15/Nov/11 00:49
Bug,MAPREDUCE-2129,12477133,Job may hang if mapreduce.job.committer.setup.cleanup.needed=false and mapreduce.map/reduce.failures.maxpercent>0,Job may hang at RUNNING state if mapreduce.job.committer.setup.cleanup.needed=false and mapreduce.map/reduce.failures.maxpercent>0. It happens when some tasks fail but havent reached failures.maxpercent.,subrotosanyal,xiaokang,Major,Closed,Fixed,12/Oct/10 13:15,17/Oct/12 18:27
Bug,MAPREDUCE-2134,12477312,ant binary-system is broken in mapreduce project.,"Build failed due to unable to copy the commons instrumented jar. I could see the following error in the log.

binary-system:
     [copy] Copying 5 files to /home/vinay/mapreduce/build-fi/system/hadoop-mapred-0.22.0-SNAPSHOT

BUILD FAILED
/home/vinay/mapreduce/build.xml:1307: Warning: Could not find file /home/vinay/mapreduce/build-fi/ivy/lib/Hadoop/system/hadoop-common-instrumented-0.22.0-SNAPSHOT.jar to copy.


It's pointing to the wrong path to copy the file. Actually the correct path is,

/home/vinay/mapreduce/build-fi/system/ivy/Hadoop/system


",cos,vinaythota,Major,Resolved,Fixed,14/Oct/10 05:22,07/Apr/11 15:40
Bug,MAPREDUCE-2137,12477448,Mapping between Gridmix jobs and the corresponding original MR jobs is needed,"Consider a trace file ""trace1"" obtained by running Rumen on a set of MR jobs' history logs. When gridmix runs simulated jobs from ""trace1"", it may skip some of the jobs from the trace file for some reason like out-of-order-jobs. Now use Rumen to generate trace2 from the history logs of gridmix's simulated jobs.
Now, to compare and analyze the gridmix's simulated jobs with original MR jobs, we need a mapping between them.",ravidotg,ravidotg,Major,Closed,Fixed,15/Oct/10 09:57,15/Nov/11 00:48
Bug,MAPREDUCE-2143,12477764,HarFileSystem is not able to handle spaces in its path,"If the Path to the HAR contains spaces, Path.getFileSystem() fails. The problem is in HarFileSystem.initialize(), which uses URI.toString() to get a string for getting to the .har suffix. URI.toString() returns a percent-encoded string when the path contains spaces. When this string is subsequently used to get the _index file, we get a FileNotFoundException. The fix is to use URI.getPath().
",rvadali,rvadali,Major,Closed,Fixed,19/Oct/10 17:26,12/Dec/11 06:19
Bug,MAPREDUCE-2146,12478035,Raid should not affect access time of a source file,"After a file is read for creating a raid parity file, the access time should be set back to the value before the read. The read by RAID code is not an application read and should not affect the access time.",rvadali,rvadali,Minor,Resolved,Fixed,21/Oct/10 21:58,07/Apr/11 15:40
Bug,MAPREDUCE-2149,12478247,Distcp : setup with update is too slow when latency is high,"
If you run distcp with '-update' option, for _each of the files_ present on source cluster setup invokes a separate RPC to destination cluster to fetch file info. 
Usually this overhead is not very noticeable when both cluster are geographically close to each other. But if the latency is large, setup could take couple of orders of magnitude longer.

E.g. : source has 10k directories, each with about 10 files, round trip latency between source and destination is 75 ms (typical for coast-to-coast clusters). 
If we run distcp on source cluster, set up would take about _2.5 hours_ irrespective of whether destination has these files or not. '-lsr' on the same dest dir from source cluster would take up to 12 min (depending on how many directories already exist on dest). 

  * A fairly simple fix to how setup() iterates should bring the set up time to same as '-lsr'. I will have a patch for this.. (though 12 min is too large).
  * A more scalable option is to differ update check to mappers.

",rangadi,rangadi,Major,Resolved,Fixed,25/Oct/10 16:14,24/Aug/17 23:55
Bug,MAPREDUCE-2172,12478950,test-patch.properties contains incorrect/version-dependent values of OK_FINDBUGS_WARNINGS and OK_RELEASEAUDIT_WARNINGS,"Running ant test-patch with an empty patch yields 25 findbugs warning and 3 release audit warnings (rather than the 0 findbugs warnings and 1 release audit warning specified in test-patch.properties):

{code}
[exec] -1 overall.  
[exec] 
[exec]     +1 @author.  The patch does not contain any @author tags.
[exec] 
[exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
[exec]                         Please justify why no new tests are needed for this patch.
[exec]                         Also please list what manual steps were performed to verify this patch.
[exec] 
[exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
[exec] 
[exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
[exec] 
[exec]     -1 findbugs.  The patch appears to introduce 25 new Findbugs warnings.
[exec] 
[exec]     -1 release audit.  The applied patch generated 3 release audit warnings (more than the trunk's current 1 warnings).
[exec] 
[exec]     +1 system test framework.  The patch passed system test framework compile.
{code}",nidaley,pkling,Major,Closed,Fixed,03/Nov/10 02:24,15/Nov/11 00:49
Bug,MAPREDUCE-2173,12478952,Race condition in TestBlockFixer causes intermittent failure,TestBlockFixer sometimes fails in reportCorruptBlocks because a corrupt block is deleted before in.readFully is called. This causes a BlockMissingException instead of the expected ChecksumException.,pkling,pkling,Major,Closed,Fixed,03/Nov/10 02:46,12/Dec/11 06:19
Bug,MAPREDUCE-2178,12479449,Race condition in LinuxTaskController permissions handling,"The linux-task-controller executable currently traverses a directory heirarchy and calls chown/chmod on the files inside. There is a race condition here which can be exploited by an attacker, causing the task-controller to improprly chown an arbitrary target file (via a symlink) to the user running a MR job. This can be exploited to escalate to root.

[this issue was raised and discussed on the security@ list over the last couple of months]",benoyantony,tlipcon,Major,Resolved,Fixed,09/Nov/10 02:18,02/May/13 02:29
Bug,MAPREDUCE-2179,12479463,RaidBlockSender.java compilation fails,"

https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk/490/consoleFull


Mapreduce trunk compilation is broken with 

compile:
     [echo] contrib: raid
    [javac] Compiling 27 source files to /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/contrib/raid/classes
    [javac] /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/src/contrib/raid/src/java/org/apache/hadoop/hdfs/server/datanode/RaidBlockSender.java:71: cannot find symbol
    [javac] symbol  : class BlockTransferThrottler
    [javac] location: class org.apache.hadoop.hdfs.server.datanode.RaidBlockSender
    [javac]   private BlockTransferThrottler throttler;
    [javac]           ^
    [javac] /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/src/contrib/raid/src/java/org/apache/hadoop/hdfs/server/datanode/RaidBlockSender.java:377: cannot find symbol
    [javac] symbol  : class BlockTransferThrottler
    [javac] location: class org.apache.hadoop.hdfs.server.datanode.RaidBlockSender
    [javac]                  BlockTransferThrottler throttler) throws IOException {
    [javac]                  ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 2 errors",rvadali,gkesavan,Blocker,Closed,Fixed,09/Nov/10 05:46,12/Dec/11 06:19
Bug,MAPREDUCE-2185,12479828,Infinite loop at creating splits using CombineFileInputFormat,"This is caused by a missing block in HDFS. So the block's locations are empty. The following code adds the block to blockToNodes map but not to rackToBlocks map. Later on when generating splits, only blocks in rackToBlocks are removed from blockToNodes map. So blockToNodes map can never become empty therefore causing infinite loop

{code}
          // add this block to the block --> node locations map
          blockToNodes.put(oneblock, oneblock.hosts);

          // add this block to the rack --> block map
          for (int j = 0; j < oneblock.racks.length; j++) {
             ..
          }
{code}",rvadali,hairong,Major,Closed,Fixed,12/Nov/10 20:12,15/Nov/11 00:48
Bug,MAPREDUCE-2187,12479979,map tasks timeout during sorting,"During the execution of a large job, the map tasks timeout:

{code}
INFO mapred.JobClient: Task Id : attempt_201010290414_60974_m_000057_1, Status : FAILED
Task attempt_201010290414_60974_m_000057_1 failed to report status for 609 seconds. Killing!
{code}

The bug is in the fact that the mapper has already finished, and, according to the logs, the timeout occurs during the merge sort phase.
The intermediate data generated by the map task is quite large. So I think this is the problem.

The logs show that the merge-sort was running for 10 minutes when the task was killed.
I think the mapred.Merger should call Reporter.progress() somewhere.",anupamseth,azaroth,Major,Closed,Fixed,15/Nov/10 17:29,19/Oct/11 00:26
Bug,MAPREDUCE-2188,12480016,The new API MultithreadedMapper doesn't call the initialize method of the RecordReader,"The wrapping RecordReader in the Multithreaded Mapper is never initialized. With HADOOP-6685, this becomes a problem because the ReflectionUtils.copy requires a non-null configuration.",omalley,omalley,Major,Closed,Fixed,15/Nov/10 23:55,12/Dec/11 06:18
Bug,MAPREDUCE-2195,12480457,New property for local conf directory in system-test-mapreduce.xml file.,As its counter-part HDFS-1167: new parameter needs to be added to the system-test configuration file to serve 'cluster restart with new  configuration' feature,cos,cos,Major,Closed,Fixed,19/Nov/10 22:14,12/Dec/11 06:19
Bug,MAPREDUCE-2199,12480742,build is broken 0.22 branch creation,hdfs and common dep versions weren't updated properly.,cos,cos,Major,Closed,Fixed,23/Nov/10 21:47,15/Nov/11 00:49
Bug,MAPREDUCE-2200,12480843,TestUmbilicalProtocolWithJobToken is failing without Krb evironment: needs to be conditional,TestUmbilicalProtocolWithJobToken requires Krb environment to be set. For testing some 'pseudo' environment is needed (similar to HDFS-1284). ,cos,cos,Major,Closed,Fixed,24/Nov/10 18:23,12/Dec/11 06:18
Bug,MAPREDUCE-2215,12492794,A more elegant FileSystem#listCorruptFileBlocks API (RAID changes),Map/reduce changes related to HADOOP-7060 and HDFS-1533.,pkling,pkling,Major,Closed,Fixed,09/Dec/10 02:34,02/May/13 02:29
Bug,MAPREDUCE-2217,12493141,The expire launching task should cover the UNASSIGNED task,"The ExpireLaunchingTask thread kills the task that are scheduled but not responded.
Currently if a task is scheduled on tasktracker and for some reason tasktracker cannot put it to RUNNING.
The task will just hang in the UNASSIGNED status and JobTracker will keep waiting for it.

JobTracker.ExpireLaunchingTask should be able to kill this task.",kasha,schen,Major,Closed,Fixed,14/Dec/10 00:19,03/Nov/14 18:05
Bug,MAPREDUCE-2219,12493368,JT should not try to remove mapred.system.dir during startup,"During startup, the JT tries to clean up mapred.system.dir by recursively removing it and then recreating it. This requires that mapred.system.dir is inside a directory owned by the mapred user. For example, if set to /system/mapred then /system must be owned by the mapred account. This isn't documented properly and also seems unnecessary. Instead we can remove the *contents* of mapred.system.dir instead of the directory itself.",tlipcon,tlipcon,Major,Closed,Fixed,15/Dec/10 22:11,12/Dec/11 06:19
Bug,MAPREDUCE-2220,12493400,Fix new API FileOutputFormat-related typos in mapred-default.xml,"there're two typos:
 * mapreduce.output.fileoutputformat.compression.type instead of mapreduce.output.fileoutputformat.compress.type
 * mapreduce.output.fileoutputformat.compression.codec instead of mapreduce.output.fileoutputformat.compress.codec

in mapred-default. Trivial patch to fix.",ruikubo,ruikubo,Minor,Closed,Fixed,16/Dec/10 07:06,11/Oct/12 17:48
Bug,MAPREDUCE-2222,12493558,Ivy resolve force mode should be turned off by default,cf. HADOOP-7068,vicaya,vicaya,Major,Closed,Fixed,17/Dec/10 22:39,12/Dec/11 06:19
Bug,MAPREDUCE-2223,12493574,TestMRCLI might fail on Ubuntu with default /etc/hosts,"Depending on the order of entries in /etc/hosts, TestCLI can fail. This is because it sets fs.default.name to ""localhost"", and then the bound IPC socket on the NN side reports its hostname as ""foobar-host"" if the entry for 127.0.0.1 lists ""foobar-host"" before ""localhost"". This seems to be the default in some versions of Ubuntu.",cos,cos,Minor,Resolved,Fixed,18/Dec/10 03:10,07/Apr/11 15:40
Bug,MAPREDUCE-2224,12493619,Synchronization bugs in JvmManager,JvmManager.JvmManagerForType has several HashMap members that are inconsistently synchronized. I've seen sporadic NPEs in the 0.20 version of this code which has similar bugs.,tlipcon,tlipcon,Critical,Closed,Fixed,19/Dec/10 03:48,05/Jun/12 07:25
Bug,MAPREDUCE-2228,12493832,Remove java5 dependencies from build,As the first short-term step let's remove JDK5 dependency from build(s),cos,cos,Major,Resolved,Fixed,22/Dec/10 05:18,12/Feb/11 14:30
Bug,MAPREDUCE-2232,12494061,Add missing methods to TestMapredGroupMappingServiceRefresh,"HADOOP-6864 added new methods to the GroupMappingServiceProvider interface, so MR trunk no longer compiles.",tlipcon,tlipcon,Major,Resolved,Fixed,25/Dec/10 21:20,07/Apr/11 15:40
Bug,HIVE-1028,12444754,typedbytes does not work for tinyint,typedbytes does not work for tinyint,namit,namit,Major,Closed,Fixed,05/Jan/10 23:03,17/Dec/11 00:05
Bug,HIVE-1029,12444759,typedbytes does not support nulls,typedbytes does not support nulls,aprabhakar,namit,Major,Closed,Fixed,06/Jan/10 01:11,05/Feb/12 01:34
Bug,HIVE-1030,12444822,Hive should use scratchDir instead of system temporary directory for storing plans,Otherwise these plan files never get deleted.,zshao,zshao,Major,Closed,Fixed,06/Jan/10 19:05,17/Dec/11 00:05
Bug,HIVE-1031,12444845,"""DESCRIBE FUNCTION array"" throws ParseException","{noformat}
hive> describe function array;
describe function array;
FAILED: Parse Error: line 1:18 cannot recognize input 'array' in describe statement

hive> describe function 'array';
describe function 'array';
OK
array(n0, n1...) - Creates an array with the given elements 
Time taken: 0.396 seconds
hive> describe function map;
describe function map;
FAILED: Parse Error: line 1:18 cannot recognize input 'map' in describe statement

hive> describe function 'map';
describe function 'map';
OK
map(key0, value0, key1, value1...) - Creates a map with the given key/value pairs 
Time taken: 0.054 seconds
hive> describe function case;
describe function case;
FAILED: Parse Error: line 1:18 cannot recognize input 'case' in describe statement

hive> describe function 'case';
describe function 'case';
OK
There is no documentation for function case
Time taken: 0.072 seconds

{noformat}",cwsteinbach,cwsteinbach,Major,Closed,Fixed,06/Jan/10 22:09,17/Dec/11 00:06
Bug,HIVE-1037,12444996,SerDe & ObjectInspector for RowContainer mismatch with the input data,"In CommonJoinOperator, RowContainer is created for each input table with the SerDe and ObjectInspector to serialize/deserialize that row to persistent storage. The serde/OI could be null in the case of the value columns are pruned by column pruner. An example query is

select count(1) from A join B on A.key=B.key;

Another case of mismatch is that the tableDesc was initialized at compile time before the column pruner take place. This could cause inconsistency in the SerDe/OI with the input data. This should be moved to execution time when the join operator is initialized. ",nzhang,nzhang,Blocker,Closed,Fixed,08/Jan/10 05:50,17/Dec/11 00:05
Bug,HIVE-1038,12445065,mapjoin dies if the join prunes all the columns ,"
The query:

select /*+ mapjoin(a) */ count(1) from src a join src b on a.key = b.key


dies.

It is a blocker for 0.5",namit,namit,Blocker,Closed,Fixed,08/Jan/10 19:53,17/Dec/11 00:06
Bug,HIVE-1039,12445206,multi-insert doesn't work for local directories,"As wd pointed out in hive-user, the following query only load data to the first local directory. Multi-insert to tables works fine. 

hive> from test
    > INSERT OVERWRITE LOCAL DIRECTORY '/home/stefdong/tmp/0' select *
where a = 1
    > INSERT OVERWRITE LOCAL DIRECTORY '/home/stefdong/tmp/1' select *
where a = 3;
",nzhang,nzhang,Major,Closed,Fixed,11/Jan/10 19:29,17/Dec/11 00:06
Bug,HIVE-1042,12445229,function in a transform with more than 1 argument fails,"                select transform(substr(key, 1, 3))
                USING '/bin/cat'
                FROM src

throws an error:

FAILED: Error in semantic analysis: AS clause has an invalid number of aliases
",pauly,namit,Major,Closed,Fixed,11/Jan/10 22:53,17/Dec/11 00:06
Bug,HIVE-1045,12445330,(bigint % int) should return bigint instead of double,"This expression should return bigint instead of double.

{code}
CREATE TABLE test (a BIGINT);
EXPLAIN SELECT a % 3 FROM test;
{code}

There must be something wrong in {{FunctionRegistry.getMethodInternal}}
",pauly,zshao,Major,Closed,Fixed,12/Jan/10 18:37,17/Dec/11 00:05
Bug,HIVE-1046,12445347,Pass build.dir.hive and other properties to subant,"Currently we are not passing properties like ""build.dir.hive"" etc to subant.
We should do that, otherwise setting ""build.dir.hive"" is not useful.


",zshao,zshao,Major,Closed,Fixed,12/Jan/10 20:05,17/Dec/11 00:05
Bug,HIVE-1056,12445708,Predicate push down does not work with UDTF's,"Predicate push down does not work with UDTF's in lateral views

{code}

hive> SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS k WHERE k=1;
FAILED: Unknown exception: null
hive>

{code}",pauly,pauly,Major,Closed,Fixed,16/Jan/10 01:32,17/Dec/11 00:03
Bug,HIVE-1059,12445715,Date/DateTime/TimeStamp types should throw an error,"Currently hive doesn't support date, datetime, or timestamp types. Using these in a create table / alter table should throw a descriptive error that suggests users to use a string type.",pauly,pauly,Major,Closed,Fixed,16/Jan/10 02:34,17/Dec/11 00:05
Bug,HIVE-1064,12445813,NPE when operating HiveCLI in distributed mode,"{code}
hive> select id, name from tab_a;
select id, name from tab_a;
10/01/18 03:55:59 INFO parse.ParseDriver: Parsing command: select id, name from tab_a
10/01/18 03:55:59 INFO parse.ParseDriver: Parse Completed
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Completed phase 1 of Semantic Analysis
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Get metadata for source tables
10/01/18 03:55:59 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
10/01/18 03:55:59 INFO metastore.ObjectStore: ObjectStore, initialize called
10/01/18 03:56:03 INFO metastore.ObjectStore: Initialized ObjectStore
10/01/18 03:56:03 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=tab_a
10/01/18 03:56:03 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:03 INFO parse.SemanticAnalyzer: Get metadata for subqueries
10/01/18 03:56:03 INFO parse.SemanticAnalyzer: Get metadata for destination tables
10/01/18 03:56:04 INFO parse.SemanticAnalyzer: Completed getting MetaData in Semantic Analysis
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for FS(2)
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for SEL(1)
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for TS(0)
10/01/18 03:56:04 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:04 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:04 INFO parse.SemanticAnalyzer: Completed plan generation
10/01/18 03:56:04 INFO ql.Driver: Semantic Analysis Completed
10/01/18 03:56:04 INFO ql.Driver: Starting command: select id, name from tab_a
Total MapReduce jobs = 1
10/01/18 03:56:04 INFO ql.Driver: Total MapReduce jobs = 1
Launching Job 1 out of 1
10/01/18 03:56:04 INFO ql.Driver: Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
10/01/18 03:56:04 INFO exec.ExecDriver: Number of reduce tasks is set to 0 since there's no reduce operator
FAILED: Unknown exception : null
10/01/18 03:56:04 ERROR ql.Driver: FAILED: Unknown exception : null
java.lang.NullPointerException
	at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:288)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:475)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:103)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:64)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:589)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:469)
	at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:329)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:317)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
	at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)

hive> 
{code}",cwsteinbach,cwsteinbach,Major,Closed,Fixed,18/Jan/10 11:56,17/Dec/11 00:03
Bug,HIVE-1066,12445881,"TestContribCliDriver failure in serde_typedbytes.q, serde_typedbytes2.q, and serde_typedbytes3.q","This will reproduce the problem:
{code}
ant test -Dqfile=serde_typedbytes.q -Dtestcase=TestContribCliDriver
ant test -Dqfile=serde_typedbytes2.q -Dtestcase=TestContribCliDriver
ant test -Dqfile=serde_typedbytes3.q -Dtestcase=TestContribCliDriver
{code}
",zshao,zshao,Major,Closed,Fixed,19/Jan/10 02:46,17/Dec/11 00:03
Bug,HIVE-1072,12446147,"""show table extended like table partition(xxx) "" will show the result of the whole table if the partition does not exist","See the following example, we should output an error for the second command.

{code}
hive> show table extended like member_count;
OK
tableName:member_count
owner:null
location:/user/hive/member_count
inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
columns:struct columns { string count}
partitioned:true
partitionColumns:struct partition_columns { string ds}
totalNumberFiles:233933
totalFileSize:32802665
maxFileSize:257
minFileSize:140
lastAccessTime:1264017438860
lastUpdateTime:1263949909703

Time taken: 125.104 seconds

hive> show table extended like member_count partition(ds = '2009-10-11');
OK
tableName:member_count
owner:null
location:/user/hive/member_count
inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
columns:struct columns { string count}
partitioned:true
partitionColumns:struct partition_columns { string ds}
totalNumberFiles:233933
totalFileSize:32802665
maxFileSize:257
minFileSize:140
lastAccessTime:1264017438860
lastUpdateTime:1263949909703

Time taken: 24.618 seconds

hive> show table extended like member_count partition(ds = '2009-12-11');
OK
tableName:member_count
owner:null
location:/user/hive/member_count
inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
columns:struct columns { string count}
partitioned:true
partitionColumns:struct partition_columns { string ds}
totalNumberFiles:3495
totalFileSize:489417
maxFileSize:257
minFileSize:140
lastAccessTime:1262676533852
lastUpdateTime:1263949909703

Time taken: 0.549 seconds
{code}
",he yongqiang,zshao,Major,Closed,Fixed,20/Jan/10 22:18,17/Dec/11 00:06
Bug,HIVE-1075,12446154,Make it possible for users to recover data when moveTask fails,"If a ""INSERT OVERWRITE"" query fails after deleting the output table content but before moving the new content into that output table, we should allow user to recover the data manually.
In order to do that, we should expose the temp location of the data and we should not remove the temp data.
",zshao,zshao,Major,Closed,Fixed,20/Jan/10 22:39,17/Dec/11 00:03
Bug,HIVE-1076,12446158,CreateTime is reset to 0 when a partition is overwritten,"Hive should keep ""CreateTime"" when a partition is overwritten. The ""CreateTime"" should be the first time the partition is created.

{code}
hive> describe extended zshao_ttp;
OK
d       string
ds      string

Detailed Table Information      Table(tableName:zshao_ttp, dbName:default, owner:zshao, createTime:1264027720, 
lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:d, type:string, comment:null)], 
location:hdfs://hdfs:9000/user/hive/zshao_ttp, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, 
outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, 
serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:
{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, 
comment:null)], parameters:{transient_lastDdlTime=1264027720})
Time taken: 3.062 seconds
hive> describe extended zshao_ttp partition(ds='2010-01-01');
OK
d       string
ds      string

Detailed Partition Information  Partition(values:[2010-01-01], dbName:default, tableName:zshao_ttp, createTime:1264027788, 
lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:d, type:string, comment:null)], location:hdfs://hdfs:9000
/user/hive/zshao_ttp/ds=2010-01-01, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, 
outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, 
serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:
{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1264027788})
Time taken: 0.436 seconds


hive> insert overwrite table zshao_ttp partition (ds='2010-01-01') select d from zshao_ttp where ds = '2010-01-01';
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_200912262300_1111, Tracking URL = http://jobtracker:50030/jobdetails.jsp?jobid=job_200912262300_1111
Kill Command = hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=jobtracker:50029 -kill job_200912262300_1111
2010-01-20 15:04:15,272 Stage-1 map = 0%,  reduce = 0%
2010-01-20 15:05:16,895 Stage-1 map = 0%,  reduce = 0%
2010-01-20 15:06:16,768 Stage-1 map = 100%,  reduce = 0%
2010-01-20 15:06:43,929 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_200912262300_1111
Launching Job 2 out of 2
Moving data to: hdfs://hdfs:9000/tmp/hive-zshao/262641680/10000
Loading data to table zshao_ttp partition {ds=2010-01-01}
Moved to trash: /user/hive/zshao_ttp/ds=2010-01-01
2 Rows loaded to zshao_ttp
OK
Time taken: 187.049 seconds


hive> describe extended zshao_ttp partition(ds='2010-01-01');
OK
d       string
ds      string

Detailed Partition Information  Partition(values:[2010-01-01], dbName:default, tableName:zshao_ttp, createTime:0, 
lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:d, type:string, comment:null)], location:hdfs://hdfs:9000
/user/hive/zshao_ttp/ds=2010-01-01, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, 
outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, 
serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:
{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:
{lastQueryTime=1264028626290,archiveFlag=false,transient_lastDdlTime=1264028626})
Time taken: 0.283 seconds
{code}
",pauly,zshao,Major,Closed,Fixed,20/Jan/10 23:11,17/Dec/11 00:05
Bug,HIVE-1085,12446366,ColumnarSerde should not be the default Serde when user specified a fileformat using 'stored as'.,,he yongqiang,he yongqiang,Major,Closed,Fixed,22/Jan/10 20:56,17/Dec/11 00:03
Bug,HIVE-1086,12446400,"Add ""-Doffline=true"" option to ant","Currently I am seeing ivy retrieve for 4 times, each time for 4 of the hadoop versions.
It takes a long time.

{code}
ivy-retrieve-hadoop-source:
[ivy:retrieve] :: Ivy 2.0.0-rc2 - 20081028224207 :: http://ant.apache.org/ivy/ ::
:: loading settings :: file = /hive/trunk/VENDOR.hive/trunk/ivy/ivysettings.xml
[ivy:retrieve] :: resolving dependencies :: org.apache.hadoop.hive#shims;working@zshao.com
[ivy:retrieve]  confs: [default]
[ivy:retrieve]  found hadoop#core;0.17.2.1 in hadoop-source
[ivy:retrieve]  found hadoop#core;0.18.3 in hadoop-source
...
{code}

We should fix this problem. Also it will help if we can add an option ""offline"" like what hadoop has.

",zshao,zshao,Major,Closed,Fixed,23/Jan/10 01:05,17/Dec/11 00:04
Bug,HIVE-1088,12446431,RCFile RecordReader's first split will read duplicate rows if the split end is < the first SYNC mark,,he yongqiang,he yongqiang,Blocker,Closed,Fixed,23/Jan/10 19:59,17/Dec/11 00:06
Bug,HIVE-1090,12446498,Skew Join does not work in distributed env.,This is due to client side operators do not get a chance to initialize them. And JoinOp's jobClose refers to two transient vars (handleSkewJoin and numberAlias).,he yongqiang,he yongqiang,Major,Closed,Fixed,25/Jan/10 04:34,17/Dec/11 00:03
Bug,HIVE-1092,12446500,Conditional task does not increase finished job counter when filter job out.,"Ended Job = 864272330, job is filtered out (removed at runtime).
Launching Job 2 out of 3 


It should be 'Launching Job 3 out of 3""

",he yongqiang,he yongqiang,Major,Closed,Fixed,25/Jan/10 04:38,17/Dec/11 00:03
Bug,HIVE-1094,12446508,Disable streaming last table if there is a skew key in previous tables.,"In JoinOperator's processOp()

{code} 
if(alias == numAliases -1){
if (sz == joinEmitInterval) {
{code} 

should be changed to
{code} 
if(alias == numAliases - 1 && !(handleSkewJoin && skewJoinKeyContext.currBigKeyTag >= 0)
{code} ",he yongqiang,he yongqiang,Major,Closed,Fixed,25/Jan/10 08:15,17/Dec/11 00:03
Bug,HIVE-1097,12446605,groupby_bigdata.q sometimes throws out of memory exception,"I would get out of memory errors like the following when running groupby_bigdata.q.

{code}
  
    [junit] plan = /data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/scratchdir/plan38413.xml
    [junit] Exception in thread ""Thread-15"" java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.util.Arrays.copyOf(Arrays.java:2882)
    [junit]     at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)
    [junit]     at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:390)
    [junit]     at java.lang.StringBuffer.append(StringBuffer.java:224)
    [junit]     at java.io.StringWriter.write(StringWriter.java:84)
    [junit]     at java.io.PrintWriter.newLine(PrintWriter.java:436)
    [junit]     at java.io.PrintWriter.println(PrintWriter.java:585)
    [junit]     at java.io.PrintWriter.println(PrintWriter.java:696)
    [junit]     at java.lang.Throwable.printStackTrace(Throwable.java:512)
    [junit]     at org.apache.hadoop.util.StringUtils.stringifyException(StringUtils.java:60)
    [junit]     at org.apache.hadoop.hive.ql.exec.ScriptOperator$StreamThread.run(ScriptOperator.java:561)
    [junit] Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.nio.HeapCharBuffer.<init>(HeapCharBuffer.java:39)
    [junit]     at java.nio.CharBuffer.allocate(CharBuffer.java:312)
    [junit]     at java.nio.charset.CharsetEncoder.isLegalReplacement(CharsetEncoder.java:319)
    [junit]     at java.nio.charset.CharsetEncoder.replaceWith(CharsetEncoder.java:267)
    [junit]     at java.nio.charset.CharsetEncoder.<init>(CharsetEncoder.java:186)
    [junit]     at java.nio.charset.CharsetEncoder.<init>(CharsetEncoder.java:209)
    [junit]     at sun.nio.cs.ISO_8859_1$Encoder.<init>(ISO_8859_1.java:116)
    [junit]     at sun.nio.cs.ISO_8859_1$Encoder.<init>(ISO_8859_1.java:113)
    [junit]     at sun.nio.cs.ISO_8859_1.newEncoder(ISO_8859_1.java:46)
    [junit]     at java.lang.StringCoding$StringEncoder.<init>(StringCoding.java:215)
    [junit]     at java.lang.StringCoding$StringEncoder.<init>(StringCoding.java:207)
    [junit]     at java.lang.StringCoding.encode(StringCoding.java:266)
    [junit]     at java.lang.String.getBytes(String.java:947)
    [junit]     at java.io.UnixFileSystem.getLength(Native Method)
    [junit]     at java.io.File.length(File.java:848)
    [junit]     at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.<init>(RawLocalFileSystem.java:375)
    [junit]     at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:359)
    [junit]     at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:245)
    [junit]     at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:643)
    [junit]     at org.apache.hadoop.hive.ql.exec.Utilities.clearMapRedWork(Utilities.java:114)
    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:680)
    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:936)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
    [junit] Traceback (most recent call last):
    [junit]   File ""../data/scripts/dumpdata_script.py"", line 6, in <module>
    [junit]     print 20000 * i + k

{code}",pauly,pauly,Major,Closed,Fixed,25/Jan/10 21:37,17/Dec/11 00:06
Bug,HIVE-1099,12446614,Queries in input40.q should be deterministic,The SELECT * queries in input40.q are not deterministic because the rows could be output in any order. The fix is to add a sort by clause as it is done in branch-0.5.,pauly,pauly,Major,Closed,Fixed,25/Jan/10 22:58,16/Jun/15 19:14
Bug,HIVE-1113,12446857,LOAD DATA LOCAL INPATH does't work on windows,"I had follow trace of this issue.

2010-01-05 01:49:04,109 ERROR ql.Driver (SessionState.java:printError(248)) - FAILED: Unknown exception: null
java.lang.IllegalArgumentException
	at java.net.URI.create(URI.java:842)
	at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:197)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:270)
	at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:315)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:307)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.net.URISyntaxException: Illegal character in path at index 42: file:/F:/hadoop/tmp/hive-Ruslan/1564735857\10000
	at java.net.URI$Parser.fail(URI.java:2809)
	at java.net.URI$Parser.checkChars(URI.java:2982)
	at java.net.URI$Parser.parseHierarchical(URI.java:3066)
	at java.net.URI$Parser.parse(URI.java:3014)
	at java.net.URI.<init>(URI.java:578)
	at java.net.URI.create(URI.java:840)
	... 13 more

",,tantra,Minor,Closed,Fixed,27/Jan/10 21:20,17/Dec/11 00:03
Bug,HIVE-1116,12446926,bug with alter table rename when table has property EXTERNAL=FALSE,"if the location is not an external location - this would be safer.

the problem right now is that it's tricky to use the drop and rename way of writing new data into a table. consider:

Initialization block:
drop table a_tmp
create table a_tmp like a;

Loading block:
load data <newdata> into a_tmp;
drop table a;
alter table a_tmp rename to a;

this looks safe. but it's not. if one runs this multiple times - then data is lost (since 'a' is pointing to 'a_tmp''s location after any iteration. and dropping table 'a' blows away loaded data in the next iteration). 

if the location is being managed by Hive - then 'rename' should switch location as well.







",jvs,jsensarma,Major,Closed,Fixed,28/Jan/10 16:49,17/Dec/11 00:03
Bug,HIVE-1124,12455022,CREATE VIEW should expand the query text consistently,"We should expand the omitted alias in the same way in ""select"" and in ""group by"".

Hive ""Group By"" recognize ""group by"" expressions by comparing the literal string.

{code}
hive> create view zshao_view as select d, count(1) as cnt from zshao_tt group by d;
OK
Time taken: 0.286 seconds
hive> select * from zshao_view;
FAILED: Error in semantic analysis: line 1:7 Expression Not In Group By Key d in definition of VIEW zshao_view [
select d, count(1) as `cnt` from `zshao_tt` group by `zshao_tt`.`d`
] used as zshao_view at line 1:14
{code}


 
",jvs,zshao,Major,Closed,Fixed,02/Feb/10 08:26,17/Dec/11 00:03
Bug,HIVE-1125,12455095,Hive CLI shows 'Ended Job=' at the beginning of the job,"Instead of ""Starting Job = "", it prints ""Ended Job =""

{code}
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Ended Job = job_201002012342_2688, Tracking URL = http://silver.data.facebook.com:50030/jobdetails.jsp?jobid=job_201002012342_2688
Kill Command = /data/users/pyang/task2/trunk/dist/shortcuts/silver.trunk/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=silver.data.facebook.com:50029 -kill job_201002012342_2688
2010-02-02 10:47:05,067 Stage-1 map = 0%,  reduce = 0%
{code}",pauly,pauly,Major,Closed,Fixed,02/Feb/10 18:52,17/Dec/11 00:03
Bug,HIVE-1129,12455228,Fix Assertion in ExecDriver.execute when assertions are enabled in HADOOP_OPTS,"I noticed that when running hive CLI, assertions are not enabled, which was causing me some confusion when debugging an issue.

So, I added the following to my environment:

export HADOOP_OPTS=""-ea -esa""

This worked, and allowed me to see assertion failures when executing via CLI.

But then I tried to run a test, and got an assertion failure from the following code in ExecDriver.execute:

    // Turn on speculative execution for reducers
    HiveConf.setVar(job, HiveConf.ConfVars.HADOOPSPECULATIVEEXECREDUCERS,
        HiveConf.getVar(job, HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS));

The assertion says it should be using getBoolVar/setBoolVar instead.
",jvs,jvs,Major,Closed,Fixed,03/Feb/10 20:22,17/Dec/11 00:03
Bug,HIVE-1140,12455682,Incorrect ambiguous column reference error message,"Whenever there is an ambiguous column name reference, the error message does not reference the proper column.
{code}
hive> FROM (SELECT key, concat(value) AS key FROM src) a SELECT a.key;
FAILED: Error in semantic analysis: line 1:25 Ambiguous Column Reference value
{code}",pauly,pauly,Minor,Closed,Fixed,08/Feb/10 20:57,17/Dec/11 00:05
Bug,HIVE-1141,12455691,Remove recursive call in FetchOperator.getNextRow,"There is an unnecessary recursive getNextRow() call that caused stack overflow in some queries.
We should remove that recursive call.
",zshao,zshao,Major,Closed,Fixed,08/Feb/10 22:14,17/Dec/11 00:05
Bug,HIVE-1142,12455692,Fix datanucleus typos in conf/hive-default.xml,"datanucleus is misspelled as ""datancucleus"" and ""datanuclues"".",pauly,jvs,Trivial,Closed,Fixed,08/Feb/10 22:18,17/Dec/11 00:03
Bug,HIVE-1167,12456210,Use TreeMap instead of Property to make explain extended deterministic,"In some places in the code, we are using Properties class in ""explain extended"".

This makes the order of the lines in the ""explain extended"" undeterministic because Properties are based on Hashtable class.

We should add another function to show the properties in sorted order.
",zshao,zshao,Major,Closed,Fixed,13/Feb/10 01:50,17/Dec/11 00:03
Bug,HIVE-1168,12456257,Fix Hive build on Hudson,"{quote}
We need to delete the .ant directory containing the old ivy version in order to fix it 
(and if we're using the same environment for both trunk and branches, either 
segregate them or script an rm to clean in between).
{quote}

It's worth noting that ant may have picked up the old version of Ivy from
somewhere else. In order Ant's classpath contains:

# Ant's startup JAR file, ant-launcher.jar
# Everything in the directory containing the version of ant-launcher.jar that's
  running, i.e. everything in ANT_HOME/lib
# All JAR files in ${user.home}/.ant/lib
# Directories and JAR files supplied via the -lib command line option.
# Everything in the CLASSPATH variable unless the -noclasspath option is used.

(2) implies that users on shared machines may have to install their own
version of ant in order to get around these problems, assuming that the
administrator has install the ivy.jar in $ANT_HOME/lib
",jvs,cwsteinbach,Critical,Closed,Fixed,14/Feb/10 07:29,17/Dec/11 00:03
Bug,HIVE-1174,12456472,"Fix job counter error if ""hive.merge.mapfiles"" equals true","if hive.merge.mapfiles is set to true, the job counter will go to 3.",he yongqiang,he yongqiang,Major,Closed,Fixed,16/Feb/10 19:40,17/Dec/11 00:04
Bug,HIVE-1176,12456570,'create if not exists' fails for a table name with 'select' in it,"hive> create table if not exists tmp_select(s string, c string, n int);
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: javax.jdo.JDOUserException JDOQL Single-String query should always start with SELECT)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTablesForDb(Hive.java:441)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTablesByPattern(Hive.java:423)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:5538)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5192)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)
        at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: MetaException(message:Got exception: javax.jdo.JDOUserException JDOQL Single-String query should always start with SELECT)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.logAndThrowMetaException(MetaStoreUtils.java:612)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTables(HiveMetaStoreClient.java:450)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTablesForDb(Hive.java:439)
        ... 15 more",aprabhakar,prasadc,Major,Closed,Fixed,17/Feb/10 18:15,02/May/13 02:29
Bug,HIVE-1182,12456834,Fix TestCliDriver too many open file error,"There is a bug in TestCliDriver that loads all .q files for each of the .q test.
We only need to load the .q file for that particular .q test.

",zshao,zshao,Critical,Closed,Fixed,19/Feb/10 20:21,17/Dec/11 00:06
Bug,HIVE-1183,12456874,hive.hwi.war.file vanished from hive-default.xml. error message prints wrong variable. does not start by default.,"hive.hwi.war.file vanished from hive-default.xml.
error message prints wrong variable.
since the full path is no longer supplied. The old documentation no longer applies

 String hwiWAR = conf.getVar(HiveConf.ConfVars.HIVEHWIWARFILE);
 String hivehome = System.getenv().get(""HIVE_HOME"");
   File hwiWARFile = new File(hivehome, hwiWAR);
   if (!hwiWARFile.exists()) {
     l4j.fatal(""HWI WAR file not found at "" + hwiWAR);
     System.exit(1);
   }",appodictic,appodictic,Blocker,Closed,Fixed,20/Feb/10 01:46,29/Jul/14 09:32
Bug,HIVE-1184,12456879,Expression Not In Group By Key error is sometimes masked,"Depending on the order of expressions, the error message for a expression not in group key is not displayed; instead it is null.

{code}
hive> select concat(value, concat(value)) from src group by concat(value);
FAILED: Error in semantic analysis: null

hive> select concat(concat(value), value) from src group by concat(value);
FAILED: Error in semantic analysis: line 1:29 Expression Not In Group By Key value

{code}",pauly,pauly,Major,Closed,Fixed,20/Feb/10 04:20,17/Dec/11 00:03
Bug,HIVE-1185,12456929,Fix RCFile resource leak when opening a non-RCFile,"See HADOOP-5476 for the bug in SequenceFile. We should do the same thing in RCFile.
",he yongqiang,zshao,Major,Closed,Fixed,21/Feb/10 08:23,17/Dec/11 00:03
Bug,HIVE-1195,12457361,Increase ObjectInspector[] length on demand,"{code}
Operator.java
  protected transient ObjectInspector[] inputObjInspectors = new ObjectInspector[Short.MAX_VALUE];
{code}

An array of 32K elements takes 256KB memory under 64-bit Java.
We are seeing hive client going out of memory because of that.
",zshao,zshao,Major,Closed,Fixed,24/Feb/10 23:06,17/Dec/11 00:03
Bug,HIVE-1200,12457506,Fix CombineHiveInputFormat to work with multi-level of directories in a single table/partition,"The CombineHiveInputFormat does not work with multi-level of directories in a single table/partition, because it uses an exact match logic, instead of the relativize logic as in MapOperator

{code}
MapOperator.java:
          if (!onepath.toUri().relativize(fpath.toUri()).equals(fpath.toUri())) {
{code}
",zshao,zshao,Major,Closed,Fixed,25/Feb/10 23:38,17/Dec/11 00:03
Bug,HIVE-1203,12457577,"HiveInputFormat.getInputFormatFromCache ""swallows""  cause exception when throwing IOExcpetion","To fix this it's simply needed to add second parameter to IOException constructor. Patches for 0.4, 0.5 and trunk are available.",klimontovich,klimontovich,Major,Closed,Fixed,26/Feb/10 14:58,16/Dec/11 23:59
Bug,HIVE-1204,12457624,typedbytes: writing to stderr kills the mapper,,namit,namit,Major,Closed,Fixed,27/Feb/10 01:36,17/Dec/11 00:03
Bug,HIVE-1207,12457816,ScriptOperator AutoProgressor does not set the interval,As title. I will show more details in the patch.,zshao,zshao,Major,Closed,Fixed,02/Mar/10 01:53,17/Dec/11 00:04
Bug,HIVE-1218,12458597,"CREATE TABLE t LIKE some_view should create a new empty base table, but instead creates a copy of view","I think it should copy only the column definitions from the view metadata.  Currently it is copying the entire descriptor, resulting in a new view instead of a new base table.
",ccy,jvs,Major,Closed,Fixed,09/Mar/10 23:23,16/Dec/11 23:56
Bug,HIVE-1242,12458867,CombineHiveInputFormat does not work for compressed text files,,namit,namit,Major,Closed,Fixed,11/Mar/10 23:23,01/Aug/14 04:51
Bug,HIVE-1247,12459317,hints cannot be passed to transform statements,"Statements like:


select /*+ MAPJOIN(a) */ transform(c1, c2) using '..' as ..


does not work",he yongqiang,namit,Major,Closed,Fixed,16/Mar/10 21:05,17/Dec/11 00:03
Bug,HIVE-1252,12459422,Task breaking bug when breaking after a filter operator,"The plan for this query has a bug,
SELECT f.key, g.value  FROM srcpart f JOIN srcpart m ON( f.key = m.key AND f.ds='2008-04-08' AND m.ds='2008-04-08' ) JOIN srcpart g ON(g.value = m.value AND g.ds='2008-04-08' AND m.ds='2008-04-08');
The first file sink operator's table desc contains all columns.

",he yongqiang,he yongqiang,Major,Closed,Fixed,17/Mar/10 20:16,17/Dec/11 00:03
Bug,HIVE-1253,12459451,Fix Date_sub and Date_add in case of daylight saving,"date_sub('2010-03-15', 7) returns '2010-03-07'. This is because we have time shifts on 2010-03-14 for daylight saving time.

Looking at ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateSub.java, it is getting a calendar instance in UTC time zone.

def calendar = Calendar.getInstance(TimeZone.getTimeZone(""UTC""));

And use calendar.add() to substract 7 days, then conver the time to 'yyyy-MM-dd' format.

If it simply uses default timezone, the problem is solved: ""def calendar = Calendar.getInstance());""

When people use date_sub('2010-03-15', 7), I think they mean substract 7 days, instead of substracting 7*24 hours. So it should be an easy fix. The same changes should go to date_add and date_diff

",batalbot,miwang,Major,Closed,Fixed,17/Mar/10 23:00,17/Dec/11 00:04
Bug,HIVE-1257,12459590,joins between HBase tables and other tables (whether HBase or not) are broken,"Details in

http://mail-archives.apache.org/mod_mbox/hadoop-hive-user/201003.mbox/%3C9A53DDE1FE082F4D952FDF20AC87E21F021F3EBC@exchange2.t8design.com%3E
",jvs,jvs,Major,Closed,Fixed,18/Mar/10 22:57,17/Dec/11 00:03
Bug,HIVE-1258,12459600,set merge files to files when bucketing/sorting is being enforced,,he yongqiang,namit,Major,Closed,Fixed,19/Mar/10 01:20,17/Dec/11 00:04
Bug,HIVE-1261,12459746,ql.metadata.Hive#close() should check for null metaStoreClient,"{{ql.metadata.Hive#close()}} always does a {{metaStoreClient.close()}}, even though it might be null.

I'd like to reuse the same thread for multiple queries from different users. Since it's a different user, {{HiveConf}} has a new ugi. But it's not easy to get rid of (or update) the cached {{ql.metadata.Hive}}. The cleanest way seems to be {{Hive.closeCurrent()}}, which unfortunately doesn't check for the {{metaStoreClient}} being {{null}}.",bcwalrus,bcwalrus,Major,Closed,Fixed,20/Mar/10 09:05,17/Dec/11 00:03
Bug,HIVE-1268,12459912,Cannot start metastore thrift server on a specific port,"The code to get the port number is broken. Doh.

{code}
14:07:55[dsom:dist]$ METASTORE_PORT=12345 bin/hive --service metastore
Starting Hive Metastore Server
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:960)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
{code}",bcwalrus,bcwalrus,Major,Closed,Fixed,22/Mar/10 21:20,17/Dec/11 00:03
Bug,HIVE-1271,12460017,Case sensitiveness of type information specified when using custom reducer causes type mismatch,"Type information specified while using a custom reduce script is converted to lower case, and causes type mismatch during query semantic analysis . The following REDUCE query where field name = ""userId"" failed.

hive> CREATE TABLE SS (
> a INT,
> b INT,
> vals ARRAY<STRUCT<userId:INT, y:STRING>>
> );
OK

hive> FROM (select * from srcTable DISTRIBUTE BY id SORT BY id) s
> INSERT OVERWRITE TABLE SS
> REDUCE *
> USING 'myreduce.py'
> AS
> (a INT,
> b INT,
> vals ARRAY<STRUCT<userId:INT, y:STRING>>
> )
> ;
FAILED: Error in semantic analysis: line 2:27 Cannot insert into
target table because column number/types are different SS: Cannot
convert column 2 from array<struct<userId:int,y:string>> to
array<struct<userid:int,y:string>>.

The same query worked fine after changing ""userId"" to ""userid"".",aprabhakar,dilipjoseph,Major,Closed,Fixed,23/Mar/10 16:27,14/Sep/22 07:36
Bug,HIVE-1273,12460050,UDF_Percentile NullPointerException,,zshao,zshao,Major,Closed,Fixed,23/Mar/10 23:09,17/Dec/11 00:03
Bug,HIVE-1274,12460051,bug in sort merge join if the big table does not have any row,,he yongqiang,namit,Major,Closed,Fixed,23/Mar/10 23:33,17/Dec/11 00:03
Bug,HIVE-1275,12460142,TestHBaseCliDriver hangs,"TestHBaseCliDriver hangs after running hbase_joins.q

This can be reproduced by running
{code}
ant test -Dtestcase=TestHBaseCliDriver
{code}",jvs,namit,Major,Closed,Fixed,24/Mar/10 17:55,17/Dec/11 00:03
Bug,HIVE-1277,12460155,Select query with specific projection(s) fails if the local file system directory for ${hive.user.scratchdir} does not exist.,"When a simple select query with specified projections is processed, the {{MapRedTask}} attempts to serialize the execution plan to a scratch directory on the local file system specified by {{hive.user.scratchdir}} configuration property. The default value of this property points to {{/tmp/hive-${user.name}}}. On some *nix systems, the /tmp directory gets cleaned up on every reboot thus causing the query execution to fail.

*Exception stacktrace*
{quote}
hive> select foo from dummy;
Total MapReduce jobs = 1
Launching Job 1 out of 1
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.checkAndCreate(File.java:1704)
	at java.io.File.createTempFile(File.java:1792)
	at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:89)
        ...
{quote}",aprabhakar,aprabhakar,Major,Closed,Fixed,24/Mar/10 20:17,17/Dec/11 00:03
Bug,HIVE-1281,12460176,Bucketing column names in create table should be case-insensitive,"This create table fails because 'userId' != 'userid'

{code}
CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS;
{code}",he yongqiang,pauly,Minor,Closed,Fixed,24/Mar/10 23:53,17/Dec/11 00:03
Bug,HIVE-1286,12460485,Remove debug message from stdout in ColumnarSerDe,"'Found class for org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
should go to stderr where other informational messages are sent.",he yongqiang,he yongqiang,Minor,Closed,Fixed,28/Mar/10 23:14,17/Dec/11 00:03
Bug,HIVE-1290,12461000,sort merge join does not work with bucketizedhiveinputformat,The mappers are assigned in the order of the sizes of the files which violates the output bucketing of the result of sort merge join,namit,namit,Major,Closed,Fixed,02/Apr/10 07:10,17/Dec/11 00:04
Bug,HIVE-1291,12461174,Fix UDAFPercentile ndexOutOfBoundsException,"The counts array can be empty. We should directly return null in that case.

{code}
org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.hive.serde2.io.DoubleWritable org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.terminate()  on object org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator@530d0eae of class org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator with arguments {} of size 0
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:725)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge$GenericUDAFBridgeEvaluator.terminate(GenericUDAFBridge.java:181)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.evaluate(GenericUDAFEvaluator.java:157)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(GroupByOperator.java:838)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:885)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:539)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.close(ExecReducer.java:300)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:701)
	... 9 more
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.hadoop.hive.ql.udf.UDAFPercentile.getPercentile(UDAFPercentile.java:97)
	at org.apache.hadoop.hive.ql.udf.UDAFPercentile.access$300(UDAFPercentile.java:44)
	at org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.terminate(UDAFPercentile.java:196)
	... 14 more
{code}
",zshao,zshao,Major,Closed,Fixed,05/Apr/10 20:49,17/Dec/11 00:03
Bug,HIVE-1294,12461492,HIVE_AUX_JARS_PATH interferes with startup of Hive Web Interface,"The Hive Webserver fails to startup with the following error message, if HIVE_AUX_JARS_PATH environment variable is set (works fine if unset).   

$ build/dist/bin/hive --service hwi
Exception in thread ""main"" java.io.IOException: Error opening job jar: -libjars
       at org.apache.hadoop.util.RunJar.main(RunJar.java:90)
Caused by: java.util.zip.ZipException: error in opening zip file
       at java.util.zip.ZipFile.open(Native Method)
       at java.util.zip.ZipFile.<init>(ZipFile.java:114)
       at java.util.jar.JarFile.<init>(JarFile.java:133)
       at java.util.jar.JarFile.<init>(JarFile.java:70)
       at org.apache.hadoop.util.RunJar.main(RunJar.java:88)

Slightly modifying the command line to launch hadoop in hwi.sh solves the problem:

$ diff bin/ext/hwi.sh  /tmp/new-hwi.sh
28c28
<   exec $HADOOP jar $AUX_JARS_CMD_LINE ${HWI_JAR_FILE} $CLASS $HIVE_OPTS ""$@""
---
>   exec $HADOOP jar ${HWI_JAR_FILE}  $CLASS $AUX_JARS_CMD_LINE $HIVE_OPTS ""$@""",appodictic,dilipjoseph,Blocker,Closed,Fixed,08/Apr/10 05:42,17/Dec/11 00:03
Bug,HIVE-1298,12461695,unit test symlink_text_input_format.q needs ORDER BY for determinism,"Failed on my Mac; I'll add the test output in the comments.
",nzhang,jvs,Critical,Closed,Fixed,09/Apr/10 17:59,17/Dec/11 00:03
Bug,HIVE-1302,12461905,describe parse_url throws an error,"descHive history file=/tmp/njain/hive_job_log_njain_201004121528_1840617354.txt
hive> describe parse_url;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
hive> describe extended parse_url;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
hive> [njain@dev029 clientpositive]$ ",,namit,Major,Closed,Fixed,12/Apr/10 22:30,16/Dec/11 23:57
Bug,HIVE-1305,12461991,add progress in join and groupby,"The operators join and groupby can consume a lot of rows before producing any output. 
All operators which do not have a output for every input should report progress periodically.
Currently, it is only being done for ScriptOperator and FilterOperator.",sdong,namit,Major,Closed,Fixed,13/Apr/10 21:29,16/Dec/11 23:59
Bug,HIVE-1308,12462071,<boolean> = <boolean> throws NPE,"Workaround is to just use <boolean> or NOT <boolean>

{code}
hive> select true=true from src;
FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper.<init>(GenericUDFUtils.java:212)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:138)
        at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:153)
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:587)
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:708)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:128)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6136)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:1831)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:1663)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:4911)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:5421)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5952)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:304)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:377)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:303)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
{code}",pauly,pauly,Major,Closed,Fixed,14/Apr/10 19:08,17/Dec/11 00:03
Bug,HIVE-1311,12462107,Bug is use of hadoop supports splittable,"CombineHiveInputFormat: getSplits()
     if (this.mrwork != null && this.mrwork.getHadoopSupportsSplittable()) 


should check if hadoop supports splittable is false",namit,namit,Major,Closed,Fixed,15/Apr/10 06:26,17/Dec/11 00:03
Bug,HIVE-1312,12462114,hive trunk does not compile with hadoop 0.17 any more,"This is caused by HIVE-1295.

{code}
compile:
     [echo] Compiling: hive
    [javac] Compiling 527 source files to /hadoop_hive_trunk/.ptest_0/build/ql/classes
    [javac] /hadoop_hive_trunk/.ptest_0/ql/src/java/org/apache/hadoop/hive/ql/io/HiveNullValueSequenceFileOu\
tputFormat.java:69: cannot find symbol
    [javac] symbol  : method getBytes()
    [javac] location: class org.apache.hadoop.io.BytesWritable
    [javac]           keyWritable.set(bw.getBytes(), 0, bw.getLength());
    [javac]                             ^
    [javac] /hadoop_hive_trunk/.ptest_0/ql/src/java/org/apache/hadoop/hive/ql/io/HiveNullValueSequenceFileOu\
tputFormat.java:69: cannot find symbol
    [javac] symbol  : method getLength()
    [javac] location: class org.apache.hadoop.io.BytesWritable
    [javac]           keyWritable.set(bw.getBytes(), 0, bw.getLength());
    [javac]                                               ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 2 errors
{code}
",jvs,zshao,Major,Closed,Fixed,15/Apr/10 07:55,17/Dec/11 00:03
Bug,HIVE-1315,12462510,bucketed sort merge join breaks after dynamic partition insert,"bucketed sort merge join produces wrong bucket number due to HIVE-1002 patch, which breaks HIVE-1290.",nzhang,nzhang,Major,Closed,Fixed,20/Apr/10 05:19,17/Dec/11 00:03
Bug,HIVE-1317,12462718,CombineHiveInputFormat throws exception when partition name contains special characters to URI,"If a partition name contains characters such as ':' and '|' which have special meaning in URI (hdfs uses URI internally for Path), CombineHiveInputFormat throws an exception. URI was created in CombineHiveInputFormat to compare a path belongs to a partition in partitionToPathInfo. We should bypass URI creation by just string comparisons. ",nzhang,nzhang,Major,Closed,Fixed,22/Apr/10 01:02,17/Dec/11 00:03
Bug,HIVE-1320,12462825,NPE with lineage in a query of union alls on joins.,"The following query generates a NPE in the lineage ctx code

EXPLAIN
INSERT OVERWRITE TABLE dest_l1
SELECT j.*
FROM (SELECT t1.key, p1.value
      FROM src1 t1
      LEFT OUTER JOIN src p1
      ON (t1.key = p1.key)
      UNION ALL
      SELECT t2.key, p2.value
      FROM src1 t2
      LEFT OUTER JOIN src p2
      ON (t2.key = p2.key)) j;

The stack trace is:

FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
at org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx$Index.mergeDependency(LineageCtx.java:116)
at org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory$UnionLineage.process(OpProcFactory.java:396)
at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
at org.apache.hadoop.hive.ql.optimizer.lineage.Generator.transform(Generator.java:72)
at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:83)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5976)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:48)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
",athusoo,athusoo,Major,Closed,Fixed,22/Apr/10 22:50,17/Dec/11 00:04
Bug,HIVE-1321,12462826,"bugs with temp directories, trailing blank fields in HBase bulk load","HIVE-1295 had two bugs discovered during testing with production data:

(1) extra directories may be present in the output directory depending on how the cluster is configured; we need to walk down these to find the column family directory

(2) if a record ends with fields which are blank strings, the text format omits the corresponding Control-A delimiters, so we need to fill in blanks for these fields (instead of throwing ArrayIndexOutOfBoundsException)
",jvs,jvs,Major,Closed,Fixed,22/Apr/10 23:14,17/Dec/11 00:03
Bug,HIVE-1322,12462892,Cached FileSystem can lead to persistant IOExceptions,"In the metastore, the FileSystem object is created through Path.getFileSytem(), which caches the created instance for performance. For DFS, the cached FileSystem seems to retain the original IP (resolved from the supplied hostname). If the hostname->IP mapping changes, then FS operations will throw IOExceptions. Because the FileSystem is cached, re-creating the object has no effect and will continue to result in IOExceptions.

One solution is to call close on an IOException. That will remove the entry in the cache.",pauly,pauly,Minor,Closed,Fixed,23/Apr/10 18:25,17/Dec/11 00:03
Bug,HIVE-1325,12462919,dynamic partition insert should throw an exception if the number of target table columns + dynamic partition columns does not equal to the number of select columns,,nzhang,nzhang,Major,Closed,Fixed,24/Apr/10 07:04,17/Dec/11 00:03
Bug,HIVE-1326,12462969,RowContainer uses hard-coded '/tmp/' path for temporary files,"In our production hadoop environment, the ""/tmp/"" is actually pretty small, and we encountered a problem when a query used the RowContainer class and filled up the /tmp/ partition.  I tracked down the cause to the RowContainer class putting temporary files in the '/tmp/' path instead of using the configured Hadoop temporary path.  I've attached a patch to fix this.

Here's the traceback:

2010-04-25 12:05:05,120 INFO org.apache.hadoop.hive.ql.exec.persistence.RowContainer: RowContainer created temp file /tmp/hive-rowcontainer-1244151903/RowContainer7816.tmp
2010-04-25 12:05:06,326 INFO ExecReducer: ExecReducer: processing 10000000 rows: used memory = 385520312
2010-04-25 12:05:08,513 INFO ExecReducer: ExecReducer: processing 11000000 rows: used memory = 341780472
2010-04-25 12:05:10,697 INFO ExecReducer: ExecReducer: processing 12000000 rows: used memory = 301446768
2010-04-25 12:05:12,837 INFO ExecReducer: ExecReducer: processing 13000000 rows: used memory = 399208768
2010-04-25 12:05:15,085 INFO ExecReducer: ExecReducer: processing 14000000 rows: used memory = 364507216
2010-04-25 12:05:17,260 INFO ExecReducer: ExecReducer: processing 15000000 rows: used memory = 332907280
2010-04-25 12:05:19,580 INFO ExecReducer: ExecReducer: processing 16000000 rows: used memory = 298774096
2010-04-25 12:05:21,629 INFO ExecReducer: ExecReducer: processing 17000000 rows: used memory = 396505408
2010-04-25 12:05:23,830 INFO ExecReducer: ExecReducer: processing 18000000 rows: used memory = 362477288
2010-04-25 12:05:25,914 INFO ExecReducer: ExecReducer: processing 19000000 rows: used memory = 327229744
2010-04-25 12:05:27,978 INFO ExecReducer: ExecReducer: processing 20000000 rows: used memory = 296051904
2010-04-25 12:05:28,155 FATAL ExecReducer: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:150)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:132)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:121)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1013)
	at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)
	at org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat$1.write(HiveSequenceFileOutputFormat.java:70)
	at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:343)
	at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.add(RowContainer.java:163)
	at org.apache.hadoop.hive.ql.exec.JoinOperator.processOp(JoinOperator.java:118)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:456)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:244)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)
	at org.apache.hadoop.mapred.Child.main(Child.java:158)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:260)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:197)
	... 22 more
",michaelklatt,michaelklatt,Major,Closed,Fixed,25/Apr/10 20:50,17/Dec/11 00:03
Bug,HIVE-1330,12463290,fatal error check omitted for reducer-side operators,,nzhang,nzhang,Major,Closed,Fixed,29/Apr/10 04:35,17/Dec/11 00:03
Bug,HIVE-1331,12463357,select * does not work if different partitions contain different formats,Will try to come up with a concrete test - but looks like we are using the table's input format,namit,namit,Major,Closed,Fixed,29/Apr/10 17:41,17/Dec/11 00:03
Bug,HIVE-1338,12463824,Fix bin/ext/jar.sh to work with hadoop 0.20 and above,{{bin/ext/jar.sh}} is not working with hadoop 0.20 and above.,zshao,zshao,Major,Closed,Fixed,05/May/10 21:42,17/Dec/11 00:03
Bug,HIVE-1341,12464270,Filter Operator Column Pruning should preserve the column order,"The column pruning process for the filter operator should preserve the order of input columns, otherwise it could result in miss match in columns in the down stream operators. 
",nzhang,nzhang,Major,Closed,Fixed,11/May/10 18:00,17/Dec/11 00:04
Bug,HIVE-1342,12464329,Predicate push down get error result when sub-queries have the same alias name ,"Query is over-optimized by PPD when sub-queries have the same alias name, see the query:

-------------------------------
create table if not exists dm_fact_buyer_prd_info_d (
		category_id string
		,gmv_trade_num  int
		,user_id    int
		)
PARTITIONED BY (ds int);

set hive.optimize.ppd=true;
set hive.map.aggr=true;

explain select category_id1,category_id2,assoc_idx
from (
		select 
			category_id1
			, category_id2
			, count(distinct user_id) as assoc_idx
		from (
			select 
				t1.category_id as category_id1
				, t2.category_id as category_id2
				, t1.user_id
			from (
				select category_id, user_id
				from dm_fact_buyer_prd_info_d
				group by category_id, user_id ) t1
			join (
				select category_id, user_id
				from dm_fact_buyer_prd_info_d
				group by category_id, user_id ) t2 on t1.user_id=t2.user_id 
			) t1
			group by category_id1, category_id2 ) t_o
			where category_id1 <> category_id2
			and assoc_idx > 2;

-----------------------------
The query above will fail when execute, throwing exception: ""can not cast UDFOpNotEqual(Text, IntWritable) to UDFOpNotEqual(Text, Text)"". 

I explained the query and the execute plan looks really wired ( only Stage-1, see the highlighted predicate):

-------------------------------
Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        t_o:t1:t1:dm_fact_buyer_prd_info_d 
          TableScan
            alias: dm_fact_buyer_prd_info_d
            Filter Operator
              predicate:
                  expr: *(category_id <> user_id)*
                  type: boolean
              Select Operator
                expressions:
                      expr: category_id
                      type: string
                      expr: user_id
                      type: bigint
                outputColumnNames: category_id, user_id
                Group By Operator
                  keys:
                        expr: category_id
                        type: string
                        expr: user_id
                        type: bigint
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Reduce Output Operator
                    key expressions:
                          expr: _col0
                          type: string
                          expr: _col1
                          type: bigint
                    sort order: ++
                    Map-reduce partition columns:
                          expr: _col0
                          type: string
                          expr: _col1
                          type: bigint
                    tag: -1
      Reduce Operator Tree:
        Group By Operator
          keys:
                expr: KEY._col0
                type: string
                expr: KEY._col1
                type: bigint
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Select Operator
            expressions:
                  expr: _col0
                  type: string
                  expr: _col1
                  type: bigint
            outputColumnNames: _col0, _col1
            File Output Operator
              compressed: true
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
 ----------------------------------

If disabling predicate push down (set hive.optimize.ppd=true), the error is gone; I tried disabling map side aggregate, the error is gone,too. 

*Changing the alias of subquery 't1' (either the inner one or the join result), the bug disappears, too.*
",ccy,tedxu,Critical,Closed,Fixed,12/May/10 08:41,16/Dec/11 23:57
Bug,HIVE-1345,12464572,TypedBytesSerDe fails to create table with multiple columns.,"Creating a table with more than one columns fails when the row format SerDe is TypedBytesSerDe. 


{code}
hive> CREATE TABLE test (a STRING, b STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe';      
Found class for org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe                                                       
FAILED: Error in metadata: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1                                           
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask                                          
hive> 
{code}

",aprabhakar,aprabhakar,Major,Closed,Fixed,14/May/10 23:53,17/Dec/11 00:03
Bug,HIVE-1350,12464836,hive.query.id is not unique ,if commands are executed by the same user within a second,namit,namit,Major,Closed,Fixed,18/May/10 22:26,17/Dec/11 00:03
Bug,HIVE-1352,12464940,rcfilecat should use '\t' to separate columns and print '\r\n' at the end of each row.,"Talked to Venky, rcfilecat needs to add column and line delimiters. ",he yongqiang,he yongqiang,Major,Closed,Fixed,19/May/10 20:24,17/Dec/11 00:03
Bug,HIVE-1353,12464946,load_dyn_part*.q tests need ORDER BY for determinism,"Just now got a spurious failure from this while testing something else.

    [junit] diff -a -I file: -I /tmp/ -I invalidscheme: -I lastUpdateTime -I la\
stAccessTime -I owner -I transient_lastDdlTime -I java.lang.RuntimeException -I\
 at org -I at sun -I at java -I at junit -I Caused by: -I [.][.][.] [0-9]* more\
 /data/users/jsichi/open/commit-trunk/.ptest_0/build/ql/test/logs/clientpositiv\
e/load_dyn_part14.q.out /data/users/jsichi/open/commit-trunk/.ptest_0/ql/src/te\
st/results/clientpositive/load_dyn_part14.q.out                                 
    [junit] 261,262d260                                                         
    [junit] < k1        __HIVE_DEFAULT_PARTITION__                              
    [junit] < k1        __HIVE_DEFAULT_PARTITION__                              
    [junit] 264a263,264                                                         
    [junit] > k1        __HIVE_DEFAULT_PARTITION__                              
    [junit] > k1        __HIVE_DEFAULT_PARTITION__                              
    [junit] Exception: Client execution results failed with error code = 1      
",jvs,jvs,Major,Closed,Fixed,19/May/10 20:54,17/Dec/11 00:03
Bug,HIVE-1354,12464965,partition level properties honored if it exists,"drop table partition_test_partitioned;

create table partition_test_partitioned(key string, value string) partitioned by (dt string);

alter table partition_test_partitioned set fileformat rcfile;
insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;
show table extended like partition_test_partitioned partition(dt=101);

alter table partition_test_partitioned set fileformat Sequencefile;
insert overwrite table partition_test_partitioned partition(dt=102) select * from src1;
show table extended like partition_test_partitioned partition(dt=102);

insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;
show table extended like partition_test_partitioned partition(dt=101);

drop table partition_test_partitioned;


Partition (dt=101) still points to RCFile, since it was created as a RCFile",namit,namit,Major,Closed,Fixed,19/May/10 23:02,17/Dec/11 00:03
Bug,HIVE-1363,12465204,'SHOW TABLE EXTENDED LIKE' command does not strip single/double quotes,"{code}
hive> SHOW TABLE EXTENDED LIKE pokes;
OK
tableName:pokes
owner:carl
location:hdfs://localhost/user/hive/warehouse/pokes
inputformat:org.apache.hadoop.mapred.TextInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
columns:struct columns { i32 num}
partitioned:false
partitionColumns:
totalNumberFiles:0
totalFileSize:0
maxFileSize:0
minFileSize:0
lastAccessTime:0
lastUpdateTime:1274517075221

hive> SHOW TABLE EXTENDED LIKE ""p*"";
FAILED: Error in metadata: MetaException(message:Got exception: javax.jdo.JDOUserException ')' expected at character 54 in ""database.name == dbName && ( tableName.matches(""(?i)""p.*""""))"")
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask

hive> SHOW TABLE EXTENDED LIKE 'p*';
OK

hive> SHOW TABLE EXTENDED LIKE `p*`;
OK
tableName:pokes
owner:carl
location:hdfs://localhost/user/hive/warehouse/pokes
inputformat:org.apache.hadoop.mapred.TextInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
columns:struct columns { i32 num}
partitioned:false
partitionColumns:
totalNumberFiles:0
totalFileSize:0
maxFileSize:0
minFileSize:0
lastAccessTime:0
lastUpdateTime:1274517075221

{code}",ctang,cwsteinbach,Major,Closed,Fixed,23/May/10 03:10,13/Nov/14 19:43
Bug,HIVE-1364,12465284,"Increase the maximum length of various metastore fields, and remove TYPE_NAME from COLUMNS primary key","The value component of a SERDEPROPERTIES key/value pair is currently limited
to a maximum length of 767 characters. I believe that the motivation for limiting the length to 
767 characters is that this value is the maximum allowed length of an index in
a MySQL database running on the InnoDB engine: http://bugs.mysql.com/bug.php?id=13315

* The Metastore OR mapping currently limits many fields (including SERDEPROPERTIES.PARAM_VALUE) to a maximum length of 767 characters despite the fact that these fields are not indexed.
* The maximum length of a VARCHAR value in MySQL 5.0.3 and later is 65,535.
* We can expect many users to hit the 767 character limit on SERDEPROPERTIES.PARAM_VALUE when using the hbase.columns.mapping serdeproperty to map a table that has many columns.

I propose increasing the maximum allowed length of SERDEPROPERTIES.PARAM_VALUE to 8192.

",cwsteinbach,cwsteinbach,Major,Closed,Fixed,24/May/10 16:32,27/Oct/15 16:27
Bug,HIVE-1365,12465314,Bug in SMBJoinOperator which may causes a final part  of the results in some cases.,,he yongqiang,he yongqiang,Major,Closed,Fixed,24/May/10 22:05,17/Dec/11 00:04
Bug,HIVE-1366,12465325,inputFileFormat error if the merge job takes a different input file format than the default output file format,If the input file format is say SequenceFileFormat and the default fileformat is RCFile. the merge job after the MR job assumes the input format is SequenceFile format rather than RCFile. This is probably introduced in HIVE-1357. ,namit,nzhang,Major,Closed,Fixed,25/May/10 01:35,17/Dec/11 00:03
Bug,HIVE-1367,12465337,cluster by multiple columns does not work if parenthesis is present,"The following query:

select ...  from src cluster by (key, value)

throws a compile error:

whereas the query


select ...  from src cluster by key, value



works fine",zhenxiao,namit,Major,Closed,Fixed,25/May/10 06:27,29/Aug/13 09:22
Bug,HIVE-1371,12465497,remove blank in rcfilecat,,he yongqiang,he yongqiang,Major,Closed,Fixed,26/May/10 19:11,17/Dec/11 00:03
Bug,HIVE-1373,12465517,Missing connection pool plugin in Eclipse classpath,"In a recent checkin, connection pool dependency was introduced but eclipse .classpath file was not updated.  This causes launch configurations from within Eclipse to fail.

{code}
hive> show tables;
show tables;
10/05/26 14:59:46 INFO parse.ParseDriver: Parsing command: show tables
10/05/26 14:59:46 INFO parse.ParseDriver: Parse Completed
10/05/26 14:59:46 INFO ql.Driver: Semantic Analysis Completed
10/05/26 14:59:46 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)
10/05/26 14:59:46 INFO ql.Driver: query plan = file:/tmp/vinithra/hive_2010-05-26_14-59-46_058_1636674338194744357/queryplan.xml
10/05/26 14:59:46 INFO ql.Driver: Starting command: show tables
10/05/26 14:59:46 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
10/05/26 14:59:46 INFO metastore.ObjectStore: ObjectStore, initialize called
FAILED: Error in metadata: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
10/05/26 14:59:47 ERROR exec.DDLTask: FAILED: Error in metadata: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.hive.ql.metadata.Hive.getTablesForDb(Hive.java:491)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTablesByPattern(Hive.java:472)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllTables(Hive.java:458)
	at org.apache.hadoop.hive.ql.exec.DDLTask.showTables(DDLTask.java:504)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:176)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:107)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:631)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:504)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:382)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:303)
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
	at org.datanucleus.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:395)
	at org.datanucleus.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:547)
	at org.datanucleus.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1956)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1951)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1159)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:803)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:698)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:191)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:208)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:153)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:128)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:276)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.executeWithRetry(HiveMetaStore.java:228)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:374)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:166)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:131)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:83)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:1077)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:1087)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTablesForDb(Hive.java:489)
	... 12 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:576)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:324)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:190)
	at org.datanucleus.store.mapped.MappedStoreManager.<init>(MappedStoreManager.java:139)
	at org.datanucleus.store.rdbms.RDBMSManager.<init>(RDBMSManager.java:265)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:576)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:300)
	at org.datanucleus.store.FederationManager.initialiseStoreManager(FederationManager.java:106)
	at org.datanucleus.store.FederationManager.<init>(FederationManager.java:68)
	at org.datanucleus.ObjectManagerFactoryImpl.initialiseStoreManager(ObjectManagerFactoryImpl.java:152)
	at org.datanucleus.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:529)
	... 38 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the ""DBCP"" plugin to create a ConnectionPool gave an error : The connection pool plugin of type ""DBCP"" was not found in the CLASSPATH!
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initDataSourceTx(ConnectionFactoryImpl.java:169)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:91)
	... 57 more
Caused by: org.datanucleus.exceptions.NucleusUserException: The connection pool plugin of type ""DBCP"" was not found in the CLASSPATH!
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initDataSourceTx(ConnectionFactoryImpl.java:143)
	... 58 more

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
10/05/26 14:59:47 ERROR ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
hive> 
{code}",vinithra,vinithra,Major,Closed,Fixed,26/May/10 22:32,17/Dec/11 00:03
Bug,HIVE-1376,12465676,Simple UDAFs with more than 1 parameter crash on empty row query ,"Simple UDAFs with more than 1 parameter crash when the query returns no rows. Currently, this only seems to affect the percentile() UDAF where the second parameter is the percentile to be computed (of type double). I've also verified the bug by adding a dummy parameter to ExampleMin in contrib. 

On an empty query, Hive seems to be trying to resolve an iterate() method with signature {null,null} instead of {null,double}. You can reproduce this bug using:

CREATE TABLE pct_test ( val INT );
SELECT percentile(val, 0.5) FROM pct_test;

which produces a lot of errors like: 

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public boolean org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.iterate(org.apache.hadoop.io.LongWritable,double)  on object org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator@11d13272 of class org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator with arguments {null, null} of size 2",nzhang,mayanklahiri,Major,Closed,Fixed,28/May/10 21:10,17/Dec/11 00:01
Bug,HIVE-1377,12465679,getPartitionDescFromPath() in CombineHiveInputFormat should handle matching by path,"The use case is:
{code}
dir = hdfs://host:9000/user/warehouse/tableName/abc
pathToPartitionInfo = {/user/warehouse/tableName : myPart}
{code}

Then calling 
{code} 
getPartitionDescFromPath(dir, pathToPartitionInfo)
{code}

will throw an IOException because /user/warehouse/tableName is not a prefix of hdfs://host:9000/user/warehouse/tableName/abc. Currently, this is not an issue but will come up if CombineFileInputFormat is modified so what the scheme and authority are not stripped out  when generating splits (see MAPREDUCE-1806).

The proposed solution is add a case where matching is done by just the path component of the URI's.
",pauly,pauly,Major,Closed,Fixed,28/May/10 21:55,17/Dec/11 00:03
Bug,HIVE-1385,12466037,UDF field() doesn't work,"I tried it against one of my table:

hive> desc r;
OK
key int
value string
a string

hive> select * from r;
OK
4 val_356 NULL
4 val_356 NULL
484 val_169 NULL
484 val_169 NULL
2000 val_169 NULL
2000 val_169 NULL
3000 val_169 NULL
3000 val_169 NULL
4000 val_125 NULL
4000 val_125 NULL

hive> select *, field(value, 'val_169') from r; 
OK
4 val_356 NULL 0
4 val_356 NULL 0
484 val_169 NULL 0
484 val_169 NULL 0
2000 val_169 NULL 0
2000 val_169 NULL 0
3000 val_169 NULL 0
3000 val_169 NULL 0
4000 val_125 NULL 0
4000 val_125 NULL 0
",sdong,sdong,Minor,Closed,Fixed,03/Jun/10 01:58,16/Dec/11 23:59
Bug,HIVE-1388,12466138,combinehiveinputformat does not work if files are of different types,"rop table t1;
drop table t2;


create table t1 (key string, value string) partitioned by (ds string, hr string);
create table t2 (key string, value string) partitioned by (ds string);

insert overwrite table t1 partition (ds='1', hr='1') select key, value from src cluster by key;
insert overwrite table t1 partition (ds='1', hr='2') select key, value from src cluster by key;
insert overwrite table t1 partition (ds='1', hr='2') select key, value from t1 where ds = '1' and hr = '2';

desc extended t1;
desc extended t1 partition (ds='1', hr='1');
desc extended t1 partition (ds='1', hr='2');

alter table t2 add partition (ds='1') location '/data/users/njain/hive3/hive3/build/ql/test/data/warehouse/t1/ds=1';
select count(1) from t2 where ds='1';

set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

select count(1) from t2 where ds='1';

drop table t1;
drop table t2;



Consider the above testcase, some files are generated by mappers, whereas some others are generated by reducers. 
It is therefore possible that some files contain Text in their key, whereas others contain BytesWritable.
Due to that, combinehiveInputFormat record reader may get an error.

Note that, this works in hiveinputformat because different files are not combined in the same mapper - it even works if
we query 't1' because different partitions are not combined in the same mapper",namit,namit,Major,Closed,Fixed,03/Jun/10 22:56,17/Dec/11 00:04
Bug,HIVE-1399,12466678,Nested UDAFs cause Hive Internal Error (NullPointerException),"This query does not make ""real-world"" sense, and I'm guessing it's not even supported by HQL/SQL, but I'm pretty sure that it shouldn't be causing an internal error with a NullPointerException. ""normal"" just has one column called ""val"". I'm running on trunk, svn updated 5 minutes ago, ant clean package.

SELECT percentile(val, percentile(val, 0.5)) FROM normal;

FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:153)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:587)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:708)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:128)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6241)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapGroupByOperator(SemanticAnalyzer.java:2301)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggr1MR(SemanticAnalyzer.java:2860)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:5002)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:5524)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:6055)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:304)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:377)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:303)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)



I've also recreated this error with a GenericUDAF I'm writing, and also with the following:

SELECT percentile(val, percentile()) FROM normal;   
SELECT avg(variance(dob_year)) FROM somedata;     // this makes no sense, but still a NullPointerException",afokken,mayanklahiri,Major,Closed,Fixed,10/Jun/10 18:21,11/Jan/13 00:25
Bug,HIVE-1403,12466924,Reporting progress to JT during closing files in FileSinkOperator,"If there are too many files need to be closed in FileSinkOperator (e.g., if DynamicPartition/FileSpray is turned on), there could be many files generated by each task and they need to be closed at the FileSinkOperator.closeOp(). If the NN is overloaded each file close could take more than 1 sec. This sometimes make JT think the task is dead since it takes too long to close all the files and without any progress report. We need to report progress after a while during file closing. ",nzhang,nzhang,Major,Closed,Fixed,14/Jun/10 20:15,17/Dec/11 00:03
Bug,HIVE-1407,12466947,Add hadoop-*-tools.jar to Eclipse classpath,"A recent change to Hadoop20Shims.java added a dependency to o.a.h.tools.HadoopArchives.
Consequently, we need to add hadoop-*-tools.jar to the eclipse classpath.
",cwsteinbach,cwsteinbach,Major,Closed,Fixed,15/Jun/10 00:40,17/Dec/11 00:03
Bug,HIVE-1409,12466962,File format information is retrieved from first partition,"Currently, if no partitions match the partition predicate, the first partition is used to retrieve the file format. This can cause an problem if the table is set to use RCFile, but the first partition uses SequenceFile:

{code}
java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.hive.ql.io.RCFile$KeyBuffer.()
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)
	at org.apache.hadoop.mapred.SequenceFileRecordReader.createKey(SequenceFileRecordReader.java:65)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.createKey(CombineHiveRecordReader.java:76)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.createKey(CombineHiveRecordReader.java:42)
	at org.apache.hadoop.hive.shims.Hadoop20Shims$CombineFileRecordReader.createKey(Hadoop20Shims.java:212)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.createKey(MapTask.java:167)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:45)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:159)
Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.hive.ql.io.RCFile$KeyBuffer.()
	at java.lang.Class.getConstructor0(Class.java:2706)
	at java.lang.Class.getDeclaredConstructor(Class.java:1985)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)
	... 9 more

{code}

The proposed change is to use the table's metadata in such cases.",pauly,pauly,Major,Closed,Fixed,15/Jun/10 03:26,17/Dec/11 00:03
Bug,HIVE-1411,12467046,DataNucleus throws NucleusException if core-3.1.1 JAR appears more than once on CLASSPATH,"DataNucleus barfs when the core-3.1.1 JAR file appears more than once on the CLASSPATH:

{code}
2010-03-06 12:33:25,565 ERROR exec.DDLTask (SessionState.java:printError(279)) - FAILED: Error in metadata: javax.jdo.JDOFatalInter 
nalException: Unexpected exception caught. 
NestedThrowables: 
java.lang.reflect.InvocationTargetException 
org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDOFatalInternalException: Unexpected exception caught. 
NestedThrowables: 
java.lang.reflect.InvocationTargetException 
at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:258) 
at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:879) 
at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:103) 
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:379) 
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:285) 
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123) 
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181) 
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597) 
at org.apache.hadoop.util.RunJar.main(RunJar.java:156) 
Caused by: javax.jdo.JDOFatalInternalException: Unexpected exception caught. 
NestedThrowables: 
java.lang.reflect.InvocationTargetException 
at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1186)
at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:803) 
at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:698) 
at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:164) 
at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:181)
at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:125) 
at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:104) 
at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62) 
at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) 
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:130)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:146)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:118) 
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.(HiveMetaStore.java:100) 
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.(HiveMetaStoreClient.java:74) 
at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:783) 
at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:794) 
at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:252) 
... 12 more 
Caused by: java.lang.reflect.InvocationTargetException 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597) 
at javax.jdo.JDOHelper$16.run(JDOHelper.java:1956) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.jdo.JDOHelper.invoke(JDOHelper.java:1951) 
at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1159)
... 28 more 
Caused by: org.datanucleus.exceptions.NucleusException: Plugin (Bundle) ""org.eclipse.jdt.core"" is already registered. Ensure you do 
nt have multiple JAR versions of the same plugin in the classpath. The URL ""file:/Users/hadop/hadoop-0.20.1+152/build/ivy/lib/Hadoo 
p/common/core-3.1.1.jar"" is already registered, and you are trying to register an identical plugin located at URL ""file:/Users/hado 
p/hadoop-0.20.1+152/lib/core-3.1.1.jar."" 
at org.datanucleus.plugin.NonManagedPluginRegistry.registerBundle(NonManagedPluginRegistry.java:437)
at org.datanucleus.plugin.NonManagedPluginRegistry.registerBundle(NonManagedPluginRegistry.java:343)
at org.datanucleus.plugin.NonManagedPluginRegistry.registerExtensions(NonManagedPluginRegistry.java:227)
at org.datanucleus.plugin.NonManagedPluginRegistry.registerExtensionPoints(NonManagedPluginRegistry.java:159)
at org.datanucleus.plugin.PluginManager.registerExtensionPoints(PluginManager.java:82) 
at org.datanucleus.OMFContext.(OMFContext.java:164) 
at org.datanucleus.OMFContext.(OMFContext.java:145) 
at org.datanucleus.ObjectManagerFactoryImpl.initialiseOMFContext(ObjectManagerFactoryImpl.java:143)
at org.datanucleus.jdo.JDOPersistenceManagerFactory.initialiseProperties(JDOPersistenceManagerFactory.java:317)
at org.datanucleus.jdo.JDOPersistenceManagerFactory.(JDOPersistenceManagerFactory.java:261)
at org.datanucleus.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:174)
... 36 more 

2010-03-06 12:33:25,575 ERROR ql.Driver (SessionState.java:printError(279)) - FAILED: Execution Error, return code 1 from org.apach 
e.hadoop.hive.ql.exec.DDLTask 
2010-03-06 12:42:30,457 ERROR exec.DDLTask (SessionState.java:printError(279)) - FAILED: Error in metadata: javax.jdo.JDOFatalInter 
nalException: Unexpected exception caught. 
NestedThrowables: 
java.lang.reflect.InvocationTargetException 
org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDOFatalInternalException: Unexpected exception caught. 
NestedThrowables:
{code}",cwsteinbach,cwsteinbach,Major,Closed,Fixed,15/Jun/10 23:56,17/Dec/11 00:03
Bug,HIVE-1412,12467255,CombineHiveInputFormat bug on tablesample,CombineHiveInputFormat should combine all files inside one partition to form a split but should not takes files cross partition boundary. This works for regular table and partitions since all input paths are directory. However this breaks when the input is files (in which case tablesample could be the use case). CombineHiveInputFormat should adjust to the case when input could also be non-directories. ,nzhang,nzhang,Major,Closed,Fixed,17/Jun/10 19:44,17/Dec/11 00:03
Bug,HIVE-1417,12467394,Archived partitions throw error with queries calling getContentSummary,"Assuming you have a src table with a ds='1' partition that is archived in HDFS, the following query will throw an exception

{code}
select count(1) from src where ds='1' group by key;
{code}
",pauly,pauly,Major,Closed,Fixed,19/Jun/10 00:41,17/Dec/11 00:03
Bug,HIVE-1418,12467397,column pruning not working with lateral view,"select myCol from tmp_pyang_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;",he yongqiang,he yongqiang,Major,Closed,Fixed,19/Jun/10 02:47,17/Dec/11 00:03
Bug,HIVE-1421,12467518,problem with sequence and rcfiles are mixed for null partitions,,namit,he yongqiang,Major,Closed,Fixed,22/Jun/10 01:13,17/Dec/11 00:03
Bug,HIVE-1422,12467519,skip counter update when RunningJob.getCounters() returns null,"Under heavy load circumstances on some Hadoop versions, we may get a NPE from trying to dereference a null Counters object.  I don't have a unit test which can reproduce it, but here's an example stack from a production cluster we saw today:

10/06/21 13:01:10 ERROR exec.ExecDriver: Ended Job = job_201005200457_701060 with exception 'java.lang.NullPointerException(null)'
java.lang.NullPointerException
at org.apache.hadoop.hive.ql.exec.Operator.updateCounters(Operator.java:999)
at org.apache.hadoop.hive.ql.exec.ExecDriver.updateCounters(ExecDriver.java:503)
at org.apache.hadoop.hive.ql.exec.ExecDriver.progress(ExecDriver.java:390)
at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:697)
at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:107)
at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)
at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:47)
",jsensarma,jvs,Major,Closed,Fixed,22/Jun/10 01:33,17/Dec/11 00:00
Bug,HIVE-1425,12467619,hive.task.progress should be added to conf/hive-default.xml,"It is defined in HiveConf, and referenced in hive-default.xml by hive.mapjoin.maxsize, but it itself is not defined in hive-default.xml.
",jvs,jvs,Major,Closed,Fixed,22/Jun/10 21:17,17/Dec/11 00:03
Bug,HIVE-1428,12467649,ALTER TABLE ADD PARTITION fails with a remote Thrift metastore,"If the hive cli is configured to use a remote metastore, ALTER TABLE ... ADD PARTITION commands will fail with an error similar to the following:

[pradeepk@chargesize:~/dev/howl]hive --auxpath ult-serde.jar -e ""ALTER TABLE mytable add partition(datestamp = '20091101', srcid = '10',action) location '/user/pradeepk/mytable/20091101/10';""
10/06/16 17:08:59 WARN conf.Configuration: DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively
Hive history file=/tmp/pradeepk/hive_job_log_pradeepk_201006161709_1934304805.txt
FAILED: Error in metadata: org.apache.thrift.TApplicationException: get_partition failed: unknown result
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
[pradeepk@chargesize:~/dev/howl]

This is due to a check that tries to retrieve the partition to see if it exists. If it does not, an attempt is made to pass a null value from the metastore. Since thrift does not support null return values, an exception is thrown.",pradeepkth,pauly,Major,Closed,Fixed,23/Jun/10 01:02,17/Dec/11 00:03
Bug,HIVE-1435,12467851,Upgraded naming scheme causes JDO exceptions,"We recently upgraded from Datanucleus 1.0 to 2.0, which changed some of the defaults for how field names get mapped to datastore identifiers. Because of this change, connecting to an existing database would throw exceptions such as:

2010-06-24 17:59:09,854 ERROR exec.DDLTask (SessionState.java:printError(277)) - FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object ""org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4ccd21c"" using statement ""INSERT INTO `SDS` (`SD_ID`,`NUM_BUCKETS`,`INPUT_FORMAT`,`OUTPUT_FORMAT`,`LOCATION`,`SERDE_ID`,`ISCOMPRESSED`) VALUES (?,?,?,?,?,?,?)"" failed : Unknown column 'ISCOMPRESSED' in 'field list'
NestedThrowables:
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'ISCOMPRESSED' in 'field list'
org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDODataStoreException: Insert of object ""org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4ccd21c"" using statement ""INSERT INTO `SDS` (`SD_ID`,`NUM_BUCKETS`,`INPUT_FORMAT`,`OUTPUT_FORMAT`,`LOCATION`,`SERDE_ID`,`ISCOMPRESSED`) VALUES (?,?,?,?,?,?,?)"" failed : Unknown column 'ISCOMPRESSED' in 'field list'
NestedThrowables:
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'ISCOMPRESSED' in 'field list'
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:325)
        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:2012)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:144)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:107)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:633)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:506)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:384)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:302)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
",pauly,pauly,Major,Closed,Fixed,25/Jun/10 02:59,17/Dec/11 00:03
Bug,HIVE-1440,12467966,FetchOperator(mapjoin) does not work with RCFile,"RCFile needs column prunning's results. But when initializing the mapjoin's fetch operator, the cp's result is not passed to record reader.",he yongqiang,he yongqiang,Major,Closed,Fixed,26/Jun/10 05:57,16/Dec/11 23:59
Bug,HIVE-1444,12468297,"""hdfs"" is hardcoded in few places in the code which inhibits use of other file systems","In quite a few places ""hdfs"" is hardcoded, which is OK for majority of the cases, except when it is not really hdfs, but s3 or any other file system.

The place where it really breaks is:
in ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java :

method: private void applyConstraints(URI fromURI, URI toURI, Tree ast, boolean isLocal)

First few lines are check for file system:
    if (!fromURI.getScheme().equals(""file"")
        && !fromURI.getScheme().equals(""hdfs"")) {
      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(ast,
          ""only \""file\"" or \""hdfs\"" file systems accepted""));
    }

""hdfs"" is hardcoded. 

I don't think you need to have this check at all as you are checking whether filesystem is local or not later on anyway and in regards to non locla file system - if one would be bad one you would get problems or have it look like local before you even come to ""applyConstraints"" method.

",appodictic,yufeldman,Critical,Closed,Fixed,30/Jun/10 21:26,09/Jan/13 10:24
Bug,HIVE-1453,12468766,Make Eclipse launch templates auto-adjust to Hive version number changes,"The changes to prepare for branching out 0.6.0 required [changes to build configuration|http://svn.apache.org/viewvc/hadoop/hive/trunk/build.properties?r1=952877&r2=956430] which caused the launch configurations to break as the jars they referred to were renamed automatically. As a result, none of the launch configurations are working at this point.",aprabhakar,aprabhakar,Major,Closed,Fixed,07/Jul/10 22:13,16/Dec/11 23:59
Bug,HIVE-1454,12468843,insert overwrite and CTAS fail in hive local mode,this is because of the changes in HIVE-543. We switched to using local storage for intermediate data for local mode queries. However there are code paths that are incorrectly allocating intermediate storage where they should be allocating external file system storage (based on table/directory uri). This is causing regressions in running queries in local mode.,jsensarma,jsensarma,Blocker,Closed,Fixed,08/Jul/10 19:13,17/Dec/11 00:04
Bug,HIVE-1455,12468888,lateral view does not work with column pruning ,,pauly,he yongqiang,Major,Closed,Fixed,09/Jul/10 05:00,17/Dec/11 00:03
Bug,HIVE-1461,12469197,Clean up references to 'hive.metastore.local',"'hive.metastore.local' should not be referred directly as a string. Instead, a HiveConf.ConfVar entry should be created and used.",ashutoshc,pauly,Minor,Closed,Fixed,13/Jul/10 21:15,16/Dec/11 23:56
Bug,HIVE-1462,12469202,Reporting progress in FileSinkOperator works in multiple directory case,"HIVE-1403 fixes the issue of timing out issue when closing too many files but it doesn't cover the case that files are under different directories. For the case of dynamic partitioning, it is usually the case so that we still get time-out.",sdong,sdong,Major,Closed,Fixed,13/Jul/10 21:57,17/Dec/11 00:01
Bug,HIVE-1465,12469270,hive-site.xml ${user.name} not replaced for local-file derby metastore connection URL,"Seems that for this parameter

{code}
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:derby:;databaseName=/var/lib/hive/metastore/${user.name}_db;create=true</value>
<description>JDBC connect string for a JDBC metastore</description>
</property>
{code}

$\{user.name\} is never replaced by the actual user name:

{code}
$ ls -la /var/lib/hive/metastore/
total 24
drwxrwxrwt 3 root root 4096 Apr 30 12:37 .
drwxr-xr-x 3 root root 4096 Apr 30 12:25 ..
drwxrwxr-x 5 hadoop hadoop 4096 Apr 30 12:37 ${user.name}_db
{code}",cwsteinbach,cwsteinbach,Blocker,Closed,Fixed,14/Jul/10 17:55,17/Dec/11 00:01
Bug,HIVE-1470,12469504,percentile_approx() fails with more than 1 reducer,The larger issue is that a UDAF that has variable return types needs two inner Evaluator classes. This patch fixes a NullPointerException bug that is only encountered when partial aggregations are invoked.,mayanklahiri,mayanklahiri,Major,Closed,Fixed,16/Jul/10 22:37,16/Dec/11 23:59
Bug,HIVE-1471,12469513,CTAS should unescape the column name in the select-clause. ,"The following query 
{{{
create table T as select `to` from S;
}}}
failed since `to` should be unescaped before creating the table. 
",nzhang,nzhang,Major,Closed,Fixed,17/Jul/10 01:05,16/Dec/11 23:59
Bug,HIVE-1473,12469726,plan file should have a high replication factor,it should be set to 10 or something like that (just like job.xml). ,,jsensarma,Minor,Closed,Fixed,20/Jul/10 16:03,16/Dec/11 23:59
Bug,HIVE-1475,12469832,.gitignore files being placed in test warehouse directories causing build failure,"from last comments on HIVE-1463:

test warehouse is being polluted with .gitignore files. these need to be removed.",jsensarma,jsensarma,Critical,Closed,Fixed,21/Jul/10 15:47,17/Dec/11 00:00
Bug,HIVE-1489,12470220,TestCliDriver -Doverwrite=true does not put the file in the correct directory,"When adding a new file in clientpositive with -Doverwrite=true, the output file was in ql/ rather than ql/src/test/results/clientpositive. ",nzhang,nzhang,Major,Closed,Fixed,27/Jul/10 00:48,17/Dec/11 00:00
Bug,HIVE-1491,12470293,fix or disable loadpart_err.q,"This test fails sporadically due to a race condition, which is annoying since it hinders pre-commit testing of patches.  I'm going to disable it unless someone has a fix.
",jvs,jvs,Major,Closed,Fixed,27/Jul/10 20:53,16/Dec/11 23:59
Bug,HIVE-1492,12470397,FileSinkOperator should remove duplicated files from the same task based on file sizes,FileSinkOperator.jobClose() calls Utilities.removeTempOrDuplicateFiles() to retain only one file for each task. A task could produce multiple files due to failed attempts or speculative runs. The largest file should be retained rather than the first file for each task. ,nzhang,nzhang,Major,Closed,Fixed,29/Jul/10 00:48,17/Dec/11 00:03
Bug,HIVE-1494,12470556,Index followup: remove sort by clause and fix a bug in collect_set udaf,,he yongqiang,he yongqiang,Major,Closed,Fixed,30/Jul/10 23:34,17/Dec/11 00:01
Bug,HIVE-1501,12470567,"when generating reentrant INSERT for index rebuild, quote identifiers using backticks","Yongqiang, you mentioned that you weren't able to do this due to SORT BY not accepting them.  The SORT BY is gone now as of HIVE-1494 (and SORT BY needs to be fixed anyway).
",sberghel,jvs,Major,Closed,Fixed,31/Jul/10 00:56,16/Dec/11 23:59
Bug,HIVE-1508,12470678,Add cleanup method to HiveHistory class,"Running hive server for long time > 90 minutes results in too many open file-handles, eventually causing the server to crash as the server runs out of file handle.
Actual bug as described by Carl Steinbach:
the hive_job_log_* files are created by the HiveHistory class. This class creates a PrintWriter for writing to the file, but never closes the writer. It looks like we need to add a cleanup method to HiveHistory that closes the PrintWriter and does any other necessary cleanup. ",appodictic,anuragphadke,Blocker,Closed,Fixed,02/Aug/10 21:49,17/Dec/11 00:01
Bug,HIVE-1509,12470776,Monitor the working set of the number of files ,,nzhang,namit,Major,Closed,Fixed,04/Aug/10 00:07,16/Dec/11 23:59
Bug,HIVE-1510,12470780,HiveCombineInputFormat should not use prefix matching to find the partitionDesc for a given path,"set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

drop table combine_3_srcpart_seq_rc;

create table combine_3_srcpart_seq_rc (key int , value string) partitioned by (ds string, hr string) stored as sequencefile;

insert overwrite table combine_3_srcpart_seq_rc partition (ds=""2010-08-03"", hr=""00"") select * from src;

alter table combine_3_srcpart_seq_rc set fileformat rcfile;
insert overwrite table combine_3_srcpart_seq_rc partition (ds=""2010-08-03"", hr=""001"") select * from src;

desc extended combine_3_srcpart_seq_rc partition(ds=""2010-08-03"", hr=""00"");
desc extended combine_3_srcpart_seq_rc partition(ds=""2010-08-03"", hr=""001"");

select * from combine_3_srcpart_seq_rc where ds=""2010-08-03"" order by key;

drop table combine_3_srcpart_seq_rc;


will fail.",he yongqiang,he yongqiang,Major,Closed,Fixed,04/Aug/10 00:50,16/Dec/11 23:59
Bug,HIVE-1520,12471140,hive.mapred.local.mem should only be used in case of local mode job submissions,"Currently - whenever we submit a map-reduce job via a child jvm process, hive sets HADOOP_HEAPSIZE to hive.mapred.local.mem (thereby limiting the max heap memory of the child jvm). the assumption being that we are submitting a job for local mode execution and different memory limits apply for that.

however - one can submit jobs via a child jvm for non local mode execution as well. This is useful, for example, if hive wants to submit jobs via different hadoop clients (for sending jobs to different hadoop clusters). in such case, we can use the 'hive.exec.submitviachild' and 'hadoop.bin.path' to dispatch job via an alternate hadoop client install point. however in such case, we don't need to set HADOOP_HEAPSIZE. all we are using the child jvm is to run the small bit of hive code that submits the job (and not for local mode execution).

in this case - we shouldn't be setting the child jvm's memory limit and should leave it to what the parent's value is.",,jsensarma,Major,Closed,Fixed,09/Aug/10 19:38,16/Dec/11 23:59
Bug,HIVE-1523,12471157,ql tests no longer work in miniMR mode,"as per title. here's the first exception i see:


2010-08-09 18:05:11,259 ERROR hive.log (MetaStoreUtils.java:logAndThrowMetaException(743)) - Got exception: java.io.FileNotFoun\
dException File file:/build/ql/test/data/warehouse/dest_j1 does not exist.
2010-08-09 18:05:11,259 ERROR hive.log (MetaStoreUtils.java:logAndThrowMetaException(746)) - java.io.FileNotFoundException: Fil\
e file:/build/ql/test/data/warehouse/dest_j1 does not exist.
  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:361)
  at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:245)
  at org.apache.hadoop.hive.metastore.Warehouse.mkdirs(Warehouse.java:136)
  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:677)
",jsensarma,jsensarma,Major,Closed,Fixed,10/Aug/10 01:13,02/May/13 02:29
Bug,HIVE-1524,12471165,parallel execution failed if mapred.job.name is set,"The plan file name was generated based on mapred.job.name. If the user specify mapred.job.name before the query, two parallel queries will have conflict plan file name. ",nzhang,nzhang,Major,Closed,Fixed,10/Aug/10 06:32,17/Dec/11 00:03
Bug,HIVE-1532,12471366,Replace globStatus with listStatus inside Hive.java's replaceFiles.,"globStatus expects a regular expression,  so if there is special characters (like '{' , '[') in the filepath, this function will fail.

We should be able to replace this call with listStatus easily since we are not passing regex to replaceFiles(). The only places replaceFiles is called is in loadPartition and Table's replaceFiles.",he yongqiang,he yongqiang,Major,Closed,Fixed,12/Aug/10 02:42,16/Dec/11 23:59
Bug,HIVE-1534,12471377,Join filters do not work correctly with outer joins," SELECT * FROM T1 LEFT OUTER JOIN T2 ON (T1.c1=T2.c2 AND T1.c1 < 10)
and  SELECT * FROM T1 RIGHT OUTER JOIN T2 ON (T1.c1=T2.c2 AND T2.c1 < 10)
do not give correct results.",amareshwari,amareshwari,Major,Closed,Fixed,12/Aug/10 06:05,06/Sep/16 23:39
Bug,HIVE-1535,12471477,alter partition should throw exception if the specified partition does not exist.,,he yongqiang,he yongqiang,Major,Closed,Fixed,13/Aug/10 01:12,16/Dec/11 23:59
Bug,HIVE-1538,12471503,FilterOperator is applied twice with ppd on.,"With hive.optimize.ppd set to true, FilterOperator is applied twice. And it seems second operator is always filtering zero rows.",amareshwari,amareshwari,Major,Closed,Fixed,13/Aug/10 07:44,28/Jan/16 03:01
Bug,HIVE-1539,12471563,Concurrent metastore threading problem ,"When running hive as a service and running a high number of queries concurrently I end up with multiple threads running at 100% cpu without any progress.

Looking at these threads I notice this thread(484e):
at org.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:598)

But on a different thread(63a2):
at org.apache.hadoop.hive.metastore.model.MStorageDescriptor.jdoReplaceField(MStorageDescriptor.java)
",bennies,bennies,Major,Resolved,Fixed,13/Aug/10 15:36,28/Sep/16 18:58
Bug,HIVE-1547,12471770,Unarchiving operation throws NPE,"Unarchiving a partition throws a null pointer exception similar to the following:

2010-08-16 12:44:18,801 ERROR exec.DDLTask (SessionState.java:printError(277)) - Failed with exception null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

This error seems to be DFS specific, as local file system in the unit tests don't catch this.",pauly,pauly,Major,Closed,Fixed,17/Aug/10 02:09,17/Dec/11 00:01
Bug,HIVE-1548,12471782,populate inputs and outputs for all statements,"Currently, they are only populated for queries - and not for most of the DDLs.

The pre and post execution hooks do not get the correct values.
It would also be very useful for locking",namit,namit,Major,Closed,Fixed,17/Aug/10 06:24,17/Dec/11 00:01
Bug,HIVE-1556,12471921,Fix TestContribCliDriver test,"Due to https://issues.apache.org/jira/browse/HIVE-1548, TestContribCliDriver is broken. Some test results need to be updated
",namit,namit,Major,Closed,Fixed,18/Aug/10 16:51,16/Dec/11 23:59
Bug,HIVE-1561,12471952,smb_mapjoin_8.q returns different results in miniMr mode,"follow on to HIVE-1523:

ant -Dclustermode=miniMR -Dtestcase=TestCliDriver -Dqfile=smb_mapjoin_8.q test

POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key

official results:
4 val_356 NULL  NULL
NULL  NULL  484 val_169
2000  val_169 NULL  NULL
NULL  NULL  3000  val_169
4000  val_125 NULL  NULL


in minimr mode:
2000  val_169 NULL  NULL
4 val_356 NULL  NULL
2000  val_169 NULL  NULL
4000  val_125 NULL  NULL
NULL  NULL  5000  val_125
",he yongqiang,jsensarma,Major,Closed,Fixed,18/Aug/10 22:05,17/Dec/11 00:01
Bug,HIVE-1563,12471959,HBase tests broken,"Broken by HIVE-1548, which did not update all log files.
",jvs,jvs,Major,Closed,Fixed,19/Aug/10 00:55,17/Dec/11 00:00
Bug,HIVE-1564,12471968,bucketizedhiveinputformat.q fails in minimr mode,"followup to HIVE-1523:

ant -Dtestcase=TestCliDriver -Dqfile=bucketizedhiveinputformat.q -Dclustermode=miniMR  clean-test test 

    [junit] Begin query: bucketizedhiveinputformat.q
    [junit] Exception: null
    [junit] java.lang.AssertionError
    [junit]   at org.apache.hadoop.hive.ql.exec.ExecDriver.showJobFailDebugInfo(ExecDriver.java:788)
    [junit]   at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:624)
    [junit]   at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:120)

ExecDriver.java:788
        // These tasks should have come from the same job.                                                                  
        assert(ti.getJobId() == jobId);
",he yongqiang,jsensarma,Major,Closed,Fixed,19/Aug/10 07:06,21/Jun/12 21:54
Bug,HIVE-1570,12472043,referencing an added file by it's name in a transform script does not work in hive local mode,"Yongqiang tried this and it fails in local mode:

add file ../data/scripts/dumpdata_script.py;

select count(distinct subq.key) from
(FROM src MAP src.key USING 'python dumpdata_script.py' AS key WHERE src.key = 10) subq;


this needs to be fixed because it means we cannot choose local mode automatically in case of transform scripts (since different paths need to be used for cluster vs. local mode execution)",jsensarma,jsensarma,Major,Closed,Fixed,19/Aug/10 22:23,16/Dec/11 23:59
Bug,HIVE-1578,12472132,Add conf. property hive.exec.show.job.failure.debug.info to enable/disable displaying link to the task with most failures,"If a job fails, Hive currently displays a link to the task with the most number of failures for easy access to the error logs. However, generating the link may require many RPC's to get all the task completion events, adding a delay of up to 30 minutes. This patch adds a configuration variable to control whether the link is generated. Turning off this feature would also disable automatic debugging tips generated by heuristics reading from the error logs.",pauly,pauly,Major,Closed,Fixed,20/Aug/10 23:47,16/Dec/11 23:59
Bug,HIVE-1580,12472156,cleanup ExecDriver.progress,"a few problems:

- if a job is retired - then counters cannot be obtained and a stack trace is printed out (from history code). this confuses users
- too many calls to getCounters. after a job has been detected to be finished - there are quite a few more calls to get the job status and the counters. we need to figure out a way to curtail this - in busy clusters the gap between the job getting finished and the hive client noticing is very perceptible and impacts user experience.

calls to getCounters are very expensive in 0.20 as they grab a jobtracker global lock (something we have fixed internally at FB)",jsensarma,jsensarma,Major,Closed,Fixed,21/Aug/10 11:38,17/Dec/11 00:01
Bug,HIVE-1583,12472225,Hive should not override Hadoop specific system properties,"Currently Hive overrides Hadoop specific system properties such as HADOOP_CLASSPATH.
It does the following in bin/hive script :
{code}
# pass classpath to hadoop
export HADOOP_CLASSPATH=${CLASSPATH}
{code}
Instead, It should honor the value of HADOOP_CLASSPATH set by client by appending CLASSPATH to it.",thiruvel,amareshwari,Major,Closed,Fixed,23/Aug/10 04:18,17/Dec/11 00:01
Bug,HIVE-1584,12472292,wrong log files in contrib client positive,TestContribCliDriver still gets some diffs,namit,namit,Major,Closed,Fixed,23/Aug/10 20:22,16/Dec/11 23:59
Bug,HIVE-1589,12472415,Add HBase/ZK JARs to Eclipse classpath,The eclipse configuration was broken by the addition of HBase and ZK JARs in HIVE-1293.,cwsteinbach,cwsteinbach,Major,Closed,Fixed,24/Aug/10 21:34,16/Dec/11 23:59
Bug,HIVE-1592,12472424,ProxyFileSystem.close calls super.close twice.,"  public void close() throws IOException {
    super.close();
    super.close();
  }
",ashutoshc,jvs,Minor,Closed,Fixed,24/Aug/10 22:54,16/Dec/11 23:56
Bug,HIVE-1593,12472432,udtf_explode.q is an empty file,"jsichi-mac:clientpositive jsichi$ pwd
/Users/jsichi/open/hive-trunk/ql/src/test/queries/clientpositive
jsichi-mac:clientpositive jsichi$ cat udtf_explode.q 
jsichi-mac:clientpositive jsichi$ 
",cwsteinbach,jvs,Minor,Closed,Fixed,25/Aug/10 01:38,16/Dec/11 23:59
Bug,HIVE-1594,12472492,Typo of hive.merge.size.smallfiles.avgsize prevents change of value,"The setting is described as <name>hive.merge.size.smallfiles.avgsize</name>, however common/src/java/org/apache/hadoop/hive/conf/HiveConf.java reads it as ""hive.merge.smallfiles.avgsize"" (note the missing '.size.') so the user's setting has no effect and the value is stuck at the default of 16MB.",goosmurf,goosmurf,Minor,Closed,Fixed,25/Aug/10 15:44,17/Dec/11 00:03
Bug,HIVE-1595,12472499,job name for alter table <T> archive partition <P> is not correct,"For some internal runs, I saw the job name as hadoop-0.20.1-tools.jar, which makes it difficult to identify",sohanjain,namit,Major,Closed,Fixed,25/Aug/10 16:36,16/Dec/11 23:55
Bug,HIVE-1598,12472547,use SequenceFile rather than TextFile format for hive query results,"Hive query's result is written to a temporary directory first and then FetchTask takes the files and display it to the users. Currently the file format used for the resulting file is TextFile format. This could cause incorrect result display if some string typed column contains new lines, which are used as record delimiters in TextInputFormat. Switching to SequenceFile format will solve this problem. ",nzhang,nzhang,Major,Closed,Fixed,26/Aug/10 00:35,08/Aug/14 02:31
Bug,HIVE-1600,12472649,need to sort hook input/output lists for test result determinism,"Begin forwarded message:

From: Ning Zhang <nzhang@facebook.com>
Date: August 26, 2010 2:47:26 PM PDT
To: John Sichi <jsichi@facebook.com>
Cc: ""hive-dev@hadoop.apache.org"" <hive-dev@hadoop.apache.org>
Subject: Re: failure in load_dyn_part1.q

Yes I saw this error before but if it does not repro. So it's probably an ordering issue in POSTHOOK. 

On Aug 26, 2010, at 2:39 PM, John Sichi wrote:

I'm seeing this failure due to a result diff when running tests on latest trunk:

POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
-POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=11
-POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=12
POSTHOOK: Output: default@nzhang_part1@ds=2008-04-08/hr=11
POSTHOOK: Output: default@nzhang_part1@ds=2008-04-08/hr=12
+POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=11
+POSTHOOK: Output: default@nzhang_part2@ds=2008-12-31/hr=12

Did something change recently?  Or are we missing a Java-level sort on the input/output list for determinism?

JVS

",jvs,jvs,Major,Closed,Fixed,26/Aug/10 21:53,16/Dec/11 23:59
Bug,HIVE-1601,12472665,Hadoop 0.17 ant test broken by HIVE-1523,"
compile-test:
   [javac] /data/users/jsichi/open/hive-trunk/build-common.xml:304: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
   [javac] Compiling 33 source files to /data/users/jsichi/open/hive-trunk/build/ql/test/classes

BUILD FAILED
/data/users/jsichi/open/hive-trunk/build.xml:168: The following error occurred while executing this line:
/data/users/jsichi/open/hive-trunk/build.xml:105: The following error occurred while executing this line:
/data/users/jsichi/open/hive-trunk/build-common.xml:304: /data/users/jsichi/open/hive-trunk/build/hadoopcore/hadoop-0.17.2.1/lib/jsp-2.1 does not exist.",jsensarma,jvs,Major,Closed,Fixed,27/Aug/10 01:57,16/Dec/11 23:59
Bug,HIVE-1607,12472892,Reinstate and deprecate IMetaStoreClient methods removed in HIVE-675,"Several methods were removed from the IMetaStoreClient interface as part of HIVE-675:

{code}

  /**
   * Drop the table.
   *
   * @param tableName
   *          The table to drop
   * @param deleteData
   *          Should we delete the underlying data
   * @throws MetaException
   *           Could not drop table properly.
   * @throws UnknownTableException
   *           The table wasn't found.
   * @throws TException
   *           A thrift communication error occurred
   * @throws NoSuchObjectException
   *           The table wasn't found.
   */
  public void dropTable(String tableName, boolean deleteData)
      throws MetaException, UnknownTableException, TException,
      NoSuchObjectException;

  /**
   * Get a table object.
   *
   * @param tableName
   *          Name of the table to fetch.
   * @return An object representing the table.
   * @throws MetaException
   *           Could not fetch the table
   * @throws TException
   *           A thrift communication error occurred
   * @throws NoSuchObjectException
   *           In case the table wasn't found.
   */
  public Table getTable(String tableName) throws MetaException, TException,
      NoSuchObjectException;

  public boolean tableExists(String databaseName, String tableName) throws MetaException,
      TException, UnknownDBException;

{code}

These methods should be reinstated with a deprecation warning.
",cwsteinbach,cwsteinbach,Major,Closed,Fixed,30/Aug/10 23:44,16/Dec/11 23:59
Bug,HIVE-1608,12472958,use sequencefile as the default for storing intermediate results,"The only argument for having a text file for storing intermediate results seems to be better debuggability.

But, tailing a sequence file is possible, and it should be more space efficient",ctang,namit,Major,Closed,Fixed,31/Aug/10 18:52,21/Jun/16 15:53
Bug,HIVE-1613,12473263,hive --service jar looks for hadoop version but was not defined,hive --service jar fails. I have to open another ticket to clean up the scripts and unify functions like version detection.,appodictic,appodictic,Blocker,Closed,Fixed,03/Sep/10 17:52,17/Dec/11 00:03
Bug,HIVE-1614,12473265,UDTF json_tuple should return null row when input is not a valid JSON string,"If the input column is not a valid JSON string, json_tuple will not return anything but this will prevent the downstream operators to access the left-hand side table. We should output a NULL row instead, similar to when the input column is a NULL value. ",nzhang,nzhang,Major,Closed,Fixed,03/Sep/10 17:56,16/Dec/11 23:59
Bug,HIVE-1615,12473302,Web Interface JSP needs Refactoring for removed meta store methods,Some meta store methods being called from JSP have been removed. Really should prioritize compiling jsp into servlet code again.,appodictic,appodictic,Blocker,Closed,Fixed,04/Sep/10 04:46,17/Dec/11 00:03
Bug,HIVE-1628,12473838,Fix Base64TextInputFormat to be compatible with commons codec 1.4,Commons-codec 1.4 made an incompatible change to the Base64 class that made line-wrapping default (boo!). This breaks the Base64TextInputFormat in contrib. This patch adds some simple reflection to use the new constructor that uses the old behavior.,tlipcon,tlipcon,Major,Closed,Fixed,10/Sep/10 18:35,17/Dec/11 00:01
Bug,HIVE-1629,12473852,Patch to fix hashCode method in DoubleWritable class,"A patch to fix the hashCode() method of DoubleWritable class of Hive.
It prevents the HashMap (of type DoubleWritable) from behaving as LinkedList.",vaggarw,vaggarw,Major,Closed,Fixed,10/Sep/10 20:17,16/Dec/11 23:59
Bug,HIVE-1630,12473878,bug in NO_DROP,"If the table is marked NO_DROP, we should still be able to drop old partitions.",sdong,namit,Major,Closed,Fixed,10/Sep/10 23:18,17/Dec/11 00:01
Bug,HIVE-1631,12473884,"JDBC driver returns wrong precision, scale, or column size for some data types","For some data types, these methods return values that do not conform to the JDBC spec:

org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getPrecision(int)
org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getScale(int)
org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnDisplaySize(int)
org.apache.hadoop.hive.jdbc.JdbcColumn.getColumnSize()
",phunt,slider,Minor,Closed,Fixed,11/Sep/10 00:44,16/Dec/11 23:55
Bug,HIVE-1632,12473944,Column length not sufficient for large STRUCT definitions,"Can be reproduced by adding the following table:
{code}hive> CREATE TABLE test (big struct<prop1: int,
                                    prop2: int,
                                    prop3: int,
                                    prop4: int,
                                    prop5: int,
                                    prop6: int,
                                    prop7: int,
                                    prop8: int,
                                    prop9: int,
                                    prop10: int,
                                    prop10: int,
                                    prop11: int,
                                    prop12: int,
                                    prop13: int,
                                    prop14: int,
                                    prop15: int,
                                    prop16: int,
                                    prop17: int,
                                    prop18: int,
                                    prop19: int>);{code}

Error:
{noformat}FAILED: Error in metadata: javax.jdo.JDODataStoreException: Add request failed : INSERT INTO COLUMNS (SD_ID,COMMENT,""COLUMN_NAME"",TYPE_NAME,INTEGER_IDX) VALUES (?,?,?,?,?) 
NestedThrowables:
java.sql.SQLDataException: A truncation error was encountered trying to shrink VARCHAR 'struct<prop1:int,prop2:int,prop3:int,prop4:int,prop5:int,pro&' to length 128.
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask{noformat}

Workaround:
Change column length in metastore. Derby example: {{ALTER TABLE columns ALTER type_name SET DATA TYPE VARCHAR(1000);}}",,wnagele,Trivial,Closed,Fixed,12/Sep/10 21:27,02/May/13 02:29
Bug,HIVE-1633,12473954,"CombineHiveInputFormat fails with ""cannot find dir for emptyFile""",,sreekanth,amareshwari,Major,Closed,Fixed,13/Sep/10 06:17,19/Jan/16 21:47
Bug,HIVE-1639,12474206,ExecDriver.addInputPaths() error if partition name contains a comma,"The ExecDriver.addInputPaths() calls FileInputFormat.addPaths(), which takes a comma-separated string representing a set of paths. If the path name of a input file contains a comma, this code throw an exception: java.lang.IllegalArgumentException: Can not create a Path from an empty string.

Instead of calling FileInputFormat.addPaths(), ExecDriver.addInputPaths should iterate all paths and call FileInputFormat.addInputPath. ",nzhang,nzhang,Major,Closed,Fixed,15/Sep/10 19:13,16/Dec/11 23:59
Bug,HIVE-1647,12474306,Incorrect initialization of thread local variable inside IOContext ( implementation is not threadsafe ) ,"Bug in org.apache.hadoop.hive.ql.io.IOContext
in relation to initialization of thread local variable.
 
public class IOContext {
 
  private static ThreadLocal<IOContext> threadLocal = new ThreadLocal<IOContext>(){ };
 
  static {
    if (threadLocal.get() == null) {
      threadLocal.set(new IOContext());
    }
  }
 
In a multi-threaded environment, the thread that gets to load the class first for the JVM (assuming threads share the classloader),
gets to initialize itself correctly by executing the code in the static block. Once the class is loaded, 
any subsequent threads would  have their respective threadlocal variable as null.  Since IOContext
is set during initialization of HiveRecordReader, In a scenario where multiple threads get to acquire
 an instance of HiveRecordReader, it would result in a NPE for all but the first thread that gets to load the class in the VM.
 
Is the above scenario of multiple threads initializing HiveRecordReader a typical one ?  or we could just provide the following fix...
 
  private static ThreadLocal<IOContext> threadLocal = new ThreadLocal<IOContext>(){
    protected synchronized IOContext initialValue() {
      return new IOContext();
    }  
  };
",liyin,ramang,Major,Closed,Fixed,16/Sep/10 18:31,16/Dec/11 23:59
Bug,HIVE-1650,12474357,TestContribNegativeCliDriver fails,,namit,namit,Major,Closed,Fixed,17/Sep/10 06:12,16/Dec/11 23:59
Bug,HIVE-1656,12474461,All TestJdbcDriver test cases fail in Eclipse unless a property is added in run config,"All TestJdbcDriver test cases fail in Eclipse, unless I add the following property in the TestJdbc run configuration (""Arguments"" tab --> ""VM arguments"" box):

-Dtest.warehouse.dir=""${workspace_loc:trunk}/build/ql/test/data/warehouse""
",slider,slider,Major,Closed,Fixed,18/Sep/10 01:49,17/Dec/11 00:00
Bug,HIVE-1657,12474579,join results are displayed wrongly for some complex joins using select *,"Noticed that the output of the join is displayed wrongly for queries involving more than one table and mixture of left and outer joins, with different join conditions.

For ex: SELECT * from T1 a RIGHT OUTER JOIN T2 b ON (a.value=b.value) LEFT OUTER JOIN T3 c ON (b.key=c.key);
displays the table T2 first, then T1 and T3.
",amareshwari,amareshwari,Major,Closed,Fixed,20/Sep/10 10:06,17/Dec/11 00:01
Bug,HIVE-1658,12474647,Fix describe [extended] column formatting,"When displaying the column schema, the formatting should follow should be 

name<TAB>type<TAB>comment<NEWLINE>

to be inline with the previous formatting style for backward compatibility.",thiruvel,pauly,Major,Closed,Fixed,21/Sep/10 01:19,16/Dec/11 23:59
Bug,HIVE-1663,12474759,ql/src/java/org/apache/hadoop/hive/ql/parse/SamplePruner.java is empty,we should remove this empty file,he yongqiang,he yongqiang,Major,Closed,Fixed,21/Sep/10 21:46,16/Dec/11 23:59
Bug,HIVE-1664,12474879,Eclipse build broken,"After updating trunk to r999644, Eclipse build is broken.
",slider,slider,Major,Closed,Fixed,22/Sep/10 22:39,17/Dec/11 00:01
Bug,HIVE-1670,12475097,MapJoin throws EOFExeption when the mapjoined table has 0 column selected,select /*+mapjoin(b) */ sum(a.key) from src a join src b on (a.key=b.key); throws EOFException,nzhang,nzhang,Major,Closed,Fixed,25/Sep/10 03:37,16/Dec/11 23:59
Bug,HIVE-1671,12475177,multithreading on Context.pathToCS,"we having 2 threads running at 100%

With a stacktrace like this:

""Thread-16725"" prio=10 tid=0x00007ff410662000 nid=0x497d runnable [0x00000000442eb000]
   java.lang.Thread.State: RUNNABLE
        at java.util.HashMap.get(HashMap.java:303)
        at org.apache.hadoop.hive.ql.Context.getCS(Context.java:524)
        at org.apache.hadoop.hive.ql.exec.Utilities.getInputSummary(Utilities.java:1369)
        at org.apache.hadoop.hive.ql.exec.MapRedTask.estimateNumberOfReducers(MapRedTask.java:329)
        at org.apache.hadoop.hive.ql.exec.MapRedTask.setNumberOfReducers(MapRedTask.java:297)
        at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:84)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:47)
",bennies,bennies,Major,Closed,Fixed,27/Sep/10 10:03,17/Dec/11 00:00
Bug,HIVE-1673,12475440,Create table bug causes the row format property lost when serde is specified.,"An example:

create table src_rc_serde_yongqiang(key string, value string) ROW FORMAT  DELIMITED FIELDS TERMINATED BY '\\0' stored as rcfile; 
will lost the row format information.",he yongqiang,he yongqiang,Major,Closed,Fixed,29/Sep/10 19:55,16/Dec/11 23:59
Bug,HIVE-1674,12475467,count(*) returns wrong result when a mapper returns empty results,select count(*) from src where false; will return # of mappers rather than 0. ,nzhang,nzhang,Major,Closed,Fixed,30/Sep/10 01:12,16/Dec/11 23:59
Bug,HIVE-1678,12475577,NPE in MapJoin ,"The query with two map joins and a group by fails with following NPE:

Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:177)
        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:464)
",amareshwari,amareshwari,Major,Closed,Fixed,01/Oct/10 05:41,16/Dec/11 23:59
Bug,HIVE-1681,12475585,ObjectStore.commitTransaction() does not properly handle transactions that have already been rolled back,"Here's the code for ObjectStore.commitTransaction() and ObjectStore.rollbackTransaction():

{code}
  public boolean commitTransaction() {
    assert (openTrasactionCalls >= 1);
    if (!currentTransaction.isActive()) {
      throw new RuntimeException(
          ""Commit is called, but transaction is not active. Either there are""
              + "" mismatching open and close calls or rollback was called in the same trasaction"");
    }
    openTrasactionCalls--;
    if ((openTrasactionCalls == 0) && currentTransaction.isActive()) {
      transactionStatus = TXN_STATUS.COMMITED;
      currentTransaction.commit();
    }
    return true;
  }

  public void rollbackTransaction() {
    if (openTrasactionCalls < 1) {
      return;
    }
    openTrasactionCalls = 0;
    if (currentTransaction.isActive()
        && transactionStatus != TXN_STATUS.ROLLBACK) {
      transactionStatus = TXN_STATUS.ROLLBACK;
      // could already be rolled back
      currentTransaction.rollback();
    }
  }

{code}

Now suppose a nested transaction throws an exception which results
in the nested pseudo-transaction calling rollbackTransaction(). This causes
rollbackTransaction() to rollback the actual transaction, as well as to set 
openTransactionCalls=0 and transactionStatus = TXN_STATUS.ROLLBACK.
Suppose also that this nested transaction squelches the original exception.
In this case the stack will unwind and the caller will eventually try to commit the
transaction by calling commitTransaction() which will see that currentTransaction.isActive() returns
FALSE and will throw a RuntimeException. The fix for this problem is
that commitTransaction() needs to first check transactionStatus and return immediately
if transactionStatus==TXN_STATUS.ROLLBACK.
",cwsteinbach,cwsteinbach,Major,Closed,Fixed,01/Oct/10 08:29,17/Dec/11 00:03
Bug,HIVE-1688,12475649,"In the MapJoinOperator, the code uses tag as alias, which is not always true","In the MapJoinOperator and SMBMapJoinOperator, the code uses tag as alias, which is not always true.
Actually, alias = order[tag]

",liyin,liyin,Major,Closed,Fixed,01/Oct/10 17:51,16/Dec/11 23:59
Bug,HIVE-1691,12475866,ANALYZE TABLE command should check columns in partition spec,"ANALYZE TABEL PARTITION (col1, col2,...) should check whether col1, col2 etc are partition columns.",nzhang,nzhang,Major,Closed,Fixed,05/Oct/10 17:02,16/Dec/11 23:59
Bug,HIVE-1697,12476719,Migration scripts should increase size of PARAM_VALUE in PARTITION_PARAMS,The migration scripts should increase the size of column PARAM_VALUE in the table PARTITION_PARAMS to 4000 chars to follow the description in package.jdo.,pauly,pauly,Major,Closed,Fixed,06/Oct/10 23:02,17/Dec/11 00:03
Bug,HIVE-1699,12477102,incorrect partition pruning ANALYZE TABLE,"If table T is partitioned, ANALYZE TABLE T PARTITION (...) COMPUTE STATISTICS; will gather stats for all partitions even though partition spec only chooses a subset. 

",nzhang,nzhang,Major,Closed,Fixed,12/Oct/10 06:13,16/Dec/11 23:59
Bug,HIVE-1707,12477245,bug when different partitions are present in different dfs,"The following does not work:

create table T -> default location dfs1
insert overwrite T partition (ds='1') select * from src;

alter table T location 'dfs2';
insert overwrite T partition (ds='1') select * from src;


It tries to insert back in dfs1 - due to which the move task fails.
It would be cleaner to keep the same semantics as fileformat - whenever a partition is being inserted into, it
inherits the properties from the table. So, after the insert, the partition should belong to dfs1.
It does not matter whether the partition exists before or not,.",he yongqiang,namit,Major,Closed,Fixed,13/Oct/10 15:54,16/Dec/11 23:59
Bug,HIVE-1711,12477289,CREATE TABLE LIKE should not set stats in the new table,CREATE TABLE T LIKE S; will copy every parameters from T to S. It should not copy table level stats.,nzhang,nzhang,Major,Closed,Fixed,13/Oct/10 23:32,16/Dec/11 23:59
Bug,HIVE-1712,12477376,Migrating metadata from derby to mysql thrown NullPointerException,"Exported derby data to csv, loaded data into mysql and ran hive query which worked in derby and got the following exception

2010-10-16 08:57:29,080 INFO  metastore.ObjectStore (ObjectStore.java:setConf(106)) - Initialized ObjectStore
2010-10-16 08:57:29,552 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logStartFunction(171)) - 0: get_table : db=default tbl=testimport
2010-10-16 08:57:30,140 ERROR metadata.Hive (Hive.java:getTable(395)) - java.lang.NullPointerException
        at java.util.Hashtable.put(Hashtable.java:394)
        at java.util.Hashtable.putAll(Hashtable.java:466)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:520)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:489)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:381)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:333)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:683)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5200)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)
        at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)",bterm,bterm,Major,Closed,Fixed,14/Oct/10 17:17,16/Dec/11 23:59
Bug,HIVE-1713,12477379,duplicated MapRedTask in Multi-table inserts mixed with FileSinkOperator and ReduceSinkOperator,A multi-table insert with FileSinkOperator and ReduceSinkOperator have the same MapRedTask added to the rootTask twice. This results in failed execution or wrong results. ,nzhang,nzhang,Major,Closed,Fixed,14/Oct/10 17:42,16/Dec/11 23:59
Bug,HIVE-1716,12477488,make TestHBaseCliDriver use dynamic ports to avoid conflicts with already-running services,"ant test -Dhadoop.version=0.20.0 -Dtestcase=TestHBaseCliDriver:

.... 
   [junit] org.apache.hadoop.hbase.client.NoServerForRegionException: Timed out trying to locate root region
    [junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRootRegion(HConnectionManager.java:976)
    [junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:625)
    [junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.relocateRegion(HConnectionManager.java:607)
    [junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:738)
    [junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:634)
    [junit]     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:601)
    [junit]     at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:128)
    [junit]     at org.apache.hadoop.hive.hbase.HBaseTestSetup.setUpFixtures(HBaseTestSetup.java:87)
    [junit]     at org.apache.hadoop.hive.hbase.HBaseTestSetup.preTest(HBaseTestSetup.java:59)
    [junit]     at org.apache.hadoop.hive.hbase.HBaseQTestUtil.<init>(HBaseQTestUtil.java:31)
    [junit]     at org.apache.hadoop.hive.cli.TestHBaseCliDriver.setUp(TestHBaseCliDriver.java:43)
    [junit]     at junit.framework.TestCase.runBare(TestCase.java:125)
    [junit]     at junit.framework.TestResult$1.protect(TestResult.java:106)
    [junit]     at junit.framework.TestResult.runProtected(TestResult.java:124)
    [junit]     at junit.framework.TestResult.run(TestResult.java:109)
    [junit]     at junit.framework.TestCase.run(TestCase.java:118)
    [junit]     at junit.framework.TestSuite.runTest(TestSuite.java:208)
    [junit]     at junit.framework.TestSuite.run(TestSuite.java:203)
",jvs,nzhang,Major,Closed,Fixed,15/Oct/10 17:38,17/Dec/11 00:01
Bug,HIVE-1717,12477494,ant clean should delete stats database,"If a test failed, the derby database used for storing intermediate stats may be in an inconsistent state. This database is shared with future tests and prevent them from connecting to the database. 
",nzhang,nzhang,Major,Closed,Fixed,15/Oct/10 18:55,16/Dec/11 23:59
Bug,HIVE-1720,12477521,hbase_stats.q is failing,"Saw this failure on Hudson and in my own sandbox.

https://hudson.apache.org/hudson/job/Hive-trunk-h0.20/392/
",nzhang,jvs,Major,Closed,Fixed,16/Oct/10 00:22,16/Dec/11 23:59
Bug,HIVE-1737,12477944,Two Bugs for Estimating Row Sizes in GroupByOperator,"Two bugs:
1. if UDAF uses string type, Group-by will break as it tries to insert an ArrayList<Object> to a HashMap<KeyWrapper, AggregationBuffer[]>.
2. The code to sample size of keys only handles String type and Text type, while in most cases, they are org.apache.hadoop.hive.serde2.lazy.LazyString, so that 0 is always used.",sdong,sdong,Major,Closed,Fixed,21/Oct/10 00:58,17/Dec/11 00:01
Bug,HIVE-1742,12478038,Fix Eclipse templates (and use Ivy metadata to generate Eclipse library dependencies),"A previous commit broke the eclipse templates.

Also, we should use the library version information in ivy/libraries.properties in
ivy.xml files as well as the eclipse .classpath file.
",cwsteinbach,cwsteinbach,Major,Closed,Fixed,21/Oct/10 22:35,16/Dec/11 23:59
Bug,HIVE-1748,12478280,Statistics broken for tables with size in excess of Integer.MAX_VALUE,ANALYZE TABLE x COMPUTE STATISTICS would fail to update the table size if it exceeded Integer.MAX_VALUE because it used parseInt instead of parseLong.,paulbutler,paulbutler,Major,Closed,Fixed,25/Oct/10 23:42,16/Dec/11 23:59
Bug,HIVE-1753,12478376,HIVE 1633 hit for Stage2 jobs with CombineHiveInputFormat,Errors are the same as HIVE-1633 but I see them for Stage-2 jobs.,sreekanth,thiruvel,Major,Closed,Fixed,26/Oct/10 18:21,17/Dec/11 00:01
Bug,HIVE-1756,12478508,failures in fatal.q in TestNegativeCliDriver,This is probably caused by HIVE-1641,liyin,namit,Major,Closed,Fixed,27/Oct/10 22:06,16/Dec/11 23:59
Bug,HIVE-1759,12478679,Many important broken links on Hive web page,"The change log links are broken, perhaps because of the move to a TLP, and the Jira issue log links all point to the 0.5 issue log. Also, all of the documentation links are broken.",appodictic,hammer,Major,Closed,Fixed,29/Oct/10 17:38,16/Dec/11 23:59
Bug,HIVE-1760,12478704,Mismatched open/commit transaction calls in case of connection retry,"Consider the create table function (parts removed for simplicity):

{code}

    private void create_table_core(final RawStore ms, final Table tbl)
        throws AlreadyExistsException, MetaException, InvalidObjectException {

      Path tblPath = null;
      boolean success = false, madeDir = false;
      try {
        ms.openTransaction();

        // get_table checks whether database exists, it should be moved here
        if (is_table_exists(tbl.getDbName(), tbl.getTableName())) {
          throw new AlreadyExistsException(""Table "" + tbl.getTableName()
              + "" already exists"");
        }

        ms.createTable(tbl);
        success = ms.commitTransaction();

      } finally {
        if (!success) {
          ms.rollbackTransaction();
          if (madeDir) {
            wh.deleteDir(tblPath, true);
          }
        }
      }
    }

{code}

A potential openTransaction() / commitTransaction() mismatch can occur if the is_table_exits() method call experiences a connection failure. 

Since get_table() in is_table_exists() uses executeWithRetry(),  the transaction will be rolled back and get_table() will be called again if the is a connection problem. However, this rollback and retry will reset the global openTransactionCalls counter back to 0, effectively canceling out the openTransaction() call. 

Then later in the method when commitTransaction() is called, Hive will throw an error similar to the following:

Caused by: java.lang.RuntimeException: commitTransaction was called but openTransactionCalls = 0. This probably indicates that there are unbalanced calls to openTransaction/commitTransaction

A similar problem exists with create_type_core()",pauly,pauly,Major,Closed,Fixed,29/Oct/10 22:55,21/Nov/13 03:30
Bug,HIVE-1767,12479141,Merge files does not work with dynamic partition,,he yongqiang,he yongqiang,Major,Closed,Fixed,04/Nov/10 20:27,17/Dec/11 00:01
Bug,HIVE-1769,12479235,pcr.q output is non-deterministic,"Noticed this after merging the patch.

Can you take care of this ?

Also, can you run it on a smaller data set ? - the output is just too big.",sdong,namit,Major,Closed,Fixed,05/Nov/10 17:32,16/Dec/11 23:59
Bug,HIVE-1771,12479263,ROUND(infinity) chokes,"Since 1-arg ROUND returns an integer, it's hard to fix this without either losing data (return NULL) or making a backwards-incompatible change (return DOUBLE instead of BIGINT).

In any case, we should definitely fix 2-arg ROUND to preserve infinity/NaN/etc, since it is already returning double.
",jvs,jvs,Major,Closed,Fixed,05/Nov/10 21:49,17/Dec/11 00:00
Bug,HIVE-1775,12479330,Assertation on inputObjInspectors.length in Groupy operator,"In the Groupby Operator:
Line 188: assert (inputObjInspectors.length == 1); 
But this assertion may not necessary true",liyin,liyin,Major,Closed,Fixed,08/Nov/10 00:29,16/Dec/11 23:59
Bug,HIVE-1776,12479421,parallel execution and auto-local mode combine to place plan file in wrong file system,"A query (that i can't reproduce verbatim) submits a job to a MR cluster with a plan file that is resident on  the local file system. This job obviously fails.

This seems to result from an interaction between the parallel execution (which is trying to run one local and one remote job at the same time). Turning off either the parallel execution mode or the auto-local mode seems to fix the problem.",jsensarma,jsensarma,Major,Closed,Fixed,08/Nov/10 22:06,16/Dec/11 23:59
Bug,HIVE-1777,12479434,Outdated comments for GenericUDTF.close(),"In a GenericUDTF, rows can be forward()'ed on close(), contrary to what the comment says.",pauly,pauly,Major,Closed,Fixed,08/Nov/10 23:17,16/Dec/11 23:59
Bug,HIVE-1780,12479548,Typo in hive-default.xml,"'CombineHiveInputFormat' is spelt incorrectly in the hive-default.xml:

It should be 'CombineHiveInputFormat' instead of  'CombinedHiveInputFormat'.",warwithin,warwithin,Trivial,Closed,Fixed,10/Nov/10 01:39,17/Dec/11 00:01
Bug,HIVE-1781,12479614,outputs not populated for dynamic partitions at compile time,"OSTHOOK: query: create table tstsrcpart like srcpart
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: default@tstsrcpart
PREHOOK: query: from srcpart
insert overwrite table tstsrcpart partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
PREHOOK: type: QUERY
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
POSTHOOK: query: from srcpart



As is evident from above, the outputs are not populated at all at compile time.

This may create a problem for many components that depend on outputs: locking, authorization etc.
However, the exact set of outputs may be needed for some other components (for example. the
internal deployment in Facebook has a replication hook which is used for replication which needs the
exact set of outputs). It may  be a good idea to extend WriteEntity to include a flag which indicates
whether the output is complete or not, and then the hook can look at that flag if needed
",namit,namit,Major,Closed,Fixed,10/Nov/10 18:04,17/Dec/11 00:01
Bug,HIVE-1794,12480024,GenericUDFOr and GenericUDFAnd cannot receive boolean typed object,If a UDF returns Java's native boolean and passed into a logic AND or OR. The execution will break.,sdong,sdong,Major,Closed,Fixed,16/Nov/10 04:12,16/Dec/11 23:59
Bug,HIVE-1795,12480157,outputs not correctly populated for alter table,"For any :

alter table <T> partition <p> ...

The table <T> is added in the output. It leads to problems with locking, and will lead to problems in future for authorization.
The partition should be in the output, not the table.",namit,namit,Major,Closed,Fixed,16/Nov/10 22:47,17/Dec/11 00:00
Bug,HIVE-1804,12480608,Mapjoin will fail if there are no files associating with the join tables,"If there are some empty tables without any file associated, the map join will fail.",liyin,liyin,Major,Closed,Fixed,22/Nov/10 20:31,17/Dec/11 00:00
Bug,HIVE-1806,12480715,The merge criteria on dynamic partitons should be per partiton,"Currently the criteria of whether a merge job should be fired on dynamic generated partitions are is the average file size of files across all dynamic partitions. It is very common that some dynamic partitions contains mostly large files and some contains mostly small files. Even though the average size of the total files are larger than the hive.merge.smallfiles.avgsize, we should merge those partitions containing small files only. ",nzhang,nzhang,Major,Closed,Fixed,23/Nov/10 19:22,16/Dec/11 23:59
Bug,HIVE-1807,12480721,No Element found exception in BucketMapJoinOptimizer,,he yongqiang,he yongqiang,Major,Closed,Fixed,23/Nov/10 19:53,17/Dec/11 00:01
Bug,HIVE-1808,12480736,bug in auto_join25.q,"In this test case, there are 2 SET statements:
set hive.mapjoin.localtask.max.memory.usage = 0.0001;
set hive.mapjoin.check.memory.rows = 2;

But in HiveConf, the names of these 2 conf variable do not match with each other.

",liyin,liyin,Major,Closed,Fixed,23/Nov/10 20:55,16/Dec/11 23:59
Bug,HIVE-1809,12480749,Hive comparison operators are broken for NaN values,"Comparisons between NaN values and doubles do not work as expected:

hive> select 'NaN' = 4.3 from data_one limit 1;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Execution log at: /tmp/pbutler/pbutler_20101123145656_d23f9b77-8907-4ed3-aef9-8b99a1cc3138.log
Job running in-process (local Hadoop)
2010-11-23 14:56:40,488 null map = 100%,  reduce = 0%
Ended Job = job_local_0001
OK
true
Time taken: 9.47 seconds
hive> select 4 <> 'NaN' from data_one limit 1;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Execution log at: /tmp/pbutler/pbutler_20101123145858_0d243ac2-f745-4e25-9a38-509bef3bb370.log
Job running in-process (local Hadoop)
2010-11-23 14:58:45,689 null map = 100%,  reduce = 0%
Ended Job = job_local_0001
OK
false
Time taken: 3.938 seconds",paulbutler,paulbutler,Major,Closed,Fixed,23/Nov/10 22:59,16/Dec/11 23:59
Bug,HIVE-1812,12491578,spurious rmr failure messages when inserting with dynamic partitioning,"Running a test such as load_dyn_part1.q, there is a lot of noise like this:

test:
    [junit] Copying data from file:/Users/jsichi/open/hive-trunk/data/files/kv1.txt
    [junit] Loading data to table srcpart partition (ds=2008-04-08, hr=11)
    [junit] rmr: cannot remove pfile:/Users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11: No such file or directory.

This shows up in production logs as well, which confuses users, making them thinking that the job is encountering problems when it isn't.  Is there a way we can squelch these?
",nzhang,jvs,Major,Closed,Fixed,29/Nov/10 20:35,16/Dec/11 23:59
Bug,HIVE-1825,12491939,Different defaults for hive.metastore.local,"hive-default.xml sets {{hive.metastore.local}} to {{true}}. In the code however there is this:

{code:title=HiveMetaStoreClient.java}
boolean localMetaStore = conf.getBoolean(""hive.metastore.local"", false);
{code}

This leads to different behaviour depending on whether hbase-default.xml is on the classpath or not.....which can lead to some confusion ;-)

I can supply a patch - should be pretty similar. I just don't  know what the ""real"" default should be. My guess would be {{true}}.",,larsfrancke,Major,Closed,Fixed,02/Dec/10 23:29,16/Dec/11 23:55
Bug,HIVE-1828,12492019,show locks should not use getTable()/getPartition ,,he yongqiang,namit,Major,Closed,Fixed,03/Dec/10 19:17,17/Dec/11 00:01
Bug,HIVE-1829,12492031,Fix intermittent failures in TestRemoteMetaStore,"Notice how Running metastore! appears twice.
{noformat}
test:
    [junit] Running org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore
    [junit] BR.recoverFromMismatchedToken
    [junit] Tests run: 11, Failures: 0, Errors: 0, Time elapsed: 36.697 sec
    [junit] Running org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore
    [junit] Running metastore!
    [junit] Running metastore!
    [junit] org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:29083.
    [junit] 	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:98)
    [junit] 	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
    [junit] 	at org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.<init>(TServerSocketKeepAlive.java:34)
    [junit] 	at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:2189)
    [junit] 	at org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore$RunMS.run(TestRemoteHiveMetaStore.java:35)
    [junit] 	at java.lang.Thread.run(Thread.java:619)
    [junit] Running org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore FAILED (crashed)
{noformat}",cwsteinbach,appodictic,Major,Closed,Fixed,03/Dec/10 21:12,16/Dec/11 23:59
Bug,HIVE-1830,12492130,mappers in group followed by joins may die OOM,,liyin,namit,Major,Closed,Fixed,06/Dec/10 07:01,25/Jan/15 00:23
Bug,HIVE-1844,12492791,Hanging hive client caused by TaskRunner's OutOfMemoryError,,he yongqiang,he yongqiang,Major,Closed,Fixed,09/Dec/10 02:14,16/Dec/11 23:59
Bug,HIVE-1845,12492870,Some attributes in the Eclipse template file is deprecated  ,"In the eclipse template file, it will reference this jar file, which is deprecated.
/@PROJECT@/build/metastore/hive-model-@HIVE_VERSION@.jar

So the correct one should be:
/@PROJECT@/build/metastore/hive-metastore-@HIVE_VERSION@.jar

Just update all the eclipse template files.",liyin,liyin,Major,Closed,Fixed,09/Dec/10 19:08,17/Dec/11 00:01
Bug,HIVE-1846,12492885,change hive assumption that local mode mappers/reducers always run in same jvm ,we are trying out a version of hadoop local mode that runs multiple mappers/reducers by spawning jvm's for them. In this mode hive mappers fail in reading the plan file. it seems that we assume (in the setMapredWork call) that local mode mappers/reducers will run in the same jvm (we can cache the current plan in a global var and don't serialize to a path). this needs to get fixed.,rvadali,jsensarma,Major,Closed,Fixed,09/Dec/10 22:50,17/Dec/11 00:01
Bug,HIVE-1848,12493133,bug in MAPJOIN,"explain
FROM srcpart c
JOIN srcpart d
ON ( c.key=d.key AND c.ds='2008-04-08' AND  d.ds='2008-04-08')
SELECT /*+ MAPJOIN(d) */ DISTINCT c.campaign_id;

The above query throws an error:


FAILED: Error in semantic analysis: line 0:-1 Invalid Function TOK_MAPJOIN
",he yongqiang,namit,Major,Closed,Fixed,13/Dec/10 23:20,16/Dec/11 23:59
Bug,HIVE-1849,12493139,add more logging to partition pruning,"In facebook, we are seeing some intermittent errors, where it seems that either all the partitions are not returned by the metastore
or some of them are pruned wrongly.

This patch adds more logging for debugging such scenarios.",namit,namit,Major,Closed,Fixed,13/Dec/10 23:56,16/Dec/11 23:59
Bug,HIVE-1850,12493182,alter table set serdeproperties bypasses regexps checks (leaves table in a non-recoverable state?),"{code}
create table aa ( test STRING )
  ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
  WITH SERDEPROPERTIES (""input.regex"" = ""[^\\](.*)"", ""output.format.string"" = ""$1s"");
{code}
This will fail. Great!

{code}
create table aa ( test STRING )
  ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
  WITH SERDEPROPERTIES (""input.regex"" = ""(.*)"", ""output.format.string"" = ""$1s"");
{code}
Works, no problem there.
{code}
alter table aa set serdeproperties (""input.regex"" = ""[^\\](.*)"", ""output.format.string"" = ""$1s"");
{code}
Wups... I can set that without any problems!

{code}
alter table aa set serdeproperties (""input.regex"" = ""(.*)"", ""output.format.string"" = ""$1s"");
FAILED: Hive Internal Error: java.util.regex.PatternSyntaxException(Unclosed character class near index 7
[^\](.*)
       ^)
java.util.regex.PatternSyntaxException: Unclosed character class near index 7
[^\](.*)
       ^
	at java.util.regex.Pattern.error(Pattern.java:1713)
	at java.util.regex.Pattern.clazz(Pattern.java:2254)
	at java.util.regex.Pattern.sequence(Pattern.java:1818)
	at java.util.regex.Pattern.expr(Pattern.java:1752)
	at java.util.regex.Pattern.compile(Pattern.java:1460)
	at java.util.regex.Pattern.<init>(Pattern.java:1133)
	at java.util.regex.Pattern.compile(Pattern.java:847)
	at org.apache.hadoop.hive.contrib.serde2.RegexSerDe.initialize(RegexSerDe.java:101)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:199)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:253)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:484)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:161)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:803)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSerdeProps(DDLSemanticAnalyzer.java:558)
	at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:232)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:335)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:686)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:142)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:370)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
{code}
After this, all further commands on the table fails, including drop table :)

1. The alter table command should probably check the regexp just like the create table command does
2. Even though the regexp is bad, it should be possible to do things like set the regexp again or drop the table.",amareshwari,terjem,Major,Closed,Fixed,14/Dec/10 13:18,16/Dec/11 23:56
Bug,HIVE-1853,12493399,downgrade JDO version,"After HIVE-1609, we are seeing some table not found errors intermittently.
We have a test case where 5 processes are concurrently issueing the same query - 
explain extended insert .. select from <T>

and once in a while, we get a error <T> not found - 
When we revert back the JDO version, the error is gone.

We can investigate later to find the JDO bug, but for now this is a show-stopper for facebook, and needs
to be reverted back immediately.

This also means, that the filters will not be pushed to mysql.",pauly,namit,Major,Closed,Fixed,16/Dec/10 06:38,02/May/13 02:30
Bug,HIVE-1854,12493624,Temporarily disable metastore tests for listPartitionsByFilter(),"After the JDO downgrade in HIVE-1853, the tests for the disabled function listPartitionByFilter() should be disabled as well.",pauly,pauly,Minor,Closed,Fixed,19/Dec/10 06:53,16/Dec/11 23:59
Bug,HIVE-1857,12493712,mixed case tablename on lefthand side of LATERAL VIEW results in query failing with confusing error message,"For the modified query below in lateral_view.q, the exception ""org.apache.hadoop.hive.ql.parse.SemanticException: line 3:7 Invalid Table Alias or Column Reference myCol"" is thrown.  The query should succeed.

SELECT myCol from tmp_PYANG_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;
",jvs,jvs,Major,Closed,Fixed,20/Dec/10 22:55,16/Dec/11 23:59
Bug,HIVE-1862,12493928,Revive partition filtering in the Hive MetaStore,HIVE-1853 downgraded the JDO version. This makes the feature of partition filtering in the metastore unusable. This jira is to keep track of the lost feature and discussing approaches to bring it back.,macyang,ddas,Major,Closed,Fixed,22/Dec/10 21:46,20/Aug/12 22:57
Bug,HIVE-1864,12493947,test load_overwrite.q fails,"After HIVE-1852, the above test is failing",cwsteinbach,namit,Major,Closed,Fixed,23/Dec/10 06:42,16/Dec/11 23:59
Bug,HIVE-1867,12494049,Add mechanism for disabling tests with intermittent failures,"{code}
[junit] Begin query: dyn_part_empty.q
    [junit] Running org.apache.hadoop.hive.cli.TestNegativeCliDriver
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.hive.cli.TestNegativeCliDriver FAILED (crashed)
{code}

dyn_part_empty.q has been intermittently failing on Hudson. I was able to reproduce locally,
and with different versions of JUnit (3.8.1, 4.5, 4.8.2).",marcelk,cwsteinbach,Major,Closed,Fixed,24/Dec/10 22:22,16/Dec/11 23:59
Bug,HIVE-1869,12494164,TestMTQueries failing on jenkins,"TestMTQueries has been failing intermittently on Hudson. The first failure I can find
a record of on Hudson is from svn rev 1052414 on December 24th, but it's 
likely that the failures actually started earlier.",brocknoland,cwsteinbach,Major,Resolved,Fixed,28/Dec/10 06:06,12/Feb/15 23:40
Bug,HIVE-1870,12494165,TestRemoteHiveMetaStore.java accidentally deleted during commit of HIVE-1845,"TestRemoteHiveMetaStore.java was removed by the commit of HIVE-1845. This change was not part of
the patch for HIVE-1845.",cwsteinbach,cwsteinbach,Major,Closed,Fixed,28/Dec/10 06:38,16/Dec/11 23:59
Bug,HIVE-1871,12494278,bug introduced by HIVE-1806,"  LoadMultiFilesDesc lmfd = new LoadMultiFilesDesc(toMove,
          
      targetDir, lfd.getIsDfsDir(), lfd.getColumns(), lfd.getColumnTypes());

The toMove includes full paths for each partition.  But the targetDir is a root path that does not contain any partition part.

So it is move, for example,
from
hdfs://dfs2.data.facebook.com:9000/tmp/hive-heyongqiang/hive_2010-12-29_13-31-58_051_54619753122187226/-ext-10002/ds=2010-12-28/hr=17/offset=16-435944367480
to
hdfs://dfs2.data.facebook.com:9000/tmp/hive-heyongqiang/hive_2010-12-29_13-31-58_051_54619753122187226/-ext-10000

And the final path after the move is 
hdfs://dfs2.data.facebook.com:9000/tmp/hive-heyongqiang/hive_2010-12-29_13-31-58_051_54619753122187226/-ext-10000/
offset=16-435944367480",he yongqiang,namit,Major,Closed,Fixed,29/Dec/10 23:00,16/Dec/11 23:59
Bug,HIVE-1873,12494341,Fix 'tar' build target broken in HIVE-1526,My patch for HIVE-1526 broke the 'tar' build target.,cwsteinbach,cwsteinbach,Major,Closed,Fixed,31/Dec/10 03:34,17/Dec/11 00:00
Bug,HDFS-868,12444505,Link to Hadoop Upgrade Wiki is broken,The link to the Hadoop Upgrade wiki is broken in the xdocs. Trivial patch forthcoming which addresses the issue.,,chrismattmann,Trivial,Closed,Fixed,02/Jan/10 17:42,24/Aug/10 20:50
Bug,HDFS-871,12444808,Balancer can hang in PendingBlockMove,"We started the balancer, with default options (-threshold 10), and it ran fine for a few hours, then hung. The process was still alive but no balancing was taking place.

At the time of the hang, jstack showed there were three threads in RUNNABLE status. Subsequent jstacks taken minutes and hours later showed the same three threads running in the same place, so I don't think this was a case where requests were being restarted, it looks like hangs. My best guess is, there's no timeout in the request to the namenode for these requests, and there needs to be.

I'll attach the full jstack output, but here's a sample thread, they are all stuck in the same place.

""pool-1-thread-972"" prio=10 tid=0x00002aaafc23a800 nid=0x27a8 runnable [0x00002a
ab0a9a2000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        - locked <0x00002aaaebdbe158> (a java.io.BufferedInputStream)
        at java.io.DataInputStream.readShort(DataInputStream.java:295)
        at org.apache.hadoop.hdfs.server.balancer.Balancer$PendingBlockMove.receiveResponse(Balancer.java:371)
        at org.apache.hadoop.hdfs.server.balancer.Balancer$PendingBlockMove.dispatch(Balancer.java:326)
        at org.apache.hadoop.hdfs.server.balancer.Balancer$PendingBlockMove.access$1800(Balancer.java:232)
        at org.apache.hadoop.hdfs.server.balancer.Balancer$PendingBlockMove$1.run(Balancer.java:393)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",,andrewr,Major,Resolved,Fixed,06/Jan/10 17:00,29/Jul/14 21:48
Bug,HDFS-872,12444814,DFSClient 0.20.1 is incompatible with HDFS 0.20.2,"After upgrading to that latest HDFS 0.20.2 (r896310 from /branches/branch-0.20), old DFS clients (0.20.1) seem to not work anymore. HBase uses the 0.20.1 hadoop core jars and the HBase master will no longer startup. Here is the exception from the HBase master log:

{code}
2010-01-06 09:59:46,762 WARN org.apache.hadoop.hdfs.DFSClient: DFS Read: java.io.IOException: Could not obtain block: blk_338051
2596555557728_1002 file=/hbase/hbase.version
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:1788)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:1616)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1743)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1673)
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
        at java.io.DataInputStream.readUTF(DataInputStream.java:572)
        at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:189)
        at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:208)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:208)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.master.HMaster.doMain(HMaster.java:1241)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1282)

2010-01-06 09:59:46,763 FATAL org.apache.hadoop.hbase.master.HMaster: Not starting HMaster because:
java.io.IOException: Could not obtain block: blk_3380512596555557728_1002 file=/hbase/hbase.version
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:1788)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:1616)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1743)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1673)
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
        at java.io.DataInputStream.readUTF(DataInputStream.java:572)
        at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:189)
        at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:208)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:208)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.master.HMaster.doMain(HMaster.java:1241)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1282)
{code}

If I switch the hadoop jars in the hbase/lib directory with 0.20.2 version it works well, which what led me to open this bug here and not in the HBASE project.",tlipcon,bassam,Major,Resolved,Fixed,06/Jan/10 18:08,26/Jan/10 23:12
Bug,HDFS-874,12444844,TestHDFSFileContextMainOperations fails on weirdly configured DNS hosts,"On an internal build machine I see exceptions like this:
java.lang.IllegalArgumentException: Wrong FS: hdfs://localhost:47262/data/1/scratch/patchqueue/patch-worker-20518/patch_21/svnrepo/build/test/data/test/test/testRenameWithQuota/srcdir, expected: hdfs://localhost.localdomain:47262

""hostname"" and ""hostname -f"" both show the machine's FQDN (not localhost). /etc/hosts is stock after CentOS 5 install. ""host 127.0.0.1"" reverses to ""localhost""",tlipcon,tlipcon,Major,Closed,Fixed,06/Jan/10 22:08,12/Dec/11 06:19
Bug,HDFS-877,12444859,Client-driven block verification not functioning,"This is actually the reason for HDFS-734 (TestDatanodeBlockScanner timing out). The issue is that DFSInputStream relies on readChunk being called one last time at the end of the file in order to receive the lastPacketInBlock=true packet from the DN. However, DFSInputStream.read checks pos < getFileLength() before issuing the read. Thus gotEOS never shifts to true and checksumOk() is never called.

",tlipcon,tlipcon,Major,Closed,Fixed,07/Jan/10 01:20,24/Aug/10 20:50
Bug,HDFS-880,12444967,TestNNLeaseRecovery fails on windows,TestNNLeaseRecovery fails on windows trying to delete name-node storage directory.,cos,shv,Major,Closed,Fixed,07/Jan/10 21:46,24/Aug/10 20:50
Bug,HDFS-885,12445058,Datanode toString() NPEs on null dnRegistration,"{{Datanode toString() }} assumes the {{dnRegistration}} value is never null. This is not always true, and when it is not the case, the {{toString()}} operator NPEs. ",stevel@apache.org,stevel@apache.org,Minor,Closed,Fixed,08/Jan/10 18:22,24/Aug/10 20:51
Bug,HDFS-886,12445079,TestHDFSTrash fails on Windows,"TestHDFSTrash fails on Windows because drive letter ""C:"" is inserted in the path when Trash is trying to move it under "".Trash/current"".",,shv,Major,Resolved,Fixed,08/Jan/10 22:54,29/Jul/14 22:09
Bug,HDFS-887,12445087,TestHDFSServerPorts fails on Windows,"{{TestHDFSServerPorts}} fails becuase it lets two {{HttpServers}} start on the same pot. That is two NameNode instances or NameNode and BackupNode can start on the same port. I see failures on my windows box, and could not reproduce it on a linux so far, but I am not sure this is pure windows problem.",,shv,Major,Resolved,Fixed,09/Jan/10 00:28,29/Jul/14 22:09
Bug,HDFS-888,12445088,TestModTime fails intermittently,"TestModTime fails pretty often. It creates 2 directories in hdfs dir1 and dir2, and them moves file1 from dir1 to dir2. This should change mod time for dir2, but sometimes it doesn't.
I am reproducing it on Windows, but this does not seem to be windows specific.",,shv,Major,Resolved,Fixed,09/Jan/10 00:34,29/Jul/14 22:09
Bug,HDFS-891,12445109,DataNode no longer needs to check for dfs.network.script,"Looking at the code for {{DataNode.instantiateDataNode())} , I see that it calls {{system.exit(-1)}} if it is not happy with the configuration

{code}
    if (conf.get(""dfs.network.script"") != null) {
      LOG.error(""This configuration for rack identification is not supported"" +
          "" anymore. RackID resolution is handled by the NameNode."");
      System.exit(-1);
    }
{code}
This is excessive. It should throw an exception and let whoever called the method decide how to handle it. The {{DataNode.main()}} method will log the exception and exit with a -1 value, but other callers (such as anything using {{MiniDFSCluster}} will now see a meaningful message rather than some Junit ""tests exited without completing"" warning. 

Easy to write a test for the correct behaviour: start a {{MiniDFSCluster}} with this configuration set, see what happens.",qwertymaniac,stevel@apache.org,Minor,Closed,Fixed,09/Jan/10 14:55,28/Sep/15 20:58
Bug,HDFS-894,12445373,DatanodeID.ipcPort is not updated when existing node re-registers,"In FSNamesystem.registerDatanode, it checks if a registering node is a reregistration of an old one based on storage ID. If so, it simply updates the old one with the new registration info. However, the new ipcPort is lost when this happens.

I produced manually this by setting up a DN with IPC port set to 0 (so it picks an ephemeral port) and then restarting the DN. At this point, the NN's view of the ipcPort is stale, and clients will not be able to achieve pipeline recovery.

This should be easy to fix and unit test, but not sure when I'll get to it, so anyone else should feel free to grab it if they get to it first.",tlipcon,tlipcon,Blocker,Closed,Fixed,13/Jan/10 03:15,24/Aug/10 20:51
Bug,HDFS-897,12445425,ReplicasMap remove has a bug in generation stamp comparison,"In {{ReplicasMap.remove(block)}}, instead of comparing generation stamp of the entry in the map to that of the given block, the generation stamp of entry in the map is compared to itself. ",sureshms,sureshms,Major,Closed,Fixed,13/Jan/10 19:00,24/Aug/10 20:51
Bug,HDFS-900,12445462,Corrupt replicas are not tracked correctly through block report from DN,"This one is tough to describe, but essentially the following order of events is seen to occur:

# A client marks one replica of a block to be corrupt by telling the NN about it
# Replication is then scheduled to make a new replica of this node
# The replication completes, such that there are now 3 good replicas and 1 corrupt replica
# The DN holding the corrupt replica sends a block report. Rather than telling this DN to delete the node, the NN instead marks this as a new *good* replica of the block, and schedules deletion on one of the good replicas.

I don't know if this is a dataloss bug in the case of 1 corrupt replica with dfs.replication=2, but it seems feasible. I will attach a debug log with some commentary marked by '============>', plus a unit test patch which I can get to reproduce this behavior reliably. (it's not a proper unit test, just some edits to an existing one to show it)",shv,tlipcon,Blocker,Closed,Fixed,14/Jan/10 00:41,12/Dec/11 06:19
Bug,HDFS-903,12445701,NN should verify images and edit logs on startup,"I was playing around with corrupting fsimage and edits logs when there are multiple dfs.name.dirs specified. I noticed that:
 * As long as your corruption does not make the image invalid, eg changes an opcode so it's an invalid opcode HDFS doesn't notice and happily uses a corrupt image or applies the corrupt edit.
* If the first image in dfs.name.dir is ""valid"" it replaces the other copies in the other name.dirs, even if they are different, with this first image, ie if the first image is actually invalid/old/corrupt metadata than you've lost your valid metadata, which can result in data loss if the namenode garbage collects blocks that it thinks are no longer used.

How about we maintain a checksum as part of the image and edit log and check those on startup and refuse to startup if they are different. Or at least provide a configuration option to do so if people are worried about the overhead of maintaining checksums of these files. Even if we assume dfs.name.dir is reliable storage this guards against operator errors.",hairong,eli,Critical,Closed,Fixed,15/Jan/10 23:27,19/Jul/14 00:02
Bug,HDFS-908,12445992,TestDistributedFileSystem fails with Wrong FS on weird hosts,"On the same host where I experienced HDFS-874, I also experience this failure for TestDistributedFileSystem:

Testcase: testFileChecksum took 0.492 sec
  Caused an ERROR
Wrong FS: hftp://localhost.localdomain:59782/filechecksum/foo0, expected: hftp://127.0.0.1:59782
java.lang.IllegalArgumentException: Wrong FS: hftp://localhost.localdomain:59782/filechecksum/foo0, expected: hftp://127.0.0.1:59782
  at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:310)
  at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:222)
  at org.apache.hadoop.hdfs.HftpFileSystem.getFileChecksum(HftpFileSystem.java:318)
  at org.apache.hadoop.hdfs.TestDistributedFileSystem.testFileChecksum(TestDistributedFileSystem.java:166)

Doesn't appear to occur on trunk or branch-0.21.",tlipcon,tlipcon,Minor,Resolved,Fixed,20/Jan/10 02:43,16/Nov/10 18:30
Bug,HDFS-909,12446120,Race condition between rollEditLog or rollFSImage ant FSEditsLog.write operations  corrupts edits log,"closing the edits log file can race with write to edits log file operation resulting in OP_INVALID end-of-file marker being initially overwritten by the concurrent (in setReadyToFlush) threads and then removed twice from the buffer, losing a good byte from edits log.

Example:
{code}
FSNameSystem.rollEditLog() -> FSEditLog.divertFileStreams() -> FSEditLog.closeStream() -> EditLogOutputStream.setReadyToFlush()
FSNameSystem.rollEditLog() -> FSEditLog.divertFileStreams() -> FSEditLog.closeStream() -> EditLogOutputStream.flush() -> EditLogFileOutputStream.flushAndSync()
OR
FSNameSystem.rollFSImage() -> FSIMage.rollFSImage() -> FSEditLog.purgeEditLog() -> FSEditLog.revertFileStreams() -> FSEditLog.closeStream() ->EditLogOutputStream.setReadyToFlush() 
FSNameSystem.rollFSImage() -> FSIMage.rollFSImage() -> FSEditLog.purgeEditLog() -> FSEditLog.revertFileStreams() -> FSEditLog.closeStream() ->EditLogOutputStream.flush() -> EditLogFileOutputStream.flushAndSync()

VERSUS

FSNameSystem.completeFile -> FSEditLog.logSync() -> EditLogOutputStream.setReadyToFlush()
FSNameSystem.completeFile -> FSEditLog.logSync() -> EditLogOutputStream.flush() -> EditLogFileOutputStream.flushAndSync()
OR 
Any FSEditLog.write
{code}

Access on the edits flush operations is synchronized only in the FSEdits.logSync() method level. However at a lower level access to EditsLogOutputStream setReadyToFlush(), flush() or flushAndSync() is NOT synchronized. These can be called from concurrent threads like in the example above

So if a rollEditLog or rollFSIMage is happening at the same time with a write operation it can race for EditLogFileOutputStream.setReadyToFlush that will overwrite the the last byte (normally the FSEditsLog.OP_INVALID which is the ""end-of-file marker"") and then remove it twice (from each thread) in flushAndSync()! Hence there will be a valid byte missing from the edits log that leads to a SecondaryNameNode silent failure and a full HDFS failure upon cluster restart. 

We got to this point after investigating a corrupted edits file that made HDFS unable to start with 

{code:title=namenode.log}
java.io.IOException: Incorrect data format. logVersion is -20 but writables.length is 768. 
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadEditRecords(FSEditLog.java:450
{code}

EDIT: moved the logs to a comment to make this readable
",tlipcon,clehene,Blocker,Closed,Fixed,20/Jan/10 18:04,24/Aug/10 20:51
Bug,HDFS-912,12446378, sed in build.xml fails,This is the HDFS version of HADOOP-6505.,aw,aw,Minor,Resolved,Fixed,22/Jan/10 22:10,03/Aug/10 00:46
Bug,HDFS-913,12446382,TestRename won't run automatically from 'run-test-hdfs-faul-inject' target,"Fault injection test classes suppose to have {{TestFi}} prefix. Otherwise, JUnit target won't pick them up as a part of the batch test run. Although, it's still possible to run test with different names with {{-Dtestcase=}} directive.",sureshms,cos,Major,Closed,Fixed,22/Jan/10 22:53,24/Aug/10 20:51
Bug,HDFS-922,12446631,Remove extra semicolon from HDFS-877 that really annoys Eclipse,HDFS-877 introduced an extra semicolon on an empty line that Eclipse treats as a syntax error and hence messes up its compilation.  ,jghoman,jghoman,Minor,Resolved,Fixed,26/Jan/10 03:08,27/Jan/10 06:00
Bug,HDFS-927,12446734,DFSInputStream retries too many times for new block locations,"I think this is a regression caused by HDFS-127 -- DFSInputStream is supposed to only go back to the NN max.block.acquires times, but in trunk it goes back twice as many - the default is 3, but I am counting 7 calls to getBlockLocations before an exception is thrown.",tlipcon,tlipcon,Critical,Closed,Fixed,26/Jan/10 21:36,24/Aug/10 20:51
Bug,HDFS-938,12454987,Replace calls to UGI.getUserName() with UGI.getShortUserName(),"HADOOP-6526 details why UGI.getUserName() will not work to identify users. Until the proposed UGI.getLocalName() is implemented, calls to getUserName() should be replaced with the short name. ",jghoman,jghoman,Major,Closed,Fixed,01/Feb/10 22:35,24/Aug/10 20:51
Bug,HDFS-939,12455001,libhdfs test is broken,"The libhdfs test currently does not run because hadoop.tmp.dir is specified as a relative path, and it looks like a side-effect of HDFS-873 was that relative paths get made absolute, so build/test/libhdfs gets turned into /test/libhdfs, which the NN can not create. Let's make the test generate conf files that use the appropriate directory (build/test/libhdfs) specified by fully qualified URIs. 

Also, are relative paths in conf files supported? If not rather than fail we should detect this and print a warning.",eli,eli,Blocker,Closed,Fixed,02/Feb/10 01:30,02/May/13 02:29
Bug,HDFS-955,12455702,FSImage.saveFSImage can lose edits,This is a continuation of a discussion from HDFS-909. The FSImage.saveFSImage function (implementing dfsadmin -saveNamespace) can corrupt the NN storage such that all current edits are lost.,shv,tlipcon,Blocker,Resolved,Fixed,09/Feb/10 00:01,06/Apr/10 21:21
Bug,HDFS-961,12455787,dfs_readdir incorrectly parses paths,"fuse-dfs dfs_readdir assumes that DistributedFileSystem#listStatus returns Paths with the same scheme/authority as the dfs.name.dir used to connect. If NameNode.DEFAULT_PORT port is used listStatus returns Paths that have authorities without the port (see HDFS-960), which breaks the following code. 

{code}
// hack city: todo fix the below to something nicer and more maintainable but
// with good performance
// strip off the path but be careful if the path is solely '/'
// NOTE - this API started returning filenames as full dfs uris
const char *const str = info[i].mName + dfs->dfs_uri_len + path_len + ((path_len == 1 && *path == '/') ? 0 : 1);
{code}

Let's make the path parsing here more robust. listStatus returns normalized paths so we can find the start of the path by searching for the 3rd slash. A more long term solution is to have hdfsFileInfo maintain a path object or at least pointers to the relevant URI components.",eli,eli,Major,Closed,Fixed,09/Feb/10 16:33,24/Aug/10 20:51
Bug,HDFS-965,12455830,TestDelegationToken fails in trunk,TestDelegationToken is failing in trunk because of superuser authorization check. The superuser group and ip are required to be configured in the test.,jnp,jnp,Major,Closed,Fixed,09/Feb/10 21:09,24/Aug/10 20:51
Bug,HDFS-966,12455839,NameNode recovers lease even in safemode,The NameNode recovers a lease even when it is in safemode. ,dhruba,dhruba,Major,Closed,Fixed,09/Feb/10 22:42,24/Aug/10 20:51
Bug,HDFS-970,12456084,FSImage writing should always fsync before close,"Without an fsync, it's common that filesystems will delay the writing of metadata to the journal until all of the data blocks have been flushed. If the system crashes while the dirty pages haven't been flushed, the file is left in an indeterminate state. In some FSs (eg ext4) this will result in a 0-length file. In others (eg XFS) it will result in the correct length but any number of data blocks getting zeroed. Calling FileChannel.force before closing the FSImage prevents this issue.",tlipcon,tlipcon,Critical,Closed,Fixed,11/Feb/10 23:37,12/Dec/11 06:19
Bug,HDFS-977,12456227,DataNode.createInterDataNodeProtocolProxy() guards a log at the wrong level,"My IDE tells me that this code snippet in {{DataNode.createInterDataNodeProtocolProxy()}} is guarding the info log entry at debug level, and it should be reviewed
{code}
    if (InterDatanodeProtocol.LOG.isDebugEnabled()) {
      InterDatanodeProtocol.LOG.info(""InterDatanodeProtocol addr="" + addr);
    }
{code}",qwertymaniac,stevel@apache.org,Trivial,Resolved,Fixed,13/Feb/10 15:30,06/Jun/11 11:22
Bug,HDFS-988,12456712,saveNamespace race can corrupt the edits log,"The adminstrator puts the namenode is safemode and then issues the savenamespace command. This can corrupt the edits log. The problem is that  when the NN enters safemode, there could still be pending logSycs occuring from other threads. Now, the saveNamespace command, when executed, would save a edits log with partial writes. I have seen this happen on 0.20.

https://issues.apache.org/jira/browse/HDFS-909?focusedCommentId=12828853&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#action_12828853",eli,dhruba,Blocker,Closed,Fixed,18/Feb/10 21:11,12/Jul/13 18:34
Bug,HDFS-995,12456970,Replace usage of FileStatus#isDir(),"HADOOP-6585 is going to deprecate FileStatus#isDir(). This jira is for replacing all uses of isDir() in HDFS with checks of isDirectory(), isFile(), or isSymlink() as needed.",eli,eli,Blocker,Closed,Fixed,22/Feb/10 03:38,24/Aug/10 20:51
Bug,HDFS-996,12457056,JUnit tests should never depend on anything in conf,Similar to MAPREDUCE-1369 we need to make sure that nothing in conf is used in the unit tests. ,cos,cos,Blocker,Resolved,Fixed,22/Feb/10 19:09,21/Apr/11 23:32
Bug,HDFS-1000,12457269,libhdfs needs to be updated to use the new UGI,libhdfs needs to be updated w.r.t the APIs in the new UGI.,ddas,ddas,Blocker,Closed,Fixed,24/Feb/10 09:05,24/Aug/10 20:51
Bug,HDFS-1001,12457390,DataXceiver and BlockReader disagree on when to send/recv CHECKSUM_OK,"Running the TestPread with additional debug statements reveals that the BlockReader sends CHECKSUM_OK when the DataXceiver doesn't expect it. Currently it doesn't matter since DataXceiver closes the connection after each op, and CHECKSUM_OK is the last thing on the wire. But if we want to cache connections, they need to agree on the exchange of CHECKSUM_OK.",bcwalrus,bcwalrus,Minor,Resolved,Fixed,25/Feb/10 08:24,19/Nov/10 09:12
Bug,HDFS-1006,12457632,getImage/putImage http requests should be https for the case of security enabled.,should use https:// and port 50475,boryas,boryas,Major,Closed,Fixed,27/Feb/10 02:36,12/Dec/11 06:19
Bug,HDFS-1007,12457689,HFTP needs to be updated to use delegation tokens,HFTPFileSystem should be updated to use the delegation tokens so that it can talk to the secure namenodes.,ddas,ddas,Major,Closed,Fixed,28/Feb/10 23:24,12/Dec/11 06:19
Bug,HDFS-1010,12457702,HDFSProxy: Retrieve group information from UnixUserGroupInformation instead of LdapEntry,"LdapIpFilter in HDFSProxy shouldn't refer to the userClass attribute for group information, instead should retrieve the group association for the user based on the Unix ugi. 

",sriksun,sriksun,Major,Closed,Fixed,01/Mar/10 07:30,24/Aug/10 20:51
Bug,HDFS-1014,12457805,Error in reading delegation tokens from edit logs., When delegation tokens are read from the edit logs...same object is used to read the identifier and is stored in the token cache. This is wrong because same object is getting updated.,jnp,jnp,Major,Closed,Fixed,02/Mar/10 00:42,24/Aug/10 20:52
Bug,HDFS-1015,12457901,Intermittent failure in TestSecurityTokenEditLog,This test fails sometimes in hadoop-0.20.100-secondary build. It doesn't fail in trunk or  hadoop-0.20.100 build.,jnp,jnp,Major,Closed,Fixed,02/Mar/10 19:31,24/Aug/10 20:52
Bug,HDFS-1017,12457935,browsedfs jsp should call JspHelper.getUGI rather than using createRemoteUser(),"Currently the JSP for browsing the file system calls getRemoteUser(), which doesn't correctly auth the user on the web, causing failures when trying to browse the web.  It should call the utility method JspHelper.getUGI instead.",jghoman,jghoman,Major,Closed,Fixed,03/Mar/10 05:00,12/Dec/11 06:19
Bug,HDFS-1019,12458040,Incorrect default values for delegation tokens in hdfs-default.xml, The default values for delegation token parameters in hdfs-default.xml are incorrect.,jnp,jnp,Major,Closed,Fixed,03/Mar/10 21:32,12/Dec/11 06:18
Bug,HDFS-1020,12458041,The canceller and renewer for delegation tokens should be long names.," Currently delegation token API uses short names for renewer and canceller. Long names should be used instead, because job-tracker will set long name for the renewer in the token.",jnp,jnp,Major,Resolved,Fixed,03/Mar/10 21:56,05/Aug/10 21:18
Bug,HDFS-1021,12458046,specify correct server principal for RefreshAuthorizationPolicyProtocol and RefreshUserToGroupMappingsProtocol protocols in DFSAdmin (for HADOOP-6612),,boryas,boryas,Major,Closed,Fixed,03/Mar/10 22:45,12/Dec/11 06:19
Bug,HDFS-1022,12458190,Merge under-10-min tests specs into one file,"Build target test-commit test target invokes macro-test-runner three times with three different files. This is a problem because macro-test-runner deletes logs before each run.

The proposed solution is to merge all tests (common, hdfs, mapred) into one files since it doesn't seem to be possible to call macro-test-runner with three files as argument (or to change macro-test-runner to make it possible).",zasran,zasran,Major,Closed,Fixed,04/Mar/10 23:23,02/May/13 02:29
Bug,HDFS-1024,12458299,SecondaryNamenode fails to checkpoint because namenode fails with CancelledKeyException,The secondary namenode fails to retrieve the entire fsimage from the Namenode. It fetches a part of the fsimage but believes that it has fetched the entire fsimage file and proceeds ahead with the checkpointing. Stack traces will be attached below.,dms,dhruba,Blocker,Closed,Fixed,05/Mar/10 21:51,24/Aug/10 20:52
Bug,HDFS-1027,12458343,Update  year to 2010.,"Copyright year needs to be updated from 2009 to 2010.


{code:xml}
<!-- The following are used to construct a copyright statement -->
  <year>2009</year>
  <vendor>The Apache Software Foundation.</vendor>
  <copyright-link>http://www.apache.org/licenses/</copyright-link>
{code}",raviphulari,raviphulari,Trivial,Resolved,Fixed,06/Mar/10 07:01,31/May/10 22:28
Bug,HDFS-1029,12458471,Image corrupt with number of files = 1,"Last week I recovered a corrupt namenode image that was completely sane except that the ""number of files"" in the header was set to 1, rather than the correct number (many million). The NN in question had been running for some time, so I believe the 2NN uploaded this broken image as a checkpoint. After this point, of course, no further checkpoints occurred, and the NN failed to load its image upon restart.

Not sure how this happens - my only thought is that we may need to add synchronization on the nsCount field in INodeDirectoryWithQuota, but that's a long shot.",,tlipcon,Major,Resolved,Fixed,08/Mar/10 19:00,12/Mar/15 02:00
Bug,HDFS-1036,12458860,in DelegationTokenFetch dfs.getURI returns no port,dfs.getUri().getPort() returns -1.,boryas,boryas,Major,Closed,Fixed,11/Mar/10 22:24,12/Dec/11 06:19
Bug,HDFS-1038,12458883,In nn_browsedfscontent.jsp fetch delegation token only if security is enabled.,"nn_browsedfscontent.jsp  calls getDelegationToken even if security is disabled, which causes NPE. ",jnp,jnp,Major,Closed,Fixed,12/Mar/10 01:31,12/Dec/11 06:18
Bug,HDFS-1039,12459169,Service should be set in the token in JspHelper.getUGI," The delegation token added to the UGI in getUGI method in the JspHelper does not have service set. Therefore, this token cannot be used to connect to the namenode.",jnp,jnp,Major,Closed,Fixed,15/Mar/10 17:13,12/Dec/11 06:19
Bug,HDFS-1041,12459427,DFSClient does not retry in getFileChecksum(..),"If connection to the first datanode fails, DFSClient does not retry in getFileChecksum(..).",szetszwo,szetszwo,Major,Closed,Fixed,17/Mar/10 20:39,06/May/12 19:34
Bug,HDFS-1044,12459464,Cannot submit mapreduce job from secure client to unsecure sever,Looks like it tries to get DelegationToken and fails because SecureManger on Server doesn't start in non-secure environment.,boryas,boryas,Major,Closed,Fixed,18/Mar/10 00:27,12/Dec/11 06:19
Bug,HDFS-1045,12459471,"In secure clusters, re-login is necessary for https clients before opening connections",Ticket credentials expire and therefore clients opening https connections (only the NN and SNN doing image/edits exchange) should re-login before opening those connections.,jghoman,jghoman,Major,Closed,Fixed,18/Mar/10 01:55,12/Dec/11 06:18
Bug,HDFS-1046,12459549,Build fails trying to download an old version of tomcat,"It looks like HDFSProxy is trying to get an old version of tomcat (6.0.18).  
/grid/0/hudson/hudson-slave/workspace/Hadoop-Hdfs-trunk/trunk/src/contrib/hdfsproxy/build.xml:292: org.codehaus.cargo.container.ContainerException: Failed to download [http://apache.osuosl.org/tomcat/tomcat-6/v6.0.18/bin/apache-tomcat-6.0.18.zip]

Looking at http://apache.osuosl.org/tomcat/tomcat-6/ , it looks like the only two version available are 6.0.24 and 6.0.26.


",sriksun,garymurry,Blocker,Closed,Fixed,18/Mar/10 17:51,24/Aug/10 20:52
Bug,HDFS-1072,12460865,AlreadyBeingCreatedException with HDFS_NameNode as the lease holder,"TestReadWhileWriting may fail by AlreadyBeingCreatedException with HDFS_NameNode as the lease holder, which indicates that lease recovery is in an inconsistent state.",zasran,szetszwo,Major,Closed,Fixed,31/Mar/10 21:59,24/Aug/10 20:52
Bug,HDFS-1074,12461034,TestProxyUtil fails,"TestProxyUtil failed a few Hudson builds, including [#289|http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/289/testReport/org.apache.hadoop.hdfsproxy/TestProxyUtil/testSendCommand/], [#287|http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/289/testReport/org.apache.hadoop.hdfsproxy/TestProxyUtil/testSendCommand/], etc.
{noformat}
junit.framework.AssertionFailedError: null
	at org.apache.hadoop.hdfsproxy.TestProxyUtil.testSendCommand(TestProxyUtil.java:43)
{noformat}",sriksun,szetszwo,Major,Closed,Fixed,02/Apr/10 17:00,24/Aug/10 20:52
Bug,HDFS-1080,12461191,SecondaryNameNode image transfer should use the defined http address rather than local ip address,"Currently when telling the NN where to get the merged image, SNN uses getLocalHost.getAddr(), which may not return the correct ip addr.  ",jghoman,jghoman,Major,Closed,Fixed,05/Apr/10 23:22,12/Dec/11 06:19
Bug,HDFS-1085,12461316,hftp read  failing silently,"When performing a massive distcp through hftp, we saw many tasks fail with 

{quote}
2010-04-06 17:56:43,005 INFO org.apache.hadoop.tools.DistCp: FAIL 2010/0/part-00032 : java.io.IOException: File size not matched: copied 193855488 bytes (184.9m) to tmpfile (=hdfs://omehost.com:8020/somepath/part-00032)
but expected 1710327403 bytes (1.6g) from hftp://someotherhost/somepath/part-00032
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:435)
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:543)
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:310)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
        at org.apache.hadoop.mapred.Child.main(Child.java:159)
{quote}

This means that read itself didn't fail but the resulted file was somehow smaller.
",szetszwo,knoguchi,Major,Closed,Fixed,06/Apr/10 22:49,16/Jul/12 17:57
Bug,HDFS-1088,12461481,Prevent renaming a symlink to its target ,"Here's the HDFS side of HADOOP-6563: 
* Small bug fix for renaming a symlink to it's target
* Hdfs#getFileBlockLocations needs to use getUriPath() to ensure an absolute path is passed in. See minor change to INode#getPathNames, which now asserts instead of returning null and causing NPEs. This was exposed via calling FileContext#getFileBlockLocations using a symlink.
* Comment in FSImage around symlinks and image loading
",eli,eli,Major,Closed,Fixed,08/Apr/10 02:10,02/May/13 02:29
Bug,HDFS-1097,12462110,"Many allocated memory pointera of libhdfs maybe null, which maybe cause coredump.","Many allocated memory pointera of libhdfs maybe null, which maybe cause coredump.",,mry.maillist,Major,Resolved,Fixed,15/Apr/10 07:01,30/Jul/14 17:45
Bug,HDFS-1101,12462503,TestDiskError.testLocalDirs() fails,{{TestDiskError.testLocalDirs()}} fails with {{FileNotFoundException}},cdouglas,shv,Major,Closed,Fixed,19/Apr/10 22:56,24/Aug/10 20:52
Bug,HDFS-1104,12462712,Fsck triggers full GC on NameNode,"A NameNode at one of our clusters fell into full GC while fsck was performed. Digging into the problem shows that it is caused by how NameNode handles the access time of a file.

Fsck calls open on every file in the checked directory to get the file's block locations. Each open changes the file's access time and then leads to writing a transaction entry to the edit log. The current code optimizes open so that it returns without issuing synchronizing the edit log to the disk. It happened that in our cluster no other jobs were running while fsck was performed. No edit log sync was ever called. So all open transactions were kept in memory. When the edit log buffer got full, it automatically doubled its space by allocating a new buffer.  Full GC happened when no contiguous space were found when allocating a new bigger buffer.",hairong,hairong,Major,Closed,Fixed,21/Apr/10 22:12,24/Aug/10 20:52
Bug,HDFS-1109,12462911,HFTP and URL Encoding,"We just saw this error happen in our cluster. If there is a file that has a ""+"" sign in the name it is not readable through HFTP protocol.
The problem is when we are reading a file with HFTP we are passing a name of the file as a parameter in request and + gets undecoded into space on the server side. So the datanode receiving the streamFile request tries to access a file with space instead of + in the name and doesn't find that file.

The proposed solution is to pass the filename as a part of URL as with all the other HFTP commands, since this is the only place where it is not being treated this way. Are there any objections to this?",dms,dms,Major,Closed,Fixed,23/Apr/10 23:40,29/Aug/12 07:32
Bug,HDFS-1112,12463145,Edit log buffer should not grow unboundedly,"Currently HDFS does not impose an upper limit on the edit log buffer. In case there are a large number of open operations coming in with access time update on, since open does not call sync automatically, there is a possibility that the buffer grow to a large size, therefore causes memory leak and full GC in extreme cases as described in HDFS-1104. 

The edit log buffer should be automatically flushed when the buffer becomes full.",hairong,hairong,Major,Closed,Fixed,27/Apr/10 20:32,28/Feb/12 07:21
Bug,HDFS-1118,12463277,DFSOutputStream socket leak when cannot connect to DataNode,"The offending code is in {{DFSOutputStream.nextBlockOutputStream}}

This function retries several times to call {{createBlockOutputStream}}. Each time when it fails, it leaves a {{Socket}} object in {{DFSOutputStream.s}}.
That object is never closed, but overwritten the next time {{createBlockOutputStream}} is called.
",zshao,zshao,Major,Closed,Fixed,29/Apr/10 00:18,19/Oct/11 00:26
Bug,HDFS-1130,12463740,Pass Administrator acl to HTTPServer for common servlet access.,"Once HADOOP-6748 is done, HDFS should pass administrator acl when HTTPServer is constructed.",ddas,amareshwari,Major,Closed,Fixed,05/May/10 05:53,12/Dec/11 06:19
Bug,HDFS-1138,12464171,Modification times are being overwritten when FSImage loads,"A very easy way to spot the bug is to do a second restart in TestRestartDFS and check that the modification time on root is the same as it was before the second restart.

The problem is modifying time of the parent if the modification time of the child is greater than parent's in addToParent.

So if you have /DIR/File then on creation of a file modification time of the DIR will be set, but on cluster restart, or when secondary is checkpointing and reading the image it will add DIR to ""/"" and write the new modification time for ""/"" which is the modification time of DIR.

This is clearly a bug. I will attach a patch with one more parameter being passed from the loadFSImage that says to not propagate the time.",dms,dms,Major,Closed,Fixed,10/May/10 19:20,12/Dec/11 06:19
Bug,HDFS-1141,12464189,completeFile does not check lease ownership,completeFile should check that the caller still owns the lease of the file that it's completing. This is for the 'testCompleteOtherLeaseHoldersFile' case in HDFS-1139.,tlipcon,tlipcon,Blocker,Closed,Fixed,10/May/10 23:08,19/Oct/11 00:26
Bug,HDFS-1145,12464283,When NameNode is shutdown it tries to exit safemode,"Suppose the NameNode is in safemode. Then we try to shuut it down by invoking NameNode.stop(). The stop() method interrupts all waiting threads, which in turn, causes the SafeMode monitor to exit and thus triggering replicating/deleting of blocks.",dhruba,dhruba,Major,Closed,Fixed,11/May/10 19:38,12/Dec/11 06:19
Bug,HDFS-1146,12464292,Javadoc for getDelegationTokenSecretManager in FSNamesystem,Javadoc is missing for public method getDelegationTokenSecretManager in FSNamesystem.,jnp,jnp,Major,Closed,Fixed,11/May/10 20:29,12/Dec/11 06:18
Bug,HDFS-1149,12464382,Lease reassignment is not persisted to edit log,"During lease recovery, the lease gets reassigned to a special NN holder. This is not currently persisted to the edit log, which means that after an NN restart, the original leaseholder could end up allocating more blocks or completing a file that has already started recovery.",atm,tlipcon,Major,Resolved,Fixed,12/May/10 19:55,26/Apr/12 20:33
Bug,HDFS-1153,12464494,dfsnodelist.jsp should handle invalid input parameters,Navigation to dfsnodelist.jsp  with invalid input parameters produces NPE and HTTP 500 error. ,raviphulari,raviphulari,Minor,Closed,Fixed,14/May/10 01:13,07/Sep/12 21:09
Bug,HDFS-1157,12464702,Modifications introduced by HDFS-1150 are breaking aspect's bindings,Modifications introduced by HDFS-1150 to DataNode class brakes the binding of some of Herriot (test framework) bindings. This JIRA is to track the fix.,cos,cos,Major,Closed,Fixed,17/May/10 17:51,02/May/13 02:29
Bug,HDFS-1159,12464748,clean-cache target removes wrong ivy cache,"target clean-cache removes Common's cache instead of DHFS one. Thus, veryclean target is rendered useless.",cos,cos,Major,Closed,Fixed,18/May/10 02:32,24/Aug/10 20:52
Bug,HDFS-1163,12464845,normalize property names for JT/NN kerberos principal names in configuration (from HADOOP 6633),,boryas,boryas,Major,Closed,Fixed,18/May/10 23:57,12/Dec/11 06:20
Bug,HDFS-1164,12464902,TestHdfsProxy is failing,"TestHdfsProxy is failing on trunk, seen in HDFS-1132 and HDFS-1143. It doesn't look like hudson posts test results for contrib and it's hard to see what's going on from the raw console output. Can someone with access to hudson upload the individual test output for TestHdfsProxy so we can see what the issue is?",tlipcon,eli,Major,Closed,Fixed,19/May/10 15:44,19/Oct/11 00:25
Bug,HDFS-1166,12464988,0.21 nightly snapshot build has dependency on 0.22 snapshot,The POM generated in https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-hdfs/0.21.0-SNAPSHOT/ has a reference to hadoop-core 0.22.0-SNAPSHOT,,kimballa,Major,Resolved,Fixed,20/May/10 02:43,30/Jul/14 19:39
Bug,HDFS-1172,12465184,Blocks in newly completed files are considered under-replicated too quickly,"I've seen this for a long time, and imagine it's a known issue, but couldn't find an existing JIRA. It often happens that we see the NN schedule replication on the last block of files very quickly after they're completed, before the other DNs in the pipeline have a chance to report the new block. This results in a lot of extra replication work on the cluster, as we replicate the block and then end up with multiple excess replicas which are very quickly deleted.",iwasakims,tlipcon,Major,Resolved,Fixed,22/May/10 06:20,30/Aug/16 01:43
Bug,HDFS-1173,12465304,Fix references to 0.22 in 0.21 branch,Build files in the 0.21 branch incorrectly refer to 0.22.,tomwhite,tomwhite,Major,Closed,Fixed,24/May/10 20:15,02/May/13 02:29
Bug,HDFS-1176,12465522,Unsupported symbols in ClientProtocol.java (line 602),JavaDoc of setSafeMode method contains illegal UTF-8 symbol ,,cos,Major,Resolved,Fixed,27/May/10 01:01,11/Aug/11 16:27
Bug,HDFS-1181,12465811,Move configuration and script files post project split,,tomwhite,tomwhite,Blocker,Closed,Fixed,31/May/10 21:43,24/Aug/10 20:52
Bug,HDFS-1186,12466120,0.20: DNs should interrupt writers at start of recovery,"When block recovery starts (eg due to NN recovering lease) it needs to interrupt any writers currently writing to those blocks. Otherwise, an old writer (who hasn't realized he lost his lease) can continue to write+sync to the blocks, and thus recovery ends up truncating data that has been sync()ed.",tlipcon,tlipcon,Blocker,Closed,Fixed,03/Jun/10 20:03,19/Oct/11 00:25
Bug,HDFS-1189,12466319,Quota counts missed between clear quota and set quota,"HDFS Quota counts will be missed between a clear quota operation and a set quota.

When setting quota for a dir, the INodeDirectory will be replaced by INodeDirectoryWithQuota and dir.isQuotaSet() becomes true. When INodeDirectoryWithQuota  is newly created, quota counting will be performed. However, when clearing quota, the quota conf is set to -1 and dir.isQuotaSet() becomes false while INodeDirectoryWithQuota will NOT be replaced back to INodeDirectory.

FSDirectory.updateCount just update the quota count for inodes that isQuotaSet() is true. So after clear quota for a dir, its quota counts will not be updated and it's reasonable. But when re seting quota for this dir, quota counting will not be performed and some counts will be missed.",johnvijoe,xiaokang,Major,Closed,Fixed,07/Jun/10 07:37,02/Sep/11 22:16
Bug,HDFS-1192,12466383,refreshSuperUserGroupsConfiguration should use server side configuration for the refresh (for HADOOP-6815),"Currently refreshSuperUserGroupsConfiguration is using client side Configuration.
One of the issues with this is that if the cluster is restarted it will loose the ""refreshed' values (unless they are copied to the NameNode/JobTracker machine).
",boryas,boryas,Major,Closed,Fixed,07/Jun/10 21:04,12/Dec/11 06:19
Bug,HDFS-1193,12466430,-mvn-system-deploy target is broken which inturn fails the mvn-deploy task leading to unstable mapreduce build.,"-mvn-system-deploy:
[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-http:jar:1.0-beta-2:runtime
[artifact:deploy] Deploying to https://repository.apache.org/content/repositories/snapshots
[artifact:deploy] [INFO] Retrieving previous build number from apache.snapshots.https
[artifact:deploy] Uploading: org/apache/hadoop/hadoop-hdfs-instrumented/0.22.0-SNAPSHOT/hadoop-hdfs-instrumented-0.22.0-20100608.071421-8.jar to apache.snapshots.https
[artifact:deploy] Uploaded 990K
[artifact:deploy] [INFO] Retrieving previous metadata from apache.snapshots.https
[artifact:deploy] [INFO] Uploading repository metadata for: 'snapshot org.apache.hadoop:hadoop-hdfs-instrumented:0.22.0-SNAPSHOT'
[artifact:deploy] [INFO] Retrieving previous metadata from apache.snapshots.https
[artifact:deploy] [INFO] Uploading repository metadata for: 'artifact org.apache.hadoop:hadoop-hdfs-instrumented'
[artifact:deploy] [INFO] Uploading project information for hadoop-hdfs-instrumented 0.22.0-20100608.071421-8
[artifact:deploy] [INFO] Retrieving previous build number from apache.snapshots.https
[artifact:deploy] Uploading: org/apache/hadoop/hadoop-hdfs-instrumented/0.22.0-SNAPSHOT/hadoop-hdfs-instrumented-0.22.0-20100608.071421-8-sources.jar to apache.snapshots.https
[artifact:deploy] Uploaded 610K
[artifact:deploy] An error has occurred while processing the Maven artifact tasks.
[artifact:deploy]  Diagnosis:
[artifact:deploy] 
[artifact:deploy] Invalid reference: 'hadoop.hdfs.instrumented.test'
",gkesavan,gkesavan,Major,Closed,Fixed,08/Jun/10 07:17,24/Aug/10 20:52
Bug,HDFS-1197,12466669,"Blocks are considered ""complete"" prematurely after commitBlockSynchronization or DN restart","I saw this failure once on my internal Hudson job that runs the append tests 48 times a day:
junit.framework.AssertionFailedError: expected:<114688> but was:<98304>
	at org.apache.hadoop.hdfs.AppendTestUtil.check(AppendTestUtil.java:112)
	at org.apache.hadoop.hdfs.TestFileAppend3.testTC2(TestFileAppend3.java:116)
",tlipcon,tlipcon,Major,Resolved,Fixed,10/Jun/10 16:57,16/Mar/12 17:41
Bug,HDFS-1198,12466681,Resolving cross-realm principals,This jira covers hdfs changes corresponding to HADOOP-6603. Kerberos has bug in resolving cross realm principals. This jira provides a work-around for that.,jnp,jnp,Major,Closed,Fixed,10/Jun/10 19:05,12/Dec/11 06:19
Bug,HDFS-1202,12466791,DataBlockScanner throws NPE when updated before initialized,Missing an isInitialized() check in updateScanStatusInternal,tlipcon,tlipcon,Major,Closed,Fixed,11/Jun/10 23:45,19/Oct/11 00:26
Bug,HDFS-1204,12466793,"0.20: Lease expiration should recover single files, not entire lease holder",This was brought up in HDFS-200 but didn't make it into the branch on Apache.,rash37,tlipcon,Major,Closed,Fixed,12/Jun/10 00:09,19/Oct/11 00:26
Bug,HDFS-1206,12467014,TestFiHFlush fails intermittently,"When I was testing HDFS-1114, the patch passed all tests except TestFiHFlush.  Then, I tried to print out some debug messages, however, TestFiHFlush succeeded after added the messages.

TestFiHFlush probably depends on the speed of BlocksMap.  If BlocksMap is slow enough, then it will pass.",cos,szetszwo,Major,Closed,Fixed,15/Jun/10 17:34,12/Dec/11 06:18
Bug,HDFS-1207,12467049,0.20-append: stallReplicationWork should be volatile,"the stallReplicationWork member in FSNamesystem is accessed by multiple threads without synchronization, but isn't marked volatile. I believe this is responsible for about 1% failure rate on TestFileAppend4.testAppendSyncChecksum* on my 8-core test boxes (looking at logs I see replication happening even though we've supposedly disabled it)",tlipcon,tlipcon,Major,Closed,Fixed,16/Jun/10 01:01,19/Oct/11 00:26
Bug,HDFS-1212,12467064,Harmonize HDFS JAR library versions with Common,HDFS part of HADOOP-6800.,tomwhite,tomwhite,Blocker,Closed,Fixed,16/Jun/10 05:33,24/Aug/10 20:52
Bug,HDFS-1215,12467125,TestNodeCount infinite loops on branch-20-append,"HDFS-409 made some minicluster changes, which got incorporated into one of the earlier 20-append patches. This breaks TestNodeCount so it infinite loops on the branch. This patch fixes it.",tlipcon,tlipcon,Major,Resolved,Fixed,16/Jun/10 21:11,16/Jun/10 21:22
Bug,HDFS-1218,12467129,20 append: Blocks recovered on startup should be treated with lower priority during block synchronization,"When a datanode experiences power loss, it can come back up with truncated replicas (due to local FS journal replay). Those replicas should not be allowed to truncate the block during block synchronization if there are other replicas from DNs that have _not_ restarted.",tlipcon,tlipcon,Critical,Closed,Fixed,16/Jun/10 21:56,29/Mar/12 09:12
Bug,HDFS-1240,12467239,TestDFSShell failing in branch-20,"After the backport of HDFS-909 into branch 20, TestDFSShell fails since it relies on resetting the base dir for minicluster through a system property. The backport changed MiniDFSCluster to read the property from an initializer instead of from the constructor.",tlipcon,tlipcon,Critical,Resolved,Fixed,17/Jun/10 16:53,02/May/13 02:29
Bug,HDFS-1249,12467319,"with fuse-dfs, chown which only has owner (or only group) argument fails with Input/output error.","with fuse-dfs, chown which only has owner (or only group) argument fails with Input/output error.
----------
/mnt/hdfs/tmp# chown root file1
chown: changing ownership of `file1': Input/output error
/mnt/hdfs/tmp# chown root:root file1
/mnt/hdfs/tmp# chown :root file1
chown: changing group of `file1': Input/output error
----------
I think it should be treated as unchanged for missing part(owner or group) instead of returning an error.

I took fuse_dfs log and it is saying
----------
unique: 25, opcode: SETATTR (4), nodeid: 14, insize: 128
chown /tmp/file1 0 4294967295
could not lookup group -1
   unique: 25, error: -5 (Input/output error), outsize: 16
unique: 26, opcode: SETATTR (4), nodeid: 14, insize: 128
chown /tmp/file1 0 0
getattr /tmp/file1
   unique: 26, success, outsize: 120
unique: 27, opcode: SETATTR (4), nodeid: 14, insize: 128
chown /tmp/file1 4294967295 0
could not lookup userid -1
   unique: 27, error: -5 (Input/output error), outsize: 16
----------

therefore this should happen because dfs_chown() in src/contrib/fuse-dfs/src/fuse_impls_chown.c has following
----------
...
  user = getUsername(uid);
  if (NULL == user) {
    syslog(LOG_ERR,""Could not lookup the user id string %d\n"",(int)uid); 
    fprintf(stderr, ""could not lookup userid %d\n"", (int)uid); 
    ret = -EIO;
  }

  if (0 == ret) {
    group = getGroup(gid);
    if (group == NULL) {
      syslog(LOG_ERR,""Could not lookup the group id string %d\n"",(int)gid); 
      fprintf(stderr, ""could not lookup group %d\n"", (int)gid); 
      ret = -EIO;
    } 
  }
...
----------

but actually, hdfsChown() in src/c++/libhdfs/hdfs.c has this
----------
...
    if (owner == NULL && group == NULL) {
      fprintf(stderr, ""Both owner and group cannot be null in chown"");
      errno = EINVAL;
      return -1;
    }
...
----------

and also, setOwner seems allowing NULL
----------
username - If it is null, the original username remains unchanged.
groupname - If it is null, the original groupname remains unchanged.
----------
according to the api document.

therefore, I think fuse_impls_chown.c should not treat only user(or only group) lookup fail as an error.",cmccabe,kentaro,Minor,Closed,Fixed,18/Jun/10 07:26,11/Oct/12 17:46
Bug,HDFS-1250,12467379,Namenode accepts block report from dead datanodes,When a datanode heartbeat times out namenode marks it dead. The subsequent heartbeat from the datanode is rejected with a command to datanode to re-register. However namenode accepts block report from the datanode although it is marked dead.,sureshms,sureshms,Major,Closed,Fixed,18/Jun/10 22:01,12/Dec/11 06:19
Bug,HDFS-1254,12467517,0.20: mark dfs.support.append to be true by default for the 0.20-append branch,The 0.20-append branch supports append/sync for HDFS. Change the default configuration to enable append.,dhruba,dhruba,Major,Resolved,Fixed,22/Jun/10 01:09,12/Jul/10 18:11
Bug,HDFS-1255,12467535,test-libhdfs.sh fails,This is a consequence of bin scripts having moved (see HADOOP-6794).,tomwhite,tomwhite,Blocker,Closed,Fixed,22/Jun/10 04:39,24/Aug/10 20:52
Bug,HDFS-1256,12467538,libhdfs is missing from the tarball,"It is being compiled, but is not added to the distribution.",tomwhite,tomwhite,Blocker,Closed,Fixed,22/Jun/10 05:18,24/Aug/10 20:52
Bug,HDFS-1257,12467604,Race condition on FSNamesystem#recentInvalidateSets introduced by HADOOP-5124,"HADOOP-5124 provided some improvements to FSNamesystem#recentInvalidateSets. But it introduced unprotected access to the data structure recentInvalidateSets. Specifically, FSNamesystem.computeInvalidateWork accesses recentInvalidateSets without read-lock protection. If there is concurrent activity (like reducing replication on a file) that adds to recentInvalidateSets, the name-node crashes with a ConcurrentModificationException.",epayne,rvadali,Major,Closed,Fixed,22/Jun/10 18:34,28/Dec/11 10:03
Bug,HDFS-1258,12467613,"Clearing namespace quota on ""/"" corrupts FS image","The HDFS root directory starts out with a default namespace quota of Integer.MAX_VALUE. If you clear this quota (using ""hadoop dfsadmin -clrQuota /""), the fsimage gets corrupted immediately. Subsequent 2NN rolls will fail, and the NN will not come back up from a restart.",atm,atm,Blocker,Closed,Fixed,22/Jun/10 20:06,23/Sep/14 04:02
Bug,HDFS-1260,12467648,0.20: Block lost when multiple DNs trying to recover it to different genstamps,"Saw this issue on a cluster where some ops people were doing network changes without shutting down DNs first. So, recovery ended up getting started at multiple different DNs at the same time, and some race condition occurred that caused a block to get permanently stuck in recovery mode. What seems to have happened is the following:
- FSDataset.tryUpdateBlock called with old genstamp 7091, new genstamp 7094, while the block in the volumeMap (and on filesystem) was genstamp 7093
- we find the block file and meta file based on block ID only, without comparing gen stamp
- we rename the meta file to the new genstamp _7094
- in updateBlockMap, we do comparison in the volumeMap by oldblock *without* wildcard GS, so it does *not* update volumeMap
- validateBlockMetaData now fails with ""blk_7739687463244048122_7094 does not exist in blocks map""

After this point, all future recovery attempts to that node fail in getBlockMetaDataInfo, since it finds the _7094 gen stamp in getStoredBlock (since the meta file got renamed above) and then fails since _7094 isn't in volumeMap in validateBlockMetadata

Making a unit test for this is probably going to be difficult, but doable.",tlipcon,tlipcon,Critical,Closed,Fixed,23/Jun/10 00:29,19/Oct/11 00:26
Bug,HDFS-1264,12467743,0.20: OOME in HDFS client made an unrecoverable HDFS block,"Ran into a bad issue in testing overnight. One of the writers experienced an OOME in the middle of writing a checksum chunk to the stream inside a sync() call. It then proceeded to retry recovery on each DN in the pipeline, but each recovery failed because its internal checksum buffer was borked in some way - on the DNs I see ""Unexpected checksum mismatch"" errors after each recovery attempt.

When another client tried to recover the file using appendFile, it got the ""Partial CRC 3766269197 does not match value computed the  last time file was closed"" error (plus there was only one replica left in targets). It thus failed to set up the append pipeline, and ran into HDFS-1262.

This was on 0.20-append, though it may happen on trunk as well.",,tlipcon,Major,Resolved,Fixed,23/Jun/10 19:50,30/Jul/14 21:02
Bug,HDFS-1267,12467771,fuse-dfs does not compile,"Looks like since libhdfs was updated to use the new UGI (HDFS-1000) fuse-dfs no longer compiles:

{noformat}
     [exec] fuse_connect.c: In function 'doConnectAsUser':
     [exec] fuse_connect.c:40: error: too many arguments to function 'hdfsConnectAsUser'
{noformat}

Any takers to fix this please?",ddas,tomwhite,Critical,Closed,Fixed,24/Jun/10 05:27,24/Aug/10 20:52
Bug,HDFS-1283,12468682,ant eclipse-files has drifted again,"Library versions specified by ant eclipse-files have drifted from those used by ivy again.  Until HDFS-1035 meanders into the code, we need to make sure we keep these updated, otherwise bedlam breaks loose for Eclipse developers.",jghoman,jghoman,Major,Closed,Fixed,07/Jul/10 01:34,12/Dec/11 06:19
Bug,HDFS-1284,12468762,TestBlockToken fails,"Hudson runs fail several tests. {{TestBlockToken.testBlockTokenRpc}} is one of them.
[See here|http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/423/testReport/junit/org.apache.hadoop.hdfs.security.token.block/TestBlockToken/testBlockTokenRpc/]",kzhang,shv,Major,Closed,Fixed,07/Jul/10 21:50,12/Dec/11 06:19
Bug,HDFS-1289,12468859,Datanode secure mode is broken,"HDFS-520 introduced a new DataNode constructor, which tries to set up an RPC connection to the NN before a Kerberos login is done. This causes datanode to fail.",kzhang,kzhang,Major,Closed,Fixed,08/Jul/10 21:43,12/Dec/11 06:19
Bug,HDFS-1292,12468906,Allow artifacts to be published to the staging Apache Nexus Maven Repository,HDFS companion issue to HADOOP-6847.,gkesavan,tomwhite,Blocker,Resolved,Fixed,09/Jul/10 10:52,10/Nov/10 18:51
Bug,HDFS-1296,12469125,using delegation token over hftp for long running clients (spawn from hdfs-1007).,"this patch incorporates patches 
https://issues.apache.org/jira/secure/attachment/12446280/hdfs-1007-long-running-hftp-client.patch
and 
https://issues.apache.org/jira/secure/attachment/12446362/hdfs-1007-securityutil-fix.patch

patches are attached.",boryas,boryas,Major,Closed,Fixed,12/Jul/10 21:54,12/Dec/11 06:19
Bug,HDFS-1299,12469225,'compile-fault-inject' never should be called directly.,Although target 'compile-fault-inject' isn't listed as a top-level target for the build it is still can be called if a person can lurk into {{src/test/build/aop.xml}} file. The target doesn't set all needed environment and never should be called directly. ,cos,cos,Major,Closed,Fixed,14/Jul/10 06:47,24/Aug/10 20:52
Bug,HDFS-1301,12469307,TestHDFSProxy need to use server side conf for ProxyUser stuff.,"currently TestHdfsProxy sets hadoop.proxyuser.USER.groups in local copy of configuration. 
But ProxyUsers only looks at the server side config.
For test we can uses static method in ProxyUsers to load the config.
",boryas,boryas,Major,Closed,Fixed,15/Jul/10 00:25,02/May/13 02:29
Bug,HDFS-1308,12469666, job conf key for the services name of DelegationToken for HFTP url is constructed incorrectly in HFTPFileSystem (part of MR-1718),change HFTP init code that checks for existing delegation tokens,boryas,boryas,Major,Closed,Fixed,19/Jul/10 22:35,12/Dec/11 06:20
Bug,HDFS-1311,12469779,Running tests with 'testcase' cause triple execution of the same test case,The change introduced in HDFS-1199 has the following side effect: if a test case name is set via {{-Dtestcase}} the same test case will be executed three times.,cos,cos,Minor,Closed,Fixed,21/Jul/10 06:46,24/Aug/10 20:52
Bug,HDFS-1313,12469889,HdfsProxy changes from HDFS-481 missed in y20.1xx,Some changes went missing between https://issues.apache.org/jira/secure/attachment/12441028/HDFS-481-bp-y20s.patch and https://issues.apache.org/jira/secure/attachment/12442210/HDFS-481-NEW.patch. The missing changes went into y20.100 but not into y20.1xx. This patch fixes it. Without this hdfsproxy does not work.  ,rohini,rohini,Major,Closed,Fixed,22/Jul/10 06:33,24/Aug/10 20:52
Bug,HDFS-1314,12469946,dfs.blocksize accepts only absolute value,"Using ""dfs.block.size=8388608"" works 
but ""dfs.block.size=8mb"" does not.

Using ""dfs.block.size=8mb"" should throw some WARNING on NumberFormatException.
(http://pastebin.corp.yahoo.com/56129)
",sho.shimauchi,karims,Minor,Closed,Fixed,22/Jul/10 20:07,29/Feb/16 09:26
Bug,HDFS-1317,12469980,HDFSProxy needs additional changes to work after changes to streamFile servlet in HDFS-1109,"Before HDFS-1109, streamFile had filename passed as a parameter in the request. With HDFS-1109, the filename is part of the request path similar to listPaths and data servlets. The AuthorizationFilter in HdfsProxy needs updating to pick up the path from the request path instead of  looking for filename parameter. ",rohini,rohini,Major,Closed,Fixed,23/Jul/10 07:33,02/May/13 02:29
Bug,HDFS-1321,12470379,"If service port and main port are the same, there is no clear log message explaining the issue.","With the introduction of a service port to the namenode, there is now a chance for user error to set the two port equal.  This will cause the namenode to fail to start up.  It would be nice if there was a log message explaining the port clash.  Or just treat things as if the service port was not specified. ",jimplush,garymurry,Minor,Resolved,Fixed,28/Jul/10 21:31,26/Jun/11 12:44
Bug,HDFS-1322,12470502,Document umask in DistributedFileSystem#mkdirs javadocs ,"The javadoc of DFSClient.mkdirs() says that the permissions of created dir will be set to dirPermission, which is not done currently.  This should talk about umask.",cmccabe,ravidotg,Major,Closed,Fixed,30/Jul/10 10:13,15/Feb/13 13:12
Bug,HDFS-1327,12470681,TestNameNodeMetrics fails under windows. ,"On a windows machine running ant test -Dtestcase=testNameNodeMetrics causes failures.  The following error is ""Pathname /c:/workspace/<snip>/build/test/data from c:/workspace/<snip>/build/test/data is not a valid DFS filename"" is presented.  This error does not occur on Linux.",,garymurry,Minor,Resolved,Fixed,02/Aug/10 22:20,30/Jul/14 23:19
Bug,HDFS-1331,12470841,dfs -test should work like /bin/test,"hadoop dfs -test doesn't act like its shell equivalent, making it difficult to actually use if you are used to the real test command:

hadoop:
$hadoop dfs -test -d /nonexist; echo $?
test: File does not exist: /nonexist
255

shell:
$ test -d /nonexist; echo $?
1

a) Why is it spitting out a message? Even so, why is it saying file instead of directory when I used -d?

b) Why is the return code 255? I realize this is documented as '0' if true.  But docs basically say the value is undefined if it isn't.

c) where is -f?

d) Why is empty -z instead of -s ?  Was it a misunderstanding of the man page?
",adi2,aw,Minor,Closed,Fixed,04/Aug/10 21:48,12/May/16 18:13
Bug,HDFS-1334,12471018,open in HftpFileSystem does not add delegation tokens to the url.,"open method in HftpFileSystem uses ByteRangeInputStream for url connection. But it does not add the delegation tokens, even if security is enabled, to the url before passing it to the ByteRangeInputStream. Therefore request fails if security is enabled.",jnp,jnp,Major,Closed,Fixed,06/Aug/10 23:56,12/Dec/11 06:19
Bug,HDFS-1340,12471456,A null delegation token is appended to the url if security is disabled when browsing filesystem.,"  When filesystem is being browsed and if security is disabled a null delegation token is added to the url. Also if a user changes the url and adds any random string for delegation token, it is retained in the links on returned html page. If security is disabled no delegation token parameter is required on the url.",jnp,jnp,Major,Closed,Fixed,12/Aug/10 20:29,12/Dec/11 06:19
Bug,HDFS-1346,12471929,DFSClient receives out of order packet ack,"When running 0.20 patched with HDFS-101, we sometimes see an error as follow:
WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor exception for block blk_-2871223654872350746_21421120java.io.IOException: Responseprocessor: Expecting seq
no for block blk_-2871223654872350746_21421120 10280 but received 10281
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2570)

This indicates that DFS client expects an ack for packet N, but receives an ack for packet N+1.",hairong,hairong,Major,Closed,Fixed,18/Aug/10 18:18,19/Oct/11 00:26
Bug,HDFS-1347,12471957,TestDelegationToken uses mortbay.log for logging,needs to be changed to commons.log,boryas,boryas,Major,Closed,Fixed,18/Aug/10 23:18,12/Dec/11 06:19
Bug,HDFS-1349,12472119,Remove empty java files,"Doug identified some empty java files that should be deleted:

src/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatisticsMBean.java
src/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeStatistics.java
src/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatistics.java
src/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeStatisticsMBean.java",eli,tomwhite,Major,Closed,Fixed,20/Aug/10 18:32,12/Dec/11 06:19
Bug,HDFS-1352,12472381,Fix jsvc.location,"The jsvc specified in build.xml 404s, causing the build to fail, because version 1.0.2 has been archived. Let's update the url, not sure we want to move to 1.0.3 or play the game where the build breaks with every jsvc dot release.",eli,eli,Major,Closed,Fixed,24/Aug/10 14:48,12/Dec/11 06:18
Bug,HDFS-1355,12472514,ant veryclean (clean-cache) doesn't clean enough,"Looks like since HDFS-1159, ant veryclean no longer work as expected for the case when hadoop common jars are changed. The proposed patch does a more through cleaning on hadoop jars.",vicaya,vicaya,Major,Closed,Fixed,25/Aug/10 18:41,12/Dec/11 06:18
Bug,HDFS-1357,12472527,HFTP traffic served by DataNode shouldn't use service port on NameNode ,"HDFS-599 introduced a new service port on NameNode to separate system traffic (e.g., heartbeats/blockreports) from client file access requests so that they can be prioritized.  All Datanode traffic now goes to the service port. However, datanode also serves as a proxy for HFTP requests from client (served by StreamFile servlet). These HFTP traffic should continue to use the client port on NameNode. Moreover, using the service port for HFTP is incompatible with the existing way of selecting delegation tokens.",kzhang,kzhang,Major,Closed,Fixed,25/Aug/10 21:53,12/Dec/11 06:20
Bug,HDFS-1360,12472758,TestBlockRecovery should bind ephemeral ports,"TestBlockRecovery starts up a DN, but doesn't configure the various ports to be ephemeral, so the test fails if run on a machine where another DN is already running.",tlipcon,tlipcon,Minor,Resolved,Fixed,27/Aug/10 23:04,20/Apr/11 23:32
Bug,HDFS-1361,12472764,Add -fileStatus operation to NNThroughputBenchmark,getFileStatus() is a frequently used operation in HDFS. It important to benchmark the name-node throughput on it.,shv,shv,Major,Closed,Fixed,28/Aug/10 01:22,12/Dec/11 06:19
Bug,HDFS-1363,12472975,startFileInternal should return the last block of the file opened for append as an under-construction block,{{FSNamesystem.startFileInternal}} should convert the last block of the file opened for append to an under-construction block and return it. This will let remove the second synchronized section in {{FSNamesystem.appendFile()}} and avoid redundant computations and potential inconsistencies as stated in HDFS-1152.,shv,shv,Major,Resolved,Fixed,31/Aug/10 21:53,15/Sep/10 01:24
Bug,HDFS-1364,12472977,HFTP client should support relogin from keytab,"If a user starts a long-running HFTP client using a keytab, we should do relogin automatically whenever TGT expires. Currently, HFTP client uses TGT to fetch a delegation token and cache that delegation token for HFTP operations. The delegation token is automatically renewed/refetched using TGT. However, when TGT expires, delegation token renewal/refetch will fail and no further HFTP operation is possible. This is unsatisfactory since the user has given us her keytab. We should be able to relogin from keytab and continue.",jnp,kzhang,Major,Closed,Fixed,31/Aug/10 22:22,12/Dec/11 06:18
Bug,HDFS-1371,12473250,One bad node can incorrectly flag many files as corrupt,"On our cluster, 12 files were reported as corrupt by fsck even though the replicas on the datanodes were healthy.
Turns out that all the replicas (12 files x 3 replicas per file) were reported corrupt from one node.

Surprisingly, these files were still readable/accessible from dfsclient (-get/-cat) without any problems.

",tanping,knoguchi,Major,Resolved,Fixed,03/Sep/10 16:10,21/May/11 12:38
Bug,HDFS-1377,12473344,Quota bug for partial blocks allows quotas to be violated ,"There's a bug in the quota code that causes them not to be respected when a file is not an exact multiple of the block size. Here's an example:

{code}
$ hadoop fs -mkdir /test
$ hadoop dfsadmin -setSpaceQuota 384M /test
$ ls dir/ | wc -l   # dir contains 101 files
101
$ du -ms dir        # each is 3mb
304	dir
$ hadoop fs -put dir /test
$ hadoop fs -count -q /test
        none             inf       402653184      -550502400            2          101          317718528 hdfs://haus01.sf.cloudera.com:10020/test
$ hadoop fs -stat ""%o %r"" /test/dir/f30
134217728 3    # three 128mb blocks
{code}

INodeDirectoryWithQuota caches the number of bytes consumed by it's children in {{diskspace}}. The quota adjustment code has a bug that causes {{diskspace}} to get updated incorrectly when a file is not an exact multiple of the block size (the value ends up being negative). 

This causes the quota checking code to think that the files in the directory consumes less space than they actually do, so the verifyQuota does not throw a QuotaExceededException even when the directory is over quota. However the bug isn't visible to users because {{fs count -q}} reports the numbers generated by INode#getContentSummary which adds up the sizes of the blocks rather than use the cached INodeDirectoryWithQuota#diskspace value.

In FSDirectory#addBlock the disk space consumed is set conservatively to the full block size * the number of replicas:

{code}
updateCount(inodes, inodes.length-1, 0,
    fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);
{code}

In FSNameSystem#addStoredBlock we adjust for this conservative estimate by subtracting out the difference between the conservative estimate and what the number of bytes actually stored was:

{code}
//Updated space consumed if required.
INodeFile file = (storedBlock != null) ? storedBlock.getINode() : null;
long diff = (file == null) ? 0 :
    (file.getPreferredBlockSize() - storedBlock.getNumBytes());

if (diff > 0 && file.isUnderConstruction() &&
    cursize < storedBlock.getNumBytes()) {
...
    dir.updateSpaceConsumed(path, 0, -diff*file.getReplication());
{code}

We do the same in FSDirectory#replaceNode when completing the file, but at a file granularity (I believe the intent here is to correct for the cases when there's a failure replicating blocks and recovery). Since oldnode is under construction INodeFile#diskspaceConsumed will use the preferred block size  (vs of Block#getNumBytes used by newnode) so we will again subtract out the difference between the full block size and what the number of bytes actually stored was:

{code}
long dsOld = oldnode.diskspaceConsumed();
...
//check if disk space needs to be updated.
long dsNew = 0;
if (updateDiskspace && (dsNew = newnode.diskspaceConsumed()) != dsOld) {
  try {
    updateSpaceConsumed(path, 0, dsNew-dsOld);
...
{code}

So in the above example we started with diskspace at 384mb (3 * 128mb) and then we subtract 375mb (to reflect only 9mb raw was actually used) twice so for each file the diskspace for the directory is - 366mb (384mb minus 2 * 375mb). Which is why the quota gets negative and yet we can still write more files.

So a directory with lots of single block files (if you have multiple blocks on the final partial block ends up subtracting from the diskspace used) ends up having a quota that's way off.

I think the fix is to in FSDirectory#replaceNode not have the diskspaceConsumed calculations differ when the old and new INode have the same blocks. I'll work on a patch which also adds a quota test for blocks that are not multiples of the block size and warns in INodeDirectory#computeContentSummary if the computed size does not reflect the cached value.",eli,eli,Blocker,Closed,Fixed,05/Sep/10 18:33,08/Mar/12 02:10
Bug,HDFS-1381,12473498,HDFS javadocs hard-code references to dfs.namenode.name.dir and dfs.datanode.data.dir parameters,"The javadoc for MiniDFSCluster makes repeated references to setting dfs.name.dir and dfs.data.dir.  These should be replaced with references to DFSConfigKeys' DFS_NAMENODE_NAME_DIR_KEY and DFS_DATANODE_DATA_DIR_KEY, respectively.  The old values are deprecated in DFSConfigKeys, but we should switch to the new values where ever we can.

Also, a quick search the code shows that TestDFSStorageStateRecovery.java and UpgradeUtilities.java should be updated as well.",jimplush,jghoman,Major,Resolved,Fixed,07/Sep/10 22:00,29/Jul/11 19:31
Bug,HDFS-1382,12473506,A transient failure with edits log and a corrupted fstime together could lead to a data loss,"We experienced a data loss situation that due to double failures.
One is transient disk failure with edits logs and the other is corrupted fstime.
 
Here is the detail:
 
1. NameNode has 2 edits directory (say edit0 and edit1)
 
2. During an update to edit0, there is a transient disk failure,
making NameNode bump the fstime and mark edit0 as stale
and continue working with edit1. 
 
3. NameNode is shut down. Now, and unluckily fstime in edit0
is corrupted. Hence during NameNode startup, the log in edit0
is replayed, hence data loss.

This bug was found by our Failure Testing Service framework:
http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html
For questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and 
Haryadi Gunawi (haryadi@eecs.berkeley.edu)",,thanhdo,Major,Resolved,Fixed,07/Sep/10 22:28,30/Jul/14 20:23
Bug,HDFS-1386,12473747,TestJMXGet fails in jdk7,,jeagles,tanping,Blocker,Resolved,Fixed,10/Sep/10 00:12,20/Nov/13 13:27
Bug,HDFS-1390,12473873,Remove or support dfs.web.ugi,"The HDFS permissions guide still refers to {{dfs.web.ugi}}, and there are some vestiges in the code, but it no longer works post-security.

We should either remove the vestiges entirely and document the servletfilter approach to securing the web UI, or preferably, we could make the StaticUserWebFilter (HADOOP-6832) respect this configuration parameter (possibly with deprecation).",,tlipcon,Critical,Resolved,Fixed,10/Sep/10 22:53,30/Jul/14 23:53
Bug,HDFS-1399,12474105,Distinct minicluster services (e.g. NN and JT) overwrite each other's service policies,HDFS portion of HADOOP-6951.,atm,atm,Major,Closed,Fixed,14/Sep/10 18:42,12/Dec/11 06:19
Bug,HDFS-1404,12474329,TestNodeCount logic incorrect in branch-0.20,"The logic in TestNodeCount is wrong in one spot, causing it to sometimes time out and fail. This got fixed in trunk as a side effect of HDFS-409 but not in branch-20.",tlipcon,tlipcon,Minor,Resolved,Fixed,16/Sep/10 22:03,16/Sep/10 22:34
Bug,HDFS-1406,12474429,TestCLI fails on Ubuntu with default /etc/hosts,"Depending on the order of entries in /etc/hosts, TestCLI can fail. This is because it sets fs.default.name to ""localhost"", and then the bound IPC socket on the NN side reports its hostname as ""foobar-host"" if the entry for 127.0.0.1 lists ""foobar-host"" before ""localhost"". This seems to be the default in some versions of Ubuntu.",cos,tlipcon,Minor,Resolved,Fixed,17/Sep/10 19:01,21/Apr/11 23:32
Bug,HDFS-1409,12474552,"The ""register"" method of the BackupNode class should be ""UnsupportedActionException(""register"")""","The register method of the BackupNode class should be ""UnsupportedActionException(""register"")"" rather than  ""UnsupportedActionException(""journal"")"".
",chingshen,chingshen,Trivial,Resolved,Fixed,20/Sep/10 03:33,23/Nov/11 06:21
Bug,HDFS-1411,12474560,"The startup command of the Backup Node is ""bin/hdfs namenode -backup""","The startup command of the Backup Node is ""bin/hdfs namenode -backup""",chingshen,chingshen,Minor,Resolved,Fixed,20/Sep/10 04:32,23/Sep/10 20:09
Bug,HDFS-1413,12474649,Broken links to HDFS Wiki in hdfs site and documentation.,"# hdfs/site wiki tab points to ""http://wiki.apache.org/hadoop/DFS"", should be HDFS.
# hdfs documentation wiki tab points to ""http://wiki.apache.org/hadoop/hdfs"", should be HDFS.",shv,shv,Major,Resolved,Fixed,21/Sep/10 01:38,23/Sep/10 20:09
Bug,HDFS-1416,12474963,CHANGES.txt does not reflect the release version 0.21.0.,"CHANGES.txt still claims version 0.21.0 - Unreleased. It should show the release date instead and include section for for 0.21.1 - Unreleased. Latest changes, that did not make into 0.21.0, should be moved under 0.21.1 section.
This should also be fixed for common and mapreduce.",tomwhite,shv,Major,Resolved,Fixed,23/Sep/10 19:12,05/Oct/10 23:48
Bug,HDFS-1419,12475052,Federation: Three test cases need minor modification after the new block id change,"Three test cases

M       src/test/hdfs/org/apache/hadoop/hdfs/TestClientBlockVerification.java
M       src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailure.java
M       src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestBlockTokenWithDFS.java

needs blockpoolID to be added into the test cases.  

Minor change in all three of them is just to add a dummy ""test-blockpoolid""


     String file = BlockReader.getFileName(targetAddr,
+        ""test-blockpoolid"",
         block.getBlockId());
",tanping,tanping,Minor,Resolved,Fixed,24/Sep/10 17:01,28/Sep/10 09:48
Bug,HDFS-1420,12475068,Clover build doesn't generate per-test coverage,Because of the way the structure of Hadoop's builds is done Clover can't properly detect test classes among the sources. As the result clover reports are incomplete and do not provide viral per-test coverage info.,cos,cos,Major,Resolved,Fixed,24/Sep/10 19:12,05/Oct/10 20:45
Bug,HDFS-1422,12475085,hadoop-hdfs-trunk is broken due to HADOOP-6951 change to hadoop-common-trunk,"The recent check in to hadoop-common breaks hadoop-hdfs trunk.  The change was made to 

src/java/org/apache/hadoop/ipc/Server.java src/java/org/apache/hadoop/ipc/Server.java

The change makes refresh() method to be non-static

-  public static synchronized void refresh(Configuration conf,
+  public synchronized void refresh(Configuration conf,
                                           PolicyProvider provider) {

This breaks the invocation of this method in DataNode.java, NameNode.java.  Compilation errors : 

compile-hdfs-classes:
    [javac] Compiling 206 source files to /home/tanping/src/SVN/hadoop-hdfs-trunk/build/classes
    [javac] /home/tanping/src/SVN/hadoop-hdfs-trunk/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java:461: non-static method refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider) cannot be referenced from a static context
    [javac]       ServiceAuthorizationManager.refresh(conf, new HDFSPolicyProvider());
    [javac]                                  ^
    [javac] /home/tanping/src/SVN/hadoop-hdfs-trunk/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java:356: non-static method refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider) cannot be referenced from a static context
    [javac]       ServiceAuthorizationManager.refresh(conf, new HDFSPolicyProvider());
    [javac]                                  ^
    [javac] /home/tanping/src/SVN/hadoop-hdfs-trunk/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java:1420: non-static method refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider) cannot be referenced from a static context
    [javac]     ServiceAuthorizationManager.refresh(
    [javac]                                ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 3 errors
",,tanping,Major,Resolved,Fixed,24/Sep/10 22:17,25/Sep/10 02:55
Bug,HDFS-1433,12475525,Fix test failures - TestPread and TestFileLimit,,sureshms,sureshms,Major,Closed,Fixed,30/Sep/10 17:49,12/Dec/11 06:19
Bug,HDFS-1439,12475807,Fix compilation error in TestFiHftp,TestFiHftp has a compilation error.,sureshms,sureshms,Major,Resolved,Fixed,04/Oct/10 21:12,16/Jun/11 19:44
Bug,HDFS-1440,12475818,TestComputeInvalidateWork fails intermittently,"TestComputeInvalidateWork fails intermittently. This is due to incorrect synchronization introduced due to HDFS-1093. The test uses blocks synchronized on FSNamesystem monitor, however with HDFS-1093, the FSNamesystem uses explicit read/write lock for synchronization.",sureshms,sureshms,Major,Closed,Fixed,04/Oct/10 22:46,12/Dec/11 06:19
Bug,HDFS-1444,12476806,Test related code of build.xml is error-prone and needs to be re-aligned.,"Test related parts of build.xml introduce at least two places (effectively different) for test classes destination compilation.
Then some extra logic is applied at say test-jar creation step where the content of one is copied over to another. Etc.

This seems to be overcomplicated and is better be fixed to prevent possible issues with future build modificaitons.",cos,cos,Minor,Resolved,Fixed,07/Oct/10 21:55,08/Oct/10 23:40
Bug,HDFS-1449,12477031,HDFS federation: Fix unit test failures,Unit test failures are failing due to ExtendedBlock#getBlockName() returning an invalid block file name.,sureshms,sureshms,Major,Resolved,Fixed,11/Oct/10 15:20,19/Oct/10 17:44
Bug,HDFS-1452,12477078,ant compile-contrib is broken,"ant compile-contrib is broken, looks like commit a0a62d971fb35de7f021ecbd6ceb8d08ef923ed5 HDFS-1444
",cos,jghoman,Major,Resolved,Fixed,11/Oct/10 22:18,12/Oct/10 01:36
Bug,HDFS-1463,12477778,accessTime updates should not occur in safeMode,"FSNamesystem.getBlockLocations sometimes need to update the accessTime of files. If the namenode is in safemode, this call should fail.",dhruba,dhruba,Major,Resolved,Fixed,19/Oct/10 20:52,20/Apr/11 23:32
Bug,HDFS-1466,12477900,TestFcHdfsSymlink relies on /tmp/test not existing,The test case testLinkAcrossFileSystems works with files in a directory /tmp/test. On our shared build systems this sometimes breaks because such a directory already exists but owned by some other user. The test case should use the build test dir instead.,eli,tlipcon,Minor,Resolved,Fixed,20/Oct/10 19:05,15/Nov/10 07:32
Bug,HDFS-1467,12477901,Append pipeline never succeeds with more than one replica,"TestPipelines appears to be failing on trunk:

Should be RBW replica after sequence of calls append()/write()/hflush() expected:<RBW> but was:<FINALIZED>
junit.framework.AssertionFailedError: Should be RBW replica after sequence of calls append()/write()/hflush() expected:<RBW> but was:<FINALIZED>
        at org.apache.hadoop.hdfs.TestPipelines.pipeline_01(TestPipelines.java:109)
",tlipcon,tlipcon,Blocker,Closed,Fixed,20/Oct/10 19:12,12/Dec/11 06:19
Bug,HDFS-1474,12477952,ant binary-system is broken,Build failed due to unable to copy the commons instrumented jar. I could see the following error in the log.,cos,cos,Major,Resolved,Fixed,21/Oct/10 05:37,22/Oct/10 00:10
Bug,HDFS-1480,12478406,All replicas of a block can end up on the same rack when some datanodes are decommissioning.,"It appears that all replicas of a block can end up in the same rack. The likelihood of such replicas seems to be directly related to decommissioning of nodes. 

Post rolling OS upgrade (decommission 3-10% of nodes, re-install etc, add them back) of a running cluster, all replicas of about 0.16% of blocks ended up in the same rack.

Hadoop Namenode UI etc doesn't seem to know about such incorrectly replicated blocks. ""hadoop fsck .."" does report that the blocks must be replicated on additional racks.

Looking at ReplicationTargetChooser.java, following seem suspect:

snippet-01:
{code}
    int maxNodesPerRack =
      (totalNumOfReplicas-1)/clusterMap.getNumOfRacks()+2;
{code}

snippet-02:
{code}
      case 2:
        if (clusterMap.isOnSameRack(results.get(0), results.get(1))) {
          chooseRemoteRack(1, results.get(0), excludedNodes,
                           blocksize, maxNodesPerRack, results);
        } else if (newBlock){
          chooseLocalRack(results.get(1), excludedNodes, blocksize,
                          maxNodesPerRack, results);
        } else {
          chooseLocalRack(writer, excludedNodes, blocksize,
                          maxNodesPerRack, results);
        }
        if (--numOfReplicas == 0) {
          break;
        }
{code}

snippet-03:
{code}
    do {
      DatanodeDescriptor[] selectedNodes =
        chooseRandom(1, nodes, excludedNodes);
      if (selectedNodes.length == 0) {
        throw new NotEnoughReplicasException(
                                             ""Not able to place enough replicas"");
      }
      result = (DatanodeDescriptor)(selectedNodes[0]);
    } while(!isGoodTarget(result, blocksize, maxNodesPerRack, results));
{code}",tlipcon,mary,Major,Resolved,Fixed,26/Oct/10 23:49,24/Aug/11 13:48
Bug,HDFS-1483,12478532,DFSClient.getBlockLocations returns BlockLocations with no indication that the corresponding blocks are corrupt,"When there are no uncorrupted replicas of a block, FSNamesystem.getBlockLocations returns LocatedBlocks corresponding to corrupt blocks. When DFSClient converts these to BlockLocations, the information that the corresponding block is corrupt is lost. We should add a field to BlockLocation to indicate whether the corresponding block is corrupt in order to warn the client that reading this block will fail. This would be especially useful for tools such as RAID FSCK, which could then easily inspect whether data or parity blocks are corrupted without having to make direct RPC calls.",pkling,pkling,Major,Closed,Fixed,28/Oct/10 01:50,02/May/13 02:29
Bug,HDFS-1487,12478967,FSDirectory.removeBlock() should update diskspace count of the block owner node,"This bug leads a quota computation error. When calling abandonBlock(), the cached diskspace count of INodeDirectoryWithQuota is larger than the actual consumed space value.",zhong,zhong,Major,Closed,Fixed,03/Nov/10 08:28,04/May/12 15:49
Bug,HDFS-1490,12479244,TransferFSImage should timeout,"Sometimes when primary crashes during image transfer secondary namenode would hang trying to read the image from HTTP connection forever.
It would be great to set timeouts on the connection so if something like that happens there is no need to restart the secondary itself.
In our case restarting components is handled by the set of scripts and since the Secondary as the process is running it would just stay hung until we get an alarm saying the checkpointing doesn't happen.",vinayakumarb,dms,Minor,Closed,Fixed,05/Nov/10 18:33,04/Sep/14 00:59
Bug,HDFS-1498,12479896,FSDirectory#unprotectedConcat calls setModificationTime on a file,"The HDFSConcat test fails when asserts are enabled because FSDirectory#unprotectedConcat calls INode#setModificationTime on a file, this method asserts that the argument is a directory. It should use setModificationTimeForce since we know the target is a file, it's mod time should be set to now unconditionally since we know we're modifying it. The behavior should be equivalent.",eli,eli,Minor,Resolved,Fixed,14/Nov/10 05:06,15/Nov/10 05:02
Bug,HDFS-1500,12479967,TestOfflineImageViewer failing on trunk,"Testcase: testOIV took 22.679 sec
	FAILED
Failed reading valid file: No image processor to read version -26 is available.
junit.framework.AssertionFailedError: Failed reading valid file: No image processor to read version -26 is available.
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer.outputOfLSVisitor(TestOfflineImageViewer.java:171)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer.testOIV(TestOfflineImageViewer.java:86)
",tlipcon,tlipcon,Major,Closed,Fixed,15/Nov/10 15:57,12/Dec/11 06:18
Bug,HDFS-1502,12480243,TestBlockRecovery triggers NPE in assert,"{noformat}
Testcase: testRBW_RWRReplicas took 10.333 sec
        Caused an ERROR
null
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:1881)
        at org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery.testSyncReplicas(TestBlockRecovery.java:144)
        at org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery.testRBW_RWRReplicas(TestBlockRecovery.java:305)
{noformat}

{noformat}
        Block reply = r.datanode.updateReplicaUnderRecovery(
            r.rInfo, recoveryId, newBlock.getNumBytes());
        assert reply.equals(newBlock) &&
               reply.getNumBytes() == newBlock.getNumBytes() :
          ""Updated replica must be the same as the new block."";    <----- line 1881
{noformat}

Not sure how reply could be null since  updateReplicaUnderRecovery always returns a newly instantiated object.",hairong,eli,Minor,Resolved,Fixed,17/Nov/10 19:35,21/Apr/11 23:32
Bug,HDFS-1503,12480244,TestSaveNamespace fails,"Will attach the full log. Here's the relevant snippet:

{noformat}
Exception in thread ""FSImageSaver for /home/eli/src/hdfs4/build/test/data/dfs/na
me1 of type IMAGE_AND_EDITS"" java.lang.RuntimeException: Injected fault: saveFSI
mage second time
....
        at java.lang.Thread.run(Thread.java:619)
Exception in thread ""FSImageSaver for /home/eli/src/hdfs4/build/test/data/dfs/na
me2 of type IMAGE_AND_EDITS"" java.lang.StackOverflowError
        at java.util.Arrays.copyOf(Arrays.java:2882)
{noformat}",tlipcon,eli,Minor,Closed,Fixed,17/Nov/10 19:47,21/Apr/11 23:32
Bug,HDFS-1504,12480249,"FSImageSaver should catch all exceptions, not just IOE","FSImageSaver currently just catches IOE. This means that if some other error like OOME or failed assert happens in saving one of the images, the coordinating thread won't know there was a problem.",tlipcon,tlipcon,Minor,Resolved,Fixed,17/Nov/10 20:22,21/Apr/11 23:32
Bug,HDFS-1505,12480252,saveNamespace appears to succeed even if all directories fail to save,"After HDFS-1071, saveNamespace now appears to ""succeed"" even if all of the individual directories failed to save.",atm,tlipcon,Blocker,Closed,Fixed,17/Nov/10 20:34,12/Dec/11 06:18
Bug,HDFS-1507,12480269,TestAbandonBlock should abandon a block,"TestAbandonBlock as written just tests that if one client tries to abandon another it will fail because the new client doesn't have a lease on the other client's file, which doesn't cover the block abandonement code path.",eli,eli,Minor,Resolved,Fixed,17/Nov/10 23:21,18/Nov/10 21:58
Bug,HDFS-1511,12480402,98 Release Audit warnings on trunk and branch-0.22,There are 98 release audit warnings on trunk. See attached txt file. These must be fixed or filtered out to get back to a reasonably small number of warnings. The OK_RELEASEAUDIT_WARNINGS property in src/test/test-patch.properties should also be set appropriately in the patch that fixes this issue.,jghoman,nidaley,Blocker,Closed,Fixed,19/Nov/10 06:16,12/Dec/11 06:18
Bug,HDFS-1516,12480738,mvn-install is broken after 0.22 branch creation,Version HAS to be bumped for system testing framework artifacts (as mentioned in the build.xml file),cos,cos,Major,Resolved,Fixed,23/Nov/10 21:24,25/Nov/10 00:49
Bug,HDFS-1522,12491717,Merge Block.BLOCK_FILE_PREFIX and DataStorage.BLOCK_FILE_PREFIX into one constant,"Two semantically identical constant {{Block.BLOCK_FILE_PREFIX}} and {{DataStorage.BLOCK_FILE_PREFIX}} should merged into one. Should be defined in {{Block}}, imo.
Also use cases of ""blok_"", like in {{DirectoryScanner}} should be replaced by the this constant.",d9liang,shv,Major,Closed,Fixed,01/Dec/10 00:29,12/May/16 18:14
Bug,HDFS-1523,12491718,TestLargeBlock is failing on trunk,"TestLargeBlock is failing for more than a week not on 0.22 and trunk with
{noformat}
java.io.IOException: Premeture EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:118)
	at org.apache.hadoop.hdfs.BlockReader.readChunk(BlockReader.java:275)
{noformat}",cos,cos,Major,Closed,Fixed,01/Dec/10 00:43,12/Dec/11 06:19
Bug,HDFS-1524,12491796,Image loader should make sure to read every byte in image file,"When I work on HDFS-1070, I come across a very strange bug. Occasionally when loading a compressed image, NameNode throws an exception complaining that the image file is corrupt. However, the result returned by the md5sum utility matches the checksum value stored in the VERSION file. 

It turns out the image loader leaves 4 bytes unread after loading all the real data of an image. Those unread bytes may be some compression-related meta-info. The image loader should make sure to read to the end of file in order for an correct checksum.",hairong,hairong,Blocker,Closed,Fixed,01/Dec/10 18:02,12/Dec/11 06:18
Bug,HDFS-1526,12491966,Dfs client name for a map/reduce task should have some randomness,"Fsck shows one of the files in our dfs cluster is corrupt.

/bin/hadoop fsck aFile -files -blocks -locations
aFile: 4633 bytes, 2 block(s): 
aFile: CORRUPT block blk_-4597378336099313975
OK
0. blk_-4597378336099313975_2284630101 len=0 repl=3 [...]
1. blk_5024052590403223424_2284630107 len=4633 repl=3 [...]Status: CORRUPT

On disk, these two blocks are of the same size and the same content. It turns out the writer of the file is from a multiple threaded map task. Each thread may write to the same file. One possible interaction of two threads might make this to happen:
[T1: create aFile] [T2: delete aFile] [T2: create aFile][T1: addBlock 0 to aFile][T2: addBlock1 to aFile]...

Because T1 and T2 have the same client name, which is the map task id, the above interactions could be done without any lease exception, thus eventually leading to a corrupt file. To solve the problem, a mapreduce task's client name could be formed by its task id followed by a random number.",hairong,hairong,Major,Resolved,Fixed,03/Dec/10 07:13,18/Dec/11 13:34
Bug,HDFS-1527,12492052,SocketOutputStream.transferToFully fails for blocks >= 2GB on 32 bit JVM,"On 32 bit JVM, SocketOutputStream.transferToFully() fails if the block size is >= 2GB. We should fall back to a normal transfer in this case. 


{code}
2010-12-02 19:04:23,490 ERROR datanode.DataNode (BlockSender.java:sendChunks(399)) - BlockSender.sendChunks() exception: java.io.IOException: Value too large
 for defined data type
        at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
        at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:418)
        at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:519)
        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:204)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:386)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:475)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.opReadBlock(DataXceiver.java:196)
        at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.opReadBlock(DataTransferProtocol.java:356)
        at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.processOp(DataTransferProtocol.java:328)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:130)
        at java.lang.Thread.run(Thread.java:619)
{code}",pkling,pkling,Major,Closed,Fixed,04/Dec/10 01:17,12/Dec/11 06:19
Bug,HDFS-1529,12492527,Incorrect handling of interrupts in waitForAckedSeqno can cause deadlock,"In HDFS-895 the handling of interrupts during hflush/close was changed to preserve interrupt status. This ends up creating an infinite loop in waitForAckedSeqno if the waiting thread gets interrupted, since Object.wait() has a strange semantic that it doesn't give up the lock even momentarily if the thread is already in interrupted state at the beginning of the call.

We should decide what the correct behavior is here - if a thread is interrupted while it's calling hflush() or close() should we (a) throw an exception, perhaps InterruptedIOException (b) ignore, or (c) wait for the flush to finish but preserve interrupt status on exit?",tlipcon,tlipcon,Blocker,Closed,Fixed,06/Dec/10 18:49,12/Dec/11 06:18
Bug,HDFS-1531,12492662,Clean up stack traces due to duplicate MXBean registration,"In the minicluster unit tests, we try to register MXBeans for each DN, but since the JMX context is JVM-wide, we get a InstanceAlreadyExistsException for all but the first. This stack trace clutters test logs a lot.",tlipcon,tlipcon,Trivial,Resolved,Fixed,07/Dec/10 21:17,21/Apr/11 23:32
Bug,HDFS-1532,12492694,Exclude Findbugs warning in FSImageFormat$Saver,"HDFS-1473 had a followup patch which changed a method name, but I forgot to modify the findbugs exclude file to match.",tlipcon,tlipcon,Major,Closed,Fixed,08/Dec/10 01:21,12/Dec/11 06:19
Bug,HDFS-1533,12492792,A more elegant FileSystem#listCorruptFileBlocks API (HDFS portion),This is the HDFS portion of HADOOP-7060.,pkling,pkling,Major,Resolved,Fixed,09/Dec/10 02:30,02/May/13 02:29
Bug,HDFS-1540,12493379,Make Datanode handle errors to namenode.register call more elegantly,"When a datanode receives a ""Connection reset by peer"" from the namenode.register(), it exits. This causes many datanodes to die.",dhruba,dhruba,Major,Resolved,Fixed,16/Dec/10 00:41,20/Apr/11 23:33
Bug,HDFS-1542,12493447,Deadlock in Configuration.writeXml when serialized form is larger than one DFS block,"Configuration.writeXml holds a lock on itself and then writes the XML to an output stream, during which DFSOutputStream will try to get a lock on ackQueue/dataQueue. Meanwihle the DataStreamer thread will call functions like conf.getInt() and deadlock against the other thread, since it could be the same conf object.

This causes a deterministic deadlock whenever the serialized form is larger than block size.",tlipcon,tlipcon,Critical,Closed,Fixed,16/Dec/10 17:57,02/May/13 02:29
Bug,HDFS-1543,12493545,Reduce dev. cycle time by moving system testing artifacts from default build and push to maven for HDFS,"The current build always generates system testing artifacts and pushes them to Maven. Most developers have no need for these artifacts and no users need them. 

Also, fault injection tests seems to be running multiple times which increases the length of testing.",vicaya,acmurthy,Major,Resolved,Fixed,17/Dec/10 18:43,20/Apr/11 23:33
Bug,HDFS-1544,12493557,Ivy resolve force mode should be turned off by default,cf. HADOOP-7068,vicaya,vicaya,Major,Closed,Fixed,17/Dec/10 22:37,12/Dec/11 06:19
Bug,HDFS-1548,12493720,Fault-injection tests are executed multiple times if invoked with run-test-hdfs-fault-inject target,When invoked with {{run-test-hdfs-fault-inject target}} fault injection tests are getting executed 4 times.,cos,cos,Major,Resolved,Fixed,21/Dec/10 00:51,21/Apr/11 23:32
Bug,HDFS-1550,12493782,NPE when listing a file with no location,"Lines listed below will caused a NullPointerException in DFSUtil.locatedBlocks2Locations (line 208) because EMPTY_BLOCK_LOCS  will return null when calling blocks.getLocatedBlocks()
{noformat}
   /** a default LocatedBlocks object, its content should not be changed */
   private final static LocatedBlocks EMPTY_BLOCK_LOCS = new LocatedBlocks();
{noformat}",hairong,hairong,Blocker,Closed,Fixed,21/Dec/10 19:13,12/Dec/11 06:19
Bug,HDFS-1551,12493786,fix the pom template's version,pom templates in the ivy folder should be updated to the latest version hadoo-common dependencies.,gkesavan,gkesavan,Major,Resolved,Fixed,21/Dec/10 19:28,20/Apr/11 23:33
Bug,HDFS-1552,12493831,Remove java5 dependencies from build,As the first short-term step let's remove JDK5 dependency from build(s),cos,cos,Major,Resolved,Fixed,22/Dec/10 05:18,14/May/12 03:29
Bug,HDFS-1559,12494060,Add missing UGM overrides to TestRefreshUserMappings,The commit of HADOOP-6864 added new methods to GroupMappingServiceProvider and broke trunk compilation for HDFS.,tlipcon,tlipcon,Major,Resolved,Fixed,25/Dec/10 21:11,20/Apr/11 23:33
Bug,HDFS-1560,12494091,dfs.data.dir permissions should default to 700,"Currently, dfs.data.dir defaults to 755 permissions, which isn't necessary for any reason, and is a security issue if not changed on a secured cluster. We should default to 700",tlipcon,tlipcon,Minor,Closed,Fixed,27/Dec/10 03:27,21/Apr/11 23:32
Bug,HDFS-1561,12494281,BackupNode listens on default host,"Currently BackupNode uses DNS to find its default host name, and then starts RPC server listening on that address ignoring the address specified in the configuration. Therefore, there is no way to start BackupNode on a particular ip or host address. BackupNode should use the address specified in the configuration instead.",shv,shv,Blocker,Closed,Fixed,30/Dec/10 00:19,12/Dec/11 06:19
Bug,HBASE-2087,12444468,"The wait on compaction because ""Too many store files"" holds up all flushing","The method MemStoreFlusher#checkStoreFileCount is called from flushRegion.  flushRegion is called by MemStoreFlusher#run thread.  If the checkStoreFileCount finds too many store files, it'll stick around waiting on a compaction to happen.  While its hanging, the MemStoreFlusher#run is held up.  No other region can flush.  Meantime WALs will be rolling and memory will be accumulating writes.",jdcryans,stack,Major,Closed,Fixed,01/Jan/10 00:26,12/Oct/12 06:14
Bug,HBASE-2090,12444564,findbugs issues ,Findbugs issues/ fixes for a subset of them. ,kaykay.unique,kaykay.unique,Major,Closed,Fixed,04/Jan/10 07:07,20/Nov/15 13:02
Bug,HBASE-2093,12444751,[stargate] RowSpec parse bug,"From mike anderson up on hbase-user@:

{quote}
Trying to create a row in hbase from ruby with row key ""http://www.google.com"" produces this exception

{noformat}
2010-01-05 11:40:53.972::WARN:  /cached_web_pages/http%3A%2F%
2Fwww.google.com%2F/
java.lang.IllegalArgumentException: java.lang.NumberFormatException: For
input string: ""www.google.com""
at org.apache.hadoop.hbase.stargate.RowSpec.parseTimestamp(RowSpec.java:171)
at org.apache.hadoop.hbase.stargate.RowSpec.(RowSpec.java:55)
at org.apache.hadoop.hbase.stargate.RowResource.(RowResource.java:62)
at org.apache.hadoop.hbase.stargate.TableResource.getRowResource(TableResource.java:117)
[...]
{noformat}

{quote}

Move to 0.20.4 if this gets in the way of the 0.20.3 release before I can look at it.
",apurtell,apurtell,Major,Closed,Fixed,05/Jan/10 21:58,12/Oct/12 06:14
Bug,HBASE-2094,12444831,hbase-2037 breaks mapreduce jobs going from 0.20.2 to 0.20.3,hbase-2037 makes it so commons-lang is now needed on mr classpath.  Can't have fellas having to change their CLASSPATH on point release update.,,stack,Blocker,Closed,Fixed,06/Jan/10 19:35,12/Oct/12 06:14
Bug,HBASE-2097,12445000,Deadlock between HRegion.put and HRegion.close,"HBASE-2037 added a bunch of fixes but also a deadlock:

HRegion.put:
{code}
splitsAndClosesLock.readLock().lock();
newScannerLock.writeLock().lock();
{code}

HRegion.close
{code}
newScannerLock.writeLock().lock();
try {
  splitsAndClosesLock.writeLock().lock();
{code}

To recreate, start a PerformanceEvaluation on standalone and it happens roughly 75% of the time.",stack,jdcryans,Blocker,Closed,Fixed,08/Jan/10 06:52,12/Oct/12 06:14
Bug,HBASE-2101,12445115,KeyValueSortReducer collapses all values to last passed,"From mailing list by Ioannis Konstantinou:

{code}The problem is in the class org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer. 
When you add keyvalues to the treeset for sorting, you need to add keyvalue clones instead of just references. What happens now, is that in every iteration, the value that exists in the treeset gets replaced with the new value.


So, you need to replace line 41 ( map.add(kv);) 
with this line:   map.add(kv.clone())

in this case, the treeset populates correcty.{code}

I filed this against 0.20.3 so if we have to cut a new RC, we can include this fix too.",,stack,Major,Closed,Fixed,09/Jan/10 19:23,12/Oct/12 06:14
Bug,HBASE-2109,12445232,"status 'simple' should show total requests per second, also the requests/sec is wrong as is","status 'simple' doesnt give us aggregate load, leaving the user to add up numbers by hand. 

Futhermore, the per-server requests numbers are off, too high by a factor of 3 - they are using the default toString() which assumes a 1 second report rate, when the shipping default is 3 seconds.",,ryanobjc,Major,Closed,Fixed,11/Jan/10 23:08,11/Jun/22 23:19
Bug,HBASE-2111,12445238,Move to ivy broke our being able to run in-place; i.e. ./bin/start-hbase.sh in a checkout.,,stack,stack,Major,Closed,Fixed,12/Jan/10 00:19,11/Jun/22 23:19
Bug,HBASE-2112,12445239,New 'indexed' contrib is missing commons-lang.jar when packaged,The packaged hbase is missing commons-lang in the new indexed contrib.,,stack,Major,Closed,Fixed,12/Jan/10 00:30,12/Oct/12 06:14
Bug,HBASE-2113,12445240,"For indexed contrib, fast-forward to next row if no more results left... big performance improvement",,,stack,Major,Closed,Fixed,12/Jan/10 00:33,12/Oct/12 06:14
Bug,HBASE-2114,12445246,Can't start HBase in trunk,"The new Configuration update HBASE-2036 didn't change the main methods in HRegionServer and HMaster, can't start hbase.",jdcryans,jdcryans,Major,Closed,Fixed,12/Jan/10 01:08,20/Nov/15 13:01
Bug,HBASE-2115,12445250,./hbase shell would not launch due to missing jruby dependency,"./hbase shell - gave some error due to missing org/jruby/.. etc. 


Add the following to ivy.xml - 
   <dependency org=""org.jruby"" name=""jruby-complete""
              rev=""${jruby.version}"" conf=""common->default"" />    

Also - in HBASE-2114 - there is an issue about missing log4j configuration as well. ( due to the fact that build/ivy/lib/common did not contain log4j , but build/ivy/lib/test did ). 

This patch addresses either of them. 

affects trunk  
",kaykay.unique,kaykay.unique,Major,Closed,Fixed,12/Jan/10 01:56,11/Jun/22 23:19
Bug,HBASE-2118,12445319,Fix our wikipedia page; says we're slow among other errors,,,stack,Major,Closed,Fixed,12/Jan/10 17:25,11/Jun/22 23:02
Bug,HBASE-2119,12445380,Fix top-level NOTICES.txt file.  Its stale.,,stack,stack,Major,Closed,Fixed,13/Jan/10 06:02,12/Oct/12 06:14
Bug,HBASE-2120,12445463,[Stargate] Unable to delete column families,"When trying to delete a column family using Stargate, the following occurs (curl command + Stargate logging):

> curl http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute/5426359469582345882 -X DELETE
---
10/01/13 18:57:38 DEBUG stargate.RowResource: DELETE http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute/5426359469582345882
10/01/13 18:57:38 DEBUG stargate.RowResource: DELETE row=ff417a5b-c4d0-4a43-b1c7-94c356fe0b72, ts=9223372036854775807, families={(family=attribute, keyvalues=(ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882/DeleteColumn/vlen=0)}
---

> curl http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882 -X DELETE
---
10/01/13 18:57:49 DEBUG stargate.RowResource: DELETE http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882
10/01/13 18:57:49 DEBUG stargate.RowResource: DELETE row=ff417a5b-c4d0-4a43-b1c7-94c356fe0b72, ts=9223372036854775807, families={(family=attribute, keyvalues=(ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882/DeleteColumn/vlen=0)}
---

Both are attempting to delete columns instead of the intended action of deleting families. The problem occurs because RowResource.java (line 282) will always return a split of length 2, since RowSpec.java (line 122) appends a colon if it's missing.

I've patched it so that a check will occur if the second split's (split[1]) length is 0 and acts accordingly. I'll attach the patch after I've run it against the test suite.

P.S. It's my first time submitting a patch, so let me know if I screwed anything up.",greglu,greglu,Major,Closed,Fixed,14/Jan/10 00:44,12/Oct/12 06:14
Bug,HBASE-2122,12445474,[Stargate] Initializing scanners column families doesn't work,"Similar to HBASE-2120 the handling of column families with scanner does not work correctly.

The issue is in ScannerResultGenerator.java (line 62), and I'm attaching a patch for both trunk and the 0.20 branch.",greglu,greglu,Major,Closed,Fixed,14/Jan/10 05:01,12/Oct/12 06:14
Bug,HBASE-2123,12445480,Remove 'master' command-line option from PE.,,,stack,Major,Closed,Fixed,14/Jan/10 06:12,12/Oct/12 06:14
Bug,HBASE-2126,12445571,Fix build break - ec2,"Because all contrib/** reuses build-contrib.xml -, internally they reuse ivy-retrieve.xml and hence need the presence of an ivy.xml in ec2 directory to succeed. 

Temporary patch with no dependencies in . Ideal patch should be to refactor build\*.xml as appropriate. ",kaykay.unique,kaykay.unique,Major,Closed,Fixed,14/Jan/10 23:52,11/Jun/22 23:02
Bug,HBASE-2127,12445575,randomWrite mode of PerformanceEvaluation benchmark program writes only to a small range of keys,"""randomWrite"" mode of PerformanceEvaluation (PE), with nclients > 1, does random writes only within a small range rather than across all rows.

e.g, for:

./bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows=1000 randomWrite 10

[Note: As per convention in all modes of PE, --rows is the number of rows per client]

So for the above, with # of clients set to 10, currently all clients generate writes to keys in the 0..999 range instead of the 0..9999 range.

[Fix appears to be simple. Will provide a patch.]",,kannanm,Minor,Closed,Fixed,15/Jan/10 00:41,12/Oct/12 06:14
Bug,HBASE-2128,12445582,ant tar build broken since switch to Ivy,"Running ant tar produces a very small tar file because all .jar dependencies are missing. This happens since the switch to Ivy.

Adding common.ivy.lib.dir to the build.xml fixes some of it but some things still don't work:
{code:xml}
    <mkdir dir=""${dist.dir}/lib""/>
    <copy todir=""${dist.dir}/lib"">
      <fileset dir=""${build.lib}"" />
      <fileset dir=""${common.ivy.lib.dir}""/>
    </copy>
{code}

The jars for the contrib apps still seem to be missing. At the moment this is only stargate but the I've got the same problem for the new thrift contrib. I am afraid I don't know enough about Ant or Ivy to be of any further assistance.",kaykay.unique,larsfrancke,Major,Closed,Fixed,15/Jan/10 01:50,20/Nov/15 13:01
Bug,HBASE-2134,12445673,Ivy nit regarding checking with latest snapshots ,"Currently - if a new jar gets published in one of the dependent m2 snapshots - ivy does not retrieve it unless the cache is cleared.  Add changing=""true"" to the dependency.  ( There has to be an alternate way to do it at the resolver level, but for now this works without a hitch). 
",kaykay.unique,kaykay.unique,Major,Closed,Fixed,15/Jan/10 20:37,20/Nov/15 13:01
Bug,HBASE-2135,12445695,ant javadoc complains about missing classes ,"Something to do with CP  - javadoc target not happy with CP ( in trunk). 

placeholder ticket to revisit it. ",kaykay.unique,kaykay.unique,Major,Closed,Fixed,15/Jan/10 22:48,20/Nov/15 13:01
Bug,HBASE-2137,12445724,javadoc warnings from 'javadoc' target ,"Some javadoc warnings: 

  [javadoc]  ../workspace/hbase/src/java/org/apache/hadoop/hbase/mapreduce/IndexConfiguration.java:149: warning - @return tag has no arguments.
  [javadoc] ../workspace/hbase/src/java/org/apache/hadoop/hbase/mapreduce/IndexConfiguration.java:149: warning - Tag @see: reference not found: Use #isAnalyze(String) for replacement.
  [javadoc] ../workspace/hbase/src/java/org/apache/hadoop/hbase/mapreduce/IndexConfiguration.java:159: warning - Tag @see: reference not found: Use #setAnalyze(String, boolean) for replacement.
  [javadoc] ../workspace/hbase/src/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java:104: warning - Tag @link: can't find getReader(org.apache.hadoop.fs.FileSystem,
  [javadoc]  org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.HBaseConfiguration) in org.apache.hadoop.hbase.regionserver.wal.HLog
",kaykay.unique,kaykay.unique,Major,Closed,Fixed,16/Jan/10 07:09,20/Nov/15 13:01
Bug,HBASE-2138,12445744,unknown metrics type,"Since the recent metric commits I see this on the master and RS at boot:

{code}
2010-01-16 11:24:59,730 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=RegionServer, sessionId=regionserver/10.10.1.49:60020
2010-01-16 11:24:59,732 ERROR org.apache.hadoop.metrics.MetricsUtil: unknown metrics type: org.apache.hadoop.hbase.metrics.MetricsRate
2010-01-16 11:24:59,732 ERROR org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.MetricsTimeVaryingRate
2010-01-16 11:24:59,732 ERROR org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.MetricsTimeVaryingRate
2010-01-16 11:24:59,732 ERROR org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.MetricsTimeVaryingRate
2010-01-16 11:24:59,732 ERROR org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.MetricsTimeVaryingRate
{code}

We need to clean that for 0.20.3",stack,jdcryans,Major,Closed,Fixed,16/Jan/10 21:08,12/Oct/12 06:14
Bug,HBASE-2140,12445756,findbugs issues - 2 performance warnings as suggested by findbugs,"* Integer.valueOf  favored instead of new Integer() 
 

* map.entrySet() favored instead of map.keySet()  
",kaykay.unique,kaykay.unique,Major,Closed,Fixed,17/Jan/10 07:44,20/Nov/15 13:01
Bug,HBASE-2144,12445880,[shell] Now does \x20 for spaces,,stack,stack,Major,Closed,Fixed,19/Jan/10 02:26,12/Oct/12 06:14
Bug,HBASE-2145,12445886,bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --miniCluster randomRead 1 don't work,"I see this in the 0.20.3RC.  Its been there a while I'd guess.  Not enough to sink RC I'd say.  In fact, all PE args could do with a review.

{code}
...
10/01/18 21:25:31 DEBUG zookeeper.ZooKeeperWrapper: Failed to read: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/master
10/01/18 21:25:32 INFO zookeeper.ClientCnxn: Attempting connection to server localhost/fe80:0:0:0:0:0:0:1%1:2181
10/01/18 21:25:32 WARN zookeeper.ClientCnxn: Exception closing session 0x0 to sun.nio.ch.SelectionKeyImpl@7284aa02
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:933)
10/01/18 21:25:32 WARN zookeeper.ClientCnxn: Ignoring exception during shutdown input
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:638)
        at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360)
        at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:999)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:970)
10/01/18 21:25:32 WARN zookeeper.ClientCnxn: Ignoring exception during shutdown output
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.shutdownOutput(SocketChannelImpl.java:649)
        at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:368)
        at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1004)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:970)
10/01/18 21:25:32 WARN zookeeper.ZooKeeperWrapper: Failed to create /hbase -- check quorum servers, currently=localhost:2181
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:608)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.ensureExists(ZooKeeperWrapper.java:405)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.ensureParentExists(ZooKeeperWrapper.java:428)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.writeMasterAddress(ZooKeeperWrapper.java:516)
        at org.apache.hadoop.hbase.master.HMaster.writeAddressToZooKeeper(HMaster.java:263)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:245)
        at org.apache.hadoop.hbase.LocalHBaseCluster.<init>(LocalHBaseCluster.java:94)
        at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:61)
        at org.apache.hadoop.hbase.MiniHBaseCluster.<init>(MiniHBaseCluster.java:53)
        at org.apache.hadoop.hbase.PerformanceEvaluation.runTest(PerformanceEvaluation.java:871)
        at org.apache.hadoop.hbase.PerformanceEvaluation.doCommandLine(PerformanceEvaluation.java:981)
        at org.apache.hadoop.hbase.PerformanceEvaluation.main(PerformanceEvaluation.java:1001)
10/01/18 21:25:34 INFO zookeeper.ClientCnxn: Attempting connection to server localhost/0:0:0:0:0:0:0:1:2181
10/01/18 21:25:34 WARN zookeeper.ClientCnxn: Exception closing session 0x0 to sun.nio.ch.SelectionKeyImpl@52a34783
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:933)
10/01/18 21:25:34 WARN zookeeper.ClientCnxn: Ignoring exception during shutdown input
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:638)
        at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360)
        at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:999)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:970)
10/01/18 21:25:34 WARN zookeeper.ClientCnxn: Ignoring exception during shutdown output
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.shutdownOutput(SocketChannelImpl.java:649)
        at sun.nio.ch.SocketAdaptor.shutdownOutput(SocketAdaptor.java:368)
        at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1004)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:970)
...
{code}

Over and over...",,stack,Major,Closed,Fixed,19/Jan/10 05:26,11/Jun/22 23:03
Bug,HBASE-2146,12445928,RPC related metrics are missing in 0.20.3 since recent changes,Since the recent change to the Metrics setup it seems that the RPC related stats have been completely dropped. See attached files for details.,ghelmling,larsgeorge,Major,Closed,Fixed,19/Jan/10 13:02,12/Oct/12 06:14
Bug,HBASE-2147,12445966,run zookeeper in the same jvm as master during non-distributed mode,this will avoid needing to 'ssh localhost' to start hbase on a stand-alone non-distributed machine. We should run ZK in the same JVM and also change the scripts too.,ryanobjc,ryanobjc,Major,Closed,Fixed,19/Jan/10 21:19,20/Nov/15 13:01
Bug,HBASE-2149,12446124,hbase.regionserver.global.memstore.lowerLimit is too low,"The default value of hbase.regionserver.global.memstore.lowerLimit of 25% is very wrong and in almost all cases was problematic (I've seen this in at least 3 occurrences). The cost of flushing a memstore is fairly high and when the global size reaches 40% then ALL inserts are blocked. This means that with a heap of 1GB you could be flushing for 10-20 seconds or worse.

I suggest a default setting of 38% or even 40% so that only a region or two will be flushed (the biggest ones) for maximum availability.",stack,jdcryans,Major,Closed,Fixed,20/Jan/10 19:37,12/Oct/12 06:14
Bug,HBASE-2150,12446168,Deprecated HBC(Configuration) constructor doesn't call this(),"While trying to port some 0.20 code, I found that HBC(Configuration) doesn't call the default constructor and thus never leads HBase ressources. This breaks compatibility.",jdcryans,jdcryans,Major,Closed,Fixed,21/Jan/10 00:06,20/Nov/15 13:02
Bug,HBASE-2151,12446183,Remove onelab and include generated thrift classes in javadoc,Patch is actually over in hbase-1373 named javadoc.patch.  It was done by Lars Francke so I assigned him this issue.,larsfrancke,stack,Minor,Closed,Fixed,21/Jan/10 02:23,20/Nov/15 13:02
Bug,HBASE-2154,12446272,Fix Client#next(int) javadoc,"Its not clear what signifies scanner end and noobs probably think that batch size is how much we fetch in an RPC (thats different, thats Scan#setCaching).",stack,stack,Major,Closed,Fixed,22/Jan/10 00:11,12/Oct/12 06:14
Bug,HBASE-2156,12446290,HBASE-2037 broke Scan,"Paul Ambrose wrote to the mailing list about some tests he has that doesn't pass on 0.20.3RC1-2. Looking into the issue it appears that this modification:

{code}
   public Scan addFamily(byte [] family) {
     familyMap.remove(family);
-    familyMap.put(family, null);
+    familyMap.put(family, EMPTY_NAVIGABLE_SET);
     return this;
   }
{code}

Makes it that when you use addColumn after that you put qualifiers into EMPTY_NAVIGABLE_SET which is static hence shared among all scanners after that like META scanners when calling tableExists.

This was introduced by HBASE-2037.",jdcryans,jdcryans,Blocker,Closed,Fixed,22/Jan/10 07:41,12/Oct/12 06:14
Bug,HBASE-2157,12446349,LATEST_TIMESTAMP not replaced by current timestamp in KeyValue,"I was trying to bulk load using the new HFileOutputFormat. When using a MapReduce in which map generates {{KeyValue}}s and reduce is equal to KeyValueSortReducer, and using the constructor using (byte[] row, byte[] family, byte[] qualifier, byte[] value), the (undefined) timestamp was inserted as HConstants.LATEST_TIMESTAMP/Long.MAX_VALUE into HBase. This causes all kinds of troubles, but most importantly, while the records were in the table, other MapReduces (using TableInputFormat) and Hbase shell's 'get'-command did not fetch them. Guess there is some sort of filtering of future dates.

As I understood from St.Ack, the LASTEST_TIMESTAMP is supposed to be replaced by System.currentTimeMillis(), but I don't see this reflected in the code of KeyValue, and apparently it did not happen elsewhere; perhaps because there is no actual HBase connection?",stack,mluiten,Major,Closed,Fixed,22/Jan/10 18:11,12/Oct/12 06:14
Bug,HBASE-2160,12446361,Can't put with ts in shell,"On the latest branch I can't issue a put with a ts in the shell, it does this:

{code}
hbase(main):008:0> put 't', 'r', 'f:', 'test', 123123
NameError: no constructor with arguments matching
 [class org.jruby.java.proxies.ArrayJavaProxy, class org.jruby.RubyFixnum] on 
 object #<Java::OrgApacheHadoopHbaseClient::Put:0x49239780>
{code}

It works without a ts and delete/scan aren't affected by this.",stack,jdcryans,Major,Closed,Fixed,22/Jan/10 19:21,12/Oct/12 06:14
Bug,HBASE-2163,12446434,ZK dependencies - explicitly add them until ZK artifacts are published to mvn repository,"Currently we include the binary of zookeeper but we need to add the dependencies explicitly as well ( similar to a recent issue , related to thrift ). 

zk depends on log4j / jline . 
log4j is already in. 

This patch adds jline to the dependencies explicitly. ",kaykay.unique,kaykay.unique,Major,Closed,Fixed,23/Jan/10 20:25,20/Nov/15 13:01
Bug,HBASE-2171,12446763,Alter statement in the hbase shell doesn't match documentation.,"The documentation claims this should work. Perhaps this jira could be a starting point for a more detailed explanation of alter

HBASE SHELL COMMANDS:
 alter     Alter column family schema;  pass table name and a dictionary
           specifying new column family schema. Dictionaries are described
           below in the GENERAL NOTES section.  Dictionary must include name
           of column family to alter.  For example,

           To change or add the 'f1' column family in table 't1' from defaults
           to instead keep a maximum of 5 cell VERSIONS, do:
           hbase> alter 't1', {NAME => 'f1', VERSIONS => 5}

           To delete the 'f1' column family in table 't1', do:
           hbase> alter 't1', {NAME => 'f1', METHOD => 'delete'}

           You can also change table-scope attributes like MAX_FILESIZE
           MEMSTORE_FLUSHSIZE and READONLY.

           For example, to change the max size of a family to 128MB, do:
           hbase> alter 't1', {METHOD => 'table_att', MAX_FILESIZE => '134217728'}
....
ase Shell; enter 'help<RETURN>' for list of supported commands.
Version: 0.20.3, r902334, Mon Jan 25 13:13:08 PST 2010
hbase(main):001:0> drop 't3'
0 row(s) in 0.0060 seconds
0 row(s) in 0.0050 seconds
0 row(s) in 0.1560 seconds
hbase(main):002:0> create 't3'
0 row(s) in 2.1050 seconds
hbase(main):003:0> disable 't3'
0 row(s) in 2.0980 seconds
hbase(main):004:0> alter 't3', {NAME => 'f1', VERSIONS => 5}
NativeException: java.lang.NullPointerException: null
 
",posix4e,posix4e,Major,Closed,Fixed,27/Jan/10 03:17,11/Jun/22 23:05
Bug,HBASE-2173,12446866,New idx javadoc not included with the rest,"I just figured that the new idx package javadoc isn't included in the normal build process. I think we should fix that for 0.20.4 but still regenerate the javadoc for 0.20.3 and add it to the website. It's out of the normal release process so please discuss, I'm obviously +1 on this.",stack,jdcryans,Major,Closed,Fixed,27/Jan/10 23:42,12/Oct/12 06:14
Bug,HBASE-2177,12454991,Add timestamping to gc logging options,http://forums.sun.com/thread.jspa?threadID=5165451,stack,stack,Major,Closed,Fixed,01/Feb/10 23:29,12/Oct/12 06:14
Bug,HBASE-2180,12455120,Bad random read performance from synchronizing hfile.fddatainputstream,"deep in the HFile read path, there is this code:

    synchronized (in) {
      in.seek(pos);
      ret = in.read(b, off, n);
    }


this makes it so that only 1 read per file per thread is active. this prevents the OS and hardware from being able to do IO scheduling by optimizing lots of concurrent reads. 

We need to either use a reentrant API (pread may be partially reentrant according to Todd) or use multiple stream objects, 1 per scanner/thread.",stack,ryanobjc,Major,Closed,Fixed,02/Feb/10 22:11,12/Oct/12 06:14
Bug,HBASE-2184,12455359,Calling HTable.getTableDescriptor().* on a full cluster takes a long time,"On a cluster with many tables, and consequently many regions, calling the getTableDescriptor() methods on a HTable takes a very long time, depending on the number of regions. For comparison, on a cluster with 7000 regions, getting a table descriptor ranged between 4 and 36 seconds, even when the queried table was empty.

The problem seems to lie in the HConnectionManager.getHTableDescriptor() method, which calls MetaScanner.scan() with an empty START_ROW. This means that even if we need the descriptor for a single region table, we still need to wait until the entire META is scanned. There is also a constructor for MetaScanner.scan() which takes the table name to lookup as a param.",,civascu,Major,Closed,Fixed,04/Feb/10 22:04,20/Nov/15 13:01
Bug,HBASE-2199,12455760,"hbase.client.tableindexed.IndexSpecification, lines 72-73 should be reversed","Error in constructor of  IndexSpecification, lines 72-73, where the order of ** this.makeAllColumns() and this.additionalColumns ** should be inverted. Otherwise by calling getAllColumns() only the indexed columns are returned.",,adipdia,Minor,Closed,Fixed,09/Feb/10 12:47,12/Oct/12 06:14
Bug,HBASE-2202,12455829,IdxRegion crash when binary characters,If binary characters crash in IdxRegion code.,,stack,Major,Closed,Fixed,09/Feb/10 21:07,12/Oct/12 06:15
Bug,HBASE-2203,12455844,[IHBase] Include only those columns required for indexed scan,"Include only columns required for indexed scan:
  -  Modifies only IDX contrib. files
  -  Makes sure that an indexed rebuild scan would only include columns required to rebuild the index",,stack,Major,Closed,Fixed,09/Feb/10 23:27,12/Oct/12 06:15
Bug,HBASE-2204,12455845,[IHBASE] Index expression evaluation should fail with a DoNotRetryException in case of an invalid index specification,"Index expression evaluation should fail with a DoNotRetryException in case of an invalid index specification:
  -  Modifies only IDX contrib. files",,stack,Major,Closed,Fixed,09/Feb/10 23:31,12/Oct/12 06:14
Bug,HBASE-2205,12455846,[IHBASE] Updated Idx pacakge javadocs,,,stack,Major,Closed,Fixed,09/Feb/10 23:33,12/Oct/12 06:15
Bug,HBASE-2206,12455847,[IHBASE] Idx memory allocation fix," Idx memory allocation fix
  - Includes a modification to core - MemStore was added with a numKeyValues query
  - It optimizes the number of memory allocation required to rebuild the index.  This is a major fix - it reduces the heap fragmentation and helped postpone/fix (not yet sure which of those) the region server crashing due to those long GC periods on a write-intensive setup",,stack,Major,Closed,Fixed,09/Feb/10 23:36,12/Oct/12 06:15
Bug,HBASE-2207,12455849,[IHBASE] Index partial column values ,"Index partial column values 
   - Modifies only IDX contrib. files
   - Allows extracting only part of the column value to be indexed. Can be useful if the column values are large since it allows to index only a small portion of the value and hence reduce the memory footprint",,stack,Major,Closed,Fixed,09/Feb/10 23:38,12/Oct/12 06:15
Bug,HBASE-2210,12455869,NPE in thrift deleteAll,Got a NPE when trying to delete all cells of an entire family.,larsfrancke,iranitov,Minor,Closed,Fixed,10/Feb/10 06:55,12/Oct/12 06:14
Bug,HBASE-2219,12456125,stop using code mapping for method names in the RPC,"since we use a sorted mapping of method names -> codes and send that over the wire, even trivial changes, such as adding a new call, become wire-incompatible.  This means many features which could easily have gone into a minor update must wait for a major update.  Eg: 2066, 1845, etc.

This will increase on-wire overhead, but the compatibility is worth it I think.",ryanobjc,ryanobjc,Blocker,Closed,Fixed,12/Feb/10 09:55,20/Nov/15 13:02
Bug,HBASE-2224,12456211,Broken build: TestGetRowVersions.testGetRowMultipleVersions ,Hudson is broke.,stack,stack,Major,Closed,Fixed,13/Feb/10 03:52,20/Nov/15 13:01
Bug,HBASE-2226,12456256,HQuorumPeerTest doesnt run because it doesnt start with the word Test,"pretty simple, test runner looks for classes starting with the word ""Test"".",ryanobjc,ryanobjc,Major,Closed,Fixed,14/Feb/10 06:44,20/Nov/15 12:44
Bug,HBASE-2227,12456277,[IHBASE] Idx Expression functionality is incompatible with SingleColumnValueFilter  ,"Idx comparison expressions should have 1:1 mapping to SingleColumnValueFilter.
Without this mapping users can't fully use the index expressions to optimize their scan performance.
Currently there are two main features lacking:
- Support for a != (NEQ) operator 
- Support for the 'filterIfMissing' modifier  ",,ykulbak,Major,Closed,Fixed,14/Feb/10 22:31,12/Oct/12 06:14
Bug,HBASE-2228,12456291,"Region close needs to be  fast; e.g. if compacting, abandon it","Over last week or so i've seen slow closes cause regions be off line for a good amount of time. Just now, i saw a big compaction go into effect because ""too many store files"".  This compaction took nearly two minutes on loaded server.  But during this time flushing was held up.  When the order to close came in (overloaded), we started the close -- so incoming writes were rejected -- but then we had to wait on the compaction to finish before the close went ahead... though incoming clients by now are being turned away.  Eventually the compaction completed and then the held-up flush was allowed run..... 91M in about 5 seconds.  Only now was the close allowed complete and the region deployed elsewhere.

Another time I saw the flush take a good long time because hdfs was running slow.  Probably not much we can do about this one but we should at least look into the above.  Interrupt an ongoing compaction and abandon it... or else keep region open while the compaction is going on and only when compete, then start up the close (Would require new state of CLOSING keeping up a progressable with the master).",nspiegelberg,stack,Major,Closed,Fixed,15/Feb/10 05:32,20/Nov/15 12:42
Bug,HBASE-2241,12456830,Change balancer sloppyness from 0.1 to 0.3,"This is a quick workaround until we do a better balancer.

Taking a region off line when cluster is under load is bad news.  Latency goes up as we wait on regions to come up in new locations.

The load balancer should only cut in if the cluster is way out of alignment.

I'd argue that 10% deviance from the avg. is not good enough reason moving regions around when cluster is under load.

Balancer already has a knack for cutting in at most inopportune moments: during cluster startup, when new node is added to a small cluster, or moving a region just after its been opened on a node.  We'll need to do a better balancer but meantime lets just allow that region loading can be sloppier, say 20% or 30% off the average before balancer cuts in.",stack,stack,Major,Closed,Fixed,19/Feb/10 19:53,12/Oct/12 06:14
Bug,HBASE-2244,12456875,META gets inconsistent in a number of crash scenarios,"(Forking this issue off from HBASE-2235).

During load testing, in a number of failure scenarios (unexpected region server deaths) etc., we notice that META can get inconsistent. This primarily happens for regions which are in the process of being split. Manually running add_table.rb seems to fix the tables meta data just fine. 

But it would be good to do automatic cleansing (as part of META scanners work) and/or avoid these inconsistent states altogether.

For example, for a particular startkey, I see all these entries:

{code}
test1,1204765,1266569946560 column=info:regioninfo, timestamp=1266581302018, value=REGION => {NAME => 'test1,
                             1204765,1266569946560', STARTKEY => '1204765', ENDKEY => '1441091', ENCODED => 18
                             19368969, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'test1', FAMILIES =>
                              [{NAME => 'actions', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647'
                             , BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
 test1,1204765,1266569946560 column=info:server, timestamp=1266570029133, value=10.129.68.212:60020
 test1,1204765,1266569946560 column=info:serverstartcode, timestamp=1266570029133, value=1266562597546
 test1,1204765,1266569946560 column=info:splitB, timestamp=1266581302018, value=\x00\x071441091\x00\x00\x00\x0
                             1\x26\xE6\x1F\xDF\x27\x1Btest1,1290703,1266581233447\x00\x071290703\x00\x00\x00\x
                             05\x05test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x
                             00\x00\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00
                             \x00\x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSI
                             ON\x00\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TT
                             L\x00\x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00
                             \x00\x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04t
                             rueh\x0FQ\xCF
 test1,1204765,1266581233447 column=info:regioninfo, timestamp=1266609172177, value=REGION => {NAME => 'test1,
                             1204765,1266581233447', STARTKEY => '1204765', ENDKEY => '1290703', ENCODED => 13
                             73493090, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'test1', FAMILIES =>
                              [{NAME => 'actions', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647'
                             , BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
 test1,1204765,1266581233447 column=info:server, timestamp=1266604768670, value=10.129.68.213:60020
 test1,1204765,1266581233447 column=info:serverstartcode, timestamp=1266604768670, value=1266562597511
 test1,1204765,1266581233447 column=info:splitA, timestamp=1266609172177, value=\x00\x071226169\x00\x00\x00\x0
                             1\x26\xE7\xCA,\x7D\x1Btest1,1204765,1266609171581\x00\x071204765\x00\x00\x00\x05\
                             x05test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\
                             x00\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00\x0
                             0\x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSION\
                             x00\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x
                             00\x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x0
                             0\x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true
                             \xB9\xBD\xFEO
 test1,1204765,1266581233447 column=info:splitB, timestamp=1266609172177, value=\x00\x071290703\x00\x00\x00\x0
                             1\x26\xE7\xCA,\x7D\x1Btest1,1226169,1266609171581\x00\x071226169\x00\x00\x00\x05\
                             x05test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\
                             x00\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00\x0
                             0\x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSION\
                             x00\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x
                             00\x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x0
                             0\x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true
                             \xE1\xDF\xF8p
 test1,1204765,1266609171581 column=info:regioninfo, timestamp=1266609172212, value=REGION => {NAME => 'test1,
                             1204765,1266609171581', STARTKEY => '1204765', ENDKEY => '1226169', ENCODED => 21
                             34878372, TABLE => {{NAME => 'test1', FAMILIES => [{NAME => 'actions', VERSIONS =
                             > '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMOR
                             Y => 'false', BLOCKCACHE => 'true'}]}}
{code}

",stack,kannanm,Critical,Closed,Fixed,20/Feb/10 02:08,12/Oct/12 06:15
Bug,HBASE-2248,12457095,Provide new non-copy mechanism to assure atomic reads in get and scan,"HBASE-2037 introduced a new MemStoreScanner which triggers a ConcurrentSkipListMap.buildFromSorted clone of the memstore and snapshot when starting a scan.

After upgrading to 0.20.3, we noticed a big slowdown in our use of short scans.  Some of our data repesent a time series.   The data is stored in time series order, MR jobs often insert/update new data at the end of the series, and queries usually have to pick up some or all of the series.  These are often scans of 0-100 rows at a time.  To load one page, we'll observe about 20 such scans being triggered concurrently, and they take 2 seconds to complete.  Doing a thread dump of a region server shows many threads in ConcurrentSkipListMap.biuldFromSorted which traverses the entire map of key values to copy it.  ",ryanobjc,davelatham,Blocker,Closed,Fixed,22/Feb/10 23:35,12/Oct/12 06:14
Bug,HBASE-2250,12457108,typo in the maven pom,a simple typo in the maven pom,,ryanobjc,Major,Closed,Fixed,23/Feb/10 02:07,11/Jun/22 23:08
Bug,HBASE-2252,12457199,Mapping a very big table kills region servers,"Currently TableInputFormat doesn't change the block caching behavior of scans and one of our table grew so big that using the defaults we kill a least one region server per job run (because of GCs even if we have a heap of 7GB). This doesn't scale well, we should set it by default to false.",jdcryans,jdcryans,Major,Closed,Fixed,23/Feb/10 19:22,12/Oct/12 06:14
Bug,HBASE-2255,12457241,take trunk back to hadoop 0.20,"revert the dependency on hadoop 0.21, back to hadoop 0.20 (we hardly knew ye)",ryanobjc,ryanobjc,Major,Closed,Fixed,23/Feb/10 23:33,20/Nov/15 12:41
Bug,HBASE-2256,12457251,"Delete row, followed quickly to put of the same row will sometimes fail.","Doing a Delete of a whole row, followed immediately by a put to that row will sometimes miss a cell. Attached is a test to provoke the issue.",,clint.morgan,Major,Closed,Fixed,24/Feb/10 03:00,11/Jun/22 23:07
Bug,HBASE-2258,12457263,The WhileMatchFilter doesn't delegate the call to filterRow(),"While testing the recent MemStoreScanner slowness I noticed that each scan in the randomSeekScan test takes about 19 seconds to complete.  The scan in question provides a startRow and a WhileMatchFilter containing a PageFilter that asks for 120 rows.  I would have expected this scan to return in roughly the same amount of time as a scan that specifies a startRow and stopRow that spans a similar number of rows.

As it turns out this is an issue with the WhileMatchFilter.  The WhileMatchFilter is not delegating the call the filterRow() down the the PageFilter.  As a result the PageFilter never increments the rowsAccepted counter.",stack,dan.washusen,Major,Closed,Fixed,24/Feb/10 07:54,12/Oct/12 06:14
Bug,HBASE-2259,12457271,StackOverflow in ExplicitColumnTracker when row has many columns,"When doing a ""get"" on a row with many columns and where the ""get"" also contains many columns to get a stack overflow is thrown in the ExplicitColumnTracker:

java.io.IOException: java.io.IOException: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:872)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:862)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1733)
        at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Caused by: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(ExplicitColumnTracker.java:122)
        at org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(ExplicitColumnTracker.java:123)
        at org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(ExplicitColumnTracker.java:123)
[... repeats many times ...]
        at org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(ExplicitColumnTracker.java:123)
        at org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.checkColumn(ExplicitColumnTracker.java:123)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getRegionServerWithRetries(HConnectionManager.java:1048)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:417)
",jdcryans,dlrozendaal,Major,Closed,Fixed,24/Feb/10 09:07,12/Oct/12 06:15
Bug,HBASE-2266,12457386,[stargate] missing MiniDFSCluster dependency,"This is the problem: java.lang.NoClassDefFoundError: org/apache/hadoop/net/StaticMapping
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:287)

Some dependency is missing from the sub project.

% mvn -DskipTests clean install
...
[INFO] BUILD SUCCESSFUL

% mvn -Dtest=Test00MiniCluster test
...
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0

From the test log file:

testDFSMiniCluster(org.apache.hadoop.hbase.stargate.Test00MiniCluster)  Time elapsed: 1.263 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org/apache/hadoop/net/StaticMapping
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:287)
	at org.apache.hadoop.hbase.stargate.MiniClusterTestCase.startDFS(MiniClusterTestCase.java:81)
	at org.apache.hadoop.hbase.stargate.MiniClusterTestCase.startMiniCluster(MiniClusterTestCase.java:187)
	at org.apache.hadoop.hbase.stargate.MiniClusterTestCase.setUp(MiniClusterTestCase.java:225)
	at junit.framework.TestCase.runBare(TestCase.java:132)
",apurtell,apurtell,Major,Closed,Fixed,25/Feb/10 07:34,20/Nov/15 12:43
Bug,HBASE-2268,12457454,"[stargate] ""Failed tests: warning(junit.framework.TestSuite$1)"" and DEBUG output is dumped to console since move to Mavenized build","Two problems with running unit tests since Stargate was moved to Maven, possibly related:

1) DEBUG output from tests is dumped to console.

2) ""Failed tests: warning(junit.framework.TestSuite$1)"" -- what?

{noformat}
Results :

Failed tests: 
  warning(junit.framework.TestSuite$1)

Tests run: 76, Failures: 1, Errors: 0, Skipped: 0
{noformat}


Someone who knows Maven, can you take a look at this?",apurtell,apurtell,Major,Closed,Fixed,25/Feb/10 16:28,20/Nov/15 12:42
Bug,HBASE-2269,12457468,"PerformanceEvaluation ""--nomapred"" may assign duplicate random seed over multiple testing threads","When you use PerformanceEvaluation with ""--nomapred"" option, you will end up having the same random seeds assigned over multiple testing threads. So you'll get inaccurate results from ""random~~"" tests.

{code:title=PerformanceEvaluation.java}
542:  abstract class Test {
543:     protected final Random rand = new Random(System.currentTimeMillis());
{code}

Milliseconds won't be sufficient; today's JVM is much faster to create multiple Test objects in one millisecond. You might want to use something like ""{{super.hashCode()}}"" instead. 
",,tatsuya6502,Minor,Closed,Fixed,25/Feb/10 18:21,20/Nov/15 12:42
Bug,HBASE-2276,12457663,Hbase Shell hcd() method is broken by the replication scope parameter,"Since an additional HColumnDescriptor constructor parameter (scope) was introduced, hbase shell hcd() method fails to create a HColumnDescriptor object:

hbase(main):007:0> alter 'doc_total_stats', {NAME => 'country_views', VERSIONS => 1}
ArgumentError: wrong # of arguments(8 for 9)
",kovyrin,kovyrin,Major,Closed,Fixed,28/Feb/10 07:54,20/Nov/15 12:42
Bug,HBASE-2283,12458055,row level atomicity ,"The flow during a HRegionServer.put() seems to be the following. [For now, let's just consider single row Put containing edits to multiple column families/columns.

HRegionServer.put() does a:

        HRegion.put();
       syncWal()  (the HDFS sync call).  /* this is assuming we have HDFS-200 */

HRegion.put() does a:
  for each column family 
  {
      HLog.append(all edits to the colum family);

      write all edits to Memstore;
  }

HLog.append() does a :
  foreach edit in a single column family {
    doWrite()
  }

doWrite() does a:
   this.writer.append().

There seems to be two related issues here that could result in inconsistencies.

Issue #1: A put() does a bunch of HLog.append() calls. These in turn do a bunch of ""write"" calls on the underlying DFS stream.  If we crash after having written out some append's to DFS, recovery will run and apply a partial transaction to memstore.  

Issue #2: The updates to memstore  should happen after the sync rather than before. Otherwise, there is the danger that the write to DFS (sync) fails for some reason & we return an error to the client, but we have already taken edits to the memstore. So subsequent reads will serve uncommitted data.
",kannanm,kannanm,Blocker,Closed,Fixed,03/Mar/10 23:55,20/Nov/15 12:43
Bug,HBASE-2284,12458061,fsWriteLatency metric may be incorrectly reported ,"fsWriteLatency metric is computed by maintaining writeTime & writeOps in HLog. If an HLog.append() carries multiple edits, then ""writeTime"" is computed incorrectly for the subsequent edits because doWrite() is called for each of the edits with the same start time argument (""now"").

This also causes a lot of false WARN spews to the log. Only one of the edits might have taken a long time, but every edit after that in a given HLog.append() operation will also raise these warning messages.

{code}
2010-03-03 11:00:42,247 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302227
2010-03-03 11:00:42,247 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302228
2010-03-03 11:00:42,247 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302229
2010-03-03 11:00:42,247 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302230
2010-03-03 11:00:42,247 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302231
{code}

Will submit a patch shortly. ",kannanm,kannanm,Minor,Closed,Fixed,04/Mar/10 01:06,10/Jun/19 02:26
Bug,HBASE-2287,12458178,TypeError in shell,"In trunk I currently get:

{code}
hbase(main):003:0> get 't', 'r', 'f:allo'
TypeError: can't convert Class into String
	from /Users/jdcryans/Work/HBase/Trunk/bin/../bin/HBase.rb:526:in `get'
	from /Users/jdcryans/Work/HBase/Trunk/bin/../bin/hirb.rb:404:in `get'
	from (hbase):4
{code}

That part of the code didn't change but the JRuby version was switched from 1.2 to 1.4",kovyrin,jdcryans,Major,Closed,Fixed,04/Mar/10 21:42,20/Nov/15 12:42
Bug,HBASE-2288,12458196,Shell fails on alter,"HBASE-2035 added this line to branch which doesn't make sense since it's related to a Trunk feature:

{code}
        if args[DEFERRED_LOG_FLUSH]
          htd.setDeferredLogFlush(JBoolean.valueOf(args[DEFERRED_LOG_FLUSH]))
        end
{code}

We just need to remove it.",jdcryans,jdcryans,Major,Closed,Fixed,05/Mar/10 00:41,12/Oct/12 06:15
Bug,HBASE-2291,12458300,Test Create Issue Ignore,,,stack,Major,Closed,Fixed,05/Mar/10 21:56,11/Jun/22 23:08
Bug,HBASE-2293,12458307,CME in RegionManager#isMetaServer,"{code}
2010-03-05 14:26:17,990 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 84 on 60000, call regionServerReport(address: 98.137.30.50:60020, startcode: 1267824847596, load: (requests=0, regions=163, usedHeap=3113, maxHeap=5983), [Lorg.apache.hadoop.hbase.HMsg;@5446315a, [Lorg.apache.hadoop.hbase.HRegionInfo;@4c5236ef) from 98.137.30.50:48101: error: java.io.IOException: java.util.ConcurrentModificationException
java.io.IOException: java.util.ConcurrentModificationException
       at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)
       at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)
       at org.apache.hadoop.hbase.master.RegionManager.isMetaServer(RegionManager.java:832)
       at org.apache.hadoop.hbase.master.RegionManager.regionsAwaitingAssignment(RegionManager.java:391)
       at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:196)
       at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:488)
       at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:414)
       at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:323)
       at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:724)
       at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:597)
       at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
       at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
{code}",jdcryans,stack,Major,Closed,Fixed,05/Mar/10 22:43,12/Oct/12 06:14
Bug,HBASE-2295,12458313,Row locks may deadlock with themselves,"Row locks in HRegion are keyed by a int-sized hash of the row key. It's perfectly possible for two rows to hash to the same key. So, if any client tries to lock both rows, it will deadlock with itself. Switching to a 64-bit hash is an improvement but still sketchy.",dhruba,tlipcon,Major,Closed,Fixed,05/Mar/10 23:31,20/Nov/15 12:43
Bug,HBASE-2305,12458570,Client port for ZK has no default,"From Doug Meil on the list:

{code}
config.set(""hbase.zookeeper.property.clientPort"",""2181"");

This is consistent with other references to the importance of ""hbase.zookeeper.quorum"" except that we did this testing with the intent of connecting using only parameters set in code to eliminate any classpath issues with XML files.

FYI... If the last parameter isn't set you'll get the following error....       could this be defaulted to a reasonable value?


10/03/09 10:04:55 ERROR zookeeper.ZooKeeperWrapper: no clientPort found in zoo.cfg
Exception in thread ""main"" java.io.IOException: Could not read quorum servers from zoo.cfg
     at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.<init>(ZooKeeperWrapper.java:85)
     at org.apache.hadoop.hbase.client.HConnectionManager$ClientZKWatcher.getZooKeeperWrapper(HConnectionManager.java:223)
     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getZooKeeperWrapper(HConnectionManager.java:932)
     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRootRegion(HConnectionManager.java:948)
     at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:625)
{code}

It should be easy to set it to 2181 along with the other port settings in HQuorumPeer.",,jdcryans,Major,Closed,Fixed,09/Mar/10 18:27,20/Nov/15 12:42
Bug,HBASE-2307,12458675,"hbase-2295 changed hregion size, testheapsize broke... fix it.",Both trunk and branch are broke at mo.,stack,stack,Major,Closed,Fixed,10/Mar/10 15:44,20/Nov/15 12:41
Bug,HBASE-2308,12458738,"Fix the bin/rename_table.rb script, make it work again",bin/rename_table.rb script rotted.  Bring it back up to date (Someone asked nicely for it up on IRC).,stack,stack,Major,Closed,Fixed,10/Mar/10 21:33,20/Nov/15 12:40
Bug,HBASE-2311,12458880,Not having assertions enabled causes index contrib tests to fail,,,stack,Major,Closed,Fixed,12/Mar/10 01:05,20/Nov/15 12:43
Bug,HBASE-2312,12458882,Possible data loss when RS goes into GC pause while rolling HLog,"There is a very corner case when bad things could happen(ie data loss):

1)	RS #1 is going to roll its HLog - not yet created the new one, old one will get no more writes
2)	RS #1 enters GC Pause of Death
3)	Master lists HLog files of RS#1 that is has to split as RS#1 is dead, starts splitting
4)	RS #1 wakes up, created the new HLog (previous one was rolled) and appends an edit - which is lost

The following seems like a possible solution:

1)	Master detects RS#1 is dead
2)	The master renames the /hbase/.logs/<regionserver name>  directory to something else (say /hbase/.logs/<regionserver name>-dead)
3)	Add mkdir support (as opposed to mkdirs) to HDFS - so that a file create fails if the directory doesn't exist. Dhruba tells me this is very doable.
4)	RS#1 comes back up and is not able create the new hlog. It restarts itself.
",nspiegelberg,karthik.ranga,Critical,Closed,Fixed,12/Mar/10 01:19,20/Nov/15 12:42
Bug,HBASE-2313,12458885,"Nit-pick about hbase-2279 shell fixup, if you do get with non-existant column family, throws lots of exceptions","I believe the old shell would complain once only.. .or at least, it didn't print out so many fat exceptions.  New fixedup shell will retry a bunch of times with a fat stack trace each time.  Assigning Alexey 'cos he asked for it.",kovyrin,stack,Major,Closed,Fixed,12/Mar/10 01:38,20/Nov/15 12:42
Bug,HBASE-2322,12459045,deadlock between put and cacheflusher in 0.20 branch,"{code}
Found one Java-level deadlock:
=============================
""IPC Server handler 59 on 60020"":
  waiting for ownable synchronizer 0x00007fec9eb050f8, (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
  which is held by ""IPC Server handler 54 on 60020""
""IPC Server handler 54 on 60020"":
  waiting to lock monitor 0x000000004190e950 (object 0x00007fec64f25258, a java.util.HashSet),
  which is held by ""regionserver/10.20.20.186:60020.cacheFlusher""
""regionserver/10.20.20.186:60020.cacheFlusher"":
  waiting for ownable synchronizer 0x00007fec651df998, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
  which is held by ""IPC Server handler 19 on 60020""
""IPC Server handler 19 on 60020"":
  waiting for ownable synchronizer 0x00007fec9eb050f8, (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync),
  which is held by ""IPC Server handler 54 on 60020""

Java stack information for the threads listed above:
===================================================
""IPC Server handler 59 on 60020"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007fec9eb050f8> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1299)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1281)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1789)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
""IPC Server handler 54 on 60020"":
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.request(MemStoreFlusher.java:172)
        - waiting to lock <0x00007fec64f25258> (a java.util.HashSet)
        at org.apache.hadoop.hbase.regionserver.HRegion.requestFlush(HRegion.java:1549)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1534)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1318)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1281)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1789)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
""regionserver/10.20.20.186:60020.cacheFlusher"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007fec651df998> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:235)
        - locked <0x00007fec64f25258> (a java.util.HashSet)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:149)
""IPC Server handler 19 on 60020"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007fec9eb050f8> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:980)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:873)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:241)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushSomeRegions(MemStoreFlusher.java:352)
        - locked <0x00007fec64ed96f0> (a org.apache.hadoop.hbase.regionserver.MemStoreFlusher)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.reclaimMemStoreMemory(MemStoreFlusher.java:321)
        - locked <0x00007fec64ed96f0> (a org.apache.hadoop.hbase.regionserver.MemStoreFlusher)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1783)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

Found 1 deadlock.
{code}",stack,stack,Blocker,Closed,Fixed,14/Mar/10 01:07,12/Oct/12 06:15
Bug,HBASE-2323,12459049,filter.RegexStringComparator does not work with certain bytes,"I'm trying to use {{RegexStringComparator}} in conjunction with {{RowFilter}}.  One of my row keys contained the byte 0xA, which turns out to be the ASCII code for the newline character (\n).  When the row key is converted to a string in order to use the regexp facility of the Java standard library, it becomes a string containing two lines and my regexp does not match.

I believe the solution is to compile the regexp with the {{DOTALL}} flag.  Luckily, this flag can be ""passed"" by the client by prefixing the regexp with {{(?s)}} so people working with an older version of HBase can work around this issue without having to upgrade.


Second problem: One of my row keys contained the sequence {{0x00 0x00 0x9D}} ({{0x9D}} = -99 when stored in a Java {{byte}}) but in {{compareTo}} the row key is transformed in a {{String}} using {{Bytes.toString}}, which just assumes that the byte array is an UTF8 encoded string.  Java ""cleverly"" substituted the 0x9D byte with 0x63 (character '?').  In my case, I want to use encoding ISO-8859-1 as it preserves every byte when the byte array is converted to a {{String}} and back to a byte array, unlike UTF-8 or ASCII.  Should we add a new method to {{RegexStringComparator}} to allow the user to specify their own {{Charset}} instance?",tsuna,tsuna,Major,Closed,Fixed,14/Mar/10 02:38,12/Oct/12 06:16
Bug,HBASE-2330,12459293,Put errors are burried by the RetriesExhaustedException,"When a user tries to put into non-existing column family, all the NoSuchColumnFamilyException errors get buried (many of them, since we retry 10 times by default) and then we throw useless RetriesExhaustedException that tells user nothing but the fact that the put operation has failed.",,kovyrin,Major,Closed,Fixed,16/Mar/10 17:14,11/Jun/22 23:19
Bug,HBASE-2335,12459353,mapred package docs don't say zookeeper jar is a dependency.,"in:
http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/mapred/package-summary.html

we dont say the classpath needs zookeeper-x.y.z.jar - which it does.

But this package does say so:
http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/mapreduce/package-summary.html

",stack,ryanobjc,Major,Closed,Fixed,17/Mar/10 06:05,12/Oct/12 06:15
Bug,HBASE-2336,12459379,Fix build broken with HBASE-2334,"HBASE-2334 was a bit to eager to put SLF4J in the ""test"" scope. Thrift needs SLF4J.",larsfrancke,larsfrancke,Major,Closed,Fixed,17/Mar/10 12:03,20/Nov/15 12:42
Bug,HBASE-2337,12459405,log recovery: splitLog deletes old logs prematurely,"splitLog()'s purpose is to take a bunch of commit logs of a crashed RS and create per-region logs. splitLog() runs in the master.  There are two cases where splitLog() might end up deleting an old log before actually creating (sync/closing) the newly created logs. If the master crashes in between deletion of the old log and creation of the new log, then edits could be lost irrecoverably.

More specifically here are the two issues we (Nicolas, Aravind and I) noticed:

Issue #1: The old logs are read one at a time. An in memory structure, logEntries (a map from region name to edits for the region), is populated. And the old logs are closed. Then the in-memory map is written out to per region files. Fix: We should move the file deletion to later.

Issue #2: There is another little case. The per-region log file is written under the region directory (named oldlogfile.log or the constant HREGION_OLDLOGFILE_NAME). Before the master creates the file, it checks to see if there is already a file with that name, and if so, it renames it to oldlogfile.log.old, and then creates file oldlogfile.log again, and copies over the contents of oldlogfile.log.old to oldlogfile.log. It then proceeds to delete ""oldlogfile.log.old"", even though it hasn't closed/sync'ed ""oldlogfile.log"" yet. 

--

I think we should be able to restructure the code such that all deletion of old logs happens *after* the new logs have been created (i.e. written to & closed).






",nspiegelberg,kannanm,Blocker,Closed,Fixed,17/Mar/10 17:25,20/Nov/15 12:42
Bug,HBASE-2338,12459407,log recovery: deleted items may be resurrected,"While working on HBASE-2283, noticed that if you do a put followed by a delete, and then crash the RS, and trigger log recovery to happen, then deleted entries may be resurrected. 

Suprisingly, the issue only affected delete of a specific column. Full row delete didn't run into this issue.

---

Code inspection revealed that we might have an issue with timestamps & WAL stuff for delete that come in with ""LATEST"" timestamp. [Note: The ""LATEST"" timestamp is syntax sugar/hint to the RS to convert it to ""now"". ]

Basically, in:

{code}
delete(byte [] family, List<KeyValue> kvs, boolean writeToWAL)
{code}

the ""kv.updateLatestStamp(byteNow);"" time stamp massaging (from LATEST to now) happens *after* the WAL log.append() call. So the KeyValue entries written to the HLog do not have the massaged timestamp. On recovery, when these entries are replayed, we add them back to reconstructionCache but don't do anything with timestamps. 

The above could be the potential source of the problem. But there could be more to the problem than my simple analysis. For instance, we still don't know why full row delete worked fine, but delete of a specific column didn't work ok. Forking this off as a separate issue from HBASE-2283.

[Note: Aravind is starting to take a look at this issue.]
",aravind.menon,kannanm,Major,Closed,Fixed,17/Mar/10 17:34,20/Nov/15 12:44
Bug,HBASE-2344,12459467,InfoServer and hence HBase Master doesn't fully start if you have HADOOP-6151 patch,"This is was due to a minor issue on the HBase side. The hadoop HttpServer prior to HADOOP-6151 was more tolerant than now.

In /org/apache/hadoop/hbase/util/InfoServer.java, addDefaultApps() adds a null key to the defaultContexts map. After HADOOP-6151, the HttpServer code raises a NPE. And hence HBase master doesn't fully start.

Will submit the patch shortly.
",kannanm,kannanm,Minor,Closed,Fixed,18/Mar/10 00:58,20/Nov/15 12:42
Bug,HBASE-2346,12459663,Usage of FilterList slows down scans,"When using a FilterList the scan is much slower compared to a scan with only a single filter (tested SingleColumnValueFilter and PrefixFilter).

The difference is extrem for very small ranges: if the range is only 10 rows the scan is 10 times slower when using the FilterList.

Is the cause just GC or object serialization/deserialization?

For a simple test I used the PerformanceEvaluation tool and created the TestTable with only 10(!) rows:
$ bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows=10 sequentialWrite 1

The attached test performs 100 scan using diffent filters. The filter is then wrapped into a FilterList (with only a single filter). This FilterList is then nested two more times into another FilterList. For each nested level the scan gets slower and slower.

The test created the following output:

Scan Null Filter (10): 391ms
Scan FilterList with Null Filter (0): 4788ms
Scan Nested FilterList with Null Filter (0): 8303ms
Scan Nested Nested FilterList with Null Filter (0): 11915ms

Scan SingleColumValueFilter Equal (0): 257ms
Scan FilterList with SingleColumValueFilter Equal (0): 4121ms
Scan Nested FilterList with SingleColumValueFilter Equal (0): 7965ms
Scan Nested Nested FilterList with SingleColumValueFilter Equal (0): 11600ms

Scan SingleColumValueFilter Not Equal (10): 912ms
Scan FilterList with SingleColumValueFilter Not Equal (10): 4542ms
Scan Nested FilterList with SingleColumValueFilter Not Equal (10): 8459ms
Scan Nested Nested FilterList with SingleColumValueFilter Not Equal (10): 11513ms

Scan PrefixFilter (10): 306ms
Scan FilterList with PrefixFilter (10): 3695ms
Scan Nested FilterList with PrefixFilter (10): 7762ms
Scan Nested Nested FilterList with PrefixFilter (10): 11721ms

Get: 245ms
",jdcryans,seelmann,Major,Closed,Fixed,19/Mar/10 15:28,12/Oct/12 06:15
Bug,HBASE-2352,12459800,Small values for hbase.client.retries.number and ipc.client.connect.max.retries breaks long ops in hbase shell,"After switching hbase shell to small retries numbers (to make Puts to invalida coordinates friendlier to users), we broke long operations on tables (like disabling, truncating, etc) because they use the same config values to control client retries.
Need to roll the change back (will provide a patch) and make a better fix for puts later (after HBASE-2330 is resolved)",kovyrin,kovyrin,Major,Closed,Fixed,21/Mar/10 18:14,20/Nov/15 12:43
Bug,HBASE-2353,12459817,HBASE-2283 removed bulk sync optimization for multi-row puts,"previously to HBASE-2283 we used to call flush/sync once per put(Put[]) call (ie: batch of commits).  Now we do for every row.  

This makes bulk uploads slower if you are using WAL.  Is there an acceptable solution to achieve both safety and performance by bulk-sync'ing puts?  Or would this not work in face of atomic guarantees?

discuss!",tlipcon,ryanobjc,Blocker,Closed,Fixed,22/Mar/10 03:11,20/Nov/15 12:43
Bug,HBASE-2355,12459822,unsychronized logWriters map is mutated from several threads in HLog splitting,"In splitLog, the logWriters map is an unsynchronized collection, and the spawned threads mutate it. In practice I've now seen several times a situation where one of the puts into this map is lost, and a thread ends up renaming a file it's in the process of writing to, causing those edits to be lost when the log is split.",tlipcon,tlipcon,Critical,Closed,Fixed,22/Mar/10 06:34,12/Oct/12 06:14
Bug,HBASE-2358,12459890,Store doReconstructionLog will fail if oldlogfile.log is empty and won't load region,"doReconstructionLog doesn't handle empty files correctly:
{code}
    FileStatus stat = this.fs.getFileStatus(reconstructionLog);
    if (stat.getLen() <= 0) {
      LOG.warn(""Passed reconstruction log "" + reconstructionLog +
        "" is zero-length. Deleting existing file"");
       fs.delete(reconstructionLog, false);
      return -1;
    }
{code}

Notice it actually compares the length of the array instead of the file length.

It should call getLen() and delete the file afterwards
{code}
   FileStatus stat = this.fs.getFileStatus(reconstructionLog);
    if (stat.getLen() <= 0) {
      LOG.warn(""Passed reconstruction log "" + reconstructionLog +
        "" is zero-length. Deleting existing file"");
       fs.delete(reconstructionLog, false);
      return -1;
    }
{code}

Also. This is a situation that shouldn't happen as an empty oldlogfile.log should be deleted when HMaster does the split in HLog.splitLog().
I couldn't figure what would make it leave it there as I also see in the logs that other empty logs are deleted. This might expose a thornier situation.",clehene,clehene,Major,Closed,Fixed,22/Mar/10 18:01,12/Oct/12 06:15
Bug,HBASE-2359,12459895,WALEdit doesn't implement HeapSize,"WALEdit from HBASE-2283 defines a method heapSize() but doesn't implement HeapSize.

 - Make it implement the interface.
 - Add a test to TestHeapSize.",kannanm,jdcryans,Major,Closed,Fixed,22/Mar/10 18:29,20/Nov/15 12:43
Bug,HBASE-2361,12459903,WALEdit broke replication scope,"Before HBASE-2283, each KV had a HLogKey with a replication scope. Now a key applies to a list of KVs that spans multiple families so a single scope doesn't work anymore. Multiple possible solutions:

 - Each KV have their own scope. We already ruled that out in a previous jira since that means the scope would end up in the HFiles.
 - Store pairs of scope/KV in WALEdit instead of straight KVs.
 - Have 2 parallel lists in WALEdit, one for KVs and the other for scopes.
 - Subclass KV and add the scope there, those would be created when inserted in the WAL and would contain the KV stored in the HFiles.

I'm sure there are other solutions, discuss.",jdcryans,jdcryans,Major,Closed,Fixed,22/Mar/10 19:28,20/Nov/15 12:44
Bug,HBASE-2365,12459959,Double-assignment around split,"Its looking like we have a split updating .META. with daughter regions and then before we process the split in master, one of the daughters has already been assigned.  On processing of the split, we assign daughter again.

I thought this had been fixed previously?  Doesn't seem so.  Need to look again.

Here is evidence for region named:

{code}summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017{code}

First master-side:

{code}
2010-03-15 16:06:52,153 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017 to cactus208,60020,12686305486412010-03-15 16:06:52,156 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 245 row(s) of meta region {server: 172.16.1.209:60020, regionname: .META.,,1, startKey: <>} complete2010-03-15 16:06:52,156 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned2010-03-15 16:06:52,841 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E5\x25A4\x25A7\x25E8\x25BF\x259E\x25E5\x2588\x2598\x25E5\x25B8\x2588\x25E5\x2582\x2585\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E5\x2587\x25B9\x25E9\x2599\x25B7\x25E4\x25BF\x25AE\x25E5\x25A4\x258D\x25E6\x2596\x25BD\x25E5\x25B7\x25A5\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E7\x2594\x25A8\x25E5\x2593\x2581\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017 to cactus210,60020,12686305508862010-03-15 16:06:54,377 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT: summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E5\x25A4\x25A7\x25E8\x25BF\x259E\x25E5\x2588\x2598\x25E5\x25B8\x2588\x25E5\x2582\x2585\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E5\x2587\x25B9\x25E9\x2599\x25B7\x25E4\x25BF\x25AE\x25E5\x25A4\x258D\x25E6\x2596\x25BD\x25E5\x25B7\x25A5\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E7\x2594\x25A8\x25E5\x2593\x2581\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268611349836: Daughters; summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E5\x25A4\x25A7\x25E8\x25BF\x259E\x25E5\x2588\x2598\x25E5\x25B8\x2588\x25E5\x2582\x2585\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E5\x2587\x25B9\x25E9\x2599\x25B7\x25E4\x25BF\x25AE\x25E5\x25A4\x258D\x25E6\x2596\x25BD\x25E5\x25B7\x25A5\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E7\x2594\x25A8\x25E5\x2593\x2581\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017, summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017 from cactus209,60020,1268630548451; 1 of 32010-03-15 16:06:54,388 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017 to cactus209,60020,1268630548451
{code}

Its hard to read but above is an assignment, the split message, then what seems to be same region being assigned again.

Here is RS side on 209 server:

{code}
2010-03-15 16:06:29,727 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017
2010-03-15 16:06:29,792 INFO org.apache.hadoop.hbase.regionserver.HRegion: region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017/1011052036 available; sequence id is 199443346
2010-03-15 16:06:29,792 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E5\x25A4\x25A7\x25E8\x25BF\x259E\x25E5\x2588\x2598\x25E5\x25B8\x2588\x25E5\x2582\x2585\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E5\x2587\x25B9\x25E9\x2599\x25B7\x25E4\x25BF\x25AE\x25E5\x25A4\x258D\x25E6\x2596\x25BD\x25E5\x25B7\x25A5\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E7\x2594\x25A8\x25E5\x2593\x2581\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017
2010-03-15 16:06:29,793 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017
2010-03-15 16:06:29,944 INFO org.apache.hadoop.hbase.regionserver.HRegion: region summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E5\x25A4\x25A7\x25E8\x25BF\x259E\x25E5\x2588\x2598\x25E5\x25B8\x2588\x25E5\x2582\x2585\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E5\x2587\x25B9\x25E9\x2599\x25B7\x25E4\x25BF\x25AE\x25E5\x25A4\x258D\x25E6\x2596\x25BD\x25E5\x25B7\x25A5\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E7\x2594\x25A8\x25E5\x2593\x2581\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017/1971466363 available; sequence id is 199443345
2010-03-15 16:06:32,750 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE_WITHOUT_REPORT: summary,SITE_0000000032\x01pt\x0120100314000000\x01\x25E7\x258C\x25AE\x25E5\x258E\x25BF\x25E5\x2586\x2580\x25E9\x25B9\x25B0\x25E6\x2591\x25A9\x25E6\x2593\x25A6\x25E6\x259D\x2590\x25E6\x2596\x2599\x25E5\x258E\x2582\x2B\x25E6\x25B1\x25BD\x25E8\x25BD\x25A6\x25E9\x2585\x258D\x25E4\x25BB\x25B6\x25EF\x25BC\x258C\x25E5\x2598\x2580\x25E9\x2593\x2583\x25E9\x2593\x2583--\x25E7\x259C\x259F\x25E5\x25AE\x259E\x25E5\x25AE\x2589\x25E5\x2585\x25A8\x25E7\x259A\x2584\x25E7\x2594\x25B5\x25E8\x25AF\x259D\x25E3\x2580\x2581\x25E7\x25BD\x2591\x25E7\x25BB\x259C\x25E4\x25BA\x2592\x25E5\x258A\x25A8\x25E4\x25BA\x25A4\x25E5\x258F\x258B\x25E7\x25A4\x25BE\x25E5\x258C\x25BA\x25EF\x25BC\x2581,1268640385017: Duplicate assignment
{code}

See how we end with 'Duplicate assignment' message?",stack,stack,Blocker,Closed,Fixed,23/Mar/10 07:39,12/Oct/12 06:15
Bug,HBASE-2369,12460052,hbase master.jsp not rooted correct in trunk,"The path seems to be /webapps/master/master.jsp not /master.jsp, plus there is no automatic redirect like there used to be.",,ryanobjc,Major,Closed,Fixed,23/Mar/10 23:45,20/Nov/15 12:41
Bug,HBASE-2370,12460054,saveVersion.sh doesnt properly grab the git revision,saveVersion.sh runs in $BASE/core which does not have a .git directory which is what it is testing for.  Needs to test for ../.git i think,ryanobjc,ryanobjc,Major,Closed,Fixed,23/Mar/10 23:50,20/Nov/15 12:42
Bug,HBASE-2371,12460157,"trunk, shell command 'list' does not work. tables exist and respond to other commands however","list show no tables, but the webui shows many tables.  describe and exists both work on specific table names that do exist.",kovyrin,ryanobjc,Major,Closed,Fixed,24/Mar/10 20:26,20/Nov/15 12:40
Bug,HBASE-2373,12460177,"Remove confusing log message of how ""BaseScanner GET got different address/startcode than SCAN""","Seeing some of these errors in the HBase master's logs:

{code}
2010-03-24 16:52:17,003 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test1,8947030000,1269474720493 got different address than SCAN: sa=10.18.34.217:60020, serverAddress=
2010-03-24 16:52:17,003 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test1,8947030000,1269474720493 got different startcode than SCAN: sc=1269456397807, serverAddress=0
2010-03-24 16:52:17,018 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test1,8953040000,1269474720493 got different address than SCAN: sa=10.18.34.215:60020, serverAddress=
2010-03-24 16:52:17,018 DEBUG org.apache.hadoop.hbase.master.BaseScanner: GET on test1,8953040000,1269474720493 got different startcode than SCAN: sc=1269456397735, serverAddress=0
{code}

Ideas? 
",stack,kannanm,Major,Closed,Fixed,24/Mar/10 23:58,12/Oct/12 06:14
Bug,HBASE-2378,12460276,Bulk insert with multiple reducers broken due to improper ImmutableBytesWritable comparator,"If I run MR to prepare HFIles with more than one reducer then some values for keys are not appeared in the table after loadtable.rb script execution. With one reducer everything works fine.

References:
http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/mapreduce/package-summary.html#bulk
- the row id must be formatted as a ImmutableBytesWritable
- MR job should ensure a total ordering among all keys

MAPREDUCE-366  (patch-5668-3.txt)
- TotalOrderPartitioner that uses the new API (attached)

HBASE-2063
- patched HFileOutputFormat (attached)

Input data (attached):
* my_sample_log_1k.txt - sample data, input for MyHFilesWriter

Source (attached):
* MyKeyComparator.java - comparator for my ImmutableBytesWritable keys
* TestTotalOrderPartitionerForMyKeys.java - test case for my keys (note that I've set up MyKeyComparator to pass that test)
* MyHFilesWriter.java	 - My MR job to prepare HFiles
* HFileOutputFormat.java - from MAPREDUCE-366
* TotalOrderPartitioner.java - from MAPREDUCE-366
* MySampler.java - My RandomSampler based on Sampler from MAPREDUCE-366 BUT I've put the following string into getSample method (without that string it doesn't work):
{code}
            reader.initialize(splits.get(i), new TaskAttemptContext(job.getConfiguration(), new TaskAttemptID()));
{code}


Test case:
# comment the following string in MyHFilesWriter: //job.setSortComparatorClass(MyKeyComparator.class);
# hadoop jar keyvalue-poc.jar MyHFilesWriter -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/hfiles/01/ -r 1
# hadoop jar keyvalue-poc.jar MyHFilesWriter -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/hfiles/02/ -r 2
# hbase> create 'tst_hfiles_01', {NAME => 'vals'}
# hbase> create 'tst_hfiles_02', {NAME => 'vals'}
# hbase org.jruby.Main /usr/lib/hbase-0.20/bin/loadtable.rb tst_hfiles_01 /test_hbase/hfiles/01
# hbase org.jruby.Main /usr/lib/hbase-0.20/bin/loadtable.rb tst_hfiles_02 /test_hbase/hfiles/02
# check values for keys
# uncomment the following string in MyHFilesWriter: //job.setSortComparatorClass(MyKeyComparator.class);
# hadoop jar keyvalue-poc.jar MyHFilesWriter -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/hfiles/03/ -r 2

for example, results:
{code}
hbase(main):006:0* count 'tst_hfiles_01', 100 
Current count: 100, row: 0.14.USA.IL.602.ELMHURST.1.1.0.0                                                     
Current count: 200, row: 0.245.USA.ME.500.PORTLAND.1.1.0.0                                                    
Current count: 300, row: 0.34.USA.FL.Rollup.Rollup.1.1.0.0                                                    
Current count: 400, row: 0.443.USA.CA.803.LOS.ANGELES.1.1.0                                                   
Current count: 500, row: 0.8.USA.CO.751.CASTLE.ROCK.1.1.0                                                     
Current count: 600, row: 1.14.DZA.Rollup.Rollup.Rollup.1.1.0.1                                                
Current count: 700, row: 1.159.SWE.AB.Rollup.Rollup.1.1.0.1                                                   
Current count: 800, row: 1.17.USA.TN.659.CLARKSVILLE.1.1.0.1                                                  
Current count: 900, row: 1.220.USA.MI.505.SOUTHFIELD.1.1.0.1                                                  
999 row(s) in 0.0930 seconds
hbase(main):007:0> count 'tst_hfiles_02', 100
Current count: 100, row: 0.231.USA.GA.524.BUFORD.1.1.0.1                                                      
Current count: 200, row: 0.4.USA.VA.573.Rollup.1.1.0.0                                                        
Current count: 300, row: 0.9.ROU.B.-1.BUCHAREST.1.1.0.0                                                       
Current count: 400, row: 1.16.USA.IA.679.Rollup.1.1.1.0                                                       
Current count: 500, row: 1.245.NOR.03.-1.OSLO.1.1.0.0                                                         
Current count: 600, row: 0.245.GBR.ENG.826005.BEXLEY.1.1.0.1                                                  
Current count: 700, row: 0.48.GBR.ENG.826027.Rollup.1.1.0.1                                                   
Current count: 800, row: 1.14.SWE.Rollup.Rollup.Rollup.1.1.0.1                                                
Current count: 900, row: 1.201.GBR.ENG.826005.LONDON.1.1.0.1                                                  
999 row(s) in 0.1630 seconds
hbase(main):008:0> get 'tst_hfiles_01', '0.14.USA.IL.602.ELMHURST.1.1.0.0'
COLUMN                       CELL                                                                             
 vals:key0                   timestamp=1269542753914, value=0                                                 
 vals:key1                   timestamp=1269542753914, value=14                                                
 vals:key2                   timestamp=1269542753914, value=USA                                               
 vals:key3                   timestamp=1269542753914, value=IL                                                
 vals:key4                   timestamp=1269542753914, value=602                                               
 vals:key5                   timestamp=1269542753914, value=ELMHURST                                          
 vals:key6                   timestamp=1269542753914, value=1                                                 
 vals:key7                   timestamp=1269542753914, value=1                                                 
 vals:key8                   timestamp=1269542753914, value=0                                                 
 vals:key9                   timestamp=1269542753914, value=0                                                 
 vals:val0                   timestamp=1269542753914, value=2                                                 
11 row(s) in 0.0160 seconds
hbase(main):009:0> get 'tst_hfiles_02', '0.14.USA.IL.602.ELMHURST.1.1.0.0'
COLUMN                       CELL                                                                             
0 row(s) in 0.0220 seconds
{code}

with MyKeyComparator
{code}
java.io.IOException: Added a key not lexically larger than previous key=.103.FRA.V.-1.LYON.1.1.0.0valskey0'XXX, lastkey=1.20.USA.AOL.0.AOL.1.1.0.0valsval0'XXX
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.checkKey(HFile.java:551)
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:513)
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:481)
	at com.contextweb.hadoop.hbase.mapred.HFileOutputFormat$1.write(HFileOutputFormat.java:77)
	at com.contextweb.hadoop.hbase.mapred.HFileOutputFormat$1.write(HFileOutputFormat.java:49)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:508)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer.reduce(KeyValueSortReducer.java:46)
	at org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer.reduce(KeyValueSortReducer.java:35)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
{code}",tlipcon,lansa,Critical,Closed,Fixed,25/Mar/10 19:24,12/Oct/12 06:15
Bug,HBASE-2389,12460582,HTable - delete / put unnecessary sync. ,"HTable is not thread-safe , but some of the methods seem to have a synchronized block  (put/delete) etc. 

It might as well be better to remove them altogether. 
",,kaykay.unique,Major,Closed,Fixed,29/Mar/10 18:20,20/Nov/15 12:41
Bug,HBASE-2391,12460608,TableServers.isMasterRunning() skips checking master status if master field is not null,"We call HBaseAdmin.isMasterRunning()  to see if client has connection with HBase.

While going over TableServers.isMasterRunning() in HConnectionManager, I see this:
    public boolean isMasterRunning() {
      if (this.master == null) {
        try {
          getMaster();
         
        } catch (MasterNotRunningException e) {
          return false;
        }
      }
      return true;
    }
When isMasterRunning() is called the first time, if master is obtained successfully, master field would contain reference to HMasterInterface. Subsequent calls to isMasterRunning() wouldn't throw MasterNotRunningException even if master server stops responding to clients.

I think master.isMasterRunning() should be called if master isn't null.

J-D pointed out that:
I think this method wasn't updated when we moved to Zookeeper (since
in pre-0.20, dead master = dead cluster), also looking at when this is
called, I only see it from HMerge and HBaseAdmin.isMasterRunning()...
which in turn isn't called anywhere in the java code (I think we call
it in the shell tho).
",,ted_yu,Major,Closed,Fixed,29/Mar/10 21:34,20/Nov/15 12:41
Bug,HBASE-2397,12460858,Bytes.toStringBinary escapes printable chars,"Bytes.toStringBinary hex-escapes printable chars such as '@', '$', '#', '%', '&', '*', '(', ')', '{', '}', '[', ']', ';', ',', '~', '|'. Why?",apurtell,apurtell,Minor,Closed,Fixed,31/Mar/10 21:34,12/Oct/12 06:15
Bug,HBASE-2398,12460866,NPE in HLog.append when calling writer.getLength,"Doing some test uploads on a fresh single node setup, I'm seeing non-fatal NPEs in the region server log coming from HLog.append when calling writer.getLength. I'll post a log in a follow-up comment.

Assigning to Nicolas so that he takes a look.",kannanm,jdcryans,Major,Closed,Fixed,31/Mar/10 22:12,20/Nov/15 12:40
Bug,HBASE-2399,12460959,Forced splits only act on the first family in a table,"While working on a patch for HBASE-2375, I came across a few bugs in the existing code related to splits.

If a user triggers a manual split, it flips a forceSplit boolean to true and then triggers a compaction (this is very similar to my current implementation for HBASE-2375).  However, the forceSplit boolean is flipped back to false at the beginning of Store.compact().  So the force split only acts on the first family in the table.  If that Store is not splittable for some reason (it is empty or has only one row), then the entire region will not be split, regardless of what is in other families.

Even if there is data in the first family, the midKey is determined based solely on that family.  If it has two rows and the next family has 1M rows, we pick the split key based on the two rows.",mingma,streamy,Critical,Closed,Fixed,01/Apr/10 16:47,20/Nov/15 12:40
Bug,HBASE-2402,12460980,[stargate] set maxVersions on gets,"Support setting maxVersions on get. Append query parameter ""v"" to the end of resource URIs on GETs, e.g. ""?v=3"" for max 3 versions.",apurtell,apurtell,Minor,Closed,Fixed,01/Apr/10 23:33,12/Oct/12 06:14
Bug,HBASE-2405,12461053,"Close, split, open of regions in RegionServer are run by a single thread only.","JGray and Karthik observed yesterday that a regoin open message arrived at the regionserver but that the regionserver worker thread did not get around to the actually opening until 45 seconds later (region offline for 45 seconds).  We only run a single Worker thread in a regoinserver processing open, close, and splits.  In this case, a long running close (or two) held up the worker thread.  We need to run more than a single worker.  A pool of workers?  Should opens be prioritized?",stack,stack,Critical,Closed,Fixed,02/Apr/10 18:47,20/Nov/15 12:42
Bug,HBASE-2410,12461182,spurious warnings from util.Sleeper,"Many of this in the logs

{noformat}
WARN util.Sleeper: We slept 59993ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://wiki.apache.org/hadoop/Hbase/Troubleshooting#A9
{noformat}

except this is the MetaScanner and the sleeper has a period of 60000ms. I have one of 30000ms in Stargate which is doing the same thing now.

I see the intent but these spurious warnings defeat the intent here. Marked as blocker as this will cause no end of users writing in on the list. Should be a test of the actual sleep time against the configured interval, not a constant.",jdcryans,apurtell,Blocker,Closed,Fixed,05/Apr/10 22:09,20/Nov/15 12:43
Bug,HBASE-2413,12461310,"Master does not respect generation stamps, may result in meta getting permanently offlined","This happens if the RS is restarted before the zk node expires. The sequence is as follows:

1. RS1 dies - lets say its server string was HOST1:PORT1:TS1
2. In a few seconds RS1 is restarted, it comes up as HOST1:PORT1:TS2 (TS2 is more recent than TS1)
3. Master gets a start up message from RS1 with the server name as HOST1:PORT1:TS2
4. Master adds this as a new RS, tries to red
---- The master does not use the generation stamps to detect that RS1 has already restarted.
---- Also, if RS1 contained meta, master would try to go to HOST1:PORT1:TS1. It would end up talking to HOST1:PORT1:TS2, which spews a bunch of not serving region exceptions.
5. zk node expires for HOST1:PORT1:TS1
6. Master tries to process shutdown for HOST1:PORT1:TS1 - this probably interferes with HOST1:PORT1:TS2  and ends up somehow removing the reassign meta in the master's queue.
---- Meta never comes online and master continues logging the following exception indefinitely:

2010-04-06 11:02:23,988 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of test1,7094000000,1270220428234, false, reassign: true
2010-04-06 11:02:23,988 DEBUG org.apache.hadoop.hbase.master.ProcessRegionClose$1: Exception in RetryableMetaOperation:
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:64)
        at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:63)
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:494)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:429)

",stack,karthik.ranga,Major,Closed,Fixed,06/Apr/10 22:20,20/Nov/15 12:41
Bug,HBASE-2414,12461334,Enhance test suite to be able to specify distributed scenarios,"We keep finding good cases that are reasonably hard to test, yet the test suite does not encode these. 
For example: 
HBASE-2413 Master does not respect generation stamps, may result in meta getting permanently offlined
HBASE-2312 Possible data loss when RS goes into GC pause while rolling HLog

I am sure there are many more such ""scenarios"" we should put into the unit tests. 

",stack,karthik.ranga,Blocker,Closed,Fixed,07/Apr/10 00:44,20/Nov/15 12:42
Bug,HBASE-2417,12461437,"HCM.locateRootRegion fails hard on ""Connection refused""","While running some tests on replication, I saw that our client does something dumb if it tries to contact a dead region server that held the ROOT region in HCM.locateRootRegion. Will post stack trace in a comment.

The problem here is that we don't retry at all, the exception will come straight out of HCM like it's the end of the world.",jdcryans,jdcryans,Major,Closed,Fixed,07/Apr/10 17:58,12/Oct/12 06:15
Bug,HBASE-2421,12461453,Put hangs for 10 retries on failed region servers,"Since MultiPut got in, instead of calling getRegionLocationForRowWithRetries we now call getRegionServerWithRetries to send an array list of Puts. The problem is that if the region server failed, we'll still retry the 10 times in a backoff fashion even tho we get connections refused. This is also true for a single put since it's the same code path.

Marking as critical since it almost disables our responsiveness to machine failures in certain cases where we are already sending a batch of edits when the server fails. Assigning to Ryan since he's been there recently.",ryanobjc,jdcryans,Critical,Closed,Fixed,07/Apr/10 20:10,20/Nov/15 12:40
Bug,HBASE-2422,12461473,Remove fragmentation indicator for 0.20.4... fix in 0.20.5.,HBASE-2165 is about working on fragmentation indicator to make it less intrusive.  Currently it can get in way of displaying UI on big cluster.,jdcryans,stack,Major,Closed,Fixed,07/Apr/10 23:41,12/Oct/12 06:15
Bug,HBASE-2428,12461564,NPE in ProcessRegionClose because meta is offline kills master and thus the cluster,"This issue was born of study done in hbase-2413.  The meta went offline and we were processing a region close at the same time.  The close processing fell into a NPE loop and wouldn't get out of it killing master and effectivly killing the cluster:

{code}
2010-03-31 17:50:57,004 INFO org.apache.hadoop.hbase.master.ServerManager: hbasetest020.X.X.X,60020,1270077892989 znode expired
2010-03-31 17:50:57,004 INFO org.apache.hadoop.hbase.master.RegionManager: META region removed from onlineMetaRegions
2010-03-31 17:51:15,385 INFO org.apache.hadoop.hbase.master.ServerManager: Received start message from: hbasetest020.X.X.X,60020,1270083075377
2010-03-31 17:51:15,399 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Updated ZNode /hbase/rs/1270083075377 with data 10.18.35.215:60020
2010-03-31 17:51:15,870 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server is overloaded: load=5, avg=3.0, slop=0.3
2010-03-31 17:51:15,870 DEBUG org.apache.hadoop.hbase.master.RegionManager: Choosing to reassign 2 regions. mostLoadedRegions has 5 regions in it.
2010-03-31 17:51:15,870 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region test1,3147000000,1270081876965
2010-03-31 17:51:15,870 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region test1,9352000000,1270080893514
2010-03-31 17:51:15,870 INFO org.apache.hadoop.hbase.master.RegionManager: Skipped 0 region(s) that are in transition states
2010-03-31 17:51:15,878 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: test1,3147000000,1270081876965 from hbasetest019.X.X.X,60020,1270082983630; 1 of 2
2010-03-31 17:51:15,879 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: test1,9352000000,1270080893514 from hbasetest019.X.X.X,60020,1270082983630; 2 of 2
2010-03-31 17:51:44,897 DEBUG org.apache.hadoop.hbase.master.ProcessRegionClose$1: Trying to contact region server for regionName '.META.,,1', but failed after 10 attempts.
Exception 1:
java.io.IOException: Call to /10.18.35.215:60020 failed on local exception: java.io.EOFExceptionException 1:
java.net.ConnectException: Connection refusedException 1:
java.net.ConnectException: Connection refusedException 1:
java.net.ConnectException: Connection refusedException 1:
java.net.ConnectException: Connection refusedException 1:
java.net.ConnectException: Connection refusedException 1:
java.net.ConnectException: Connection refusedException 1:
org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: .META.,,1
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2282)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.delete(HRegionServer.java:1989)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Exception 1:
org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: .META.,,1
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2282)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.delete(HRegionServer.java:1989)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: .META.,,1
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2282)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.delete(HRegionServer.java:1989)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:577)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:94)
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:74)
        at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:63)
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:494)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:429)
2010-03-31 17:51:44,899 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of test1,1230000000,1270081673808, false, reassign: true
2010-03-31 17:51:44,899 DEBUG org.apache.hadoop.hbase.master.ProcessRegionClose$1: Exception in RetryableMetaOperation:
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:64)
        at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:63)
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:494)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:429)
2010-03-31 17:51:44,900 WARN org.apache.hadoop.hbase.master.HMaster: Processing pending operations: ProcessRegionClose of test1,1230000000,1270081673808, false, reassign: true
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:96)
        at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:63)
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:494)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:429)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:64)
        ... 3 more
{code}

... and so on.

Marking a blocker for 0.20.5.
{code}",stack,stack,Blocker,Closed,Fixed,08/Apr/10 18:37,12/Oct/12 06:15
Bug,HBASE-2431,12461608,ZK settings for initLimit/syncLimit shouldn't have been removed from hbase-default.xml,"The initLimit/syncLimit ZK settings were removed from hbase-default.xml as part of HBASE-2392 (upgrade to ZK 3.3). 

But these settings are needed. Without them, if you try to start a ZK quorum of more than 1 server, you'll get the following error:

{code}
java.lang.IllegalArgumentException: initLimit is not set
at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parseProperties(QuorumPeerConfig.java:246)
at org.apache.hadoop.hbase.zookeeper.HQuorumPeer.main(HQuorumPeer.java:76)
{code}",apurtell,kannanm,Major,Closed,Fixed,09/Apr/10 00:36,20/Nov/15 12:41
Bug,HBASE-2437,12461944,Refactor HLog splitLog,"the HLog.splitLog got really long and complex and hard to verify for correctness. 
I started to refactor it and also ported changes from hbase-2337 that deals with premature deletion of log files in case of errors. Further improvements will be possible, however the scope of this issue is to clean the code and make it behave correctly (i.e. not lose any edits)  

Added a suite of unit tests that might be ported to 0.20 as well.

Added a setting (hbase.skip.errors - feel free to suggest a better name) that, when set to false will make the process less tolerant to failures or corrupted files:  in case a log file is corrupted or an error stops the process from consistently splitting the log, will abort the entire operation to avoid losing any edits. When hbase.skip.errors is on any corrupted files will be partially parsed and then moved to the corrupted logs archive (see hbase-2337). 

Like hbase-2337 the splitLog method will first split all the logs and then proceed to archive them. If any splitted log file (oldlogfile.log) that is the result of an earlier splitLog attempt is found in the region directory, it will be deleted - this is safe since we won't move the original log files until the splitLog process completes.",clehene,clehene,Major,Closed,Fixed,13/Apr/10 12:06,20/Nov/15 12:43
Bug,HBASE-2439,12461995,HBase can get stuck if updates to META are blocked,"(We noticed this on a import-style test in a small test cluster.)

If compactions are running slow, and we are doing a lot of region splits, then, since META has a much smaller hard-coded memstore flush size (16KB), it quickly accumulates lots of store files. Once this exceeds ""hbase.hstore.blockingStoreFiles"", flushes to META become no-ops. This causes METAs memstore footprint to grow. Once this exceeds ""hbase.hregion.memstore.block.multiplier * 16KB"", we block further updates to META.

In my test setup:
  hbase.hregion.memstore.block.multiplier = 4.
and,
  hbase.hstore.blockingStoreFiles = 15.

And we saw messages of the form:

{code}
2010-04-09 18:37:39,539 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 23 on 60020' on region .META.,,1: memstore size 64.2k is >= than blocking 64.0k size
{code}

Now, if around the same time the CompactSplitThread does a compaction and determines it is going split the region. As part of finishing the split, it wants to update META about the daughter regions. 

It'll end up waiting for the META to become unblocked. The single CompactSplitThread is now held up, and no further compactions can proceed.  META's compaction request is itself blocked because the compaction queue will never get cleared.

This essentially creates a deadlock and the region server is able to not progress any further. Eventually, each region server's CompactSplitThread ends up in the same state.

",kannanm,kannanm,Major,Closed,Fixed,13/Apr/10 21:55,20/Nov/15 12:40
Bug,HBASE-2442,12462006,Log lease recovery catches IOException too widely,"In HLog.recoverLog, all IOExceptions are caught. This code should only spin for the case of AlreadyBeingCreatedException, but currently will spin on any IOException, even though others may well be permanent state (eg the log directory has been chowned to someone else)",tlipcon,tlipcon,Major,Closed,Fixed,14/Apr/10 01:41,20/Nov/15 12:40
Bug,HBASE-2443,12462007,IPC client can throw NPE if socket creation fails,"If the socket factory fails to create a socket (eg Too many open files), then HBaseClient.handleConnectionFailure will throw NPE instead of an IOE subclass",tlipcon,tlipcon,Major,Closed,Fixed,14/Apr/10 01:51,20/Nov/15 12:42
Bug,HBASE-2447,12462072,LogSyncer.addToSyncQueue doesn't check if syncer is still running before waiting,"In testing GC pause scenarios with kill -STOP, I got the regionserver into a situation where it was blocked forever while shutting down (also blocking clients, since the RPCs were still pinging). The root issue is that, if the log syncer has an error just as more edits are being done, addToSyncQueue() can go to sleep waiting on a syncer which has just died.",tlipcon,tlipcon,Critical,Closed,Fixed,14/Apr/10 19:17,20/Nov/15 12:43
Bug,HBASE-2448,12462075,Scanner threads are interrupted without acquiring lock properly,"There are a few places where scanner threads are interrupted with .interrupt() instead of .interruptIfAlive(). This means that if they're in the midst of the checkFileSystem operation, it'll end up catching the interruption there, determine that the filesystem is down, and shut down the whole server. Other nasties can also result.",tlipcon,tlipcon,Critical,Closed,Fixed,14/Apr/10 19:24,20/Nov/15 12:41
Bug,HBASE-2449,12462091,Local HBase does not stop properly,"Just tried running a local hbase instance after a 

{code}
mvn -D skipTests=true package assembly:assembly
{code}

and then unpacking the SNAPSHOT tar ball in $HBASE_HOME/target

When trying to stop it with stop-hbase.sh it would not stop but print dots continuously. 

Thread dump shows:

{noformat}
Full thread dump Java HotSpot(TM) 64-Bit Server VM (14.3-b01-101 mixed mode):

""Attach Listener"" daemon prio=9 tid=0x0000000102b52800 nid=0x14eb13000 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""481404130@qtp0-0-EventThread"" daemon prio=5 tid=0x0000000102cf1800 nid=0x152dac000 waiting on condition [0x0000000152dab000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000001075778d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:477)

""481404130@qtp0-0-SendThread(localhost:2181)"" daemon prio=5 tid=0x0000000102914800 nid=0x152a56000 runnable [0x0000000152a55000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
	- locked <0x0000000107577b58> (a sun.nio.ch.Util$1)
	- locked <0x0000000107577b70> (a java.util.Collections$UnmodifiableSet)
	- locked <0x0000000107577ae0> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1066)

""LruBlockCache.EvictionThread"" daemon prio=5 tid=0x0000000101eab800 nid=0x155452000 in Object.wait() [0x0000000155451000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000001094034c0> (a org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread.run(LruBlockCache.java:509)
	- locked <0x00000001094034c0> (a org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread)

""DestroyJavaVM"" prio=5 tid=0x0000000102800800 nid=0x100501000 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""ProcessThread:-1"" prio=5 tid=0x0000000102a5f800 nid=0x14f248000 waiting on condition [0x000000014f247000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000001091d2e40> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
	at org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:103)

""SyncThread:0"" prio=5 tid=0x0000000102a0e000 nid=0x14f145000 waiting on condition [0x000000014f144000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000001091d2d50> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
	at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:94)

""SessionTracker"" prio=5 tid=0x00000001029be000 nid=0x14f042000 in Object.wait() [0x000000014f041000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000001091e42f8> (a org.apache.zookeeper.server.SessionTrackerImpl)
	at org.apache.zookeeper.server.SessionTrackerImpl.run(SessionTrackerImpl.java:142)
	- locked <0x00000001091e42f8> (a org.apache.zookeeper.server.SessionTrackerImpl)

""NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181"" daemon prio=5 tid=0x000000010297f800 nid=0x14ef3f000 runnable [0x000000014ef3e000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
	at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
	at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
	- locked <0x00000001091c1240> (a sun.nio.ch.Util$1)
	- locked <0x00000001091c1228> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000001091ea248> (a sun.nio.ch.KQueueSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
	at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:232)

""Low Memory Detector"" daemon prio=5 tid=0x0000000101878000 nid=0x14e90d000 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""CompilerThread1"" daemon prio=9 tid=0x0000000102840800 nid=0x14e80a000 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""CompilerThread0"" daemon prio=9 tid=0x000000010283f800 nid=0x14e707000 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" daemon prio=9 tid=0x000000010283f000 nid=0x14e604000 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Surrogate Locker Thread (CMS)"" daemon prio=5 tid=0x000000010283e000 nid=0x14e501000 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" daemon prio=8 tid=0x000000010282e800 nid=0x14e157000 in Object.wait() [0x000000014e156000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000001091efa30> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
	- locked <0x00000001091efa30> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

""Reference Handler"" daemon prio=10 tid=0x000000010282d800 nid=0x14e054000 in Object.wait() [0x000000014e053000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000001091ef988> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:485)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
	- locked <0x00000001091ef988> (a java.lang.ref.Reference$Lock)

""VM Thread"" prio=9 tid=0x0000000102826800 nid=0x14df51000 runnable 

""Gang worker#0 (Parallel GC Threads)"" prio=9 tid=0x0000000102802000 nid=0x106205000 runnable 

""Gang worker#1 (Parallel GC Threads)"" prio=9 tid=0x0000000102802800 nid=0x106308000 runnable 

""Concurrent Mark-Sweep GC Thread"" prio=9 tid=0x0000000101800800 nid=0x14dc5a000 runnable 
""VM Periodic Task Thread"" prio=10 tid=0x0000000101881000 nid=0x14ea10000 waiting on condition 

""Exception Catcher Thread"" prio=10 tid=0x0000000102801800 nid=0x103101000 runnable 
JNI global references: 1175
{noformat}
",,larsgeorge,Major,Closed,Fixed,14/Apr/10 22:47,20/Nov/15 12:41
Bug,HBASE-2451,12462101,.META. by-passes cache; BLOCKCACHE=>'false',"In a new install, if I describe '.META.', it says:

{code}
DESCRIPTION                                                             ENABLED                               
 {NAME => '.META.', IS_META => 'true', MEMSTORE_FLUSHSIZE => '16384', F true                                  
 AMILIES => [{NAME => 'historian', COMPRESSION => 'NONE', VERSIONS => '                                       
 2147483647', TTL => '604800', BLOCKSIZE => '8192', IN_MEMORY => 'false                                       
 ', BLOCKCACHE => 'false'}, {NAME => 'info', COMPRESSION => 'NONE', VER                                       
 SIONS => '10', TTL => '2147483647', BLOCKSIZE => '8192', IN_MEMORY =>                                        
 'false', BLOCKCACHE => 'true'}]} 
{code}

BLOCKCACHE is 'true' for the 'info' family (Yes historian is still in 0.20 branch).

But, if I add logging to hfile and storefile and store, blockcache is 'false' -- there is no cache constructed for use by the hfile.

This is killing cluster performance.  

It looks like a problem parsing the 'true' value in columnfamily.  I'll put up a patch in the morning.  Meantime, marking as blocker on 0.20.4.",stack,stack,Blocker,Closed,Fixed,15/Apr/10 04:49,20/Nov/15 12:43
Bug,HBASE-2455,12462203,"""Cached an already cached block"" RTE in RS after split","It seems there's a race right after splits where this RTE is thrown. See logs here: http://pastebin.com/TDFpid5r
(this seems to have been introduced by HBASE-2248)",ryanobjc,tlipcon,Major,Closed,Fixed,16/Apr/10 01:52,12/Oct/12 06:15
Bug,HBASE-2456,12462208,deleteChangedReaderObserver spitting warnings after HBASE-2248,"I'm seeing very occasional ""Not in set"" warnings on region servers under heavy concurrent read/write test after HBASE-2248. Here's a log:

http://pastebin.com/1Vm9C7Uf",ryanobjc,tlipcon,Major,Closed,Fixed,16/Apr/10 03:24,20/Nov/15 12:44
Bug,HBASE-2457,12462210,RS gets stuck compacting region ad infinitum,"Testing 0.20_pre_durability@934643, I ended up in a state where one region server got stuck compacting a single region over and over again forever. This was with a special config with very low flush threshold in order to stress test flush/compact code.",,tlipcon,Critical,Closed,Fixed,16/Apr/10 05:04,20/Nov/15 12:42
Bug,HBASE-2458,12462248,"Client stuck in TreeMap,remove",Testing 0.20_pre_durability@934691 my client got permanently stuck with one thread looping inside TreeMap.remove. See attached stack.,tlipcon,tlipcon,Blocker,Closed,Fixed,16/Apr/10 13:48,20/Nov/15 12:44
Bug,HBASE-2460,12462274,add_table.rb deletes any tables for which the target table name is a prefix,"If you have a table foobar and a table foo, add_table.rb on table foo will delete all of the META entries for foobar. We're missing the ',' in the scan.",tlipcon,tlipcon,Critical,Closed,Fixed,16/Apr/10 18:29,20/Nov/15 12:42
Bug,HBASE-2461,12462302,Split doesn't handle IOExceptions when creating new region reference files,"I was testing an HDFS patch which had a bug in it, so it happened to throw an NPE during a split with the following trace:

2010-04-16 19:18:20,727 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region TestTable,-1945465867<1271449232310>,1271453785648
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.enqueueCurrentPacket(DFSClient.java:3124)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(DFSClient.java:3220)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3306)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3255)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
        at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:560)
        at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:95)
        at org.apache.hadoop.hbase.io.Reference.write(Reference.java:129)
        at org.apache.hadoop.hbase.regionserver.StoreFile.split(StoreFile.java:498)
        at org.apache.hadoop.hbase.regionserver.HRegion.splitRegion(HRegion.java:682)
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread.split(CompactSplitThread.java:162)
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:95)

After that, my region was gone, any further writes to it would fail.",stack,tlipcon,Blocker,Closed,Fixed,17/Apr/10 02:39,20/Nov/15 12:43
Bug,HBASE-2463,12462306,Various Bytes.* functions silently ignore invalid arguments,"Many functions in {{hbase.util.Bytes}} silently ignore invalid arguments.
For instance, {{Bytes.toInt(null)}} deliberately returns {{-1}}.  There are tons of cases like that.
* All functions that are given a null pointer should throw an NPE.
* All functions that are given otherwise invalid arguments should throw an {{IllegalArgumentException}}.

Anyone relying on ""special return values"" such as {{-1}} for {{Bytes.toInt(null)}} is guilty of writing broken code ({{-1}} is a valid return value for {{toInt}}!).",tsuna,tsuna,Major,Closed,Fixed,17/Apr/10 08:55,20/Nov/15 12:42
Bug,HBASE-2471,12462564,"Splitting logs, we'll make an output file though the region no longer exists","The ""human unit tester"" (Kannan) last night wondered what happens splitting logs and we come across an edit whose region has since been removed.  Taking a look, it looks like we'll create the output file and write the edits for the no-longer-extant region anyways.  This will leave litter in the filesystem -- region split files that will never be used nor removed.  This issue is about verifying that indeed this is whats happening (We do SequenceFile.createWriter with the overwrite flag set to true which tracing seems to mean create all intermediary directories -- to be verified) and if it indeed is happening, fixing split so unless the region dir exists, don't write out edits.. just drop them.",jdcryans,stack,Major,Closed,Fixed,20/Apr/10 17:56,20/Nov/15 12:41
Bug,HBASE-2474,12462576,Bug in 2248 - mixed version reads (not allowed by spec),"While doing a concurrent read/write test, the reader eventually gets a situation where the first column in the result set has the wrong 'value' than the rest of the result set (of 50 columns or so).  The test (included) does puts of 50 columns with all the same (Random) value. The reader validates that all values are equal, and fails.",,ryanobjc,Blocker,Closed,Fixed,20/Apr/10 20:42,20/Nov/15 12:42
Bug,HBASE-2475,12462586,[stargate] Required ordering of JSON name/value pairs when performing Insert/Update,"From Tyler Coffin up on hbase-user@

{quote}
I am using the Stargate REST interface to HBase for inserting data. When using JSON to transmit the query content, I have found that specific ordering of key/value pairs within the JSON string is required in order for the query to succeed (otherwise a response of 'HTTP/1.1 500 Row key is invalid' error is thrown if ""key"" and ""Cell"" are reversed).

Example:
This string receives the above error:

{noformat}
{""Row"":[{""Cell"":[{""column"":""bWVzc2FnZTptc2c="",""$"":""Zm9vYmFy""}],""key"":""MTIzNAo=""}]}
{noformat}

This is the valid equivalent string:

{noformat}
{""Row"":[{""key"":""MTIzNAo="",""Cell"":[{""column"":""bWVzc2FnZTptc2c="",""$"":""Zm9vYmFy""}]}]}
{noformat}

As you can see the only difference between these two instances is that the ""key"" and ""Cell"" name/value pairs have their order reversed.

In the equivalent XML notation, the ordering is specifically required per the schema. However with JSON Objects (i.e. name/value pairs) order is not required (JSON Arrays are ordered, but not Objects). Some JSON libraries will preserve ordering of Objects but not all  which is how I discovered this problem in the first place because I was using the Perl JSON library which does not guarantee order). 
I'm unsure if this is a bug in the REST implementation or an inconvenient ambiguity in the JSON specification. Regardless I thought I'd share this discovery with the community for feedback (or at the very least to document this for users' future reference).

For reference this is the table schema for the above query:

{noformat}
{NAME => 'reftrack', FAMILIES => [{NAME => 'message', COMPRESSION =>
'NONE', VERSIONS => '1', TTL => '2147483647', BLOCKSIZE => '65536',
IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}
{noformat}
{quote}",,apurtell,Major,Closed,Fixed,20/Apr/10 22:08,11/Jun/22 23:32
Bug,HBASE-2476,12462605,HLog sequence number is obtained outside updateLock,"In one of the overloads of HLog.append, obtainSeqNum() is called before this.updateLock is acquired. This means that it's possible for some other thread to have a higher sequence number but achieve its write first, and hence the lastSeqWritten map can become incorrect.",tlipcon,tlipcon,Critical,Closed,Fixed,21/Apr/10 02:36,20/Nov/15 12:43
Bug,HBASE-2481,12462834,Client is not getting UnknownScannerExceptions; they are being eaten,"This was reported by mudphone on IRC and confirmed by myself in quick test.  If the client takes too long going back to the RS, the RS will throw an UnknownScannerException but it doesn't get back to the client.  Instead, the client scan silently ends.  Marking this blocker.  Its actually in 0.20.4.  Thats what I was testing.  Mayhaps an RC sinker?",jdcryans,stack,Blocker,Closed,Fixed,23/Apr/10 00:26,22/Jul/16 06:46
Bug,HBASE-2482,12462835,regions in transition do not get reassigned by master when RS crashes,"Very similar to HBASE-1928, but for the general case (not just ROOT/META):

If a region is in transition on a RS when the RS crashes, the master does not remove it from regionsInTransition when processing the RS shutdown. This is fairly easy to trigger by bringing up a RS and kill -9ing it just as it starts to get regions assigned. Those regions will get permanently lost since they're stuck in regionsInTransition and thus don't get assigned by the metascanner.",tlipcon,tlipcon,Blocker,Closed,Fixed,23/Apr/10 00:38,12/Oct/12 06:15
Bug,HBASE-2483,12462841,Some tests do not use ephemeral ports,"For example, seems like most of the tests bind the master to port 60000. This doesn't work on shared build machines where multiple hbase test targets might run concurrently.",streamy,tlipcon,Blocker,Closed,Fixed,23/Apr/10 05:12,20/Nov/15 12:43
Bug,HBASE-2485,12462917,"Persist Master in-memory state so on restart or failover, new instance can pick up where the old left off","Today there was some good stuff up on IRC on how transitions won't always make it across Master failovers in multi-master deploy because transitions are kept in in-memory structure up in the Master and so on master crash, the new master will be missing state  on startup (Todd was main promulgator of this observation and of the opinion that while  master rewrite is scheduled for 0.21, some part needs to be done for 0.20.5).  A few suggestions were made: transitions should be file-backed somehow, etc.  Let this issue be about the subset we want to do for 0.20.5.

Of the in-memory state queues, there is at least the master tasks queue -- process region opens, closes, regionserver crashes, etc. -- where tasks must be done in order and IIRC, tasks are fairly idempotent (at least in the server crash case, its multi-step and we'll put the crash event back on the queue if we cannot do all steps in the one go).  Perhaps this queue could be done using the new queue facility in zk 3.3.0 (I haven't looked to check if possible, just suggesting).  Another suggestion was a file to which we'd append queue items, requeueing, and marking the file with task complete, etc.  On Master restart or fail-over, we'd replay the queue log.

There is also the Map of regions-in-transition.  Yesterday we learned that there is a bug where server shutdown processing does not iterate the Map of regions-in-transition.  This Map may hold regions that are in ""opening"" or ""opened"" state but haven't yet had the fact added to .META. by master.  Meantime the hosting server can crash.  Regions that were opening will stay in the regions-in-transition and those in opened-but-not-yet-added-to-meta will go ahead and add a crashed server to .META. (Currently regions-in-transition does not record server the region opening/open is happening on so it doesn't have enough info to be processed as part of server shutdown).

Regions-in-transition also needs to be persistant.  On startup, regions-in-transition can get kinda hectic on a big cluster.  Ordering is not so important here I believe.  A directory in zk might work (For 1M regions in a big cluster, that'd be about 2M creates and 2M deletes during startup -- thats too much?).  Or we could write a WAL-like log again of region  transitions (We'd have to develop a little vocabulary) that got reread by a new master.


",streamy,stack,Major,Closed,Fixed,24/Apr/10 04:03,20/Nov/15 12:42
Bug,HBASE-2487,12462983,Uncaught exceptions in receiving IPC responses orphan clients,"This is HADOOP-6723, see that issue for details.",tlipcon,tlipcon,Critical,Closed,Fixed,26/Apr/10 05:48,20/Nov/15 12:40
Bug,HBASE-2491,12462998,master.jsp uses absolute links to table.jsp. This broke when master.jsp moved under webapps/master,"In the latest trunks / 0.21 branch, the master's web UI sits under /webapps/master. All links to the specific tables fail to work, because the master.jsp uses absolute links for them - e.g. href=""/table.jsp"".
Although the fact that the master pages shouldn't be under /webapps/master ( see HBASE-2369), these links should still be relative to the master page - href=""table.jsp"", considering that they are not called from anywhere else.",,civascu,Minor,Closed,Fixed,26/Apr/10 08:24,20/Nov/15 12:43
Bug,HBASE-2494,12463084,Does not apply new.name parameter to CopyTable.,Checking new.name parameter is wrong.  Change 'rsClassArgKey' variable to 'newNameArgKey'.,,ohyoonsik,Minor,Closed,Fixed,27/Apr/10 08:56,20/Nov/15 12:42
Bug,HBASE-2497,12463207,ProcessServerShutdown throws NullPointerException for offline regions,"When a regionsserver dies the master can run into the following bug.

2010-04-27 17:20:37,303 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessServerShutdown of dell106.cluster,60020,1272377612991
2010-04-27 17:20:37,303 INFO org.apache.hadoop.hbase.master.RegionServerOperation: process shutdown of server dell106.cluster,60020,1272377612991: logSplit: true, rootRescanned: true, numberOfMetaRegions: 1, onlineMetaRegions.size(): 1
2010-04-27 17:20:01,637 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Log split complete, meta reassignment and scanning:
2010-04-27 17:20:01,653 DEBUG org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanRootRegion: process server shutdown scanning root region on 10.1.3.124
2010-04-27 17:20:01,664 DEBUG org.apache.hadoop.hbase.master.RegionServerOperation: process server shutdown scanning root region on 10.1.3.124 finished master
2010-04-27 17:20:01,683 DEBUG org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanMetaRegions: process server shutdown scanning .META.,,1 on 10.1.3.104:60020
2010-04-27 17:20:18,087 DEBUG org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanMetaRegions: Exception in RetryableMetaOperation:
2010-04-27 17:20:18,118 WARN org.apache.hadoop.hbase.master.HMaster: Adding to delayed queue: ProcessServerShutdown of dell106.cluster,60020,1272377612991
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:100)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown.process(ProcessServerShutdown.java:345)
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:509)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:448)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:487)
        at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:461)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown.scanMetaRegion(ProcessServerShutdown.java:147)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanMetaRegions.call(ProcessServerShutdown.java:264)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown$ScanMetaRegions.call(ProcessServerShutdown.java:250)
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:69)
        ... 3 more


The problem is in ProcessServerShutdown.java at line 148-149:

146	        String serverAddress = 
147	          Bytes.toString(values.getValue(CATALOG_FAMILY, SERVER_QUALIFIER));
148	        long startCode =
149	          Bytes.toLong(values.getValue(CATALOG_FAMILY, STARTCODE_QUALIFIER));
150	        String serverName = null;
151	        if (serverAddress != null && serverAddress.length() > 0) {
152	          serverName = HServerInfo.getServerName(serverAddress, startCode);
153	        }

It should be modified to:

146	        String serverAddress = 
147	          Bytes.toString(values.getValue(CATALOG_FAMILY, SERVER_QUALIFIER));
150	        String serverName = null;
151	        if (serverAddress != null && serverAddress.length() > 0) {
148	          long startCode =
149	            Bytes.toLong(values.getValue(CATALOG_FAMILY, STARTCODE_QUALIFIER));
152	          serverName = HServerInfo.getServerName(serverAddress, startCode);
153	        }

As Bytes.toLong cannot handle the null pointer returned by getValue for missing  STARTCODE_QUALIFIER of offline regions in META.",mkurucz,mkurucz,Major,Closed,Fixed,28/Apr/10 13:16,20/Nov/15 12:43
Bug,HBASE-2499,12463257,Race condition when disabling a table leaves regions in transition,"A lot of people reported that weren't able to add/delete a column because only some of the regions got the modification. I personally thought it was due to the CME bug in the Master, but I'm able to easily reproduce on 0.20.4 on a 1800 regions table.

Since 0.20.3, we now call disableTable after every retry to make sure we don't miss any region. This creates a race where while we scan .META. in TableOperation, a region could be reported as closed after we scanned the row. We end up processing it like if it was assigned and we put it back into regionsInTransition. We need to either query .META. before processing each region or make some more check to see if the region was closed.

This kills the RC in my book.

In the mean time, anyone getting this can restart their HBase and it will pick up the change.",jdcryans,jdcryans,Blocker,Closed,Fixed,28/Apr/10 20:45,12/Oct/12 06:14
Bug,HBASE-2503,12463371,"PriorityQueue isn't thread safe, KeyValueHeap uses it that way","In the same spirit as HBASE-2077, but a bit different (at least to me). Dave Latham had the following NPE killing a RS:

{code}
Exception in thread ""regionserver/192.168.41.2:60020.leaseChecker"" java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:127)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:117)
        at java.util.PriorityQueue.siftDownUsingComparator(PriorityQueue.java:644)
        at java.util.PriorityQueue.siftDown(PriorityQueue.java:612)
        at java.util.PriorityQueue.poll(PriorityQueue.java:523)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.close(KeyValueHeap.java:151)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.close(HRegion.java:1862)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$ScannerListener.leaseExpired(HRegionServer.java:1959)
        at org.apache.hadoop.hbase.Leases.run(Leases.java:98)
{code}

He also has the same stack traces from 2077. The PQ javadoc says this class is not thread safe, but it is used by the leaseChecker thread and the client threads. We need to use something like the BlockingPriorityQueue instead.",jdcryans,jdcryans,Critical,Closed,Fixed,29/Apr/10 20:52,12/Oct/12 06:15
Bug,HBASE-2509,12463620,"NPEs in various places, HRegion.get, HRS.close","ttr on irc reported that he was unable to get/scan sometimes, was getting NPEs.

The root cause is a delayed init of the RegionScanner.storeHeap means it can be null, not all accessors of it (specifically in close()) checked for that.",ryanobjc,ryanobjc,Major,Closed,Fixed,04/May/10 00:10,12/Oct/12 06:14
Bug,HBASE-2511,12463713,"Apply HBASE-2509 (""NPEs in various places, HRegion.get, HRS.close"") to trunk after hbase-2248 goes in",Passing to Ryan.,ryanobjc,stack,Major,Closed,Fixed,04/May/10 22:20,20/Nov/15 12:42
Bug,HBASE-2513,12463720,hbase-2414 added bug where we'd tight-loop if no root available.,,tsuna,stack,Major,Closed,Fixed,04/May/10 23:38,20/Nov/15 12:41
Bug,HBASE-2514,12463722,RegionServer should refuse to be assigned a region that use LZO when LZO isn't available,"If a RegionServer is assigned a region that uses LZO but the required libraries aren't installed on that RegionServer, the server will fail unexpectedly after throwing a {{java.lang.ClassNotFoundException: com.hadoop.compression.lzo.LzoCodec}}

{code}

2010-05-04 16:57:27,258 FATAL org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Replay of hlog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: tsdb,,1273011287339
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:994)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:887)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:255)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:142)
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: com.hadoop.compression.lzo.LzoCodec
        at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm$1.getCodec(Compression.java:91)
        at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.getCompressor(Compression.java:196)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.getCompressingStream(HFile.java:388)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.newBlock(HFile.java:374)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.checkBlockBoundary(HFile.java:345)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:517)
        at org.apache.hadoop.hbase.io.hfile.HFile$Writer.append(HFile.java:482)
        at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:558)
        at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:522)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:979)
        ... 3 more
Caused by: java.lang.ClassNotFoundException: com.hadoop.compression.lzo.LzoCodec
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:315)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:330)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:250)
        at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm$1.getCodec(Compression.java:87)
        ... 12 more
{code}",ryanobjc,tsuna,Critical,Closed,Fixed,05/May/10 00:06,20/Nov/15 12:43
Bug,HBASE-2515,12463725,ChangeTableState considers split&&offline regions as being served,"A region is considered as being serverd and unserved at the same time in the ChangeTableState (and TableOperation) class. This translates to logs like this:

{code}
2010-05-04 17:26:01,073 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT_INCLUDES_DAUGHTERS: TestTable,0000518811,1273019008135: Daughters; 
TestTable,0000518811,1273019159867, TestTable,0000584541,1273019159867 from 10.10.1.177,60020,1273018776034; 1 of 1
<<disable is called>>
2010-05-04 17:26:25,893 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Skipping region REGION => {NAME => 'TestTable,0000518811,1273019008135', STARTKEY => '0000518811', ENDKEY => '0000650817', 
ENCODED => 143183187, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'TestTable', FAMILIES => [{NAME => 'info', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 
'false', BLOCKCACHE => 'true'}]}} because it is offline and split
2010-05-04 17:26:25,902 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region TestTable,0000518811,1273019008135 to setClosing list
2010-05-04 17:26:27,008 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Skipping region REGION => {NAME => 'TestTable,0000518811,1273019008135', STARTKEY => '0000518811', ENDKEY => '0000650817', 
ENCODED => 143183187, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'TestTable', FAMILIES => [{NAME => 'info', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY =>
 'false', BLOCKCACHE => 'true'}]}} because it is offline and split
2010-05-04 17:26:27,018 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region TestTable,0000518811,1273019008135 to setClosing list
{code}

The region gets stuck in transition! More details in a comment to come.",jdcryans,jdcryans,Critical,Closed,Fixed,05/May/10 00:45,20/Nov/15 12:43
Bug,HBASE-2516,12463792,"Ugly IOE when region is being closed; rather, should NSRE","I'm running 80/20 YCSB (80% reads/20% writes).  I see this from time to time in logs (especially if I do big fat bulk upload at same time -- having trouble making ycsb do heavy loading at mo):

{code}
2010-05-05 06:57:01,165 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region, no outstanding scanners on usertable,user1431413702,1273040674721
2010-05-05 06:57:01,165 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: No more row locks outstanding on region usertable,user1431413702,1273040674721
2010-05-05 06:57:01,178 DEBUG org.apache.hadoop.hbase.regionserver.Store: closed cf
2010-05-05 06:57:01,178 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed usertable,user1431413702,1273040674721
2010-05-05 06:57:01,178 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer:
java.io.IOException: Region usertable,user1431413702,1273040674721 closed
        at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1179)
        at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1172)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2506)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2493)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1742)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
{code}

This should be a NSRE.

J-D took a look and fact that we do an IOE 'region closed' is way old, from before the time forgot, so its just always been there just more obvious now that Get is a Scan.",dploeg,stack,Major,Closed,Fixed,05/May/10 16:37,20/Nov/15 12:43
Bug,HBASE-2519,12463847,StoreFileScanner.seek swallows IOEs,"If a storefilescanner fails to seek, it silently swallows the IOE and returns false as if it were the end of the scanner.

This means that we can silently lose data if an IOE occurs for one of the store files during a compaction.",tlipcon,tlipcon,Blocker,Closed,Fixed,06/May/10 05:26,20/Nov/15 12:41
Bug,HBASE-2525,12464099,"If one-RS only and its restarted before its shutdown is completely processed, we'll never complete shutdown processing","This is why TestZooKeeper is currently broken in build.  There is a single RS only in that test.  Its restarted using the hbase.regionserver.restart.on.zk.expire facility in the RS.  HBASE-2413 added blocking new servers of same host+port until the old is processed but if the old was only server carrying root and meta, and we won't let in the new server until the old server is processed AND processing of old server shutdown inline has assignment of root and meta, we're stuck.",stack,stack,Major,Closed,Fixed,10/May/10 00:52,20/Nov/15 12:43
Bug,HBASE-2530,12464199,HBASE-2165 removed compactionQueueSize metric,"Per RS compactionQueueSize metric is no longer reported. HBASE-2165 (r932137) seems to have reverted more stuff than just the fragmentation display alone.

Would be nice to have this added back.

",ruifang,kannanm,Major,Closed,Fixed,11/May/10 03:34,12/Oct/12 06:15
Bug,HBASE-2531,12464206,32-bit encoding of regionnames waaaaaaayyyyy too susceptible to hash clashes,"Kannan tripped over two regionnames that hashed the same:

Here is code demo'ing that his two names hash the same:

{code}
package org;

import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.util.JenkinsHash;


public class Testing {
  public static void main(final String [] args) {
    System.out.println(encodeRegionName(Bytes.toBytes(""test1,6838000000,1273541236167"")));
    System.out.println(encodeRegionName(Bytes.toBytes(""test1,0520100000,1273541610201"")));
  }

  /**
   * @param regionName
   * @return the encodedName
   */
  public static int encodeRegionName(final byte [] regionName) {
    return Math.abs(JenkinsHash.getInstance().hash(regionName, regionName.length, 0));
  }
}
{code}

Need new encoding mechanism.  Will need to migrate old regions to new schema.",kannanm,stack,Blocker,Closed,Fixed,11/May/10 05:30,14/Apr/21 05:57
Bug,HBASE-2539,12464392,Cannot start ZK before the rest in tests anymore,"HBASE-2414 added a coarse check to see if the test cluster is up, but that prevents from starting ZK before everything else. Needed for replication's unit testing.",jdcryans,jdcryans,Major,Closed,Fixed,12/May/10 21:32,20/Nov/15 12:41
Bug,HBASE-2544,12464491,Forward port branch 0.20 WAL to TRUNK,Tests are failing on TRUNK.  The WAL implementation in TRUNK does not do the hookups for hdfs-200 properly.  Fix this before forward porting other issues from branch that are not in trunk.,stack,stack,Major,Closed,Fixed,13/May/10 23:54,20/Nov/15 12:43
Bug,HBASE-2545,12464505,"Unresponsive region server, potential deadlock","We have a 15-node (14RS+1Master) hbase cluster.  We just recently upgraded from 0.20.3 to 0.20.4.  This cluster does have colocated hadoop MR, but we mostly use another MR cluster to hit it.  Upon start, the cluster runs the jobs fine for about an hour.  Afterwards, an RS seems to have locked up.  Doing a get for a row in region being served by that region server hangs (cannot even ctrl+c out of the hbase shell).  Attached is the thread dump.  Verified in UI that the affect server runs on 0.20.4 and not 0.20.3.",tlipcon,kjirapinyo,Blocker,Closed,Fixed,14/May/10 05:27,12/Oct/12 06:15
Bug,HBASE-2546,12464506,Specify default filesystem in both the new and old way (needed if we are to run on 0.20 and 0.21 hadoop),"I couldn't start a distributed cluster because master wanted to keep using the local filesystem.  Setting default filesystem using both old and new way seems the way to go:

{code}
Index: core/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
===================================================================
--- core/src/main/java/org/apache/hadoop/hbase/master/HMaster.java      (revision 944113)
+++ core/src/main/java/org/apache/hadoop/hbase/master/HMaster.java      (working copy)
@@ -165,6 +165,9 @@
     // default localfs.  Presumption is that rootdir is fully-qualified before
     // we get to here with appropriate fs scheme.
     this.rootdir = FSUtils.getRootDir(this.conf);
+    // Cover both bases, the old way of setting default fs and the new.
+    // We're supposed to run on 0.20 and 0.21 anyways.
+    this.conf.set(""fs.default.name"", this.rootdir.toString());
     this.conf.set(""fs.defaultFS"", this.rootdir.toString());
     this.fs = FileSystem.get(this.conf);
     checkRootDir(this.rootdir, this.conf, this.fs);
{code}",stack,stack,Major,Closed,Fixed,14/May/10 05:29,20/Nov/15 12:43
Bug,HBASE-2553,12464577,Revisit IncrementColumnValue implementation in 0.22,"right now we are using too much of the old get code, we need to review that and constrain how this works but without breaking ICV.

Also we should be resetting the timestamp on every ICV call, and removing the older version.  Instead of 'updating' an ICV ""in place"" we should be adding a new one, removing the old one from memstore (if it is there).  This will play well with the atomic approach added in HBASE-2248 since we are only touching 1 KeyValue at a time.",ryanobjc,ryanobjc,Major,Closed,Fixed,15/May/10 00:52,20/Nov/15 12:41
Bug,HBASE-2558,12464723,"[mvn] Our javadoc overview -- ""Getting Started"", requirements, etc. -- is not carried across by mvn javadoc:javadoc target","I see this http://jira.codehaus.org/browse/MJAVADOC-278 which does not bode well.  I messed around upping the plugin version and explicitly specifying the overview.html file to use but no luck.  Come back and figure this or an alternative.

Made it a blocker.",,stack,Blocker,Closed,Fixed,17/May/10 20:54,20/Nov/15 12:41
Bug,HBASE-2560,12464740,IllegalArgumentException when manually splitting table from web UI,"Clicked split once, then again, got an error:

http://monster01.sf.cloudera.com:60010/table.jsp?action=split&name=VerifiableEditor&key=
java.lang.IllegalArgumentException: Not a host:port pair: 
	at org.apache.hadoop.hbase.HServerAddress.(HServerAddress.java:57)
	at org.apache.hadoop.hbase.master.HMaster.getTableRegions(HMaster.java:841)
	at org.apache.hadoop.hbase.master.HMaster.modifyTable(HMaster.java:981)
",tlipcon,tlipcon,Critical,Closed,Fixed,18/May/10 01:10,11/Jun/22 23:33
Bug,HBASE-2561,12464741,Scanning .META. while split in progress yields IllegalArgumentException,"Running scan '.META.' from the shell throws IllegalArgumentException if a split is running at the same time:

hbase(main):004:0> scan '.META.'
ROW                          COLUMN+CELL                                                                      
 VerifiableEditor,,127414503 column=info:regioninfo, timestamp=1274145178356, value=REGION => {NAME => 'Verifi
 3318                        ableEditor,,1274145033318', STARTKEY => '', ENDKEY => '-1942612687<1274143362177>
                             ', ENCODED => 1741581486, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'Ver
                             ifiableEditor', FAMILIES => [{NAME => 'info', REPLICATION_SCOPE => '0', COMPRESSI
                             ON => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMO
                             RY => 'false', BLOCKCACHE => 'true'}]}}                                          
 VerifiableEditor,,127414503 column=info:server, timestamp=1274145178356, value=                              
 3318                                                                                                         

ERROR: java.lang.IllegalArgumentException: offset (0) + length (8) exceed the capacity of the array: 0
",tlipcon,tlipcon,Critical,Closed,Fixed,18/May/10 01:15,20/Nov/15 12:43
Bug,HBASE-2562,12464744,bin/hbase doesn't work in-situ in maven,"on trunk I can't use the normal start-hbase.sh scripts, etc, after doing mvn install - instead have to do the whole assembly nonsense, which is a pain when trying to quickly iterate on changes.",tlipcon,tlipcon,Critical,Closed,Fixed,18/May/10 01:38,20/Nov/15 12:42
Bug,HBASE-2572,12464915, hbase/bin/set_meta_block_caching.rb:72: can't convert Java::JavaLang::String into String (TypeError) - little issue with script,,stack,stack,Major,Closed,Fixed,19/May/10 17:02,20/Nov/15 12:44
Bug,HBASE-2573,12464948,client.HConnectionManager$TableServers logs non-printable binary bytes,"Just a minor annoyance, sometimes the client code logs non-printable binary bytes that mess up my terminal when DEBUG logging is turned on.",tsuna,tsuna,Trivial,Closed,Fixed,19/May/10 21:10,20/Nov/15 12:41
Bug,HBASE-2576,12464970,TestHRegion.testDelete_mixed() failing on hudson,,streamy,streamy,Critical,Closed,Fixed,19/May/10 23:55,20/Nov/15 12:41
Bug,HBASE-2581,12464980,Bloom commit broke TestShell,TestShell is not passing on hudson after bloom commit.,streamy,streamy,Blocker,Closed,Fixed,20/May/10 00:43,20/Nov/15 12:41
Bug,HBASE-2582,12464981,TestTableSchemaModel not passing after commit of blooms,TestTableSchemaModel is failing because commit of blooms changed the existing column family setting.,apurtell,streamy,Critical,Closed,Fixed,20/May/10 00:57,20/Nov/15 12:41
Bug,HBASE-2583,12465003,Make webapps work in distributed mode again and make webapps deploy at / instead of at /webapps/master/master.jsp,UI is not showing up in right place when you run hbase unless you run it in-situ; it don't work properly when distributed mode.,stack,stack,Major,Closed,Fixed,20/May/10 08:14,20/Nov/15 12:43
Bug,HBASE-2586,12465075,Move hbase webapps to an hbase-webapps dir,Having our webapps in a dir named webapps/ is confusing the hadoop http servers in miniclusters. This was causing my MiniMRCluster jobs to not work correctly. Moving our dir to a new name fixed the issue.,tlipcon,tlipcon,Minor,Closed,Fixed,20/May/10 22:24,20/Nov/15 12:40
Bug,HBASE-2589,12465144,TestHRegion.testWritesWhileScanning flaky on trunk,"IIRC we added this test in response to a bug in HBASE-2248 in the old branch. It's failing about half the time on my internal Hudson. i=36 expected:<1000> but was:<0>
",tlipcon,tlipcon,Critical,Closed,Fixed,21/May/10 16:51,20/Nov/15 12:43
Bug,HBASE-2590,12465146,Failed parse of branch element in saveVersion.sh,"A mvn build would fail because of the string it was getting back from svn:

{code}
      [exec] not part of the command.
      Execute:Java13CommandLauncher: Executing 'sh' with arguments:
       '/home/X/Y/hadoop/branches/V.V/hbase/hbase-trunk/src/saveVersion.sh'
        '0.21.0-SNAPSHOT'
         '/home/X/Y/hadoop/branches/V.V/hbase-trunk/target/generated-sources'
        The ' characters around the executable and arguments are
         not part of the command.
         [exec] sed: -e expression #6, char 48: unterminated `s' command
         [exec] Result: 1
{code}

Path amended in the above to protect the innocent.

The failure was around parse of branch.  Branch is not used.",tsuna,stack,Major,Closed,Fixed,21/May/10 17:25,20/Nov/15 12:42
Bug,HBASE-2591,12465153,HBASE-2587 hardcoded the port that dfscluster runs on," HBASE-2587 ""Coral where tests write data when running and make sure clean target removes all written"" hardcoded the port that dfscluster runs on.  Makes it so can't run tests in shared environment.


Here's a fix:

{code}
diff --git a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index 8795ba6..238e804 100644
--- a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -173,8 +173,8 @@ public class HBaseTestingUtility {
     else this.clusterTestBuildDir = dir;
     System.setProperty(TEST_DIRECTORY_KEY, this.clusterTestBuildDir.toString());
     System.setProperty(""test.cache.data"", this.clusterTestBuildDir.toString());
-    this.dfsCluster = new MiniDFSCluster(12345, this.conf, servers, true,
-      true, true, null, null, null, null);
+    this.dfsCluster = new MiniDFSCluster(0, this.conf, servers, true, true,
+      true, null, null, null, null);
     return this.dfsCluster;
   }
{code}",stack,stack,Major,Closed,Fixed,21/May/10 19:37,20/Nov/15 12:40
Bug,HBASE-2599,12465182,"BaseScanner says ""Current assignment of X is not valid"" over and over for same region","From IRC today

{code}
12:41 < cmorgan> hey guys. I'm having a recent  issue with a single node cluster running 0.20.4. After stopping for a backup I now get region assignment churn. Seems master keeps thinking that region
                 assignment is not valid even when it is. Following is a log snippet:
12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        HMaster] DEBUG ter.RegionServerOperationQueue  - Processing todo: PendingOpenOperation from localhost.,7802,1274425405680
12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        HMaster] INFO  e.master.RegionServerOperation  - net_troove_coin_account_AccountCredentials,,1234913258116 open on 127.0.0.1:7802
12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        HMaster] INFO  e.master.RegionServerOperation  - Updated row net_troove_coin_account_AccountCredentials,,1234913258116 in region .META.,,1 with
                 startcode=1274425405680, server=127.0.0.1:7802
12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        HMaster] DEBUG ter.RegionServerOperationQueue  - Processing todo: PendingOpenOperation from localhost.,7802,1274425405680
12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        HMaster] INFO  e.master.RegionServerOperation  - net_troove_application_request_TemporaryRequest,,1234913268355 open on 127.0.0.1:7802
12:41 < cmorgan> [21/05/10 00:59:42] 3443247 [        HMaster] INFO  e.master.RegionServerOperation  - Updated row net_troove_application_request_TemporaryRequest,,1234913268355 in region .META.,,1 with
                 startcode=1274425405680, server=127.0.0.1:7802
12:41 < cmorgan> [21/05/10 00:59:42] 3443247 [ger.metaScanner] DEBUG adoop.hbase.master.BaseScanner  - Current assignment of net_troove_coin_account_AccountEntry,,1271448856984 is not valid;
                 serverAddress=127.0.0.1:7802, startCode=1274425405680 unknown.
12:41 < cmorgan> [21/05/10 00:59:42] 3443248 [ger.metaScanner] DEBUG adoop.hbase.master.BaseScanner  - Current assignment of net_troove_coin_account_AccountEntry-Base_EntryDay_DESCENDING,,1273266418876
                 is not valid;  serverAddress=127.0.0.1:7802, startCode=1274425405680 unknown.
12:41 < cmorgan> [21/05/10 00:59:42] 3443251 [ger.metaScanner] DEBUG adoop.hbase.master.BaseScanner  - Current assignment of net_troove_coin_bank_BankStatement,,1266433980935 is not valid;
                 serverAddress=127.0.0.1:7802, startCode=1274425405680 unknown.

12:58 < cmorgan> stack: I'd been running with 0.20.4 for a week or so starting/stopping every night. Now this happens...

14:11 < cmorgan> stack: some more info: On our mini production server the regionserver is getting ""My address is localhost.:7802"" (notice the dot after localhost). But the master is also sometimes
                 referring to it as 127.0.0.1. I just used the same data and config on my laptop, and its binding to my external LAN ip (""My address is 10.0.1.4:7802""). Under this setup hbase comes up
                 stable (no region assignment churn).

{code}

Looking at this, I think issue is that when we register a server we use a getServerName on a HServerInfo provided by the regionserver (though we are on the master side) but BaseScanner uses a getServerName that is made by doing a dns lookup using the IP that it finds in the server column of .META.  My sense is that is possible for the regionserver hostname and what the master finds when it does a lookup against dns can disagree, fatally.

This issue seems popular over last few weeks.  Was reported at least once more on a standalone instance and also on krispykola's 15-node ec2 cluster (He went back to 0.20.3 and then it went away?).  It made for what looked like double-assignment in his case (Our attempt at caching DNS names may be amiss -- I tihnk tht the main diff between 0.20.3 and 0.20.4 in this area).

My thought is to purge DNS from the HServerInfo passed by the RS to Master on startup and heartbeating and to use IPs only (and even then, the IP that the master tells the RS to use, its remote address as seen by the master).  We might have to do this fix for 0.20.5 since it seems to happen more in 0.20.4.

I'm looking into this.  Opinions welcome.",stack,stack,Major,Closed,Fixed,22/May/10 04:45,20/Nov/15 12:43
Bug,HBASE-2606,12465234,"NativeException: org.apache.hadoop.hbase.client.NoServerForRegionException: No server address listed in .META. for region item,,1274235159640 ","Though the region is listed when you attempt to access it you get an error that it is not assigned to a region.  In the log you find this repeating over and over:

2010-05-23 16:10:08,958 INFO org.apache.hadoop.hbase.master.ServerManager: 1 region servers, 0 dead, average load 5.0
2010-05-23 16:10:09,519 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 10.211.55.4:60020, regionname: -ROOT-,,0, startKey: <>}
2010-05-23 16:10:09,529 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 10.211.55.4:60020, regionname: -ROOT-,,0, startKey: <>} complete
2010-05-23 16:10:52,178 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {server: 10.211.55.4:60020, regionname: .META.,,1, startKey: <>}
2010-05-23 16:10:52,188 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of item,,1274235159640 is not valid;  serverAddress=, startCode=0 unknown.
2010-05-23 16:10:52,193 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 3 row(s) of meta region {server: 10.211.55.4:60020, regionname: .META.,,1, startKey: <>} complete
2010-05-23 16:10:52,193 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
2010-05-23 16:10:53,139 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region item,,1274235159640 to hbase,60020,1274655730505
2010-05-23 16:10:53,143 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: item,,1274235159640 from hbase,60020,1274655730505; 1 of 1
2010-05-23 16:10:53,143 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: PendingOpenOperation from hbase,60020,1274655730505
2010-05-23 16:10:53,143 INFO org.apache.hadoop.hbase.master.RegionServerOperation: item,,1274235159640 open on 10.211.55.4:60020
2010-05-23 16:10:53,146 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row item,,1274235159640 in region .META.,,1 with startcode=1274655730505, server=10.211.55.4:60020

Doing a flush and a major_compaction of the table resets it. I have a VM snapshot that has the behavior and it is reproducible at will.  Attached are the log files from the .META. region at jdcryans request.",,spullara,Major,Closed,Fixed,24/May/10 00:32,12/Oct/12 06:15
Bug,HBASE-2610,12465292,ValueFilter copy pasted javadoc from QualifierFilter,"I see this in ValueFilter:

{code}
  /**
   * Constructor.
   * @param valueCompareOp the compare op for column qualifier matching
   * @param valueComparator the comparator for column qualifier matching
   */
  public ValueFilter(final CompareOp valueCompareOp,
      final WritableByteArrayComparable valueComparator) {
    super(valueCompareOp, valueComparator);
  }
{code}

Obviously it's not for column qualifier matching but for value matching.",jdcryans,jdcryans,Major,Closed,Fixed,24/May/10 17:44,12/Oct/12 06:15
Bug,HBASE-2614,12465425,killing server in TestMasterTransitions causes NPEs and test deadlock,,stack,apurtell,Major,Closed,Fixed,26/May/10 03:01,20/Nov/15 12:43
Bug,HBASE-2615,12465486,M/R on bulk imported tables,"We are bulk importing using loadtable.rb and running M/R jobs using HBase as input.

We're taking the following steps:
1a. Load HBase with a M/R job using the normal API. 
OR
1b. Load HBase with bulk import.

THEN

2a. Using the shell, do a ""count"" over the table.
OR
2b. Run a M/R job that scans the whole HBase table (and nothing else).

Of the 4 combos, 3 are fine: 1a+2a, 1a+2b, 1b+2a.  We're having trouble with 1b+2b.  When we run the M/R job, it doesn't seem to read in any records, but there are no explicit errors in either the Hadoop or HBase logs.

Any ideas on what might be wrong with the bulk import to cause this problem?  We confirmed this problem exists in both hbase-0.20.3 and hbase-0.20.4.

We have created dummy data (see attached). This is the test case:

After loading the data into HDFS. In hbase shell:
create 'tiny', 'values'

Execute: 
{HBASE-HOME}/bin/hbase org.jruby.Main {HBASE-HOME}/bin/loadtable.rb tiny tinytable

Then run the simple row counter
{HADOOP-HOME}/bin/hadoop jar {HBASE-HOME}/hbase-0.20.x.jar rowcounter tiny values

Notice that map input records read is always zero. We confirmed that other mapreduce jobs do not execute the map function at all, always returning 0 records.

We also ran a major_compaction of all Hbase tables (.META. and .ROOT. as well) but this did not fix the problem.",stack,azzadev,Major,Closed,Fixed,26/May/10 16:40,12/Oct/12 06:15
Bug,HBASE-2616,12465487,TestHRegion.testWritesWhileGetting flaky on trunk,"Saw this failure on my internal hudson:

junit.framework.AssertionFailedError: expected:<\x00\x00\x00\x96> but was:<\x00\x00\x01\x00>
	at org.apache.hadoop.hbase.HBaseTestCase.assertEquals(HBaseTestCase.java:684)
	at org.apache.hadoop.hbase.regionserver.TestHRegion.testWritesWhileGetting(TestHRegion.java:2334)
",ryanobjc,tlipcon,Critical,Closed,Fixed,26/May/10 16:43,12/Oct/12 06:15
Bug,HBASE-2617,12465520,Load balancer falls into pathological state if one server under average - slop; endless churn,"I'm looking at a 0.20.4 cluster of 80 fast machines.  It runs fine for a while and then falls over into crazy balancing churn (My view on logs is sporadic but have a log before me where this is happening).  Things I see that seem to be of 0.20.4 particularly:

+ We don't reach an equilibrium or at least it takes so long, its as though it wasn't every going to happen
+ Master log filled w/ open, close of one or two regions  usually the same ones over and over (The Regions that are candidates to close are provided by the RS.  They are ordered by hash of their name.  We return the top ten from this Set every time.  So, we always close the same regions all the time even if we just opened it)
+ Often, we'll tell an RS to close a region.  It will do the job.  In 0.20.4 we made it so if RS has any work at all for the master, that we return immediately rather than wait for the reporting period to elaspse.   So, on these fast machines, it can be back near immediately if it just opened another some other region, say.  It can get assigned the region it just closed.  Seems to happen frequently enough.

For example, look at the below extract featuring a single regions life.  Its opened and closed 5 times in about 1/2 a second:

{code}
2010-05-25 11:01:05,488 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of GenericMetaStore,139757491,1274779304880, false, reassign: true
2010-05-25 11:01:05,489 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region set as unassigned: GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,490 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region GenericMetaStore,139757491,1274779304880 to a025.example.com,60020,1274744064673
2010-05-25 11:01:05,510 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: GenericMetaStore,139757491,1274779304880 from a025.example.com,60020,1274744064673; 1 of 1
2010-05-25 11:01:05,510 INFO org.apache.hadoop.hbase.master.RegionServerOperation: GenericMetaStore,139757491,1274779304880 open on 10.209.32.189:60020
2010-05-25 11:01:05,511 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row GenericMetaStore,139757491,1274779304880 in region .META.,,1 with startcode=1274744064673, server=10.209.32.189:60020
2010-05-25 11:01:05,548 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,552 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: GenericMetaStore,139757491,1274779304880 from a025.example.com,60020,1274744064673; 1 of 2
2010-05-25 11:01:05,552 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of GenericMetaStore,139757491,1274779304880, false, reassign: true
2010-05-25 11:01:05,552 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region set as unassigned: GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,556 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region GenericMetaStore,139757491,1274779304880 to a028.example.com,60020,1274747560769
2010-05-25 11:01:05,578 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: GenericMetaStore,139757491,1274779304880 from a028.example.com,60020,1274747560769; 1 of 1
2010-05-25 11:01:05,578 INFO org.apache.hadoop.hbase.master.RegionServerOperation: GenericMetaStore,139757491,1274779304880 open on 10.209.32.185:60020
2010-05-25 11:01:05,579 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row GenericMetaStore,139757491,1274779304880 in region .META.,,1 with startcode=1274747560769, server=10.209.32.185:60020
2010-05-25 11:01:05,599 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,605 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: GenericMetaStore,139757491,1274779304880 from a028.example.com,60020,1274747560769; 1 of 2
2010-05-25 11:01:05,605 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of GenericMetaStore,139757491,1274779304880, false, reassign: true
2010-05-25 11:01:05,606 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region set as unassigned: GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,607 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region GenericMetaStore,139757491,1274779304880 to sjc1c104.example.com,60020,1274747062601
2010-05-25 11:01:05,640 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: GenericMetaStore,139757491,1274779304880 from sjc1c104.example.com,60020,1274747062601; 1 of 1
2010-05-25 11:01:05,640 INFO org.apache.hadoop.hbase.master.RegionServerOperation: GenericMetaStore,139757491,1274779304880 open on 10.209.42.181:60020
2010-05-25 11:01:05,641 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row GenericMetaStore,139757491,1274779304880 in region .META.,,1 with startcode=1274747062601, server=10.209.42.181:60020
2010-05-25 11:01:05,723 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,729 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: GenericMetaStore,139757491,1274779304880 from sjc1c104.example.com,60020,1274747062601; 1 of 4
2010-05-25 11:01:05,729 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of GenericMetaStore,139757491,1274779304880, false, reassign: true
2010-05-25 11:01:05,730 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region set as unassigned: GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,731 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region GenericMetaStore,139757491,1274779304880 to sjc1c091.example.com,60020,1274747056415
2010-05-25 11:01:05,751 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: GenericMetaStore,139757491,1274779304880 from sjc1c091.example.com,60020,1274747056415; 1 of 1
2010-05-25 11:01:05,752 INFO org.apache.hadoop.hbase.master.RegionServerOperation: GenericMetaStore,139757491,1274779304880 open on 10.209.42.238:60020
2010-05-25 11:01:05,752 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row GenericMetaStore,139757491,1274779304880 in region .META.,,1 with startcode=1274747056415, server=10.209.42.238:60020
2010-05-25 11:01:05,775 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,780 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_CLOSE: GenericMetaStore,139757491,1274779304880 from sjc1c091.example.com,60020,1274747056415; 1 of 2
2010-05-25 11:01:05,780 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of GenericMetaStore,139757491,1274779304880, false, reassign: true
2010-05-25 11:01:05,780 INFO org.apache.hadoop.hbase.master.ProcessRegionClose$1: region set as unassigned: GenericMetaStore,139757491,1274779304880
2010-05-25 11:01:05,808 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region GenericMetaStore,139757491,1274779304880 to sjc1a003.example.com,60020,1274747057557
2010-05-25 11:01:05,828 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: GenericMetaStore,139757491,1274779304880 from sjc1a003.example.com,60020,1274747057557; 1 of 1
2010-05-25 11:01:05,828 INFO org.apache.hadoop.hbase.master.RegionServerOperation: GenericMetaStore,139757491,1274779304880 open on 10.209.32.148:60020
2010-05-25 11:01:05,829 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row GenericMetaStore,139757491,1274779304880 in region .META.,,1 with startcode=1274747057557, server=10.209.32.148:60020
{code}

The culprit seems to be the code that wants to bring up underloaded regionservers up to average.  That and something about the lightly loaded servers math that is off.

I'm marking this as a blocker.  I'm not sure why its not more common.  There were some issues on this cluster regards disks filling but though such an event may have provoked the issue, we should have evened out eventually.

Making this a blocker on 0.20.5.   Need to fix it for this user at least.",stack,stack,Blocker,Closed,Fixed,26/May/10 23:26,12/Oct/12 06:15
Bug,HBASE-2619,12465605,HBase shell 'alter' command cannot set table properties to False,"The alter table command in the HBase shell is broken. Specifically, attempting to set boolean table options like BLOCKCACHE to False does not result in the expected behavior.

This is due to bugs in the file src/main/ruby/hbase/admin.rb, in the hcd() function. There many statements that look like this:

arg[IN_MEMORY]? JBoolean.valueOf(arg[IN_MEMORY]): HColumnDescriptor::DEFAULT_IN_MEMORY

The intent of this code is to 1) determine if a given parameter is present in the associative array, 2) use the supplied value if it exists, or 3) use the default value if it does not exist. However, in the case of boolean parameters that have been set to False this code instead evaluates the variable (to False) and then chooses the default parameter value, which is incorrect.

The attached patch fixes the issue.",,bowlinearl,Minor,Closed,Fixed,27/May/10 22:07,20/Nov/15 12:42
Bug,HBASE-2620,12465615,REST tests don't use ephemeral ports,"Saw this failure on my hudson:

java.net.BindException: Address already in use
...elided...
	at org.apache.hadoop.hbase.rest.HBaseRESTClusterTestBase.startServletContainer(HBaseRESTClusterTestBase.java:60)
	at org.apache.hadoop.hbase.rest.HBaseRESTClusterTestBase.setUp(HBaseRESTClusterTestBase.java:27)
	at org.apache.hadoop.hbase.rest.TestTableResource.setUp(TestTableResource.java:63)
",apurtell,tlipcon,Major,Closed,Fixed,28/May/10 03:27,20/Nov/15 12:41
Bug,HBASE-2621,12465635,Fix broken link in HFile Javadoc,"There's a bad link in the HFile javadoc; should point to HBASE-61, but instead point to HBASE-3315. Also cleaned up the reference to TFile.",hammer,hammer,Major,Closed,Fixed,28/May/10 09:38,20/Nov/15 12:40
Bug,HBASE-2622,12465636,Fix link to SVN repository on HBase website,"We currently point to http://svn.apache.org/repos/asf/hadoop/hbase on the website, when we should point to http://svn.apache.org/repos/asf/hbase (and similar).",hammer,hammer,Major,Closed,Fixed,28/May/10 09:43,11/Jun/22 23:41
Bug,HBASE-2633,12465743,REST XML schema description up on the wiki is out of date,The REST XML schema description up on the wiki is out of date. Fix it. ,apurtell,apurtell,Major,Closed,Fixed,30/May/10 19:51,12/Oct/12 06:16
Bug,HBASE-2635,12465814,ImmutableBytesWritable ignores offset in several cases,"HBASE-2378 improved ImmutableBytesWritable's comparator, but there's still a bug! We assume offset = 0, which makes the comparator function incorrectly when the ImmutableBytesWritable is a slice into a larger byte array.",tlipcon,tlipcon,Blocker,Closed,Fixed,01/Jun/10 00:40,12/Oct/12 06:15
Bug,HBASE-2639,12465826,Bring root and meta over to new naming scheme (hbase-2531),hbase-2531 introduced new region naming scheme.  root and meta were left using old scheme.  This issue is about doing work to bring these over too.,,stack,Major,Closed,Fixed,01/Jun/10 04:33,11/Jun/22 23:43
Bug,HBASE-2643,12465884,Figure how to deal with eof splitting logs,"When splitting the WAL and encountering EOF, it's not clear what to do. Initial discussion of this started in http://review.hbase.org/r/74/ - summarizing here for brevity:

We can get an EOFException while splitting the WAL in the following cases:
- The writer died after creating the file but before even writing the header (or crashed halfway through writing the header)
- The writer died in the middle of flushing some data - sync() guarantees that we can see _at least_ the last edit, but we may see half of an edit that was being written out when the RS crashed (especially for large rows)
- The data was actually corrupted somehow (eg a length field got changed to be too long and thus points past EOF)

Ideally we would know when we see EOF whether it was really the last record, and in that case, simply drop that record (it wasn't synced, so therefore we dont need to split it). Some open questions:
  - Currently we ignore empty files. Is it ok to ignore an empty log file if it's not the last one?
  - Similarly, do we ignore an EOF mid-record if it's not the last log file?",nspiegelberg,stack,Blocker,Closed,Fixed,01/Jun/10 17:07,20/Nov/15 12:41
Bug,HBASE-2644,12465885,Investigate sync 'voodoo' splitting WALs,"The sequence file sync seems to actually help for some weird reason when recovering parts of edits (odd -- it looks like it just adds a marker to the file).  Investigate.  This comes of review of hbase-2437.   Below is copied from http://review.hbase.org/r/74/

{code}
no point to call .sync() here, it just wastes a bunch of IO to write ""sync markers"" which we don't make any real use of.
Cosmin Lehene 6 days, 23 hours ago (May 25th, 2010, 9:07 a.m.)
sync() used to call syncFs(). It looks like HBASE-2544 changed things a bit, but it doesn't only add the SequenceFile sync marker.

I added this after I've seen inconsistent results when running splitLog on bigger hlogs. Try copying a log from the cluster locally and run splitLog from the command line a few times without flushing it after each append. I used to get inconsistent results between runs and calling sync fixed it.

There's this ""//TODO: test the split of a large (lots of regions > 500 file). In my tests it seems without hflush""  in the TestHLogSplit. 

We could do some testing to figure out why would log entries be lost when running locally.

What would be a better way to flush the writer?
Todd Lipcon 5 days, 19 hours ago (May 26th, 2010, 1:31 p.m.)
This seems really voodoo.. if anything we're probably masking a real bug by doing this. Can you write a unit test which shows this problem (even if it takes 30 minutes to run, would be good to have in our arsenal)
Cosmin Lehene 2 days, 18 hours ago (May 29th, 2010, 2:13 p.m.)
I can't reproduce it on hdfs-0.20. I can't compile hdfs-0.21 (again) for some reason. I'll give it another try some other time. 

Added the test. Also tried with a real 60MB log file. 
I'm not sure if we should leave the test active.
{code}",,stack,Major,Closed,Fixed,01/Jun/10 17:10,20/Nov/15 12:41
Bug,HBASE-2645,12465888,HLog writer can do 1-2 sync operations after lease has been recovered for split process.,"TestHLogSplit.testLogCannotBeWrittenOnceParsed is failing. 

This test starts a thread that writes one edit to the log, syncs and counts. During this, a HLog.splitLog operation is started. splitLog recovers the log lease before reading the log, so that the original regionserver could not wake up and write after the split process started.  
The test compares the number of edits reported by the split process and by the writer thread. Writer thread (called zombie in the test) should report <=  than the splitLog (sync() might raise after the last edit gets written and the edit won't get counted by zombie thread). However it appears that the zombie counts 1-2 more edits. So it looks like it can sync without a lease.

This might be a hdfs-0.20 related issue. ",tlipcon,clehene,Blocker,Closed,Fixed,01/Jun/10 17:57,15/Oct/13 04:46
Bug,HBASE-2654,12465938,Several tests failing after bulk output commit,"Several tests are failing on Hudson after the commit of HBASE-1923 - see http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1282/testReport/
These tests passed on local build, something seems to be different about the hudson environment.",tlipcon,tlipcon,Blocker,Closed,Fixed,02/Jun/10 06:58,20/Nov/15 12:43
Bug,HBASE-2656,12465990,HMaster.getRegionTableClosest should not return null for closed regions,"Raised in the review of HBASE-2560: there are a couple functions in HMaster which return null when a region has not been deployed. Instead, they should return a Pair<HRegionInfo, HServerAddress> where only the address is null (since the info is still in meta!)",tlipcon,tlipcon,Blocker,Closed,Fixed,02/Jun/10 17:55,11/Jun/22 23:44
Bug,HBASE-2657,12465992,TestTableResource is broken in trunk,Column name is illegal -- its 'test:' -- when constructing a HCD.,stack,stack,Major,Closed,Fixed,02/Jun/10 18:36,20/Nov/15 12:41
Bug,HBASE-2658,12465993,REST (stargate) TableRegionModel Regions need to be updated to work w/ new region naming convention from HBASE-2531,"One reason TestTableResource was failing was because comparing region names as strings was failing because the two below no longer matched.  My guess is that the rest stuff is not using the new means of constructing region names.  See HBASE-2531

TestTableResource,,1275503739792.30a45563321be3ec11841b0f1e79d687.
TestTableResource,,1275503739792",stack,stack,Major,Closed,Fixed,02/Jun/10 18:41,20/Nov/15 12:43
Bug,HBASE-2662,12466136,TestScannerResource.testScannerResource broke in trunk,"{code}
Error Message

Illegal character <58>. Family names cannot contain control characters or colons: a:
Stacktrace

java.lang.IllegalArgumentException: Illegal character <58>. Family names cannot contain control characters or colons: a:
	at org.apache.hadoop.hbase.HColumnDescriptor.isLegalFamilyName(HColumnDescriptor.java:278)
	at org.apache.hadoop.hbase.HColumnDescriptor.<init>(HColumnDescriptor.java:240)
	at org.apache.hadoop.hbase.HColumnDescriptor.<init>(HColumnDescriptor.java:212)
	at org.apache.hadoop.hbase.HColumnDescriptor.<init>(HColumnDescriptor.java:169)
	at org.apache.hadoop.hbase.HColumnDescriptor.<init>(HColumnDescriptor.java:158)
	at org.apache.hadoop.hbase.rest.TestScannerResource.setUp(TestScannerResource.java:106)
	at junit.framework.TestCase.runBare(TestCase.java:132)
{code}",stack,stack,Major,Closed,Fixed,03/Jun/10 22:50,20/Nov/15 12:41
Bug,HBASE-2664,12466151,TestHRegion.testCheckAndDelete_ThatDeleteWasWritten fail in trunk,Seems like a bug in the recently committed patch. See http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1289/testReport/org.apache.hadoop.hbase.regionserver/TestHRegion/testCheckAndDelete_ThatDeleteWasWritten/,,tlipcon,Critical,Closed,Fixed,04/Jun/10 01:42,20/Nov/15 12:43
Bug,HBASE-2665,12466156,/TestStoreReconstruction broke in trunk,,stack,stack,Major,Closed,Fixed,04/Jun/10 03:22,11/Jun/22 23:44
Bug,HBASE-2667,12466163,TestHLog.testSplit failing in trunk,"Build is broke because this test won't pass.

Seems to be two issues at least:

1. we need to change the test so it keeps logs in one subdir, archives in another and split products in yet another as per regionserver.  New split code does cleanup of old logs dir if successful.  In this test it was removing the created splits so when verify went to run, the log no longer existed
2. The verify of splits tests that sequence numbers are in order.  They don't seem to be (the assertion failure was masked by a close reader exception).",stack,stack,Major,Closed,Fixed,04/Jun/10 06:35,20/Nov/15 12:41
Bug,HBASE-2669,12466236,HCM.shutdownHook causes data loss with hbase.client.write.buffer != 0,"In my application I set {{hbase.client.write.buffer}} to a reasonably small value (roughly 64 edits) in order to try to batch a few {{Put}} together before talking to HBase.  When my application does a graceful shutdown, I call {{HTable#flushCommits}} in order to flush any pending change to HBase.  I want to do the same thing when I get a {{SIGTERM}} by using {{Runtime#addShutdownHook}} but this is impossible since {{HConnectionManager}} already registers a shutdown hook that invokes {{HConnectionManager#deleteAllConnections}}.  This static method closes all the connections to HBase and then all connections to ZooKeeper.  Because all shutdown hooks run in parallel, my hook will attempt to flush edits while connections are getting closed.

There is no way to guarantee the order in which the hooks will execute, so I propose that we remove the hook in the HCM altogether and provide some user-visible API they call in their own hook after they're done flushing their stuff, if they really want to do a graceful shutdown.  I expect that a lot of users won't use a hook though, otherwise this issue would have cropped up already.  For those users, connections won't get ""gracefully"" terminated, but I don't think that would be a problem since the underlying TCP socket will get closed by the OS anyway, so things like ZooKeeper and such should realize that the connection has been terminated and assume the client is gone, and do the necessary clean-up on their side.

An alternate fix would be to leave the hook in place by default but keep a reference to it and add a user-visible API to be able to un-register the hook.  I find this ugly.

Thoughts?",stack,tsuna,Critical,Closed,Fixed,05/Jun/10 02:21,20/Nov/15 12:44
Bug,HBASE-2670,12466239,MemStore should retain multiple KVs with the same timestamp when memstoreTS differs,"There appears to be a bug in HBASE-2248 as committed to trunk. See following failing test:
http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1296/testReport/junit/org.apache.hadoop.hbase/TestAcidGuarantees/testAtomicity/
Think this is the same bug we saw early on in 2248 in the 0.20 branch, looks like the fix didn't make it over.",tlipcon,tlipcon,Blocker,Closed,Fixed,05/Jun/10 04:43,20/Nov/15 12:42
Bug,HBASE-2683,12466376,Make it obvious in the documentation that ZooKeeper needs permanent storage,"If our users let HBase manage ZK, they probably won't bother combing through hbase-default.xml to figure that they need to set hbase.zookeeper.property.dataDir to something else than /tmp. It probably happened to deinspanjer in prod today and that's a show stopper.

The fix would be, at least, to improve the Getting Started documentation to include that configuration in the ""Fully-Distributed Operation"" section.",jdcryans,jdcryans,Major,Closed,Fixed,07/Jun/10 19:52,12/Oct/12 06:15
Bug,HBASE-2684,12466386,TestMasterWrongRS flaky in trunk,"I think this is just a flaky test. I saw:

java.lang.AssertionError: expected:<2> but was:<3>
on the first:
    assertEquals(2, cluster.getLiveRegionServerThreads().size());

My guess is that the 2 second sleep is not good enough. We should probably either force a heartbeat somehow, or hook in so we can wait until there's been a heartbeat, rather than sleeping a hardcoded amount of time.",jdcryans,tlipcon,Major,Closed,Fixed,07/Jun/10 21:54,20/Nov/15 12:42
Bug,HBASE-2691,12466410,"LeaseStillHeldException totally ignored by RS, wrongly named","Currently region servers don't handle org.apache.hadoop.hbase.Leases$LeaseStillHeldException in any way that's useful so what happens right now is that it tries to report to the master and this happens:

{code}

2010-06-07 17:20:54,368 WARN  [RegionServer:0] regionserver.HRegionServer(553): Attempt=1
org.apache.hadoop.hbase.Leases$LeaseStillHeldException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:94)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.checkThrowable(RemoteExceptionHandler.java:48)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.checkIOException(RemoteExceptionHandler.java:66)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:541)
        at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.run(MiniHBaseCluster.java:173)
        at java.lang.Thread.run(Thread.java:637)
{code}

Then it will retry until the watch is triggered telling it that the session's expired! Instead, we should be a lot more proactive initiate abort procedure.",jdcryans,jdcryans,Major,Closed,Fixed,08/Jun/10 00:41,20/Nov/15 12:41
Bug,HBASE-2703,12466512,ui not working in distributed context,UI is not showing when you put hbase on a cluster; this is since we renamed webapps dir as hbase-webapps.,,stack,Major,Closed,Fixed,08/Jun/10 23:42,20/Nov/15 12:41
Bug,HBASE-2707,12466680,Can't recover from a dead ROOT server if any exceptions happens during log splitting,"There's an almost easy way to get stuck after a RS holding ROOT dies, usually from a GC-like event. It happens frequently to my TestReplication in HBASE-2223.

Some logs:

{code}
2010-06-10 11:35:52,090 INFO  [master] wal.HLog(1175): Spliting is done. Removing old log dir hdfs://localhost:55814/user/jdcryans/.logs/10.10.1.63,55846,1276194933831
2010-06-10 11:35:52,095 WARN  [master] master.RegionServerOperationQueue(183): Failed processing: ProcessServerShutdown of 10.10.1.63,55846,1276194933831; putting onto delayed todo queue
java.io.IOException: Cannot delete: hdfs://localhost:55814/user/jdcryans/.logs/10.10.1.63,55846,1276194933831
        at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1179)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown.process(ProcessServerShutdown.java:298)
        at org.apache.hadoop.hbase.master.RegionServerOperationQueue.process(RegionServerOperationQueue.java:149)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:456)
Caused by: java.io.IOException: java.io.IOException: /user/jdcryans/.logs/10.10.1.63,55846,1276194933831 is non empty
2010-06-10 11:35:52,097 DEBUG [master] master.RegionServerOperationQueue(126): -ROOT- isn't online, can't process delayedToDoQueue items
2010-06-10 11:35:53,098 DEBUG [master] master.RegionServerOperationQueue(126): -ROOT- isn't online, can't process delayedToDoQueue items
2010-06-10 11:35:53,523 INFO  [main.serverMonitor] master.ServerManager$ServerMonitor(131): 1 region servers, 1 dead, average load 14.0[10.10.1.63,55846,1276194933831]
2010-06-10 11:35:54,099 DEBUG [master] master.RegionServerOperationQueue(126): -ROOT- isn't online, can't process delayedToDoQueue items
2010-06-10 11:35:55,101 DEBUG [master] master.RegionServerOperationQueue(126): -ROOT- isn't online, can't process delayedToDoQueue items
{code}

The last lines are my own debug. Since we don't process the delayed todo if ROOT isn't online, we'll never reassign the regions. ",stack,jdcryans,Blocker,Closed,Fixed,10/Jun/10 18:44,20/Nov/15 12:42
Bug,HBASE-2710,12466705,Shell should have default terminal width when detection fails,"HBASE-2632 added code to detect terminal width. On some terminals (eg emacs shell) this returns 0, which resulted in infinite loops in the formatter. Should default to 100 when we can't detect width.",kannanm,tlipcon,Major,Closed,Fixed,11/Jun/10 01:24,20/Nov/15 12:41
Bug,HBASE-2712,12466759,Cached region location that went stale won't recover if asking for first row,"Let's say that:

 - A client cached the location of some region, not the first one in the table
 - The RS that was holding it fails
 - The first thing the client does after the failure is trying to reach the first row of that region

This will never recover, since HCM.deleteCachedLocation doesn't delete if the row we asked for is the first row in a region. This looks a lot like HBASE-1920, but there isn't enough information in that jira to say that it's the same thing.

This is a blocker, and it kills 0.20.5 RC2 (sorry).",jdcryans,jdcryans,Blocker,Closed,Fixed,11/Jun/10 18:06,12/Oct/12 06:15
Bug,HBASE-2720,12466867,TestFromClientSide fails for client region cache prewarm on Hudson,"TestFromClientSide failed by HBASE-2468 patch: http://hudson.zones.apache.org/hudson/job/HBase-TRUNK/1322/testReport/junit/org.apache.hadoop.hbase.client/TestFromClientSide/testRegionCachePreWarm/

It seems the number of actual cached regions was less than expected (as configured) on hudson. ",mingjielai,mingjielai,Major,Closed,Fixed,14/Jun/10 00:30,20/Nov/15 12:43
Bug,HBASE-2725,12466939,Shutdown hook management is gone in trunk; restore,Shutdown hook handling is gone from trunk.  Investigate (I think I did this cleaning up tests but was over-enthusiastic pruning).,stack,stack,Blocker,Closed,Fixed,14/Jun/10 22:20,20/Nov/15 12:41
Bug,HBASE-2727,12466945,Splits writing one file only is untenable; need dir of recovered edits ordered by sequenceid.,"This issue comes of tlipcon doing a bit of human unit testing.  His speculation is:

Let a region X deploy to server A.  Server A opens the region, then closes it.
Let region X now deploy to server B.  Server B now crashes.

Both server A and server B now have edits for region X in their WALs.

The processing of server crashes is currently sequential. 

If server A crashes before server B, server A will write out a file of recovered edits for region X but region X was not deployed on server A so, the file will just sit there unused.  The processing of server B crash will overwrite the recovered edits file written by the split of server A wal.  This is ok.

But if somehow, server B processing is done before server A's, then interesting issues will likely arise; in the main, there is danger that the server B's recovered edits could be overwritten.

Another issue comes up in the review of hbase-1025.  During the replay of edits on region deploy, if the hosting regionserver crashes before we have processed all of the recovered edits, we could lose some (the recovery of the regionserver that is replaying the edits could overwrite the log of edits only partially replayed).

Discussing up on IRC, whats needed is a directory of edits to replay ordered by sequenceid.  On recovery, we play the oldest through to the newest removing the edits only on successfully replay.

Making blocker on 0.21 since this is a correctness issue.",stack,stack,Blocker,Closed,Fixed,14/Jun/10 23:39,20/Nov/15 12:40
Bug,HBASE-2728,12466946,Support for HADOOP-4829,Users who have a HADOOP-4829 patched hadoop will run into the issue that closing a RS cleanly result into data loss because the FileSystem will be closed before the regions are. Cloudera is an example. We need to support those users.,jdcryans,jdcryans,Major,Closed,Fixed,15/Jun/10 00:37,12/Oct/12 06:15
Bug,HBASE-2729,12466950,flushCache should write to a tmp directory and then move into the store directory,"Currently it appears that internalFlushCache writes directly to the target spot of the flushed data. The finally() block appends the metadata and closes the file as if nothing bad went wrong in case of an exception. This is really bad, since it means that an IOE in the middle of flushing cache could easily write a valid looking file with only half the data, which would then prevent us from recovering those edits during log replay.

Instead, it should flush to a tmp location and move it into the region dir only after it's successfully written.",stack,tlipcon,Blocker,Closed,Fixed,15/Jun/10 01:56,20/Nov/15 12:40
Bug,HBASE-2732,12467033,"TestZooKeeper was broken, HBASE-2691 showed it","Since I committed HBASE-2691, TestZooKeeper began failing. Looking at it, it's because it's setup with only 1 region server and we kill it... and it's not restarting. IIRC we were relying on that feature before but looks like we're not anymore.",jdcryans,jdcryans,Major,Closed,Fixed,15/Jun/10 21:10,20/Nov/15 12:43
Bug,HBASE-2733,12467040,HBASE-2353 broke timestamp replacement on Puts when writeToWAL disabled,"In refactoring for HBASE-2353, it ended up that updateKeys() was only called on KVs if writeToWAL was set to true. This caused failure of TestGetClosestRowBefore (though the real bug was in put)",tlipcon,tlipcon,Blocker,Closed,Fixed,15/Jun/10 22:15,20/Nov/15 12:41
Bug,HBASE-2734,12467044,TestFSErrorsExposed should catch IOE as well as RTE,"Recently scanner construction changed such that it could throw IOE before iteration started. TestFSErrors is now failing since IOE is thrown, but the test only catches RTEs.",tlipcon,tlipcon,Major,Closed,Fixed,15/Jun/10 23:34,20/Nov/15 12:43
Bug,HBASE-2737,12467106,CME in ZKW introduced in HBASE-2694,"Saw this while tail'ing a log for something else:

{code}
2010-06-15 17:30:03,769 ERROR [main-EventThread] zookeeper.ClientCnxn$EventThread(490): Error while calling watcher
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.process(ZooKeeperWrapper.java:235)
{code}

Looks like the listeners list's iterator is used in an unprotected manner.",karthik.ranga,jdcryans,Major,Closed,Fixed,16/Jun/10 17:50,20/Nov/15 12:43
Bug,HBASE-2738,12467110,Fix TestTimeRangeMapRed verification now that we store multiple versions with same TS,"We recently introduced ""memstore timestamps"" which are internal, and allow a cell to exist twice with the same timestamp, from different puts. TestTimeRangeMapred now fails since it sees both the old and new version of the cells during verification. Setting the max versions of the verification scanner to 1 fixes it.",tlipcon,tlipcon,Major,Closed,Fixed,16/Jun/10 18:01,20/Nov/15 12:41
Bug,HBASE-2740,12467140,NPE in ReadWriteConsistencyControl,"A region server running 0.20.5rc3 died with this exception:

2010-06-16 12:19:03,694 FATAL org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Replay of hlog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: xxxxxxxxxx,1272579746382
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1041)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:896)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:262)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:149)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.ReadWriteConsistencyControl.getThreadReadPoint(ReadWriteConsistencyControl.java:40)
        at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.getNext(MemStore.java:532)
        at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.seek(MemStore.java:558)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:311)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.checkReseek(StoreScanner.java:297)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.peek(StoreScanner.java:143)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.updateReaders(StoreScanner.java:283)
        at org.apache.hadoop.hbase.regionserver.Store.notifyChangedReadersObservers(Store.java:621)
        at org.apache.hadoop.hbase.regionserver.Store.updateStorefiles(Store.java:607)
        at org.apache.hadoop.hbase.regionserver.Store.access$200(Store.java:88)
        at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.commit(Store.java:1605)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1019)
        ... 3 more

",ryanobjc,davelatham,Blocker,Closed,Fixed,16/Jun/10 23:56,12/Oct/12 06:15
Bug,HBASE-2741,12467141,HBaseExecutorService needs to be multi-cluster friendly,"While running TestReplication I bumped into:

{code}
2010-06-16 16:44:07,576 DEBUG [IPC Server handler 3 on 62423] master.RegionManager(357): Created UNASSIGNED zNode test,,1276731846828.de5dcd3df0fbc58207ce6ccff9ff2870. in state M2ZK_REGION_OFFLINE
2010-06-16 16:44:07,577 INFO  [RegionServer:0] regionserver.HRegionServer(511): MSG_REGION_OPEN: test,,1276731846828.de5dcd3df0fbc58207ce6ccff9ff2870.
2010-06-16 16:44:07,577 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(1358): Worker: MSG_REGION_OPEN: test,,1276731846828.de5dcd3df0fbc58207ce6ccff9ff2870.
2010-06-16 16:44:07,578 DEBUG [RegionServer:0.worker] regionserver.RSZookeeperUpdater(157): Updating ZNode /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870 with [RS2ZK_REGION_OPENING] expected version = 0
2010-06-16 16:44:07,580 DEBUG [main-EventThread] master.HMaster(1142): Event NodeDataChanged with state SyncConnected with path /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,580 DEBUG [main-EventThread] master.ZKMasterAddressWatcher(64): Got event NodeDataChanged with path /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,580 DEBUG [main-EventThread] master.ZKUnassignedWatcher(71): ZK-EVENT-PROCESS: Got zkEvent NodeDataChanged state:SyncConnected path:/1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,580 INFO  [main-EventThread] regionserver.HRegionServer(379): Got ZooKeeper event, state: SyncConnected, type: NodeDataChanged, path: /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,581 DEBUG [RegionServer:0.worker] regionserver.HRegion(294): Creating region test,,1276731846828.de5dcd3df0fbc58207ce6ccff9ff2870.
2010-06-16 16:44:07,582 DEBUG [MASTER_CLOSEREGION-master-1] handler.MasterOpenRegionHandler(70): Event = RS2ZK_REGION_OPENING, region = de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,582 DEBUG [MASTER_CLOSEREGION-master-1] handler.MasterOpenRegionHandler(81): NO-OP call to handling region opening event
2010-06-16 16:44:07,589 INFO  [RegionServer:0.worker] regionserver.HRegion(369): region test,,1276731846828.de5dcd3df0fbc58207ce6ccff9ff2870. available; sequence id is 1
2010-06-16 16:44:07,590 DEBUG [RegionServer:0.worker] regionserver.RSZookeeperUpdater(157): Updating ZNode /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870 with [RS2ZK_REGION_OPENED] expected version = 1
2010-06-16 16:44:07,591 DEBUG [main-EventThread] master.HMaster(1142): Event NodeDataChanged with state SyncConnected with path /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,591 DEBUG [main-EventThread] master.ZKMasterAddressWatcher(64): Got event NodeDataChanged with path /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,592 DEBUG [main-EventThread] master.ZKUnassignedWatcher(71): ZK-EVENT-PROCESS: Got zkEvent NodeDataChanged state:SyncConnected path:/1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,591 INFO  [main-EventThread] regionserver.HRegionServer(379): Got ZooKeeper event, state: SyncConnected, type: NodeDataChanged, path: /1/UNASSIGNED/de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,593 DEBUG [MASTER_CLOSEREGION-master-1] handler.MasterOpenRegionHandler(70): Event = RS2ZK_REGION_OPENED, region = de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,594 DEBUG [MASTER_CLOSEREGION-master-1] handler.MasterOpenRegionHandler(96): RS 10.10.1.130,62425,1276731832950 has opened region de5dcd3df0fbc58207ce6ccff9ff2870
2010-06-16 16:44:07,594 ERROR [MASTER_CLOSEREGION-master-1] server.NIOServerCnxn$Factory$1(81): Thread Thread[MASTER_CLOSEREGION-master-1,5,main] died
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.ServerManager.processRegionOpen(ServerManager.java:607)
        at org.apache.hadoop.hbase.master.handler.MasterOpenRegionHandler.handleRegionOpenedEvent(MasterOpenRegionHandler.java:99)
        at org.apache.hadoop.hbase.master.handler.MasterOpenRegionHandler.process(MasterOpenRegionHandler.java:75)
        at org.apache.hadoop.hbase.executor.HBaseEventHandler.run(HBaseEventHandler.java:215)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:637)
{code}

This looks very new. Assigning Karthik as he was there recently.",karthik.ranga,jdcryans,Major,Closed,Fixed,17/Jun/10 00:04,20/Nov/15 12:40
Bug,HBASE-2755,12467417,Duplicate assignment of a region after region server recovery,"After a region server recovery, some regions may get assigned to duplicate region servers.

Note: I am based on a slightly older trunk (prior to the HBASE-2694). Nevertheless, I think HBASE-2694 doesn't address this case.

Scenario:

* Three region server setup (store285,286,287), with about 500 regions in the table overall.
* kill -9 and restart one of the region servers (store286).
* The 170 odd regions in the failed region server got assigned out. But two of the regions got assigned to multiple region servers.
* Looking at the log entries for one such region, it appears that there is some race condition that happens between the ProcessRegionOpen (a RegionServerOperation) and BaseScanner which causes the BaseScanner to think this region needs to be reassigned.

Relevant Logs:

Master detects that the server start message (from the restarted RS) is from a server it already knows about, but startcode is different. So, it triggers server recovery. Alternatively, the recovery will be triggered by ZNODE expiry in some cases depending on which ever event (restart of RS or Znode expiry) happens first. After that it does logs splits etc. for the failed RS; it then also removes the old region server/startcode from the deadservers map.

{code}
2010-06-17 10:26:06,420 INFO org.apache.hadoop.hbase.master.ServerManager: Server start rejected; we already have 10.138.95.182:60020 registered; existingServer=serverName=store286.xyz.com,60020,1276629467680, load=(requests=22, regions=171, usedHeap=6549, maxHeap=11993), newServer=serverName=store286.xyz.com,60020,1276795566511, load=(requests=0, regions=0, usedHeap=0, maxHeap=0)
2010-06-17 10:26:06,420 INFO org.apache.hadoop.hbase.master.ServerManager: Triggering server recovery; existingServer looks stale
2010-06-17 10:26:06,420 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=store286.xyz.com,60020,1276629467680 to dead servers, added shutdown processing operation

... split log processing...

2010-06-17 10:29:51,317 DEBUG org.apache.hadoop.hbase.master.RegionServerOperation: Removed store286.xyz.com,60020,1276629467680 from deadservers Map
{code}

What follows is the relevant log snippet for one of the regions that gets double assigned.

Master tries to assign the region to store285. 

At 10:30:20,006, in ProcessRegionOpen, we update META with information about the new assignment. However, just around the same time, BaseScanner processes this entry (at 10:30:20,009), but finds that the region is still assigned to the old region server. There have been some fixes for double assignment in BaseScanner because BaseScanner might be doing a stale read depending on when it started. But looks like there is still another hole left.

{code}
2010-06-17 10:30:10,186 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. to store285.xyz.com,60020,1276629468460

2010-06-17 10:30:11,701 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_PROCESS_OPEN: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 8 of
2010-06-17 10:30:12,800 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_PROCESS_OPEN: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 7 of
2010-06-17 10:30:13,905 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_PROCESS_OPEN: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 6 of
...
2010-06-17 10:30:20,001 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 1 of 3
2010-06-17 10:30:20,001 INFO org.apache.hadoop.hbase.master.RegionServerOperation: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. open on store285.xyz.com,60020,1276629468460
2010-06-17 10:30:20,006 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. in region .META.,,1 with startcode=1276629468460, server=store285.xyz.com:60020
2010-06-17 10:30:20,009 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. is not valid;  serverAddress=store286.xyz.com:60020, startCode=1276629467680 unknown.
{code}

At this point BaseScanner calls ""this.master.getRegionManager().setUnassigned(info, true)"" to set the region to be unassigned (even though it is assigned to store285). And later, this region is given to another region server (store287).

{code}
2010-06-17 10:30:20,581 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. to store287.xyz.com,60020,1276629468678
2010-06-17 10:30:25,525 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_OPEN: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store287.xyz.com,60020,1276629468678; 6 of 6
2010-06-17 10:30:25,531 INFO org.apache.hadoop.hbase.master.RegionServerOperation: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. open on store287.xyz.com,60020,1276629468678
2010-06-17 10:30:25,534 INFO org.apache.hadoop.hbase.master.RegionServerOperation: Updated row test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. in region .META.,,1 with startcode=1276629468678, server=store287.xyz.com:60020
{code}


",streamy,kannanm,Blocker,Closed,Fixed,19/Jun/10 21:29,22/Mar/11 01:34
Bug,HBASE-2756,12467420,MetaScanner.metaScan doesn't take configurations,"HBASE-2468 added a bunch of code in MetaScanner.metaScan, and this particular line is wrong:

{code}
+    // if row is not null, we want to use the startKey of the row's region as
+    // the startRow for the meta scan.
+    if (row != null) {
+      HTable metaTable = new HTable(HConstants.META_TABLE_NAME);   <<<<<<<<<<<<<<<<<
+      Result startRowResult = metaTable.getRowOrBefore(startRow,
+          HConstants.CATALOG_FAMILY);
+      if (startRowResult == null) {
{code}

If the user specified any new configuration in his code, like ZK's parent znode, then it will miss it. This should use the HTable constructor that takes a Configuration and pass the one it already has.

I found this with my TestReplication test in HBASE-2223.",jdcryans,jdcryans,Major,Closed,Fixed,20/Jun/10 00:08,20/Nov/15 12:41
Bug,HBASE-2757,12467422,Regions not assigned after HBASE-2694 went in,"Looking into the Hudson failure, http://hudson.zones.apache.org/hudson/view/HBase/job/HBase-TRUNK/1339/ at the failing org.apache.hadoop.hbase.client.TestFromClientSide.testRegionCachePreWarm test, I seem to be seeing a case of regions being added to UNASSIGNED up in zk but then subsequently nothing.   Adjacent regions are being similarily added but these are being assigned out.  Somethings up.  It seems to be causing the above test failure.

Its hard to see in the logs.... 

Grep for this line:

    LOG.info(""Starting testRegionCachePreWarm"");

The unit test then does TEST_UTIL.createMultiRegions.

You'll see all regions being created and then they UNASSIGNED znodes are created.  Grep for the 'eee' row from testCachePrewarm table (be careful, there is also logging for testCachePrewarm2 in this log).

The last thing you'll see is:

2010-06-19 19:34:39,628 DEBUG [RegionManager.metaScanner] master.RegionManager(1006): Created UNASSIGNED zNode testCachePrewarm,eee,1276976076048.557068905bf2abfe84a5e49953a23c02. in state M2ZK_REGION_OFFLINE

For ddd and fff, you'll see those assigned out.

Maybe the test is not hanging about long enough but wonder why its skipped?",tlipcon,stack,Blocker,Closed,Fixed,20/Jun/10 02:31,20/Nov/15 12:40
Bug,HBASE-2758,12467456,META region stuck in RS2ZK_REGION_OPENED state,"In cluster testing trunk, I ended up with a situation where META was unassigned and no amount of restarting various pieces would fix it. On master startup, I see:

2010-06-20 21:08:05,431 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Current assignment of .META.,,1.1028785192 is not valid;  serverAddress=, startCode=0 unknown.
2010-06-20 21:08:05,436 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: While creating UNASSIGNED region 1028785192 exists, state = RS2ZK_REGION_OPENED
2010-06-20 21:08:05,438 WARN org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: <monster01.sf.cloudera.com:/hbase,org.apache.hadoop.hbase.master.HMaster>Failed to create ZNode /hbase/UNASSIGNED/1028785192 in ZooKeeper
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /hbase/UNASSIGNED/1028785192
2010-06-20 21:08:05,438 DEBUG org.apache.hadoop.hbase.master.RegionManager: Created UNASSIGNED zNode .META.,,1.1028785192 in state M2ZK_REGION_OFFLINE

then on the RS:
2010-06-20 21:08:05,899 ERROR org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater: ZNode /hbase/UNASSIGNED/1028785192 is not in CLOSED/OFFLINE state (state = RS2ZK_REGION_OPENED), will NOT open region.
2010-06-20 21:08:05,899 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Error opening .META.,,1.1028785192
java.io.IOException: ZNode /hbase/UNASSIGNED/1028785192 is not in CLOSED/OFFLINE state (state = RS2ZK_REGION_OPENED), will NOT open region.

and the region never opens
",karthik.ranga,tlipcon,Blocker,Closed,Fixed,21/Jun/10 04:16,20/Nov/15 12:43
Bug,HBASE-2760,12467459,MetaScanner throws TableNotFoundException when specifying empty start row of a table,"Getting errors like this:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: VerifiableEditor, row=VerifiableEditor,,00000000000000
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:104)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.prefetchRegionCache(HConnectionManager.java:733)
META contains:
{code}
hbase(main):001:0> scan '.META.'
ROW                                                      COLUMN+CELL                                                                                                                                                        
 VerifiableEditor,,1277097543936.6a2992842d685f48213bc33 column=info:regioninfo, timestamp=1277097544071, value=REGION => {NAME => 'VerifiableEditor,,1277097543936.6a2992842d685f48213bc33afa85ac94.', STARTKEY => '', ENDK
 afa85ac94.                                              EY => '', ENCODED => 6a2992842d685f48213bc33afa85ac94, TABLE => {{NAME => 'VerifiableEditor', FAMILIES => [{NAME => 'info', BLOOMFILTER => 'NONE', REPLICATION_SCOP
                                                         E => '0', COMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                        
 VerifiableEditor,,1277097543936.6a2992842d685f48213bc33 column=info:server, timestamp=1277097544317, value=monster04.sf.cloudera.com:60020                                                                                 
 afa85ac94.                                                                                                                                                                                                                 
 VerifiableEditor,,1277097543936.6a2992842d685f48213bc33 column=info:serverstartcode, timestamp=1277097544317, value=1277097337799                                                                                          
 afa85ac94.                                                                                                                                                                                                                 
1 row(s) in 0.2660 seconds
{code}",tlipcon,tlipcon,Critical,Closed,Fixed,21/Jun/10 05:23,20/Nov/15 12:41
Bug,HBASE-2763,12467527,Cross-port HADOOP-6833 IPC parameter leak bug,There's a bug where any RPC call that throws an exception ends up leaking the parameter objects of that call. This was introduced by HBASE-2360,tlipcon,tlipcon,Blocker,Closed,Fixed,22/Jun/10 02:42,20/Nov/15 12:41
Bug,HBASE-2767,12467617,setMaxRecoveryErrorCount reflection fails after HDFS-1209,HDFS-1209 adds a configuration parameter for changing the number of recovery retries. This has been applied in CDH3b2 and about to be applied in 0.20-append. HBaseTestingUtility.setMaxRecoveryErrorCount() fails once this patch has been applied.,tlipcon,tlipcon,Major,Closed,Fixed,22/Jun/10 20:42,20/Nov/15 12:43
Bug,HBASE-2769,12467630,Trivial typo in HBaseConfiguration deprecation message,"""instantinating"" for ""instantiating"" - this has been annoying my OCD side.",tlipcon,tlipcon,Trivial,Closed,Fixed,22/Jun/10 21:54,20/Nov/15 12:42
Bug,HBASE-2772,12467640,Scan doesn't recover from region server failure,"Very simple, if the region server you are scanning from dies for any reason, your scanner will die too because it will not try to get a new lease and will reuse the same id.

It doesn't happen to region that moves because we keep them opened until the scanner is closed.

It could be affecting branch, I'm just not sure yet.

I found this issue with TestReplication from HBASE-2223.",jdcryans,jdcryans,Major,Closed,Fixed,22/Jun/10 23:07,12/Oct/12 06:16
Bug,HBASE-2774,12467664,Spin in ReadWriteConsistencyControl eating CPU (load > 40) and no progress running YCSB on clean cluster startup,"
When I try to do a YCSB load, RSs will spin up massive load but make no progress.  Seems to happen to each RS in turn until they do their first flush.  They stay in the high-load mode for maybe 5-10 minutes or so and then fall out of the bad condition.

Here is my ugly YCSB command (Haven't gotten around to tidying it up yet):

{code}
$ java -cp build/ycsb.jar:/home/hadoop/current/conf/:/home/hadoop/current/hbase-0.21.0-SNAPSHOT.jar:/home/hadoop/current/lib/hadoop-core-0.20.3-append-r956776.jar:/home/hadoop/current/lib/zookeeper-3.3.1.jar:/home/hadoop/current/lib/commons-logging-1.1.1.jar:/home/hadoop/current/lib/log4j-1.2.15.jar  com.yahoo.ycsb.Client -load -db com.yahoo.ycsb.db.HBaseClient  -P workloads/5050 -p columnfamily=values -s -threads 100 -p recordcount=10000000
{code}

Cluster is 5 regionservers NOT running hadoop-core-0.20.3-append-r956776 but rather old head of branch-0.20 hadoop.

It seems that its easy to repro if you start fresh.  It might happen later in loading but it seems as though after first flush, we're ok.

It comes on pretty immediately.  The server that is taking on the upload has its load start to climb gradually up into the 40s then stays there.  Later it falls when condtion clears.

Here is content of my yahoo workload file:

{code}
recordcount=100000000
operationcount=100000000
workload=com.yahoo.ycsb.workloads.CoreWorkload

readallfields=true

readproportion=0.5
updateproportion=0.5
scanproportion=0
insertproportion=0

requestdistribution=zipfian
{code}

Here is my hbase-site.xml

{code}
  <property>
    <name>hbase.regions.slop</name>
    <value>0.01</value>
    <description>Rebalance if regionserver has average + (average * slop) regions.
    Default is 30% slop.
    </description>
  </property>

  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>XXXXXXXXX</value>
  </property>

<property>
  <name>hbase.regionserver.hlog.blocksize</name>
  <value>67108864</value>
  <description>Block size for HLog files. To minimize potential data loss,
    the size should be (avg key length) * (avg value length) * flushlogentries.
    Default 1MB.
  </description>
</property>

<property>
  <name>hbase.hstore.blockingStoreFiles</name>
  <value>25</value>
</property>

<property>
  <name>hbase.rootdir</name>
  <value>hdfs://svXXXXXX:9000/hbase</value>
  <description>The directory shared by region servers.</description>
</property>

<property>
  <name>hbase.cluster.distributed</name>
  <value>true</value>
</property>

<property>
  <name>zookeeper.znode.parent</name>
  <value>/stack</value>
  <description>
    the path in zookeeper for this cluster
  </description>
</property>

<property>
  <name>hfile.block.cache.size</name>
  <value>0.2</value>
  <description>
    The size of the block cache used by HFile/StoreFile. Set to 0 to disable.
  </description>
</property>


<property>
  <name>hbase.hregion.memstore.block.multiplier</name>
  <value>8</value>
  <description>
    Block updates if memcache has hbase.hregion.block.memcache
    time hbase.hregion.flush.size bytes.  Useful preventing
    runaway memcache during spikes in update traffic.  Without an
    upper-bound, memcache fills such that when it flushes the
    resultant flush files take a long time to compact or split, or
    worse, we OOME.
  </description>
</property>

<property>
<name>zookeeper.session.timeout</name>
<value>60000</value>
</property>


<property>
  <name>hbase.regionserver.handler.count</name>
  <value>60</value>
  <description>Count of RPC Server instances spun up on RegionServers
    Same property is used by the HMaster for count of master handlers.
    Default is 10.
  </description>
</property>

<property>
    <name>hbase.regions.percheckin</name>
    <value>20</value>
</property>

<property>
    <name>hbase.regionserver.maxlogs</name>
    <value>128</value>
</property>

<property>
    <name>hbase.regionserver.logroll.multiplier</name>
    <value>2.95</value>
</property>
{code}
",,stack,Major,Closed,Fixed,23/Jun/10 06:14,20/Nov/15 12:40
Bug,HBASE-2775,12467665,Update of hadoop jar in HBASE-2771 broke TestMultiClusters,TestMultiClusters failing following HBASE-2771,jdcryans,streamy,Blocker,Closed,Fixed,23/Jun/10 06:21,20/Nov/15 12:41
Bug,HBASE-2781,12467760,ZKW.createUnassignedRegion doesn't make sure existing znode is in the right state,"In ZKW.createUnassignedRegion I see this comment:

{code}
      // check if this node already exists - 
      //   - it should not exist
      //   - if it does, it should be in the CLOSED state
{code}

And what I got is:

{noformat}
2010-06-23 15:42:05,823 INFO  [IPC Server handler 3 on 60362] master.ServerManager(457): Processing MSG_REPORT_PROCESS_OPEN: test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. from h136.sfo.stumble.net,60365,1277332849712; 1 of 4
2010-06-23 15:42:05,867 INFO  [RegionServer:1.worker] regionserver.HRegionServer$Worker(1338): Worker: MSG_REGION_OPEN: test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:05,870 DEBUG [RegionServer:1.worker] regionserver.RSZookeeperUpdater(157): Updating ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 with [RS2ZK_REGION_OPENING] expected version = 0
2010-06-23 15:42:05,871 DEBUG [main-EventThread] master.HMaster(1158): Event NodeDataChanged with state SyncConnected with path /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:05,871 DEBUG [main-EventThread] master.ZKMasterAddressWatcher(64): Got event NodeDataChanged with path /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:05,871 DEBUG [main-EventThread] master.ZKUnassignedWatcher(95): ZK-EVENT-PROCESS: Got zkEvent NodeDataChanged state:SyncConnected path:/1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:05,872 INFO  [main-EventThread] regionserver.HRegionServer(379): Got ZooKeeper event, state: SyncConnected, type: NodeDataChanged, path: /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:05,872 DEBUG [MASTER_OPENREGION-10.10.1.136:60362-1] handler.MasterOpenRegionHandler(77): Event = RS2ZK_REGION_OPENING, region = 13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:05,874 DEBUG [RegionServer:1.worker] regionserver.HRegion(297): Creating region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:06,154 INFO  [RegionServer:1.worker] regionserver.HRegion(366): Onlined test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.; next sequenceid=1
2010-06-23 15:42:06,154 DEBUG [RegionServer:1.worker] regionserver.RSZookeeperUpdater(157): Updating ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 with [RS2ZK_REGION_OPENED] expected version = 1\
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:06,249 ERROR [RegionServer:1.worker] regionserver.HRegionServer(1488): Failed to mark region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. as opened
java.io.IOException: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
Caused by: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:06,993 DEBUG [RegionServer:1] regionserver.HRegionServer(1569): closing region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:06,993 DEBUG [RegionServer:1] regionserver.HRegion(487): Closing test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.: disabling compactions & flushes
2010-06-23 15:42:06,993 DEBUG [RegionServer:1] regionserver.HRegion(512): Updates disabled for region, no outstanding scanners on test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:06,993 DEBUG [RegionServer:1] regionserver.HRegion(519): No more row locks outstanding on region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:06,994 INFO  [RegionServer:1] regionserver.HRegion(531): Closed test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:09,105 INFO  [master] master.ProcessServerShutdown(126): Region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. was in transition 
name=test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464., state=PENDING_OPEN on dead server h136.sfo.stumble.net,60365,1277332849712 - marking unassigned
2010-06-23 15:42:10,065 INFO  [IPC Server handler 2 on 60362] master.RegionManager(340): Assigning region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. to h136.sfo.stumble.net,60363,1277332849671
2010-06-23 15:42:10,067 DEBUG [IPC Server handler 2 on 60362] zookeeper.ZooKeeperWrapper(1079): While creating UNASSIGNED region 13bef4950ac6827ac32d87682b8b2464 exists, state = RS2ZK_REGION_OPENING
2010-06-23 15:42:10,126 WARN  [IPC Server handler 2 on 60362] zookeeper.ZooKeeperWrapper(1024): <localhost:/1,org.apache.hadoop.hbase.master.HMaster>Failed to create ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 in ZooKeeper
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:10,127 DEBUG [IPC Server handler 2 on 60362] master.RegionManager(350): Created UNASSIGNED zNode test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. in state M2ZK_REGION_OFFLINE
2010-06-23 15:42:10,245 INFO  [RegionServer:0] regionserver.HRegionServer(511): MSG_REGION_OPEN: test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:11,248 INFO  [IPC Server handler 1 on 60362] master.ServerManager(457): Processing MSG_REPORT_PROCESS_OPEN: test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. from h136.sfo.stumble.net,60363,1277332849671; 7 of 13
2010-06-23 15:42:13,795 INFO  [RegionServer:0.worker] regionserver.HRegionServer$Worker(1338): Worker: MSG_REGION_OPEN: test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
2010-06-23 15:42:13,797 ERROR [RegionServer:0.worker] regionserver.RSZookeeperUpdater(107): ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 is not in CLOSED/OFFLINE state (state = RS2ZK_REGION_OPENING), will NOT open region.
2010-06-23 15:42:13,798 ERROR [RegionServer:0.worker] regionserver.HRegionServer(814): Error opening test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
java.io.IOException: ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 is not in CLOSED/OFFLINE state (state = RS2ZK_REGION_OPENING), will NOT open region.
2010-06-23 15:42:13,800 ERROR [RegionServer:0.worker] regionserver.RSZookeeperUpdater(141): Aborting open of region 13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:13,800 DEBUG [RegionServer:0.worker] regionserver.RSZookeeperUpdater(157): Updating ZNode /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464 with [RS2ZK_REGION_CLOSED] expected version = 0
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
2010-06-23 15:42:13,802 ERROR [RegionServer:0.worker] regionserver.HRegionServer(1473): Failed to abort open region test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464.
java.io.IOException: org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
Caused by: org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /1/UNASSIGNED/13bef4950ac6827ac32d87682b8b2464
{noformat}

Basically:
 # A region server was opening the region
 # It was expired just before reporting that the region is opened, leaving the znode in the state RS2ZK_REGION_OPENING
 # The region gets reassigned, it sees that state, doesn't change it, but still outputs in the end ""Created UNASSIGNED zNode test,lll,1277332918248.13bef4950ac6827ac32d87682b8b2464. in state M2ZK_REGION_OFFLINE""
 # When the region server opens the region, it sees that the state is wrong and aborts opening the region

I think that the way to fix it is to change the state to what it should be.",karthik.ranga,jdcryans,Critical,Closed,Fixed,23/Jun/10 23:43,20/Nov/15 12:44
Bug,HBASE-2785,12467817,TestScannerTimeout.test2772 is flaky,I knew that that test could be flaky but it seemed to work fine for a while. Basically if the region server takes too long to abort (more than 6 seconds) then the client will detect that the scanner expired instead of just moving. Find a less flaky way.,jdcryans,jdcryans,Major,Closed,Fixed,24/Jun/10 16:26,20/Nov/15 12:44
Bug,HBASE-2786,12467825,TestHLog.testSplit hangs,"This a blocker had it blocks and times out Hudson.

It seems that when we upgraded to latest of 0.20-append we got into a new situation where we can't recover a file that's empty if the original writer is still alive:

{noformat}
2010-06-24 10:41:20,645 DEBUG [main] wal.HLog(1281): Splitting hlog 4 of 4: hdfs://localhost:64456/hbase/testSplit/.logs/hlog.1277401279534, length=0
2010-06-24 10:41:20,645 INFO  [main] util.FSUtils(612): Recovering filehdfs://localhost:64456/hbase/testSplit/.logs/hlog.1277401279534
2010-06-24 10:41:20,647 WARN  [IPC Server handler 5 on 64456] namenode.FSNamesystem(1156): DIR* NameSystem.startFile: 
failed to create file /hbase/testSplit/.logs/hlog.1277401279534 for DFSClient_-1981892617 on client 127.0.0.1 because current leaseholder is trying to recreate file
...

2010-06-24 10:42:24,919 WARN  [IPC Server handler 0 on 64456] namenode.FSNamesystem(1156): DIR* NameSystem.startFile: 
failed to create file /hbase/testSplit/.logs/hlog.1277401279534 for DFSClient_-1981892617 on client 127.0.0.1 because current leaseholder is trying to recreate file.
2010-06-24 10:42:24,919 WARN  [main] util.FSUtils(631): Waited 64274ms for lease recovery on 
hdfs://localhost:64456/hbase/testSplit/.logs/hlog.1277401279534:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: 
failed to create file /hbase/testSplit/.logs/hlog.1277401279534 for DFSClient_-1981892617 on client 127.0.0.1 because current leaseholder is trying to recreate file.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1058)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1171)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.append(NameNode.java:396)
{noformat}

We could just not roll the latest log and it would probably fix the issue, but I wonder if we could change something in HDFS instead. Todd?",nspiegelberg,jdcryans,Major,Closed,Fixed,24/Jun/10 18:15,02/May/13 02:29
Bug,HBASE-2787,12467842,PE is confused about flushCommits,"flushCommits is currently false by default, oh and:

{code}
        final String flushCommits = ""--flushCommits="";
        if (cmd.startsWith(flushCommits)) {
          this.flushCommits = Boolean.parseBoolean(cmd.substring(flushCommits.length()));
          continue;
        }

        final String writeToWAL = ""--writeToWAL="";
        if (cmd.startsWith(writeToWAL)) {
          this.flushCommits = Boolean.parseBoolean(cmd.substring(writeToWAL.length()));
          continue;
        }
{code}",jdcryans,jdcryans,Major,Closed,Fixed,24/Jun/10 23:39,20/Nov/15 12:41
Bug,HBASE-2797,12467983,Another NPE in ReadWriteConsistencyControl,"This occurred on a cluster with 46 slaves, running a couple MR jobs.  One doing heavy writes copying everything from one table to a new table with a different schema.  After one regionserver went down, about 40 of them died within an hour before it was caught and the jobs stopped.  Let me know if any other piece of context would be particularly helpful.

This exception appears in the .out file:

Exception in thread ""regionserver/192.168.41.2:60020"" java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.ReadWriteConsistencyControl.getThreadReadPoint(ReadWriteConsistencyControl.java:40)
        at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.getNext(MemStore.java:532)
        at org.apache.hadoop.hbase.regionserver.MemStore$MemStoreScanner.seek(MemStore.java:558)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:320)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.checkReseek(StoreScanner.java:306)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.peek(StoreScanner.java:143)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:127)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:117)
        at java.util.PriorityQueue.siftDownUsingComparator(PriorityQueue.java:644)
        at java.util.PriorityQueue.siftDown(PriorityQueue.java:612)
        at java.util.PriorityQueue.poll(PriorityQueue.java:523)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.close(KeyValueHeap.java:151)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.close(HRegion.java:1971)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.closeAllRegions(HRegionServer.java:1610)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:621)
        at java.lang.Thread.run(Thread.java:619)",ryanobjc,davelatham,Blocker,Closed,Fixed,27/Jun/10 03:42,12/Oct/12 06:16
Bug,HBASE-2799,12468015,"""Append not enabled"" warning should not show if hbase root dir isn't on DFS","HBASE-2762 added a warning on the master UI if append isn't enabled. However, when HBase is in standalone mode and using a file:/// rootdir, it shows the warning, which doesn't make sense.",stack,tlipcon,Minor,Closed,Fixed,28/Jun/10 06:07,20/Nov/15 12:43
Bug,HBASE-2803,12468173,"Remove remaining Get code from Store.java,etc","There is still remaining Get code due to HBASE-2248, remove it!",ryanobjc,ryanobjc,Major,Closed,Fixed,29/Jun/10 16:29,20/Nov/15 12:40
Bug,HBASE-2806,12468274,DNS hiccups cause uncaught NPE in HServerAddress#getBindAddress,"Yesterday at the Hadoop Summit, the ""HADOOP"" wireless network was using a pair of DNS servers that couldn't resolve {{localhost.}}.  This prevented me from starting HBase as the construction of the {{HMaster}} was failing with the following rather cryptic error:
{code}
2010-06-29 14:30:24,603 ERROR org.apache.hadoop.hbase.master.HMaster: Failed to start master
java.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMaster$LocalHMasternull
        at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:1217)
        at org.apache.hadoop.hbase.LocalHBaseCluster.<init>(LocalHBaseCluster.java:112)
        at org.apache.hadoop.hbase.master.HMaster.doMain(HMaster.java:1298)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1355)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:1215)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.HServerAddress.getBindAddress(HServerAddress.java:89)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:204)
        at org.apache.hadoop.hbase.master.HMaster$LocalHMaster.<init>(HMaster.java:1230)
        ... 8 more
{code}
The {{NullPointerException}} in {{getBindAddress}} comes from the following line of code:
{code:java}
    return this.address.getAddress().getHostAddress();
{code}
where {{getAddress()}} was returning {{null}}.

I think the code should check for this case, log an appropriate error message (to point whoever is going to troubleshoot the problem in the right direction), and throw something else than an NPE.",tsuna,tsuna,Minor,Closed,Fixed,30/Jun/10 18:40,20/Nov/15 12:43
Bug,HBASE-2812,12468502,Disable 'table' fails to complete frustrating my ability to test easily,"I see this in the client after it gives up:

hbase(main):006:0> disable 'test_schema'

ERROR: org.apache.hadoop.hbase.RegionException: Retries exhausted, it took too long to wait for the table test_schema to be disabled.

Here is some help for this command:
          Disable the named table: e.g. ""hbase> disable 't1'""

and this in the server log, a set of about 5 reports it is closing per disable call:

2010-07-03 15:19:47,554 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:47,554 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:47,555 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:47,576 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:47,576 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:48,567 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:48,567 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:48,568 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:48,577 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:48,578 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:49,580 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:49,580 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:49,581 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:50,580 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:50,581 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:50,592 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:50,592 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:50,593 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:51,581 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:51,581 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:52,605 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:52,605 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:52,606 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:52,703 INFO org.apache.hadoop.hbase.master.ServerManager: 1 region servers, 0 dead, average load 3.0
2010-07-03 15:19:52,863 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>}
2010-07-03 15:19:52,867 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>} complete
2010-07-03 15:19:53,585 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:53,585 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:53,671 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=1.6292953MB (1708440), Free=196.70822MB (206263512), Max=198.33751MB (207971952), Counts: Blocks=2, Access=52, Hit=50, Miss=2, Evictions=0, Evicted=0, Ratios: Hit Ratio=96.15384340286255%, Miss Ratio=3.8461539894342422%, Evicted/Run=NaN
2010-07-03 15:19:54,617 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:54,617 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:54,618 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:54,821 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>}
2010-07-03 15:19:54,825 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 2 row(s) of meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>} complete
2010-07-03 15:19:54,825 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
2010-07-03 15:19:55,589 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:55,589 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:58,629 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing unserved regions
2010-07-03 15:19:58,629 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Processing regions currently being served
2010-07-03 15:19:58,630 DEBUG org.apache.hadoop.hbase.master.ChangeTableState: Adding region test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081. to setClosing list
2010-07-03 15:19:59,595 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:19:59,595 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_CLOSE: test_schema,,1278195322074.65c77aedf2f2a08d161a188dd2dd5081.
2010-07-03 15:20:52,706 INFO org.apache.hadoop.hbase.master.ServerManager: 1 region servers, 0 dead, average load 3.0
2010-07-03 15:20:52,866 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>}
2010-07-03 15:20:52,873 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>} complete
2010-07-03 15:20:53,671 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=1.6292953MB (1708440), Free=196.70822MB (206263512), Max=198.33751MB (207971952), Counts: Blocks=2, Access=54, Hit=52, Miss=2, Evictions=0, Evicted=0, Ratios: Hit Ratio=96.29629850387573%, Miss Ratio=3.7037037312984467%, Evicted/Run=NaN
2010-07-03 15:20:54,823 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>}
2010-07-03 15:20:54,828 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 2 row(s) of meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>} complete
2010-07-03 15:20:54,828 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
2010-07-03 15:21:52,709 INFO org.apache.hadoop.hbase.master.ServerManager: 1 region servers, 0 dead, average load 3.0
2010-07-03 15:21:52,869 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>}
2010-07-03 15:21:52,876 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 192.168.2.1:54389, regionname: -ROOT-,,0.70236052, startKey: <>} complete
2010-07-03 15:21:53,672 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=1.6292953MB (1708440), Free=196.70822MB (206263512), Max=198.33751MB (207971952), Counts: Blocks=2, Access=56, Hit=54, Miss=2, Evictions=0, Evicted=0, Ratios: Hit Ratio=96.42857313156128%, Miss Ratio=3.57142873108387%, Evicted/Run=NaN
2010-07-03 15:21:54,826 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scanning meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>}
2010-07-03 15:21:54,830 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 2 row(s) of meta region {server: 192.168.2.1:54389, regionname: .META.,,1.1028785192, startKey: <>} complete
2010-07-03 15:21:54,830 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
",,spullara,Major,Closed,Fixed,03/Jul/10 22:23,11/Jun/22 23:55
Bug,HBASE-2815,12468659,not able to run the test suite in background because TestShell gets suspended on tty output,"Can't run the test suite in background. Problem seems to be due to TestShell.

This works fine:
{code}
% mvn test -Dtest=TestShell -Dtest.output=true
{code}

But:
{code}
% mvn test -Dtest=TestShell -Dtest.output=true & 
or,
% mvn test -Dtest=TestShell -Dtest.output=true >& test.log &
{code}

causes test to hang, and eventually timeout after 3600 seconds.

The process is reported as being suspended on tty output.

[3]  + Suspended (tty output)        mvn test -Dtest=TestShell -Dtest.output=true
",kovyrin,kannanm,Major,Closed,Fixed,06/Jul/10 18:13,20/Nov/15 12:42
Bug,HBASE-2820,12468689,hbck throws an error if hbase root dir isn't on default FS,"""Wrong FS"" exception gets thrown since we construct the default FS instead of the one from rootdir",tlipcon,tlipcon,Trivial,Closed,Fixed,07/Jul/10 04:58,20/Nov/15 12:43
Bug,HBASE-2823,12468848,Entire Row Deletes not stored in Row+Col Bloom,"If the user issues a Row Delete on an family with Row+Col blooms, that information is not currently detected by shouldSeek().  Possible known solutions are:

1. adding Row as Bloom Filter Key on Row Delete, shouldSeek() should do both a Row & Row+Col query for Row+Col filters.
2. keep delete information in a separate storage element.

#1 seems like the best solution, but need to investigate further and fix this problem.",espr1t,nspiegelberg,Major,Closed,Fixed,08/Jul/10 20:25,20/Nov/15 12:41
Bug,HBASE-2825,12468934,Scans respect row locks,"In the javadoc Package org.apache.hadoop.hbase.client Description it states that ""Scans (currently) operate without respect for row locks."" I think that since 0.20.4 and HBASE-2248 this is no longer the case.",ryanobjc,bernardos,Minor,Closed,Fixed,09/Jul/10 15:48,20/Nov/15 12:41
Bug,HBASE-2828,12468963,HTable unnecessarily coupled with HMaster,"HTable constructor calls ""getCurrentNrHRS()"" to get the region server count for thread pool creation.  This code calls HBaseAdmin.getClusterStatus() [aka: the HMaster] to get the server count.  This information can be scraped from counting the ZooKeeper /hbase/rs/--- ZNodes.  Need to remove unnecessary master queries when ZooKeeper can do the same job.",nspiegelberg,nspiegelberg,Major,Closed,Fixed,09/Jul/10 23:52,20/Nov/15 12:43
Bug,HBASE-2831,12469133,Fix '$bin' path duplication in setup scripts,"I have my bash environment setup to echo absolute pathnames when a relative one is specified in 'cd'. This caused problems with all the Hadoop bash scripts because the script accidentally sets the $bin variable twice in this setup. (e.g. would set $bin=""/path/bin/hbase\n/path/bin/hbase"")

This jira is for HBase scripts. I filed a separate jira for HDFS scripts, which share the same pattern.",nspiegelberg,nspiegelberg,Trivial,Closed,Fixed,12/Jul/10 23:35,20/Nov/15 12:40
Bug,HBASE-2837,12469363,HBASE-2613 broke rolling upgrades,"HBASE-2613 broke rolling upgrades because it removed MSG_CALL_SERVER_STARTUP from the HMsg.Type enum. Even if it won't be used anywhere, we should put it back so that people can do rolling upgrades.",jdcryans,jdcryans,Blocker,Closed,Fixed,15/Jul/10 17:46,12/Oct/12 06:16
Bug,HBASE-2840,12469506,Remove the final remnants of the old Get code - the query matchers and other helper classes,"even though we no longer call the old Get code, there are QueryMatchers and other accessory classes which used to be necessary but are no longer.",,ryanobjc,Major,Closed,Fixed,16/Jul/10 23:12,20/Nov/15 12:41
Bug,HBASE-2843,12469511,Re-add bloomfilter test over-zealously removed by HBASE-2625 ,I removed TestByteBloomFilter when I shouldn't have when I removed all related to unused dynamic bloomfilters.  Readd.,stack,stack,Major,Closed,Fixed,17/Jul/10 00:34,20/Nov/15 12:40
Bug,HBASE-2846,12469553,Make rest server be same as thrift and avro servers,,stack,stack,Major,Closed,Fixed,18/Jul/10 05:18,20/Nov/15 12:43
Bug,HBASE-2849,12469634,HBase clients cannot recover when their ZooKeeper session becomes invalid,"Someone made mention of this loop last week but I don't think I filed an issue.  Here is another instance, again from a secret hbase admirer:

""It seems that when Zookeeper dies and restarts, all client applications need to be restarted too. I just restarted HBase in non-distributed mode (which includes a ZK) and now my application can't reconnect to ZK unless I restart it too.  I'm stuck in this loop:

{code}
2010-07-19 00:13:05,725 INFO org.apache.zookeeper.server.NIOServerCnxn:
  Closed socket connection for client /127.0.0.1:55153 (no session established for client)
2010-07-19 00:13:07,052 INFO org.apache.zookeeper.server.NIOServerCnxn:
  Accepted socket connection from /127.0.0.1:55154
2010-07-19 00:13:07,053 INFO org.apache.zookeeper.server.NIOServerCnxn:
  Refusing session request for client /127.0.0.1:55154 as it has seen zxid 0xf5 our last zxid is 0xd7
  client must try another server
{code}
""",tsuna,stack,Critical,Closed,Fixed,19/Jul/10 16:52,20/Nov/15 12:40
Bug,HBASE-2851,12469668,Remove testDynamicBloom() unit test,"HBASE-2843 added TestByteBloomFilter back, however the reason for removing it was spotty test results for testDynamicBloom(), which tests a class that we don't use anyways.  Need to remove the offending test.",nspiegelberg,nspiegelberg,Trivial,Closed,Fixed,19/Jul/10 22:40,20/Nov/15 12:40
Bug,HBASE-2852,12469674,Bloom filter NPE,"When a rowcol Bloom filter is being used and the user submits a query for all columns, a null pointer exception is thrown. This is because there is no checking if columns have been specified or not.",pranavkhaitan,pranavkhaitan,Major,Closed,Fixed,20/Jul/10 01:02,20/Nov/15 12:41
Bug,HBASE-2853,12469682,TestLoadIncrementalHFiles fails on TRUNK,"Its writing KVs w/ LATEST_TIMESTAMP. Here's fix:

{code}
pynchon-56:hbase stack$ git diff
diff --git a/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java b/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
index d5374d2..e02d11a 100644
--- a/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
+++ b/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
@@ -175,10 +175,11 @@ public class TestLoadIncrementalHFiles {
   {
     HFile.Writer writer = new HFile.Writer(fs, path, BLOCKSIZE, COMPRESSION,
         KeyValue.KEY_COMPARATOR);
+    long now = System.currentTimeMillis();
     try {
       // subtract 2 since iterateOnSplits doesn't include boundary keys
       for (byte[] key : Bytes.iterateOnSplits(startKey, endKey, numRows-2)) {
-        KeyValue kv = new KeyValue(key, family, qualifier, key);
+        KeyValue kv = new KeyValue(key, family, qualifier, now, key);
         writer.append(kv);
       }
     } finally {
{code}",stack,stack,Major,Closed,Fixed,20/Jul/10 05:30,20/Nov/15 12:41
Bug,HBASE-2854,12469685,broken tests on trunk,"the following tests are broken:


>>><<< org.apache.hadoop.hbase.regionserver.TestStore.testIncrementColumnValue_ICVDuringFlush

All of the TestQueryMatcher tests.
",,ryanobjc,Major,Closed,Fixed,20/Jul/10 06:58,20/Nov/15 12:41
Bug,HBASE-2856,12469746,TestAcidGuarantee broken on trunk ,"TestAcidGuarantee has a test whereby it attempts to read a number of columns from a row, and every so often the first column of N is different, when it should be the same.  This is a bug deep inside the scanner whereby the first peek() of a row is done at time T then the rest of the read is done at T+1 after a flush, thus the memstoreTS data is lost, and previously 'uncommitted' data becomes committed and flushed to disk.

One possible solution is to introduce the memstoreTS (or similarly equivalent value) to the HFile thus allowing us to preserve read consistency past flushes.  Another solution involves fixing the scanners so that peek() is not destructive (and thus might return different things at different times alas).

",amitanand,ryanobjc,Blocker,Closed,Fixed,20/Jul/10 19:28,12/Oct/12 05:35
Bug,HBASE-2858,12469768,TestReplication.queueFailover fails half the time,"TestReplication.queueFailover fails 50% of the time, it's because ZooKeeperWrapper.listZnodes (introduced in HBASE-2694 and missed by HBASE-2735) doesn't use the Watcher it's passed so sometimes ReplicationSource misses hlogs to replicate for the region server we kill. Also it uncovered an issue (while I was fixing the first one) that RepSource ignores log files too quickly when the master is a bit too slow to split logs.",jdcryans,jdcryans,Major,Closed,Fixed,21/Jul/10 00:23,20/Nov/15 12:44
Bug,HBASE-2861,12469845,regionserver's logsyncer thread hangs on DFSClient$DFSOutputStream.waitForAckedSeqno,"During loads into HBase, we are noticing that a RS is sometimes getting stuck.

The logSyncer thread:

{code}
      at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.waitForAckedSeqno(DFSClient.java:3367)
        - locked <0x00002aaac7fef748> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3301)
        at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
        at org.apache.hadoop.io.SequenceFile$Writer.syncFs(SequenceFile.java:944)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.sync(SequenceFileLogWriter.java:124)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.hflush(HLog.java:949)
{code}

A lot of other threads are stuck on:

{code}
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.addToSyncQueue(HLog.java:916)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:936)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.append(HLog.java:828)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1657)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1425)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1393)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1665)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multiPut(HRegionServer.java:2326)
{code}

Subsequently, trying to disable the table, which in turn attempts to close the region(s), caused internalFlushCache() also to get stuck here:

{code}
      at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:807)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:974)
        at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:511)
        - locked <0x00002aaab76af670> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:463)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:1468)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:1329)
{code}

I'll attach the full jstack trace soon.


",hairong,kannanm,Critical,Closed,Fixed,21/Jul/10 18:49,20/Nov/15 12:43
Bug,HBASE-2863,12469888,HBASE-2553 removed an important edge case ,"in HBASE-2553 an important edge case whereby a KV with the same TS in snapshot was lost, tests have been failing (but flakly so) indicating it as well.",ryanobjc,ryanobjc,Major,Closed,Fixed,22/Jul/10 06:11,20/Nov/15 12:41
Bug,HBASE-2866,12469949,Region permanently offlined ,"After split, master attempts to reassign a region to a region server. Occasionally, such a region can get permanently offlined.


Master:
---------
{code}
2010-07-22 01:26:00,914 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT_INCLUDES_DAUGHTERS: test1,6512200000,1279784117114.6466481aa931f8c1fa87622735487a72.: Daughters; test1,6512200000,1279787158624.6ead25ae677116cc88fc5420bb39d52e., test1,6531790000,1279787\
158624.8d5490bfc166c687657cb09203bd7d44. from test024.test.xyz.com,60020,1279780567744; 1 of 1                                                                                                                                                                                                     
2010-07-22 01:26:00,935 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Creating UNASSIGNED region 8d5490bfc166c687657cb09203bd7d44 in state = M2ZK_REGION_OFFLINE

2010-07-22 01:26:00,935 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Creating UNASSIGNED region 8d5490bfc166c687657cb09203bd7d44 in state = M2ZK_REGION_OFFLINE

2010-07-22 01:26:00,945 INFO org.apache.hadoop.hbase.master.RegionManager: Assigning region test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44. to test024.test.xyz.com,60020,1279780567744

2010-07-22 01:26:00,949 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: While updating UNASSIGNED region 8d5490bfc166c687657cb09203bd7d44 exists, state = M2ZK_REGION_OFFLINE

2010-07-22 01:26:00,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Created UNASSIGNED zNode test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44. in state M2ZK_REGION_OFFLINE
{code}

-------------------

Region Server:

{code}
2010-07-22 01:26:00,947 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44.
2010-07-22 01:26:00,947 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: test1,6512200000,1279787158624.6ead25ae677116cc88fc5420bb39d52e.
2010-07-22 01:26:00,947 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44.
2010-07-22 01:26:00,948 DEBUG org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater: Updating ZNode /hbase/UNASSIGNED/8d5490bfc166c687657cb09203bd7d44 with [RS2ZK_REGION_OPENING] expected version = 0
2010-07-22 01:26:00,952 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Got ZooKeeper event, state: SyncConnected, type: NodeDataChanged, path: /hbase/UNASSIGNED/8d5490bfc166c687657cb09203bd7d44
2010-07-22 01:26:00,974 WARN org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: <msgstorectrl001.test.xyz.com,msgstorectrl021.test.xyz.com,msgstorectrl041.test.xyz.com,msgstorectrl061.test.xyz.com,msgstorectrl081.ash2.facebook\
.com:/hbase,test024.test.xyz.com,60020,1279780567744>Failed to write data to ZooKeeper
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/UNASSIGNED/8d5490bfc166c687657cb09203bd7d44
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:106)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1038)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.writeZNode(ZooKeeperWrapper.java:1062)
        at org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater.updateZKWithEventData(RSZookeeperUpdater.java:161)
        at org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater.startRegionOpenEvent(RSZookeeperUpdater.java:115)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:1428)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:1337)
        at java.lang.Thread.run(Thread.java:619)
2010-07-22 01:26:00,975 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Error opening test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44.
java.io.IOException: org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/UNASSIGNED/8d5490bfc166c687657cb09203bd7d44
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.writeZNode(ZooKeeperWrapper.java:1072)
{code}

Meta:
-----

Relevant section of META.

Note that these are the only two entries for the problem region. The first one is the parent region (and this problem
region is its splitB).  For the next one, note that there is no ""info:server"" and ""info:serverstartcode"" columns.

{code}
 test1,6512200000,12797841 column=info:splitB, timestamp=1279787160693, value=\x00\x0A6551820000\x00
 17114.6466481aa931f8c1fa8 \x00\x00\x01)\xF9BL`@test1,6531790000,1279787158624.8d5490bfc166c687657cb
 7622735487a72.            09203bd7d44.\x00\x0A6531790000\x00\x00\x00\x05\x05test1\x00\x00\x00\x00\x
                           00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\x00\x00\x07IS_META
                           \x00\x00\x00\x05false\x00\x00\x00\x01\x08\x07actions\x00\x00\x00\x08\x00\
                           x00\x00\x0BBLOOMFILTER\x00\x00\x00\x04NONE\x00\x00\x00\x11REPLICATION_SCO
                           PE\x00\x00\x00\x010\x00\x00\x00\x0BCOMPRESSION\x00\x00\x00\x04NONE\x00\x0
                           0\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x00\x00\x00\x0A2147
                           483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x00\x09IN_ME
                           MORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true\x
                           FE\xA0\xFD\xC5

 ..

 test1,6531790000,12797871 column=info:regioninfo, timestamp=1279787160782, value=REGION => {NAME =>
 58624.8d5490bfc166c687657  'test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44.', STAR
 cb09203bd7d44.            TKEY => '6531790000', ENDKEY => '6551820000', ENCODED => 8d5490bfc166c687
                           657cb09203bd7d44, TABLE => {{NAME => 'test1', FAMILIES => [{NAME => 'acti
                           ons', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', C
                           OMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMOR
                           Y => 'false', BLOCKCACHE => 'true'}]}}
{code}


I think Karthik has a handle on the first part (i.e. why the RS ran into the version mismatch, and aborted opening the region). He'll add details to the JIRA. But what we aren't clear about at this stage is why the base scanner didn't kick in and try to reassign the region.

BTW, HBase ""hbck"" reported this as well (which was good!):

{code}
Number of Tables: 5
Number of live region servers:92
Number of dead region servers:0
.........
ERROR: Region test1,6512200000,1279784117114.6466481aa931f8c1fa87622735487a72. is not served by any region server  but is listed in META to be on server null
ERROR: Region test1,6531790000,1279787158624.8d5490bfc166c687657cb09203bd7d44. is not served by any region server  but is listed in META to be on server null
{code}


",karthik.ranga,kannanm,Blocker,Closed,Fixed,22/Jul/10 20:20,20/Nov/15 12:42
Bug,HBASE-2869,12469967,"Regularize how we log sequenceids -- sometimes its myseqid, other times its sequence id, etc.","I'm trying to trace sequenceids over time to make sure all is working properly over crashes, etc., in an HRS and its way too painful.  Regularize how we log so whenever a sequence id is mentioned in logs its named sequenceid.",stack,stack,Major,Closed,Fixed,22/Jul/10 23:48,20/Nov/15 12:42
Bug,HBASE-2874,12470069,Unnecessary double-synchronization in ZooKeeperWrapper,The {{listeners}} attribute is a synchronized collection but it's only accessed from 3 methods that are already marked as {{synchronized}}.  The double-synchronization is unnecessary and can be eliminated.,tsuna,tsuna,Trivial,Closed,Fixed,24/Jul/10 03:07,20/Nov/15 12:42
Bug,HBASE-2876,12470074,HBase hbck: false positive error reported for parent regions that are in offline state in meta after a split,"HBase Checker will sometimes report something like the following:

{code}
ERROR: Region test1,9922400000,1279934604048.8cb65b1882960f230abb97860dd13c53. is not served by any region server but is listed in META to be on server null
{code}

The region in question is a parent region that has been offlined following a split. META still contains for the above region only because there are daughter regions which still have references to the parent region. Once the daughter regions undergo compaction, these references will be gone; and the parent region's entry will be removed from META. But ""hbck"" should detect entries in this condition and not complain.

{code}
 test1,9922400000,12799346 column=info:regioninfo, timestamp=1279938675016, value=REGION => {NAME =>
 04048.8cb65b1882960f230ab  'test1,9922400000,1279934604048.8cb65b1882960f230abb97860dd13c53.', STAR
 b97860dd13c53.            TKEY => '9922400000', ENDKEY => '', ENCODED => 8cb65b1882960f230abb97860d
                           d13c53, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'test1', FAMIL
                           IES => [{NAME => 'actions', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '
                           0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZ
                           E => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
 test1,9922400000,12799346 column=info:server, timestamp=1279938675016, value=
 04048.8cb65b1882960f230ab
 b97860dd13c53.
 test1,9922400000,12799346 column=info:serverstartcode, timestamp=1279938675016, value=
 04048.8cb65b1882960f230ab
 b97860dd13c53.
 test1,9922400000,12799346 column=info:splitA, timestamp=1279938675016, value=\x00\x0A9961500000\x00
 04048.8cb65b1882960f230ab \x00\x00\x01*\x02J9'@test1,9922400000,1279938672935.94425ba581acd336d1cbd
 b97860dd13c53.            11181ee2785.\x00\x0A9922400000\x00\x00\x00\x05\x05test1\x00\x00\x00\x00\x
                           00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\x00\x00\x07IS_META
                           \x00\x00\x00\x05false\x00\x00\x00\x01\x08\x07actions\x00\x00\x00\x08\x00\
                           x00\x00\x0BBLOOMFILTER\x00\x00\x00\x04NONE\x00\x00\x00\x11REPLICATION_SCO
                           PE\x00\x00\x00\x010\x00\x00\x00\x0BCOMPRESSION\x00\x00\x00\x04NONE\x00\x0
                           0\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x00\x00\x00\x0A2147
                           483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x00\x09IN_ME
                           MORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true\x
                           B7\x04q\x18
 test1,9922400000,12799346 column=info:splitB, timestamp=1279938675016, value=\x00\x00\x00\x00\x00\x
 04048.8cb65b1882960f230ab 01*\x02J9'@test1,9961500000,1279938672935.bb521c9d8c51fd8133f145dc3c75013
 b97860dd13c53.            6.\x00\x0A9961500000\x00\x00\x00\x05\x05test1\x00\x00\x00\x00\x00\x02\x00
                           \x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\x00\x00\x07IS_META\x00\x00\x
                           00\x05false\x00\x00\x00\x01\x08\x07actions\x00\x00\x00\x08\x00\x00\x00\x0
                           BBLOOMFILTER\x00\x00\x00\x04NONE\x00\x00\x00\x11REPLICATION_SCOPE\x00\x00
                           \x00\x010\x00\x00\x00\x0BCOMPRESSION\x00\x00\x00\x04NONE\x00\x00\x00\x08V
                           ERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x00\x00\x00\x0A2147483647\x00
                           \x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x00\x09IN_MEMORY\x00\x
                           00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true""\xE8XD
{code}",stack,kannanm,Major,Closed,Fixed,24/Jul/10 04:33,20/Nov/15 12:43
Bug,HBASE-2884,12470289,TestHFileOutputFormat intermittent diff: LoadIncrementalHFiles should put expected data in table expected:<2048> but was:<1024>,"{code}
Tests run: 5, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 160.582 sec <<< FAILURE!
testMRIncrementalLoad(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat)  Time elapsed: 85.676 sec  <<< FAILURE!
java.lang.AssertionError: LoadIncrementalHFiles should put expected data in table expected:<2048> but was:<1024>
        at org.junit.Assert.fail(Assert.java:91)
        at org.junit.Assert.failNotEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:126)
        at org.junit.Assert.assertEquals(Assert.java:470)
        at org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.doIncrementalLoadTest(TestHFileOutputFormat.java:300)
        at org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.testMRIncrementalLoad(TestHFileOutputFormat.java:248)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}

Will upload full test output shortly.
",tlipcon,kannanm,Major,Closed,Fixed,27/Jul/10 19:27,20/Nov/15 12:43
Bug,HBASE-2890,12470482,Initialize RPC JMX metrics on startup,"Currently RPC call metrics are created dynamically based on cluster activity.  So when monitoring via JMX, not all RPC statistics may be present in the exported MBean, depending on what the past cluster activity has been.

HBASE-2146 has previously added code to initialize the MBean attributes on startup for all RPC methods, but in a way that depended on the defunct code -> method name mappings.

This issue is to initialize the exported MBean attributes in a cleaner way, by introspecting the RPC protocol interfaces (HMasterInterface, HMasterRegionInterface, HRegionInterface).",ghelmling,ghelmling,Minor,Closed,Fixed,30/Jul/10 00:28,20/Nov/15 12:43
Bug,HBASE-2892,12470554,Replication metrics aren't updated,"When I committed HBASE-2838, I changed the way the metrics were managed in my last patch but I forgot to put the doUpdates in place.",jdcryans,jdcryans,Major,Closed,Fixed,30/Jul/10 23:14,20/Nov/15 12:40
Bug,HBASE-2897,12470700,[stargate] RowResultGenerator should handle NoSuchColumnFamilyException,"From Sasha Maksimenko up on user@hbase:

{quote}
hi!
thanks for answer. I use very simple code

{code}
org.apache.hadoop.hbase.stargate.client.Client client = new
  org.apache.hadoop.hbase.stargate.client.Client();
Response put = client.post(""http://hostname:port/task/2/value"",
""application/octet-stream"", ""1"".getBytes());
System.out.println(put.getCode()+new String(put.getBody()));
client.shutdown();
{code}

In the first invocation I use  correct column name ""value"" and everything is OK. After that I use wrong column name""valueS"" and get exception

503 javax.ws.rs.WebApplicationException:
org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException:
[...]

Next time I change column back but problem still exist. When I re-start server problem is dissappear
{quote}

RowResultGenerator should gracefully handle NoSuchColumnFamilyException.",apurtell,apurtell,Major,Closed,Fixed,03/Aug/10 08:36,20/Nov/15 12:43
Bug,HBASE-2898,12470753,MultiPut makes proper error handling impossible and leads to corrupted data,"tl;dr version: I think the {{MultiPut}} RPC needs to be completely rewritten.  The current code makes it totally impossible for an HBase client to do proper error handling.  When an edit fails, the client has no clue as to what the problem was (certain error cases can be retried, others cannot e.g. when using a non-existent family) and the client doesn't even know which of the edits have been applied successfully.  So the client often has to retry edits without knowing whether they've been applied or not, which leads to extra unwanted versions for the {{KeyValue}} that were successfully applied (for those who care about versions, this is essentially equivalent to data corruption).  In addition, there's no way for a client to properly handle {{NotServingRegionException}}, the client has to unnecessarily invalidate cached locations of some regions and retry *all* edits.

h2. Life of a failed multi-put

Let's see why step by step what happens when a single edit in a multi-put fails.

# An HBase user calls any of the {{put}} methods on an {{HTable}} instance.
# Eventually, {{HTable#flushCommits}} is invoked to actually send the edits to the RegionServer(s).
# This takes us to {{HConnectionManager#processBatchOfPuts}} where all edits are sorted into one or more {{MultiPut}}.  Each {{MultiPut}} is aggregating all the edits that are going to a particular RegionServer.
# A thread pool is used to send all the {{MultiPut}} in parallel to their respective RegionServer.  Let's follow what happens for a single {{MultiPut}}.
# The {{MultiPut}} travels through the IPC code on the client and then through the network and then through the IPC code on the RegionServer.
# We're now in {{HRegionServer#multiPut}} where a new {{MultiPutResponse}} is created.
# Still in {{HRegionServer#multiPut}}.  Since a {{MultiPut}} is essentially a map from region name to a list of {{Put}} for that region, there's a {{for}} loop that executes each list of {{Put}} for each region sequentially.  Let's follow what happens for a single list of {{Put}} for a particular region.
# We're now in {{HRegionServer#put(byte[], List<Put>)}}.  Each {{Put}} is associated with the row lock that was specified by the client (if any).  Then the pairs of {{(Put, lock id)}} are handed to the right {{HRegion}}.
# Now we're in {{HRegion#put(Pair<Put, Integer>[])}}, which immediately takes us to {{HRegion#doMiniBatchPut}}.
# At this point, let's assume that we're doing just 2 edits.  So the {{BatchOperationInProgress}} that {{doMiniBatchPut}} contains just 2 {{Put}}.
# The {{while}} loop in {{doMiniBatchPut}} that's going to execute each {{Put}} starts.
# The first {{Put}} fails because an exception is thrown when appending the edit to the {{WAL}}.  Its {{batchOp.retCodes}} is marked as {{OperationStatusCode.FAILURE}}.
# Because there was an exception, we're back to {{HRegion#put(Pair<Put, Integer>[])}} where the {{while}} loop will test that {{batchOp.isDone}} is {{false}} and do another iteration.
# {{doMiniBatchPut}} is called again and handles the remaining {{Put}}.
# The second {{Put}} succeeds normally, so its {{batchOp.retCodes}} is marked as {{OperationStatusCode.SUCCESS}}.
# {{doMiniBatchPut}} is done and returns to {{HRegion#put(Pair<Put, Integer>[])}}, which returns to {{HRegionServer#put(byte[], List<Put>)}}.
# At this point, {{HRegionServer#put(byte[], List<Put>)}} does a {{for}} loop and extracts the index of the *first* {{Put}} that failed out of the {{OperationStatusCode[]}}.  In our case, it'll return 0 since the first {{Put}} failed.
# This index in the list of {{Put}} of the first that failed (0 in this case) is returned to {{HRegionServer#multiPut}}, which records in the {{MultiPutResponse}} - the client knows that the first {{Put}} failed but has no idea about the other one.

So the client has no reliable way of knowing which {{Put}} failed (if any) past the first failure.  All it knows is that for a particular region, they succeeded up to a particular {{Put}}, at which point there was a failure, and then the remaining may or may not have succeeded.  Its best bet is to retry all the {{Put}} past the index of the first failure for this region.  But this has an unintended consequence.  The {{Put}} that were successful during the first run will be *re-applied*.  This will unexpectedly create extra versions.  Now I realize most people don't really care about versions, so they won't notice.  But whoever relies on the versions for whatever reason will rightfully consider this to be data corruption.

As it is now, {{MultiPut}} makes proper error handling impossible.  Since this RPC cannot guarantee any atomicity other than at the individual {{Put}} level, it should return to the client specific information about which {{Put}} failed in case of a failure, so that the client can do proper error handling.

This requires us to change the {{MultiPutResponse}} so that it can indicate which {{Put}} specifically failed.  We could do this for instance by giving the index of the {{Put}} along with its {{OperationStatusCode}}.  So in the scenario above, the {{MultiPutResponse}} would essentially return something like: ""for that particular region, put #0 failed, put #1 succeeded"".  If we want to save a bit of space, we may want to omit the successes from the response and only mention the failures - so a response that doesn't mention any failure means that everything was successful.  Not sure whether that's a good idea though.

Since doing this require an incompatible RPC change, I propose that we take the opportunity to rewrite the {{MultiPut}} RPC too.  Right now it's very inefficient, it's just a hack on top of {{Put}}.  When {{MultiPut}} is written to the wire, a lot of unnecessary duplicate data is sent out.  The timestamp, the row key and the family are sent out to the wire N+1 times, where N is the number of edits for a particular row, instead of just once (!).

Alternatively, if we don't want to change the RPCs, we can fix this issue in a backward compatible way by making the {{while}} loop in {{HRegion#put(Pair<Put, Integer>[])}} stop as soon as a failure is encountered.

h2. Inability to properly handle {{NotServingRegionException}}

Since the code in {{HRegionServer#multiPut}} invokes {{HRegionServer#put(byte[], List<Put>)}} for each region for which there are edits, it's possible that edits for a first region all get successfully applied, then when moving on the 2nd region, a {{NotServingRegionException}} is thrown, which fails the RPC entirely and leaves the client with absolutely no clue as to which edits were successfully applied or not and which region caused the {{NotServingRegionException}}.  Currently the code in {{HConnectionManager}} will simply retry everything when that happens, and it'll invalidate the cached location of all the regions involved in the multi-put (even though it could well be that just a single region has to be invalidated).

I'm not sure how to best solve this problem because the Hadoop RPC protocol doesn't give us an easy way of passing data around in an exception.  The only two things I can think of are both really ugly:
# The message of the exception could contain the name of the region that caused the exception so that the client can parse the name out of the message and invalidate the right entry in its cache.
# As a special case, instead of throwing a {{NotServingRegionException}}, we'd change {{MultiPutResponse}} to also contain a list of regions no longer served by this region server, so the client could invalidate the right entries in its cache and retry just the edits that need to be retried.

I think 2. is better but it also requires an incompatible RPC change.

h2. Repro code

Simple test case to highlight the bug (against HBase trunk).  The HBase client retries hopelessly until it reaches the maximum number of attempts before bailing out.
{code}

hbase(main):001:0> describe 'mytable'
DESCRIPTION                                                              ENABLED                                
 {NAME => 'mytable', FAMILIES => [{NAME => 'myfam', BLOOMFILTER => 'NONE true                                   
 ', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '1', TT                                        
 L => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCAC                                        
 HE => 'true'}]}                                                                                                
1 row(s) in 0.7760 seconds
{code}
{code:java}

import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;

final class put {
  public static void main(String[]a) throws Exception {
    final HTable t = new HTable(HBaseConfiguration.create(), ""mytable"");
    final Put p = new Put(""somerow"".getBytes());
    p.add(""badfam"".getBytes(), ""qual2"".getBytes(), ""badvalue"".getBytes());
    t.put(p);
    t.flushCommits();
  }
}
{code}
Excerpt of the log produced when running {{put}}:
{noformat}
HConnectionManager$TableServers: Found ROOT at 127.0.0.1:54754
HConnectionManager$TableServers: Cached location for .META.,,1.1028785192 is localhost.:54754
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 1000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 1000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 1000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 2000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 2000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 4000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 4000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 8000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 16000 ms!
HConnectionManager$TableServers: locateRegionInMeta(parentTable=.META., tableName=mytable, row=[115, 111, 109, 101, 114, 111, 119] (""somerow""), useCache=true)
HConnectionManager$TableServers: Cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754
HConnectionManager$TableServers: Failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache
HConnectionManager$TableServers: Removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tableName=mytable from cache because of somerow
HConnectionManager$TableServers: processBatchOfPuts had some failures, sleeping for 32000 ms!
Exception in thread ""main"" org.apache.hadoop.hbase.client.RetriesExhaustedException: Still had 1 puts left after retrying 10 times.
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.processBatchOfPuts(HConnectionManager.java:1534)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:664)
	at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:549)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:535)
	at put.main(put.java:10)
{noformat}",ryanobjc,tsuna,Blocker,Closed,Fixed,03/Aug/10 19:00,20/Nov/15 12:41
Bug,HBASE-2899,12470772,hfile.min.blocksize.size ignored/documentation wrong,"There is a property in hbase-default.xml called {{hfile.min.blocksize.size}} set to {{65536}}.

The description says: Minimum store file block size.  The smaller you make this, the  bigger your index and the less you fetch on a random-access.  Set size down  if you have small cells and want faster random-access of individual cells.

This property is only used in the HFileOutputFormat and nowhere else. So we should at least change the description to something more meaningful.

The other option I see would be: HFile now has a DEFAULT_BLOCKSIZE field which could be moved to HConstants and HFile could somehow read the {{hfile.min.blocksize.size}} from the Configuration or use HConstansts.DEFAULT_BLOCKSIZE if it's not defined. I believe this is what's happening to the other config variables?",stack,larsfrancke,Trivial,Closed,Fixed,03/Aug/10 23:01,12/Jun/22 00:01
Bug,HBASE-2901,12470864,HBASE-2461 broke build,I broke the build.  Fix coming.,stack,stack,Major,Closed,Fixed,05/Aug/10 03:58,20/Nov/15 12:41
Bug,HBASE-2905,12471103,Nullpointer Exception is throwed when insert mass data via rest interface,"Nullpointer Exception is throwed when insert mass data via rest interface.

{code}
java.lang.NullPointerException
	at org.mortbay.io.ByteArrayBuffer.wrap(ByteArrayBuffer.java:361)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:588)
	at com.sun.jersey.spi.container.servlet.WebComponent$Writer.write(WebComponent.java:233)
	at com.sun.jersey.spi.container.ContainerResponse$CommittingOutputStream.write(ContainerResponse.java:108)
	at org.apache.hadoop.hbase.rest.provider.producer.ProtobufMessageBodyProducer.writeTo(ProtobufMessageBodyProducer.java:78)
	at com.sun.jersey.spi.container.ContainerResponse.write(ContainerResponse.java:254)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:744)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:667)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:658)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:318)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:425)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:604)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:389)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:879)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:747)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)
{code}

The issue is caused by using WeakHashMap as buffer. When get the object from the buffer, the JVM gc possible has removed the object.",sandy_yin,sandy_yin,Major,Closed,Fixed,09/Aug/10 10:36,20/Nov/15 12:44
Bug,HBASE-2906,12471245,[rest/stargate] URI decoding in RowResource,"Currently the RowResource constructor URI-decodes the rowspec string before passing it to the RowSpec constructor, which breaks rowspecs whose row, column etc identifiers contain slashes.

When addressing a row and/or column whose identifier contains a slash, the client must URI-encode the values, so for example a row whose identifier is 'http://hbase.apache.org/' would be addressed as:
  /tablename/http%3a%2f%2fhbase.apache.org%2f/column:qualifier

Currently RowResource() decodes this before passing to RowSpec(), so RowSpec() recieves:
  /tablename/http://hbase.apache.org//column:qualifier
which cannot be correctly parsed because of the extra slashes.

RowResource() should pass the string on to RowSpec() undecoded, and RowSpec() should decode the components individually after piecing apart the path.",apurtell,kbriggs,Major,Closed,Fixed,11/Aug/10 01:49,20/Nov/15 12:41
Bug,HBASE-2908,12471280,Wrong order of null-check,"In method org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(JobContext)
this.table is used before null-throw check.",korusef,korusef,Trivial,Closed,Fixed,11/Aug/10 12:52,20/Nov/15 12:40
Bug,HBASE-2909,12471350,"SoftValueSortedMap is broken, can generate NPEs","The way SoftValueSortedMap is using SoftValues, it looks like that it's able to get it's keys garbage collected along with the values themselves. We got this issue in production but I was also able to randomly generate it using YCSB with 300 threads. Here's an example on 0.20 with jdk 1.6u14:

{noformat}

java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:1036)
        at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:104)
        at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:96)
        at java.util.TreeMap.cmp(TreeMap.java:1911)
        at java.util.TreeMap.get(TreeMap.java:1835)
        at org.apache.hadoop.hbase.util.SoftValueSortedMap.get(SoftValueSortedMap.java:91)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getCachedLocation(HConnectionManager.java:788)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:651)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:634)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:601)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:128)
        at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.getTable(ThriftServer.java:262)
        at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.mutateRowTs(ThriftServer.java:585)
        at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.mutateRow(ThriftServer.java:578)
        at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRow.process(Hbase.java:2345)
        at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor.process(Hbase.java:1988)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:259)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

In this specific case, the null cannot be the passed key because it's coming from HTable which uses HConstants.EMPTY_START_ROW. It cannot be a null key that was inserted previously because we would have got the NPE at insert time. This can only mean that some key *became* null.",jdcryans,jdcryans,Blocker,Closed,Fixed,11/Aug/10 21:07,20/Nov/15 12:42
Bug,HBASE-2915,12471578,Deadlock between HRegion.ICV and HRegion.close,"HRegion.ICV gets a row lock then gets a newScanner lock.

HRegion.close gets a newScanner lock, slitCloseLock and finally waits for all row locks to finish.

If the ICV got the row lock and then close got the newScannerLock, both end up waiting on the other. This was introduced when Get became a Scan.

Stack thinks we can get rid of the newScannerLock in close since we setClosing to true.",jdcryans,jdcryans,Blocker,Closed,Fixed,13/Aug/10 19:20,20/Nov/15 12:40
Bug,HBASE-2918,12471595,SequenceFileLogWriter doesnt make it clear if there is no append by config or by missing lib/feature,"This code doesnt make the situation clear:


    Method m = null;
    if (conf.getBoolean(""dfs.support.append"", false)) {
      try {
        // function pointer to writer.syncFs()
        m = this.writer.getClass().getMethod(""syncFs"", new Class<?> []{});
      } catch (SecurityException e) {
        throw new IOException(""Failed test for syncfs"", e);
      } catch (NoSuchMethodException e) {
        // Not available
      }
    }
    this.syncFs = m;
    LOG.info((this.syncFs != null)?
      ""Using syncFs -- HDFS-200"": ""syncFs -- HDFS-200 -- not available"");


we dont know if the test failed, or if the config is turned off.  it would make debuggers life easier if the message was clearer.",,ryanobjc,Minor,Closed,Fixed,14/Aug/10 00:20,20/Nov/15 12:42
Bug,HBASE-2919,12471693,initTableReducerJob: Unused method parameter.,"In method org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(String, Class<? extends TableReducer>, Job, Class) the partitioner parameter was not passed to called org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(String, Class<? extends TableReducer>, Job, Class) method.",korusef,korusef,Major,Closed,Fixed,16/Aug/10 13:57,12/Jun/22 00:09
Bug,HBASE-2920,12471721,HTable.checkAndPut/Delete doesn't handle null values,"From John Beatty on the ML:

{quote}
Thanks Ryan, but I seem to be missing something then. It NPEs for me.
When running against 0.89.20100726 and providing a null expected value
I get the below stack trace (and works like a champ when I provide a
byte[0]. I also don't see the transformation you're referring to in
HTable.

(for reference,
http://svn.apache.org/viewvc/hbase/branches/0.89.20100726/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?view=markup)

java.io.IOException: java.io.IOException: java.lang.NullPointerException
       at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:845)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:835)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.checkAndMutate(HRegionServer.java:1754)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.checkAndPut(HRegionServer.java:1773)
       at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:576)
       at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:919)
Caused by: java.lang.NullPointerException
       at org.apache.hadoop.hbase.regionserver.HRegion.checkAndMutate(HRegion.java:1616)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.checkAndMutate(HRegionServer.java:1751)
       ... 6 more
{quote}

Looking in the code, I'm not sure either where the null conversion is done, even worse is that we don't even have unit tests! It should be put intoTestFromClientSide.",jdcryans,jdcryans,Critical,Closed,Fixed,16/Aug/10 17:27,20/Nov/15 12:42
Bug,HBASE-2923,12471830,Deadlock between HRegion.internalFlushCache and close,"HBASE-2461 added a synchronize on close(), but it's deadlocking with internalFlushCache. We should just check after getting the write locks if the region is already closed.",jdcryans,jdcryans,Major,Closed,Fixed,17/Aug/10 18:30,20/Nov/15 12:43
Bug,HBASE-2924,12471850,TestLogRolling doesn't use the right HLog half the time,"Since HBASE-2868, TestLogRolling uses 2 region servers instead of 1. The rest of the un-refactored code isn't expecting that, and only used the log from the first RS. This is why we get very inconsistent results. Fix by either coming back to 1 RS or at least use the right HLog.",jdcryans,jdcryans,Major,Closed,Fixed,17/Aug/10 21:05,20/Nov/15 12:42
Bug,HBASE-2925,12471856,LRU of HConnectionManager.HBASE_INSTANCES breaks if HBaseConfiguration is changed,"{{HConnectionManager.getConnection(config)}} caches the created {{TableServer}} in {{HBASE_INSTANCES}} (a {{LinkedHashMap}} ) which is keyed by the configuration instance itself.
Given the current implementation of {{hashCode()}} (and {{equals()}}) of {{HBaseConfiguration}}, the hash code of the configuration is changed if any of its properties are changed, which will cause the keys of {{HBASE_INSTANCES}} to be inconsistent with the hashtable that contains them, making some entries unreachable.
In this case, when the map's LRU strategy needs to remove the oldest entry, it tries to remove it based on the oldest key, which no longer gives the original hash code, therefore the lookup in {{HBASE_INSTANCES.remove(oldest)}} doesn't actually remove anything.

This has been observed to lead to OOM errors in long running clients.
",,rmahfoud,Major,Closed,Fixed,17/Aug/10 21:27,20/Nov/15 12:42
Bug,HBASE-2927,12471937,BaseScanner gets stale HRegionInfo in some race cases,"This issue is a branch of HBASE-2812, as it is a specific fix for disabling tables in some situations. The issue is that the BaseScanner can get a stale HRI by the time it checks if the region is OFFLINE, and even in trunk where we do a double checkAssigned there is no way for us to tell if the region was offlined in the mean time.",jdcryans,jdcryans,Critical,Closed,Fixed,18/Aug/10 19:31,20/Nov/15 12:43
Bug,HBASE-2928,12471944,Fault in logic in BinaryPrefixComparator leads to ArrayIndexOutOfBoundsException.,"Following statement makes an incorrect assumption that value.length >= this.value.length.

return Bytes.compareTo(this.value, 0, this.value.length, value, 0,
      this.value.length);",pranavkhaitan,pranavkhaitan,Major,Closed,Fixed,18/Aug/10 21:19,20/Nov/15 12:43
Bug,HBASE-2931,12472127,"Do not throw RuntimeExceptions in RPC/HbaseObjectWritable code, ensure we log and rethrow as IOE","When there are issues with RPC and HbaseObjectWritable, primarily when server and client have different jars, the only thing that happens is the client will receive an EOF exception.  The server does not log what happened at all and the client does not receive a server trace, rather the server seems to close the connection and the client gets an EOF because it tries to read off of a closed stream.

We need to ensure that we catch, log, and rethrow as IOE any exceptions that may occur because of an issue with RPC or HbaseObjectWritable.",karthik.ranga,streamy,Critical,Closed,Fixed,20/Aug/10 21:10,20/Nov/15 12:42
Bug,HBASE-2933,12472139,Skip EOF Errors during Log Recovery,"While testing a cluster, we hit upon the following assert during region assigment.  We were killing the master during a long run of splits.  We think what happened is that the HMaster was killed while splitting, woke up & split again.  If this happens, we will have 2 files: 1 partially written and 1 complete one.  Since encountering partial log splits upon Master failure is considered normal behavior, we should continue at the RS level if we encounter an EOFException & not an filesystem-level exception, even with skip.errors == false.

2010-08-20 16:59:07,718 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Error opening MailBox_dsanduleac,57db45276ece7ce03ef7e8d9969eb189:99900000000008@facebook.com,1280960828959.7c542d24d4496e273b739231b01885e6.
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.hadoop.io.SequenceFile$Reader.readRecordLength(SequenceFile.java:1902)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1932)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1837)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1883)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:121)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:113)
        at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEdits(HRegion.java:1981)
        at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEdits(HRegion.java:1956)
        at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEditsIfAny(HRegion.java:1915)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:344)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.instantiateRegion(HRegionServer.java:1490)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:1437)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:1345)
        at java.lang.Thread.run(Thread.java:619)
2010-08-20 16:59:07,719 ERROR org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater: Aborting open of region 7c542d24d4496e273b739231b01885e6",nspiegelberg,nspiegelberg,Critical,Closed,Fixed,21/Aug/10 01:07,20/Nov/15 12:42
Bug,HBASE-2935,12472511,"Refactor ""Corrupt Data"" Tests in TestHLogSplit","While fixing HBASE-2643, I noticed that a couple of the HLogSplit tests from HBASE-2437 were now failing.  3 tests are trying to detect proper handling of garbage data: testCorruptedFileGetsArchivedIfSkipErrors, testTrailingGarbageCorruptionLogFileSkipErrorsFalseThrows, testCorruptedLogFilesSkipErrorsFalseDoesNotTouchLogs.  However, these tests are corrupting data at the HBase level.  Data corruption should be tested at the HDFS level, because the filesystem is responsible for data validation.  These tests need to inject corrupt data at the HDFS level & then verify that ChecksumExceptions are thrown.",posix4e,nspiegelberg,Minor,Closed,Fixed,25/Aug/10 18:08,20/Nov/15 12:43
Bug,HBASE-2941,12472888,port HADOOP-6713 - threading scalability for RPC reads - to HBase,"HADOOP-6713 has  patch to fix the read scalability of hadoop rpc.  Right now a single thread accepts() then receives the RPC payload for every single RPC in hbase. Including object creation, writable deserialization, etc.

Apply the patch from that issue to our own forked HBaseRPC code.",ryanobjc,ryanobjc,Major,Closed,Fixed,30/Aug/10 22:31,20/Nov/15 12:42
Bug,HBASE-2942,12472965,Custom filters should not require registration in HbaseObjectWritable,"Some of the filter RPC serialization still requires that code -> class mappings be added to HbaseObjectWritable.  FilterList in particular requires this for it's child filters, since it calls HbaseObjectWritable.writeObject() on each.  This makes developing custom filters a big pain, as HbaseObjectWritable must be modified and the hbase core jar re-staged to the cluster.

We should fix this so that all filters can be written as a class name + data if no code exists.",ghelmling,ghelmling,Minor,Closed,Fixed,31/Aug/10 20:21,20/Nov/15 12:42
Bug,HBASE-2943,12472979,major_compact (and other admin commands) broken for .META.,Table admin commands seem to be broken against META.  Implementation is new in master rewrite branch so should wait until that goes in to see if this bug still exists.,stack,streamy,Critical,Closed,Fixed,31/Aug/10 22:48,20/Nov/15 12:41
Bug,HBASE-2944,12472985,cannot alter bloomfilter setting for a column family from hbase shell,"{code}
hbase(main):002:0> create 't1', 'cf'
create 't1', 'cf'
0 row(s) in 1.1320 seconds

hbase(main):003:0> disable 't1'
disable 't1'
0 row(s) in 1.0810 seconds

hbase(main):004:0> alter 't1', {NAME => 'cf', BLOOMFILTER => 'ROW'}
alter 't1', {NAME => 'cf', BLOOMFILTER => 'ROW'}

ERROR: no constructor with arguments matching [class org.jruby.java.proxies.ArrayJavaProxy, class org.jruby.RubyFixnum, class org.jruby.RubyString, class org.jruby.RubyBoolean, class org.jruby.RubyBoolean, class org.jruby.RubyFixnum, class org.jruby.RubyFixnum, class org.jruby.RubyBoolean, class org.jruby.RubyFixnum] on object #<Java::OrgApacheHadoopHbase::HColumnDescriptor:0x1e4218cb>
{code}",kannanm,kannanm,Minor,Closed,Fixed,01/Sep/10 00:23,20/Nov/15 12:40
Bug,HBASE-2948,12473002,bin/hbase  shell broken,"hbase shell is broken after master rewrite merge:

hbase(main):001:0> status

ERROR: undefined method `getZooKeeperWrapper' for #<#<Class:01x17eda64e>:0x73415727>

Here is some help for this command:
          Show cluster status. Can be 'summary', 'simple', or 'detailed'. The
          default is 'summary'. Examples:

            hbase> status
            hbase> status 'simple'
            hbase> status 'summary'
            hbase> status 'detailed'


hbase(main):001:0> list
TABLE                                                                                                                                                                           

ERROR: undefined method `getZooKeeperWrapper' for #<#<Class:01x63220fd1>:0x513c952f>

Here is some help for this command:
          List all tables in hbase. Optional regular expression parameter could
          be used to filter the output. Examples:

            hbase> list
            hbase> list 'abc.*'
",,anih,Major,Closed,Fixed,01/Sep/10 05:37,20/Nov/15 12:41
Bug,HBASE-2954,12473085,Fix broken build caused by hbase-2692 commit,TestReplication is broke because the replication stuff needs to be carried forward to sit on top of new zk pattern.   The TestSplitLog was a silly oversight on my part.,stack,stack,Major,Closed,Fixed,01/Sep/10 21:39,20/Nov/15 12:43
Bug,HBASE-2961,12473292,"Close zookeeper when done with it (HCM, Master, and RS)","We're not closing down zk properly, mostly in HCM.  Makes for spew in zk logs and it also causes shutdown to run longer.",stack,stack,Major,Closed,Fixed,03/Sep/10 23:30,20/Nov/15 12:42
Bug,HBASE-2962,12473316,Add missing methods to HTableInterface (and HTable),"HBASE-1845 added two new methods in HTable (batch). Those need to be in HTableInterface as well.

And in HTable we have:
* put(Put)
* put(List<Put>)
* delete(Delete)
* delete(List<Delete>)
* get(Get)

Shouldn't we add a get(List<Get>) as well for consistency?

Others that are missing:
* getRegionLocation
* getScannerCaching / setgetScannerCaching
* getStartKeys / getEndKeys / getStartEndKeys
* getRegionsInfo
* setAutoFlush
* getWriteBufferSize / setWriteBufferSize
* getWriteBuffer
* prewarmRegionCache
* serializeRegionInfo / deserializeRegionInfo

For some of those it might not make sense to add them. I'm just listing them all.

The patch is trivial once we've decided which to add, I'll prepare one for batch & get.",,larsfrancke,Major,Closed,Fixed,04/Sep/10 17:24,20/Nov/15 12:42
Bug,HBASE-2964,12473415,Deadlock when RS tries to RPC to itself inside SplitTransaction,"In testing the 0.89.20100830 rc, I ran into a deadlock with the following situation:

- All of the IPC Handler threads are blocked on the region lock, which is held by CompactSplitThread.
- CompactSplitThread is in the process of trying to edit META to create the offline parent. META happens to be on the same server as is executing the split.

Therefore, the CompactSplitThread is trying to connect back to itself, but all of the handler threads are blocked, so the IPC never happens. Thus, the entire RS gets deadlocked.",tlipcon,tlipcon,Blocker,Closed,Fixed,07/Sep/10 01:16,20/Nov/15 12:42
Bug,HBASE-2967,12473532,Failed split: IOE 'File is Corrupt!' -- sync length not being written out to SequenceFile,"We saw this on one of our clusters:

{code}
2010-09-07 18:07:16,229 WARN org.apache.hadoop.hbase.master.RegionServerOperationQueue: Failed processing: ProcessServerShutdown of sv4borg18,60020,1283516293515; putting onto delayed todo queue
java.io.IOException: File is corrupt!
        at org.apache.hadoop.io.SequenceFile$Reader.readRecordLength(SequenceFile.java:1907)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1932)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1837)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1883)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:121)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:113)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.parseHLog(HLog.java:1493)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1256)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1143)
        at org.apache.hadoop.hbase.master.ProcessServerShutdown.process(ProcessServerShutdown.java:299)
        at org.apache.hadoop.hbase.master.RegionServerOperationQueue.process(RegionServerOperationQueue.java:147)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:532)
{code}

Because it was an IOE, it got requeued.  Each time around we failed on it again.

A few things:

+ This exception needs to add filename and the position in file at which problem found.
+ Need to commit little patch over in HBASE-2889 that outputs position and ordinal of wal edit because it helps diagnose these kinds of issues.
+ We should be able to skip the bad edit; just postion ourselves at byte past the bad sync and start reading again
+ There must be something about our setup that makes it so we fail write of the sync 16 random bytes that make up the SF 'sync' marker though oddly for one of the files, the sync failure happens at 1/3rd of the way into a 64MB wal, edit #2000 out of 130k odd edits.



",stack,stack,Blocker,Closed,Fixed,08/Sep/10 06:25,20/Nov/15 12:42
Bug,HBASE-2969,12473607,missing sync in HTablePool.getTable(),"Considering that the method _getTable(String)_ in _org.apache.hadoop.hbase.client.HTablePool_ is invoked by multiple threads, it may happen that while _'queue == null'_ is true, it is possible to have a queue mapped to that name into the tables map when _'tables.put(tableName, queue)'_ is executed. 

However, I don't think it will cause any harm because the overwritten queue will eventually be garbage-collected. :-)",guiga,guiga,Minor,Closed,Fixed,08/Sep/10 17:58,20/Nov/15 12:43
Bug,HBASE-2973,12473635,NPE in LogCleaner,"From TRUNK:

{code}
2010-09-08 15:52:42,221 DEBUG org.apache.hadoop.hbase.master.LogCleaner: Add log cleaner in chain: org.apache.hadoop.hbase.master.TimeToLiveLogCleaner
2010-09-08 15:52:42,221 DEBUG org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner: Didn't find this log in ZK, deleting: null
2010-09-08 15:52:42,221 DEBUG org.apache.hadoop.hbase.master.LogCleaner: Add log cleaner in chain: org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner
2010-09-08 15:52:42,234 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: <master-sv2borg185:60000> Set watcher on existing znode /hbase/root-region-server
2010-09-08 15:52:42,235 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: <master-sv2borg185:60000> Retrieved 18 bytes of data from znode /hbase/root-region-server and set a watcher
2010-09-08 15:53:42,220 INFO org.apache.hadoop.hbase.master.ServerManager: 0 region servers, 0 dead, average load NaN[]
2010-09-08 15:53:42,227 ERROR org.apache.hadoop.hbase.master.LogCleaner: Caught exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.LogCleaner.chore(LogCleaner.java:128)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
2010-09-08 15:54:42,221 INFO org.apache.hadoop.hbase.master.ServerManager: 0 region servers, 0 dead, average load NaN[]
2010-09-08 15:54:42,225 ERROR org.apache.hadoop.hbase.master.LogCleaner: Caught exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.LogCleaner.chore(LogCleaner.java:128)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
{code}",,stack,Major,Closed,Fixed,08/Sep/10 23:32,20/Nov/15 12:43
Bug,HBASE-2974,12473650,LoadBalancer ArithmeticException: / by zero,"Having an issue so regionservers are not checking in.  Balancer meantime went to run and threw ArithmeticException: / by zero

Committing this:

{code}
Index: src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java      (revision 995307)
+++ src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java      (working copy)
@@ -140,6 +140,10 @@
       new TreeMap<HServerInfo,List<HRegionInfo>>(
           new HServerInfo.LoadComparator());
     int numServers = clusterState.size();
+    if (numServers == 0) {
+      LOG.debug(""numServers=0 so nothing to balance"");
+      return null;
+    }
     int numRegions = 0;
     // Iterate so we can count regions as we build the map
     for(Map.Entry<HServerInfo, List<HRegionInfo>> server :
{code}",stack,stack,Major,Closed,Fixed,09/Sep/10 05:42,20/Nov/15 12:41
Bug,HBASE-2975,12473651,DFSClient names in master and RS should be unique,"In the post-newmaster trunk, there's some code which gives the regionserver and masters fancy names based on hostname and port. This breaks log recovery, though, if the master starts recovering a log (ie has an append lease on a log file), then crashes and comes back on the same port. The NN doesn't see this as a new client, since the client name is the same, so it thinks it still holds a lease on the file. The new master, though, can't call append() because the NN thinks it's appending, so it loops forever.",tlipcon,tlipcon,Critical,Closed,Fixed,09/Sep/10 05:55,20/Nov/15 12:42
Bug,HBASE-2976,12473699,Running HFile tool passing fully-qualified filename I get 'IllegalArgumentException: Wrong FS',"This fixes it:

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java b/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
index 65cbb9d..3966108 100644
--- a/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
+++ b/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
@@ -1826,6 +1826,8 @@ public class HFile {
       Configuration conf = HBaseConfiguration.create();
       conf.set(""fs.defaultFS"",
         conf.get(org.apache.hadoop.hbase.HConstants.HBASE_DIR));
+      conf.set(""fs.default.name"",
+        conf.get(org.apache.hadoop.hbase.HConstants.HBASE_DIR));
       FileSystem fs = FileSystem.get(conf);
       ArrayList<Path> files = new ArrayList<Path>();
       if (cmd.hasOption(""f"")) {
{code}",stack,stack,Major,Closed,Fixed,09/Sep/10 16:28,20/Nov/15 12:43
Bug,HBASE-2978,12473767,LoadBalancer IndexOutOfBoundsException,"{code}
2010-09-09 22:49:21,500 ERROR [192.168.1.157:51901-balancerChore] hbase.Chore(69): Caught exception
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.hadoop.hbase.master.LoadBalancer.balanceCluster(LoadBalancer.java:250)
	at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:568)
	at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:480)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
{code}",stack,stack,Major,Closed,Fixed,10/Sep/10 05:51,20/Nov/15 12:41
Bug,HBASE-2979,12473768,Fix failing TestMultParrallel in hudson build,Its failing w/ a while now.,stack,stack,Major,Closed,Fixed,10/Sep/10 06:03,20/Nov/15 12:41
Bug,HBASE-2982,12473839,Maven 3 doesn't like duplicate dependencies in hbase/pom.xml,"An install of the latest maven plugin resolves dependencies with Maven 3.
Maven 3 does not like the fact that several dependencies in the main pom.xml are duplicated.

Below is the error message which can be fixed by removing:

* The 2nd com.google.guava dependency from the ""Test dependencies"" section
* both org.slf4j dependencies from the ""Avro dependencies"" section 


9/10/10 11:15:50 AM PDT: Build errors for hbase; org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:
[WARNING] 'version' contains an expression but should be a constant. @ org.apache.hbase:hbase:${hbase.version}, /opt/eclipse/troove/hbase/pom.xml
[ERROR] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.slf4j:slf4j-log4j12:jar -> duplicate declaration of version ${slf4j.version} @ org.apache.hbase:hbase:${hbase.version}, /opt/eclipse/troove/hbase/pom.xml
[ERROR] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.slf4j:slf4j-api:jar -> duplicate declaration of version ${slf4j.version} @ org.apache.hbase:hbase:${hbase.version}, /opt/eclipse/troove/hbase/pom.xml
[ERROR] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: com.google.guava:guava:jar -> duplicate declaration of version ${guava.version} @ org.apache.hbase:hbase:${hbase.version}, /opt/eclipse/troove/hbase/pom.xml
[WARNING] 'build.plugins.plugin.version' is missing for org.codehaus.mojo:build-helper-maven-plugin @ org.apache.hbase:hbase:0.89-SNAPSHOT-withHLogSplit, /opt/eclipse/troove/hbase/pom.xml
",larsfrancke,jk-public@troove.net,Major,Closed,Fixed,10/Sep/10 18:36,20/Nov/15 12:44
Bug,HBASE-2983,12473865,TestHLog unit test is mis-comparing in an assertion,The TestHLog unit test seems to be failing,posix4e,posix4e,Minor,Closed,Fixed,10/Sep/10 21:57,20/Nov/15 12:43
Bug,HBASE-2984,12473871,[shell] Altering a family shouldn't reset to default unchanged attributes,"I changed the replication on a family that was also VERSIONS => 1 and COMPRESSION => LZO. I forgot that you have to respecify everything everytime you alter a family, so both were reset to 3 and NONE. Then the regions were compacted... and it has been splitting for about 20 minutes now. Fortunately this is our MR environment so our web site isn't affected, but it's still a major pain. Oh and also the table cannot be disabled to be re-altered since split parents are always present (I hope it'll stop splitting before midnight).

The shell should use the old values for attributes that aren't changed.",jdcryans,jdcryans,Blocker,Closed,Fixed,10/Sep/10 22:48,20/Nov/15 12:42
Bug,HBASE-2985,12473874,HRegionServer.multi() no longer calls HRegion.put(List) when possible,This should result in a reduce performance of puts in batched mode,ryanobjc,ryanobjc,Major,Closed,Fixed,10/Sep/10 23:12,20/Nov/15 12:40
Bug,HBASE-2986,12473875,multi writable can npe causing client hang,"I have this backtrace:

} org.apache.hadoop.hbase.NotServingRegionException: usertable,,1284159178472.010c503fa9c9f12bd0fd9551ede360ec. is closed
        at org.apache.hadoop.hbase.regionserver.HRegion.startRegionOperation(HRegion.java:3068)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1248)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.put(HRegionServer.java:1709)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:2412)
        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:557)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1007)

2010-09-10 16:00:56,808 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 15 on 60020 caught: java.lang.NullPointerException
        at org.apache.hadoop.hbase.client.MultiResponse.write(MultiResponse.java:92)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.writeObject(HbaseObjectWritable.java:376)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.write(HbaseObjectWritable.java:242)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1029)

After this happened my client just sat there and didnt go anywhere.",ryanobjc,ryanobjc,Blocker,Closed,Fixed,10/Sep/10 23:14,20/Nov/15 12:40
Bug,HBASE-2989,12473987,[replication] RSM won't cleanup after locking if 0 peers,"Small bug in ReplicationSourceManager, it won't cleanup after locking another's RS znode if it didn't contain any queue at all. It happens in transferQueues():

{code}
LOG.info(""Moving "" + rsZnode + ""'s hlogs to my queue"");
    SortedMap<String, SortedSet<String>> newQueues =
        this.zkHelper.copyQueuesFromRS(rsZnode);
    if (newQueues == null || newQueues.size() == 0) {
      return;
    }
    this.zkHelper.deleteRsQueues(rsZnode);
{code}

That last line should be before the if, so that it deletes the lock znode and the RS znode. Currently a lot of cruft piles up in ZK after a few restarts with replication enabled and no queues, or in slave RSs.

",jdcryans,jdcryans,Minor,Closed,Fixed,13/Sep/10 17:28,20/Nov/15 12:42
Bug,HBASE-2992,12474021,[replication] MalformedObjectNameException in ReplicationMetrics,"It's possible to get an exception that looks like this when creating a new ReplicationSource:

{noformat}
javax.management.MalformedObjectNameException: Invalid character ',' in key part of property
        at javax.management.ObjectName.construct(ObjectName.java:535)
        at javax.management.ObjectName.<init>(ObjectName.java:1403)
        at org.apache.hadoop.metrics.util.MBeanUtil.getMBeanName(MBeanUtil.java:80)
        at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:51)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationStatistics.<init>(ReplicationStatistics.java:43)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceMetrics.<init>(ReplicationSourceMetrics.java:77)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.init(ReplicationSource.java:176)

...
{noformat}

Need to make sure the MBean's name is valid.",jdcryans,jdcryans,Critical,Closed,Fixed,14/Sep/10 00:42,20/Nov/15 12:42
Bug,HBASE-2995,12474112,Incorrect dependency on Log class from Jetty,"There are a few dependencies on the class {{org.mortbay.log.Log}} which is a bug:

21:05 < dj_ryan> we should not depend on that logging framework
21:05 < dj_ryan> at all
21:05 < dj_ryan> ever

I could find it in use in these classes:
* {{org.apache.hadoop.hbase.client.ScannerCallable}}
* {{org.apache.hadoop.hbase.client.TestHCM}}
* {{org.apache.hadoop.hbase.mapreduce.Export}}
* {{org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader}}",stack,larsfrancke,Major,Closed,Fixed,14/Sep/10 19:17,20/Nov/15 12:43
Bug,HBASE-2997,12474125,Performance fixes - profiler driven,while profiling hbase I found a number of slow pieces.  Here are fixes for them.,ryanobjc,ryanobjc,Critical,Closed,Fixed,14/Sep/10 21:27,20/Nov/15 12:41
Bug,HBASE-2998,12474127,rolling-restart.sh shouldn't rely on zoo.cfg,"I tried the rolling-restart script on our dev environment, which is configured with zoo.cfg for zookeeper, and it worked pretty well. Then I tried it on our MR cluster, which doesn't have a zoo.cfg, and we suffered some downtime (no biggie tho, nothing critical was running). When the script calls this line:

{code}
bin/hbase zkcli stat $zmaster
{code}

It directly runs a ZooKeeperMain which isn't modified to read from the HBase configuration files. What happens next if ZK isn't running on the master node is that it receives a ConnectionRefused, ignores it, procedes to restart the master (which waits on the znode), and the starts restarting the region servers. They can't shutdown properly under 60 seconds, since they need a master, so they get killed. What follows is pretty ugly and pretty much requires a whole restart.",stack,jdcryans,Critical,Closed,Fixed,14/Sep/10 21:53,20/Nov/15 12:42
Bug,HBASE-3003,12474231,ClassSize constants dont use 'final' ,constants should really be final.  things would be confusing if they changed later.,ryanobjc,ryanobjc,Major,Closed,Fixed,15/Sep/10 23:45,20/Nov/15 12:42
Bug,HBASE-3006,12474298,Reading compressed HFile blocks causes way too many DFS RPC calls severly impacting performance,"On some read perf tests, we noticed several perf outliers (10 second plus range). The rows were large (spanning multiple blocks, but still the numbers didn't add up). We had compression turned on.

We enabled DN clienttrace logging,
log4j.logger.org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace=DEBUG

and noticed lots of 516 byte reads at the DN level, several of them at the same offset in the block.

{code}
2010-09-16 09:28:32,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:38713, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 203000
2010-09-16 09:28:32,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:40547, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 119000
2010-09-16 09:28:32,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:40650, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 149000
2010-09-16 09:28:32,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:40861, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 135000
2010-09-16 09:28:32,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:41129, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 117000
2010-09-16 09:28:32,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:41691, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 148000
2010-09-16 09:28:32,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:42881, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 114000
2010-09-16 09:28:32,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:49511, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 153000
2010-09-16 09:28:32,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:51158, bytes: 3096, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.\
30.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 139000
{code}

This was strange coz our block size was 64k, and on disk block size after compression should generally have been around 6k.

Some print debugging at the HFile and BoundedRangeFileInputStream (which is wrapped by createDecompressionStream) revealed the following:

We are trying to read 20k from DFS @ HFile layer. The BounderRangeFileInputStream instead reads several header bytes 1 byte at a time, and then reads a 11k chunk and later a 9k chunk.

{code}
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 34386760 compressedSize = 20711 decompressedSize = 92324
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386760; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386761; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386762; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386763; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386764; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386765; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386766; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386767; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386768; bytes: 11005
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397773; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397774; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397775; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397776; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397777; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397778; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397779; bytes: 1
2010-09-16 09:21:27,913 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397780; bytes: 1
2010-09-16 09:21:27,913 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397781; bytes: 9690
{code}

Seems like it should be an easy fix to prefetch the compressed size... rather than incremental fetches.
",kannanm,kannanm,Critical,Closed,Fixed,16/Sep/10 16:40,20/Nov/15 12:42
Bug,HBASE-3008,12474336,Memstore.updateColumnValue passes wrong flag to heapSizeChange,"Memstore.updateColumnValue passes the wrong flag to heapSizeChange, making it so that the size keeps growing while the actual size is probably the same. For example, in our production environment we see tables that only take ICVs doing flushes of a few KBs when it thinks there's 64MB in:

{noformat}
Started memstore flush for region somecountingtable,,1248719998664.121282795. Current region memstore size 64.0
Added hdfs://borg9:9000/hbase/somecountingtable/121282795/counter/3564459650504019443, entries=905, sequenceid=72504291507, 
memsize=183.3k, filesize=18.5k to somecountingtable,,1248719998664.121282795
{noformat}",jdcryans,jdcryans,Major,Closed,Fixed,16/Sep/10 22:51,20/Nov/15 12:41
Bug,HBASE-3010,12474362,Can't start/stop/start... cluster using new master,"Currently you might start a small cluster the first time on TRUNK -- i.e. new master -- but second time you do the startup you run into a couple of interesting issues:

+ The old root-region-location is still in place. It gets cleaned later but for a while on startup it does not have the 'right' address.
+ Regionserver (or a client) on startup creates a catalogtracker, a class that notices changes in meta tables keeping up catalog table locations.  Starting the catalogtracker results in a check for current catalog locations.  As part of this process, since root-region-location ""exists"", catalogtracker tries to verify root's location by doing a noop against root host, only, to do this it needs to do the initial rpc proxy setup.  It can so happen that the old root address was that of the current regionserver trying to initialize so we'll be trying to connect to ourself to verify root location ONLY, we're doing this before we've setup the rpcserver and handlers -- so we block, and as it happens there is no timeout on proxy setup (Todd ran into this yesterday, I ran into it today -- its easy to manufacture).
+ So regionserver can't progress.  Meantime the master can't progress because there are no regionservers checking in.  And you can't shut it down because we're not looking at the right 'stop' flag",stack,stack,Blocker,Closed,Fixed,17/Sep/10 08:06,20/Nov/15 12:41
Bug,HBASE-3012,12474410,TOF doesn't take zk client port for remote clusters,"Currently we are only able to specify the ZK ensemble and root znode for the remote cluster in TOF, we should also be able to give the client port (like in replication). This will require a change in CopyTable's command line arguments too.",jdcryans,jdcryans,Minor,Closed,Fixed,17/Sep/10 17:21,20/Nov/15 12:42
Bug,HBASE-3015,12474463,recovered.edits files not deleted if it only contain edits that have already been flushed; hurts perf for all future opens of the region,"On RS crash, master processes the RS's logs, splits them into per region log files, and puts them in recovered.edits sub-directory of the region. 

It may be the case the some of these files contain only old edits that have already been flushed, and don't need to be reapplied again. However, in this case the file is not deleted, and stays in recovered.edits for ever. This will slow down every future ""open"" of this region, as the region will unnecessarily spend time processing this file.

In  HRegion.java:replaceRecoveredEditsIfAny(), the code below checks if the file we just processed contain any edits that were applied, and in that case flushes the memstore into which things were being recovered. 

{code}
  if (seqid > minSeqId) {
      // Then we added some edits to memory. Flush and cleanup split edit files.
      internalFlushcache(null, seqid);
      for (Path file: files) {
        if (!this.fs.delete(file, false)) {
          LOG.error(""Failed delete of "" + file);
        } else {
          LOG.debug(""Deleted recovered.edits file="" + file);
        }
      }
    }
{code}

But it is not clear why the 'for' loop to clean up the recovered.edits file is inside the if check.


",stack,kannanm,Major,Closed,Fixed,18/Sep/10 02:46,20/Nov/15 12:42
Bug,HBASE-3017,12474622,More log pruning,"This issue covers some tightening up of log messages; as is all of the zk noise tends to overwhelm.

For example, zkwatcher logs a generic ""This event happened in zk with path X and event type Y"" but just after, there will be a log from the handler of this zk event with this subsequent log more descriptive.  This change would make zkwatcher log at INFO by default rather than DEBUG cutting down on logging content (re-enabling DEBUG is easy to do if needed).",stack,stack,Major,Closed,Fixed,20/Sep/10 18:49,20/Nov/15 12:44
Bug,HBASE-3023,12474760,NPE processing server crash in MetaReader. getServerUserRegions,"{code}
2010-09-21 18:28:30,856 ERROR
org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while
processing event M_SERVER_SHUTDOWN
java.lang.NullPointerException
       at org.apache.hadoop.hbase.catalog.MetaReader.getServerUserRegions(MetaReader.java:413)
       at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:106)
       at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:150)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
{code}

Problem is that there was no server set in .META. yet for a particular row but we went ahead tried to use the server as though it non-null.",stack,stack,Major,Closed,Fixed,21/Sep/10 21:58,20/Nov/15 12:43
Bug,HBASE-3024,12474762,NPE processing server crash in MetaEditor.addDaughter,"{code}
2010-09-21 22:11:01,812 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Reassigning the 116 region(s) that sv2borg183,60020,1285106817665 was carrying.
2010-09-21 22:11:01,813 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Fixup; missing daughter [B@4b0bc3c9
2010-09-21 22:11:01,818 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.lang.NullPointerException
        at org.apache.hadoop.hbase.catalog.MetaEditor.addDaughter(MetaEditor.java:102)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.fixupDaughter(ServerShutdownHandler.java:156)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.fixupDaughters(ServerShutdownHandler.java:137)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:120)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:150)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
{code}",stack,stack,Major,Closed,Fixed,21/Sep/10 22:21,20/Nov/15 12:41
Bug,HBASE-3026,12474791,"Fixup of ""missing"" daughters on split is too aggressive","There is a bug in how we check for whether the daughters mentioned in parent region are present in .META.  The check is done when we are processing a server shutdown.  We're making the mistake of checking for presence of the daughter in the list of regions that used to live on the crashed server BUT fact of the matter is is that the daughter could just as well be rebalanced to another server.

The upshot is that we are inserting into .META. and trying to assign regions that are already assigned.",,stack,Major,Closed,Fixed,22/Sep/10 06:28,20/Nov/15 12:41
Bug,HBASE-3028,12474793,"No basescanner means no GC'ing of split, offlined parent regions",I need to add back cleanup of split parents probably as a Chore in master.  Can use new MetaReader code.,stack,stack,Major,Closed,Fixed,22/Sep/10 06:41,20/Nov/15 12:41
Bug,HBASE-3030,12474849,The return code of many filesystem operations are not checked,"The region server makes call delete/rename/mkdir calls to the FileSystem. These calls return true or false depending on whether the operation was performed successfully or not. Region server should check these return values, and either throw an exception or log it.",dhruba,dhruba,Major,Closed,Fixed,22/Sep/10 18:07,20/Nov/15 12:41
Bug,HBASE-3031,12474854,"CopyTable MR job named ""Copy Table"" in Driver","The CopyTable MR job needs to change name, currently it requires passing quotes around it since it's ""Copy Table"". Also all the other names are lower case and without and white spaces.",stack,jdcryans,Minor,Closed,Fixed,22/Sep/10 19:02,20/Nov/15 12:42
Bug,HBASE-3036,12475078,avro tests failing up on hudson (pass locally),,,stack,Major,Closed,Fixed,24/Sep/10 20:39,20/Nov/15 12:43
Bug,HBASE-3037,12475086,"When new master joins running cluster does ""Received report from unknown server -- telling it to STOP_REGIONSERVER...""",Working on it.,stack,stack,Major,Closed,Fixed,24/Sep/10 23:03,20/Nov/15 12:43
Bug,HBASE-3038,12475095,WALReaderFSDataInputStream.getPos() fails if Filesize > MAX_INT,"WALReaderFSDataInputStream.getPos() uses  this.in.available() to determine the actual length of the file.  Except that available() returns an int instead of a long.  Therefore, our current logic is broke when trying to read a split log > 2GB.",nspiegelberg,nspiegelberg,Critical,Closed,Fixed,25/Sep/10 01:58,20/Nov/15 12:42
Bug,HBASE-3039,12475116,Stuck in regionsInTransition because rebalance came in at same time as a split,"Saw this doing cluster tests:

{code}
2010-09-25 21:31:48,212 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because regions in transition: {73781e505e452221c9cd0e03585eb5d1=usertable,user800184056, 
128...
{code}

Here's the problem:

{code}
2010-09-25 08:16:48,186 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=usertable,user800184056,1285397376525.73781e505e452221c9cd0e03585eb5d1., src=su184,60020,      
1285371621579, dest=sv2borg189,60020,1285371621577

2010-09-25 08:16:48,186 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region usertable,user800184056,1285397376525.                               
73781e505e452221c9cd0e03585eb5d1. (offlining)

2010-09-25 08:16:52,656 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: usertable,user800184056,1285397376525.73781e505e452221c9cd0e03585eb5d1.:           
Daughters; usertable,user800184056,1285402609029.c05825561e7ea3cc6507c70bfb21541a., usertable,user804024623,1285402609029.28f64903a7875bdafc1e7ee344b225b0.
2010-09-25 08:17:11,414 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  usertable,user800184056,1285397376525.                              
73781e505e452221c9cd0e03585eb5d1. state=PENDING_CLOSE, ts=1285402608186
{code}


....just as we were doing a balance, the region split.

Over on RS, I see the split starting up and then in comes the balance 'close' message.  By the time the close handler runs on regionserver the split is well underway and close handler actually doesn't find an online region to split.",stack,stack,Major,Closed,Fixed,25/Sep/10 21:46,20/Nov/15 12:41
Bug,HBASE-3041,12475215,[replication] ReplicationSink shouldn't kill the whole RS when it fails to replicate,"This is kind of a funny bug, as long as you don't run into it. I thought I'd be a good idea to kill the region servers that act as sinks when they can't replicate edits on their own cluster (this is often something we do in face of fatal errors throughout the code), but not so much.

So, last friday while I was using CopyTable to replicate data from a master to a slave cluster while the new data was being replicated, one table got really slow and took too long to split which tripped RetriesExhaustedException coming out of HTable in ReplicationSink. This killed a first region server, which was itself hosting regions. Splitting the logs took a bit longer since the cluster was under high insert load, so this triggered other exceptions in the other region servers, to a point where they were all down. I restarted the cluster, the master splits all the logs that were remaining and begins assigning regions. Some of them took too long to open because each region server had a few regions to recover each and the last ones in the queue were minutes from being opened. Since the master cluster was already pushing edits to the slave, the region servers all got RetriesExhausted and all went down again. I changed the client pause from 1 to 3 and restarted, same happened. I changed it to 5, and finally was able to keep the cluster up. Fortunately, the master cluster was queueing up the HLogs so we didn't lose any data and the backlog was replicated in a few minutes.

So, instead of killing the region server, any exception coming out of HTable should just be treated as a failure to apply and the source cluster should retry later.",jdcryans,jdcryans,Major,Closed,Fixed,27/Sep/10 17:15,12/Oct/12 06:17
Bug,HBASE-3042,12475242,Use LO4J in SequenceFileLogReader,Trivial change.  The SequenceFileLogReader class accidentally uses the Mortbay logging class instead of the LOG4J.,nspiegelberg,nspiegelberg,Trivial,Closed,Fixed,27/Sep/10 21:13,20/Nov/15 12:42
Bug,HBASE-3044,12475275,[replication] ReplicationSource won't cleanup logs if there's nothing to replicate,"ReplicationSource only calls ReplicationSourceManager.logPositionAndCleanOldLogs if it replicated something, meaning that a region server that doesn't host a region with replicable edits will read through the logs but never report the position in zookeeper and won't clean the processed hlogs. This happened on one cluster here and shutting down a region server took a few minutes just to delete all the znodes.",jdcryans,jdcryans,Major,Closed,Fixed,28/Sep/10 03:52,12/Oct/12 06:17
Bug,HBASE-3047,12475350,"If new master crashes, restart is messy","If master crashes, the cluster-is-up flag is left stuck on.

On restart of cluster, regionservers may come up before the master.  They'll have registered themselves in zk by time the master assumes its role and master will think its joining an up and running cluster when in fact this is a fresh startup.  Other probs. are that there'll be a root region that is bad up in zk.  Same for meta and at moment we're not handling bad root and meta very well.

Here's sample of kinda of issues we're running into:

{code}
2010-09-25 23:53:13,938 FATAL org.apache.hadoop.hbase.master.HMaster:
Unhandled exception. Starting shutdown.
java.io.IOException: Call to /10.20.20.188:60020 failed on local
exception: java.io.IOException: Connection reset by peer
   at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:781)
   at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)
   at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:255)
   at $Proxy1.getProtocolVersion(Unknown Source)
   at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:412)
   at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:388)
   at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:435)
   at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:345)
   at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:889)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:350)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.getRootServerConnection(CatalogTracker.java:209)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:241)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:286)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:326)
   at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:157)
   at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:140)
   at org.apache.hadoop.hbase.master.AssignmentManager.rebuildUserRegions(AssignmentManager.java:753)
   at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:174)
   at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:314)
Caused by: java.io.IOException: Connection reset by peer
   at sun.nio.ch.FileDispatcher.read0(Native Method)
   at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:21)
   at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233)
   at sun.nio.ch.IOUtil.read(IOUtil.java:206)
{code}

Notice, we think its a case of processFailover so we think we can just scan meta to fixup our inmemory picture of the running cluster, only the scan of meta fails because the meta isn not assigned.

",stack,stack,Major,Closed,Fixed,28/Sep/10 21:30,20/Nov/15 12:42
Bug,HBASE-3054,12475473,Remore TestEmptyMetaInfo; it doesn't make sense any more.,"At the head of the TestEmptyMetaInfo it says ""TODO: Does this test make sense any more?""

Its a test that checks that the basescanner when it runs cleans up weirdly incomplete rows in .META.

We don't really have a scanner any more and the thing we have, does not do this kinda cleaning anyways so this test is testing something removed.

i'm removing the test.  Its a mess anyways based on TestEmptyMetaInfo.  It just started failing too.",stack,stack,Major,Closed,Fixed,30/Sep/10 03:42,20/Nov/15 12:41
Bug,HBASE-3056,12475526,Fix ordering in ZKWatcher constructor to prevent weird race condition,"A small race condition in ZKWatcher leads to an NPE:

{noformat}
2010-09-30 10:56:36,028 INFO  [Thread-217] zookeeper.ZKUtil(93): hconnection opening connection to ZooKeeper with quorum (localhost:21815)
2010-09-30 10:56:36,036 DEBUG [Thread-217-EventThread] zookeeper.ZooKeeperWatcher(184): hconnection Received ZooKeeper Event, type=None, state=SyncConnected, path=null
2010-09-30 10:56:36,036 ERROR [Thread-217-EventThread] zookeeper.ClientCnxn$EventThread(490): Error while calling watcher 
java.lang.NullPointerException
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:243)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:193)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:488)
{noformat}",streamy,streamy,Major,Closed,Fixed,30/Sep/10 17:58,20/Nov/15 12:43
Bug,HBASE-3057,12475531,Race condition when closing regions that causes flakiness in TestRestartCluster,"In {{TestRestartCluster.testClusterRestart()}} we spin up cluster, create three tables, shut it down, start it back up, and ensure we still have three regions.

A subtle race condition during the first shutdown makes it so the flush of META doesn't finish so when we start back up there are no user regions.

I'm not sure if there are reasons the ordering is as such, but the is the section of code in CloseRegionHandler around line 118:

{noformat}
      this.rsServices.removeFromOnlineRegions(regionInfo.getEncodedName());
      region.close(abort);
{noformat}

We remove from the online map of regions before actually closing.  But what the main run() loop in the RS is waiting on to determine when it can shut down is that the online region map is empty.

{noformat}
  private void waitOnAllRegionsToClose() {
    // Wait till all regions are closed before going out.
    int lastCount = -1;
    while (!this.onlineRegions.isEmpty()) {
{noformat}

Any reason not to swap these two and do the close before removing from online regions?",streamy,streamy,Major,Closed,Fixed,30/Sep/10 18:44,20/Nov/15 12:43
Bug,HBASE-3058,12475540,Fix REST tests on trunk,"Most of the REST tests do not pass on trunk.  Most likely because configuration is being generated internally within REST classes rather than being passed in, so when tests override configs they are not getting picked up.

There was a similar issue already fixed with thrift and avro.",streamy,streamy,Critical,Closed,Fixed,30/Sep/10 20:06,20/Nov/15 12:44
Bug,HBASE-3059,12475550,TestReadWriteConsistencyControl occasionally hangs,"The test hung when I ran mvn test today. The jstack shows that a Writer thread hung at

""Thread-1"" prio=10 tid=0x00002aaad81d2800 nid=0x6ce9 in Object.wait() [0x0000000040f37000]
   java.lang.Thread.State: WAITING (on object monitor)  at java.lang.Object.wait(Native Method)
  at org.apache.hadoop.hbase.regionserver.ReadWriteConsistencyControl.completeMemstoreInsert(ReadWriteConsistencyControl.java:130)
  -- locked <0x00002aaac9fa0f50> (a java.lang.Object)                
  at org.apache.hadoop.hbase.regionserver.TestReadWriteConsistencyControl$Writer.run(TestReadWriteConsistencyControl.java:56)  at java.lang.Thread.run(Thread.java:619)

It seems to be caused by a race condition in ReadWriteConsistencyControl#completeMemStoreInsert. Accesses/updates of the value of memStoreRead should be done while holding the readWaiters lock.",hairong,hairong,Major,Closed,Fixed,30/Sep/10 21:32,20/Nov/15 12:43
Bug,HBASE-3060,12475555,[replication] Reenable replication on trunk with unit tests,"Replication currently doesn't work on trunk because it needs to be ported on the new ZK utils. Tests that weren't passing were marked as DISABLED or @ignore. 

Do the port and reenable everything.",jdcryans,streamy,Blocker,Closed,Fixed,30/Sep/10 21:47,20/Nov/15 12:41
Bug,HBASE-3062,12475576,"ZooKeeper KeeperException$ConnectionLossException is a ""recoverable"" exception; we should retry a while on server startup at least.","On startup of daemons we'll sometimes fail (new master) with something like the following:

{code}
2010-09-30 23:38:36,979 INFO org.apache.hadoop.hbase.zookeeper.ZKUtil: regionserver60020 opening connection to ZooKeeper with quorum (sv2borg182:20001,sv2borg181:20001,sv2borg180:20001)
2010-09-30 23:38:36,979 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=sv2borg182:20001,sv2borg181:20001,sv2borg180:20001 sessionTimeout=60000 watcher=regionserver60020
2010-09-30 23:38:36,980 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server sv2borg182/10.20.20.182:20001
2010-09-30 23:38:36,980 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to sv2borg182/10.20.20.182:20001, initiating session
2010-09-30 23:38:36,981 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
2010-09-30 23:38:37,083 ERROR org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: regionserver60020 Unexpected KeeperException creating base node
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase
    at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
    at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
    at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:637)
    at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndFailSilent(ZKUtil.java:807)
    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:107)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeZooKeeper(HRegionServer.java:438)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.initialize(HRegionServer.java:420)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:305)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2436)
    at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:60)
    at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
    at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2460)
{code}

I'll see it over on master too the odd time.

Currently we fail out (though in above case, because of order in which we do startup the regionserver actually hung up because RPC was started before zk)

We should retry this error some at least on startup because its 'recoverable' (See http://wiki.apache.org/hadoop/ZooKeeper/ErrorHandling).  In fact, we should probably retry always until we get a session expired.",stack,stack,Major,Closed,Fixed,01/Oct/10 04:59,20/Nov/15 12:43
Bug,HBASE-3063,12475578,TestThriftServer failing in TRUNK,Delete of a table is removing regions from the filesystem while they are trying to close.  Not sure yet how to fix.,stack,stack,Major,Closed,Fixed,01/Oct/10 06:07,20/Nov/15 12:40
Bug,HBASE-3064,12475612,Long sleeping in HConnectionManager after thread is interrupted,"We run sometimes into the problem that when a thread running HBase client code is interrupted, it hangs. The problem is it is sleeping in HConnectionManager, in the methods locateRegionInMeta and getRegionServerWithRetries, where there is code like this:

{code}
try{
  Thread.sleep(getPauseTime(tries));
} catch (InterruptedException e) {
  // continue
}
{code}

which is located in a for-loop, so it will keep retrying even when someone requested the thread to stop its work.

The attached patch proposes as fix to re-assert the interrupted status of the thread and to throw an IOException. Some other cases of InterruptedException-handling in the same class do a similar thing, though sometimes returning null or breaking. I found returning null causes NPE's in other locations so I think it is better to throw an informative exception.

Side thought: I would not be against propagating the InterruptedException all the way up to the client APIs (HTable/HBaseAdmin), so that users who want to support interruptable threads do not have to check the interrupted flag. I'd need to check some more but I have the impression that now sometimes methods like HTable.get() simply return null when a thread is interrupted.

Some background on good ways of handling InterruptedExceptions can be found here:
http://www.ibm.com/developerworks/java/library/j-jtp05236.html",bruno,bruno,Major,Closed,Fixed,01/Oct/10 13:32,20/Nov/15 12:41
Bug,HBASE-3065,12475630,Retry all 'retryable' zk operations; e.g. connection loss,"The 'new' master refactored our zk code tidying up all zk accesses and coralling them behind nice zk utility classes.  One improvement was letting out all KeeperExceptions letting the client deal.  Thats good generally because in old days, we'd suppress important state zk changes in state.  But there is at least one case the new zk utility could handle for the application and thats the class of retryable KeeperExceptions.  The one that comes to mind is conection loss.  On connection loss we should retry the just-failed operation.  Usually the retry will just work.  At worse, on reconnect, we'll pick up the expired session event. 

Adding in this change shouldn't be too bad given the refactor of zk corralled all zk access into one or two classes only.

One thing to consider though is how much we should retry.  We could retry on a timer or we could retry for ever as long as the Stoppable interface is passed so if another thread has stopped or aborted the hosting service, we'll notice and give up trying.  Doing the latter is probably better than some kinda timeout.

HBASE-3062 adds a timed retry on the first zk operation.  This issue is about generalizing what is over there across all zk access.",liyin,stack,Blocker,Closed,Fixed,01/Oct/10 15:51,20/Nov/15 12:43
Bug,HBASE-3066,12475655,We don't put the port for hregionserver up into znode since new master,Found by jd,stack,stack,Major,Closed,Fixed,01/Oct/10 19:21,20/Nov/15 12:43
Bug,HBASE-3068,12475663,"IllegalStateException when new server comes online, is given 200 regions to open and 200th region gets timed out of regions in transition","Yesterday we committed a change that makes it so the master will crash is a zk transition that is unexpected.   Its extreme but good for highlighting bad state changes (we also started marking these as illegalstateexceptions yesterday too).

So, testing new master I brought up a new server.  Balancer tried to give new server 256 regions.

{code}
2010-10-01 16:01:42,972 INFO org.apache.hadoop.hbase.master.LoadBalancer: Calculated a load balance in 0ms. Moving 256 regions off of 7 overloaded servers onto 1 less loaded servers
{code}

Turns out we failed complete open of all 256 servers within the regions-in-transition timeout period so we tried to reassign.  The master aborted because region was in the PENDING_OPEN state when we went about assigning.

{code}
2010-10-01 16:02:28,809 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  usertable,user1128734802,1285701924906.006696a9bf346f8593df66728e18e029. state=PENDING_OPEN, ts=1285948921051
2010-10-01 16:02:28,809 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN or OPENING for too long, reassigning region=usertable,user1128734802,1285701924906.006696a9bf346f8593df66728e18e029.
2010-10-01 16:02:28,811 FATAL org.apache.hadoop.hbase.master.HMaster: Unexpected state trying to OFFLINE; usertable,user1128734802,1285701924906.006696a9bf346f8593df66728e18e029. state=PENDING_OPEN, ts=1285948921051
java.lang.IllegalStateException
    at org.apache.hadoop.hbase.master.AssignmentManager.setOfflineInZooKeeper(AssignmentManager.java:662)
    at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:632)
    at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:560)
    at org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor.chore(AssignmentManager.java:1102)
    at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
{code}
",stack,stack,Major,Closed,Fixed,01/Oct/10 20:51,20/Nov/15 12:41
Bug,HBASE-3073,12475677,"New APIs for Result, faster implementation for some calls","Our existing API for Result hasn't been given much love in the last year.  In the mean time, inefficiencies in the existing implementation have come to light, causing issues with benchmarks.  Furthermore, some people are finding the API both difficult to use as well as not useful enough (See: HBASE-1937).

I propose the following new APIs:
public List<KeyValue> getColumn(byte [] family, byte [] qualifier);
public KeyValue getColumnLatest(byte [] family, byte [] qualifier);

The implementation of these use a binary search on the underlying kvs array (which is sorted).  I also have new implementations for
public boolean containsColumn(byte [] family, byte [] qualifier);
public byte [] getValue(byte [] family, byte [] qualifier);

Which in the small case run faster, but in the big case seem to run a bit slower.  That is if you call getValue() 10 times for a Result it will be faster with the new implementation, but if you call getValue() 100 times for the same Result it is faster using the old implementation.  My tests indicated about 10% slower on 'getValue' 100x with an overall 1000x iteration on 1000 different Result objects.  Considering most people use getValue() to retrieve named columns and iteration when the qualifier list is unknown I think this is a reasonable trade off.

Along with the new API, there is a recommendation to use raw() to get the list of KeyValue objects for iteration.  This increases the visibility of KeyValue, and also is much faster to iterate (4.9 times on my mini benchmark, 100 columns per Result, redone 1000 times on different Result objects).

Given my recent major speed boost by changing YCSB to use the raw() interface, I think that this is a must have for 0.90.  ",ryanobjc,ryanobjc,Major,Closed,Fixed,01/Oct/10 23:54,12/Oct/12 06:17
Bug,HBASE-3074,12475696,Zookeeper test failing on hudson,"We're failing here because .META. moved:

{code}
retrying after sleep of 5000 because: Connection refused
2010-10-02 00:50:49,728 DEBUG [main] client.HConnectionManager$HConnectionImplementation(717): locateRegionInMeta attempt 2 of 4 failed; retrying after sleep of 5000 because: Connection refused
2010-10-02 00:50:59,730 DEBUG [main] client.HConnectionManager$HConnectionImplementation(717): locateRegionInMeta attempt 0 of 4 failed; retrying after sleep of 5000 because: Connection refused
2010-10-02 00:51:04,731 DEBUG [main] client.HConnectionManager$HConnectionImplementation(717): locateRegionInMeta attempt 1 of 4 failed; retrying after sleep of 5000 because: Connection refused
2010-10-02 00:51:09,732 DEBUG [main] client.HConnectionManager$HConnectionImplementation(717): locateRegionInMeta attempt 2 of 4 failed; retrying after sleep of 5000 because: Connection refused
2010-10-02 00:51:14,734 WARN  [main] client.HConnectionManager$HConnectionImplementation(597): Encounted problems when prefetch META table: 
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server vesta.apache.org:54172 for region .META.,,1, row 'test1285980613475,,99999999999999', but failed after 4 attempts.
Exceptions:
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused

	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:946)
	at org.apache.hadoop.hbase.client.HTable.getRowOrBefore(HTable.java:500)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:104)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:594)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:645)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:539)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:507)
	at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:287)
	at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:207)
	at org.apache.hadoop.hbase.TestZooKeeper.testSanity(TestZooKeeper.java:140)
...

{code}

I'm not sure why we're not picking up new locations.",stack,stack,Major,Closed,Fixed,02/Oct/10 23:45,20/Nov/15 12:43
Bug,HBASE-3079,12475789,Shell displaying uninformative exceptions,"The shell seems to hang longer than the normal client and then display uninformative messages when doing wrong things.

For example, inserting into a non-existing family or reading from a disabled table.  I believe in both these cases, HTable will throw informative exceptions like InvalidFamilyException and TableDisabledException (need to confirm exactly what it does).

But in the shell, I get things like:

Inserting to a family that does not exist:
{noformat}
ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedException: Still had 1 puts left after retrying 7 times.
{noformat}

Reading from a disabled table (this takes a long time before anything is displayed):
{noformat}

ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.0.0.4:62505 for region sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a., row '', but failed after 7 attempts.
Exceptions:
org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2221)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1812)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:557)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1007)

org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
org.apache.hadoop.hbase.client.RegionOfflineException: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a.
{noformat}",ryanobjc,streamy,Major,Closed,Fixed,04/Oct/10 18:40,12/Oct/12 06:17
Bug,HBASE-3080,12475836,TestAdmin hanging on hudson,TestAdmin is hanging in the enable/disable exercising code.,stack,stack,Major,Closed,Fixed,05/Oct/10 04:59,20/Nov/15 12:41
Bug,HBASE-3081,12475878,Log Splitting & Replay: Distinguish between Network IOE and Parsing IOE,"Originally, if HBase got an IOE from HDFS while splitting or opening a region, it would abort the operation. The assumption being that this is a network failure that will likely disappear at a later time or different partition of the network. However, if HBase gets parsing exceptions, we want to log the problem and continue opening/splitting the region anyways, because parsing is an idempotent problem and retries won't fix this issue.",nspiegelberg,nspiegelberg,Major,Closed,Fixed,05/Oct/10 18:54,12/Oct/12 06:17
Bug,HBASE-3085,12475908,TestSchemaResource broken on TRUNK up on HUDSON,,stack,stack,Major,Closed,Fixed,06/Oct/10 00:01,20/Nov/15 12:40
Bug,HBASE-3088,12476790,TestAvroServer and TestThriftServer broken because use same table in all tests and tests enable/disable/delete,"There is dross left up in zk when you disable a table so if you go create same table name in new test, it'll fail to enable because zk says table disabled.  Clean up any table mentions in memory and up in zk on delete of a table (as well as all the other stuff we do on table delete).",stack,stack,Major,Closed,Fixed,07/Oct/10 18:10,20/Nov/15 12:41
Bug,HBASE-3089,12476809,REST tests are broken locally and up in hudson,The RESTServlet is a singleton.  In our rest tests the singleton is carried over between tests that start a new mini cluster each time.  A while back we added cleanup of zk when we let go of a connection... Whats happen is that the singleton is using a cleaned up zookeeperwatcher,stack,stack,Major,Closed,Fixed,07/Oct/10 22:19,20/Nov/15 12:41
Bug,HBASE-3091,12476829,Fix TestKillingServersFromMaster in TRUNK; it just hangs since new master went in,This test depends on mechanisms much changed after new master went in so it just hangs; its not getting the confirmations it used expect.  This issue is about recasting the test -- because what it tests is useful (just reading the test I found bug in new master) -- so need to get it going again.  Marking critiical for 0.90.,stack,stack,Critical,Closed,Fixed,08/Oct/10 05:48,20/Nov/15 12:42
Bug,HBASE-3094,12476891,Fixes for miscellaneous broken tests,"So, tests have been failing so long with in particular, some tests running w/o finishing hiding behind them tests that have been broke for ages.  Broken tests has let a raft of brokenness to creep in.  This issue is a grab back of fixes for all those failing up on hudson.",stack,stack,Major,Closed,Fixed,08/Oct/10 19:37,20/Nov/15 12:41
Bug,HBASE-3095,12476892,Client needs to reconnect if it expires its zk session,"Clients use an HConnection down in their guts to connect to the hbase cluster.  Master-is-running and root-region-location are up in zk.   Setup of a new HConnection sets up a connection to ZooKeeper.  If the session with ZK expires for whatever reason -- in tests they would expire because zk ensemble was restarted across tests or we might expire because of a long GC, well, it'll be frustrating to users if we do not just try and resetup the zk connection.",jdcryans,stack,Major,Closed,Fixed,08/Oct/10 20:02,20/Nov/15 12:44
Bug,HBASE-3098,12476911,TestMetaReaderEditor is broken in TRUNK; hangs,It gets stuck.  Lars F reported it today.,stack,stack,Critical,Closed,Fixed,09/Oct/10 03:44,20/Nov/15 12:41
Bug,HBASE-3100,12476932,TestMergeTable failing in TRUNK,"This is an old crufty test that is failing in TRUNK.  We create regions -- user, root, and meta regions offline -- but then come along and disable them after starting up minicluster.  Confuses new master.  This test also does not emit logging up in hudson because its written against old-style testing... even has a subclass of the HBaseTestCase to do this crazy manual creation.",stack,stack,Major,Closed,Fixed,09/Oct/10 19:13,20/Nov/15 12:40
Bug,HBASE-3101,12476938,bin assembly doesn't include -tests or -source jars,"Currently the bin assembly tries to include the hbase-VERSION-tests.jar but there's a typo ""test"" instead of ""tests"" in the assembly descriptor, so it doesn't do so. Also, I think we should probably ship the -sources jar even in the -bin assembly - it's useful for people to point their IDE at it to get API javadocs, etc.",tlipcon,tlipcon,Major,Closed,Fixed,10/Oct/10 00:35,20/Nov/15 12:41
Bug,HBASE-3104,12477160,Disable TestMultiClusters; it doesn't really test anything and HBaseTestingUtility needs more work for this to pass anyways,,,stack,Major,Closed,Fixed,12/Oct/10 17:54,20/Nov/15 12:42
Bug,HBASE-3105,12477171,Scan.toString fails to escape binary start/stop rows,"Scan.toString calls Bytes.toString(this.startRow)  - same for stopRow.
Prints out ugly binary stuff in the logs.  Replace it with Bytes.toStringBinary()",,davelatham,Minor,Closed,Fixed,12/Oct/10 18:44,12/Jun/22 00:12
Bug,HBASE-3110,12477309,TestReplicationSink failing in TRUNK up on Hudson,,,stack,Major,Closed,Fixed,14/Oct/10 04:02,20/Nov/15 12:43
Bug,HBASE-3111,12477313,TestTableMapReduce broken up on hudson,"So, test has failed for various reasons since fixed over last week or so.  It is currently failing when the reducer starts up.  Its failing because its not picking up the zk servers location; its using stale config.",stack,stack,Major,Closed,Fixed,14/Oct/10 05:57,20/Nov/15 12:42
Bug,HBASE-3112,12477377,Enable and disable of table needs a bit of loving in new master,"The tools are in place to do a more reliable enable/disable of tables.  Some work has been done to hack in a basic enable/disable but its not enough -- see the test avro/thrift tests where a disable/enable/disable switchback can confuse the table state (and has been disabled until this issue addressed).

This issue is about finishing off enable/disable in the new master.   I think we need to add to the table znode an enabling/disabling state rather than have them binary with a watcher that will stop an enable (or disable) starting until the previous completes (Currently we atomically switch the state though the region close/open lags -- some work in enable/disable handlers helps in that they won't complete till all regions have transitioned.. but its not enough).

Need to add tests too.

Marking issue critical bug because loads of the questions we get on lists are about enable/disable probs.",stack,stack,Critical,Closed,Fixed,14/Oct/10 17:17,20/Nov/15 12:41
Bug,HBASE-3113,12477400,Don't reassign regions if cluster is being shutdown,"On stop of a cluster, handling a close message, we'll go ahead and reassign regions as per normal though the cluster up flag is false.  This is what cause the TestRegionRebalancing test to fail up on hudson just now, #1546.",stack,stack,Major,Closed,Fixed,14/Oct/10 21:18,20/Nov/15 12:41
Bug,HBASE-3114,12477425,Test up on hudson are leaking zookeeper ensembles,"Here is from a recent run up on Hudson:

{code}
2010-10-14 23:31:56,482 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21818
2010-10-14 23:31:56,483 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21819
2010-10-14 23:31:56,483 INFO  [main] zookeeper.MiniZooKeeperCluster(111): Failed binding ZK Server to client port: 21820
2010-10-14 23:31:56,522 INFO  [main] zookeeper.MiniZooKeeperCluster(125): Started MiniZK Server on client port: 21821
{code}

See how we start trying to bind to 21818 but we don't get a free port till we get to 21821?

Some test or tests is not cleaning up after itself leaving a running zk cluster or two about.

TestReplication looks to be suspect.  Here is its @AfterClass method:

{code}
  /**
   * @throws java.lang.Exception
   */
  @AfterClass
  public static void tearDownAfterClass() throws Exception {
    /* REENABLE
    utility2.shutdownMiniCluster();
    utility1.shutdownMiniCluster();
    */
  }
{code}

",stack,stack,Major,Closed,Fixed,15/Oct/10 06:08,20/Nov/15 12:43
Bug,HBASE-3121,12477677,[rest] Do not perform cache control when returning results,"The REST interface currently provides MaxAge hints to HTTP cache servers when returning results, but does not do so in a way that makes much sense. For some other responses such as scanner results or schema, the REST interface provides a NoCache hint. That seems appropriate. Otherwise, especially given the rich configuration languages of caching servers such as Varnish, it is probably not appropriate to manage cache policy in the REST interface. ",apurtell,apurtell,Major,Closed,Fixed,18/Oct/10 22:38,20/Nov/15 12:42
Bug,HBASE-3128,12477783,"On assign, if ConnectException, reassign another server","{code}

2010-10-19 13:29:56,116 DEBUG [Thread-216-EventThread] master.AssignmentManager(321): Handling transition=M_ZK_REGION_OFFLINE, server=172.24.152.112:51845, region=1028785192/.META.
2010-10-19 13:29:56,116 ERROR [Master:0;172.24.152.112:51845] master.ServerManager(573): Error connecting to region server
org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed setting up proxy interface org.apache.hadoop.hbase.ipc.HRegionInterface to /172.24.152.112:51850 after attempts=1
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:351)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:936)
	at org.apache.hadoop.hbase.master.ServerManager.getServerConnection(ServerManager.java:568)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:511)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:726)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:640)
	at org.apache.hadoop.hbase.master.AssignmentManager.assignMeta(AssignmentManager.java:935)
	at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:430)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:377)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:266)
	at java.lang.Thread.run(Thread.java:637)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:310)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:860)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:728)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:255)
	at $Proxy10.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:412)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:388)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:435)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:345)
	... 10 more
2010-10-19 13:29:56,116 WARN  [Master:0;172.24.152.112:51845] master.AssignmentManager(730): Failed assignment of .META.,,1.1028785192 to serverName=172.24.152.112,51850,1287520189982, load=(requests=0, regions=0, usedHeap=60, maxHeap=123)
java.lang.RuntimeException: Fatal error connection to RS
	at org.apache.hadoop.hbase.master.ServerManager.getServerConnection(ServerManager.java:574)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:511)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:726)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:640)
	at org.apache.hadoop.hbase.master.AssignmentManager.assignMeta(AssignmentManager.java:935)
	at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:430)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:377)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:266)
	at java.lang.Thread.run(Thread.java:637)
Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed setting up proxy interface org.apache.hadoop.hbase.ipc.HRegionInterface to /172.24.152.112:51850 after attempts=1
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:351)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:936)
	at org.apache.hadoop.hbase.master.ServerManager.getServerConnection(ServerManager.java:568)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:310)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:860)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:728)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:255)
	at $Proxy10.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:412)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:388)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:435)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:345)
	... 10 more

{code}",stack,stack,Major,Closed,Fixed,19/Oct/10 21:24,20/Nov/15 12:42
Bug,HBASE-3130,12477785,[replication] ReplicationSource can't recover from session expired on remote clusters,"Currently ReplicationSource cannot recover when its zookeeper connection to its remote cluster expires. HLogs are still being tracked, but a cluster restart is required to continue replication (or a rolling restart).",ctrezzo,jdcryans,Major,Closed,Fixed,19/Oct/10 21:38,20/Nov/15 12:41
Bug,HBASE-3136,12477936,Stale reads from ZK can break the atomic CAS operations we have in ZKAssign,"With ZK based region transitions, we rely on atomic state changes of regions in transition.  For example, an RS needs to atomically switch a node from OFFLINE to OPENING, or the master needs to delete nodes that are in OPENED state, etc...

The way we implement this is by:
- Read existing data (returns byte[] and version in Stat)
- Verify data is in expected state
- Update to the new state, passing the expected version previously read

This doesn't always work as expected because that initial read of the existing data could be a stale read (in ZK, writes are quorum writes but reads are not so you can get stale data).

Can provide a more explicit example if anyone is interested, but a fix is coming.",streamy,streamy,Blocker,Closed,Fixed,20/Oct/10 23:19,21/Jun/20 22:35
Bug,HBASE-3139,12477962,Server shutdown processor stuck because meta not online,"Playing with rolling restart I see that the server hosting root and meta can go down close to each other.  In below, note how we are processing server hosting -ROOT- and part of its processing involves reading .META. content to see what servers it was carrying.  Well, note that .META. is offline at time (our verification attempt failed because server had just been shutdown and verification got ConnectException).  So we pause the server shutdown processing till .META. comes back online -- only it never does.

{code}
2010-10-21 07:32:23,931 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper
2010-10-21 07:32:23,953 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Splitting logs for sv2borg182,60020,1287645693959                                                                                       2010-10-21 07:32:23,994 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper
2010-10-21 07:32:24,020 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x12bcd5b344b0115 Creating (or updating) unassigned node for 70236052 with OFFLINE state                                                    2010-10-21 07:32:24,045 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan for -ROOT-,,0.70236052 so generated a random one; hri=-ROOT-,,0.70236052, src=, dest=sv2borg181,60020,1287646329081; 8 (online=8, exclude=null) available servers                                                                                                                                                                                         2010-10-21 07:32:24,045 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region -ROOT-,,0.70236052 to sv2borg181,60020,1287646329081                                                                              2010-10-21 07:32:24,048 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Failed verification of .META.,,1; java.net.ConnectException: Connection refused
2010-10-21 07:32:24,048 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Current cached META location is not valid, resetting
2010-10-21 07:32:24,079 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=sv2borg181,60020,1287646329081, region=70236052/-ROOT-                                            2010-10-21 07:32:24,162 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=sv2borg181,60020,1287646329081, region=70236052/-ROOT-
2010-10-21 07:32:24,212 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=sv2borg181,60020,1287646329081, region=70236052/-ROOT-                                             2010-10-21 07:32:24,212 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for 70236052; deleting unassigned node                                                                             2010-10-21 07:32:24,212 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x12bcd5b344b0115 Deleting existing unassigned node for 70236052 that is in expected state RS_ZK_REGION_OPENED                              2010-10-21 07:32:24,238 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region -ROOT-,,0.70236052                                                                                                         2010-10-21 07:32:27,902 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=sv2borg183,60020,1287646347597, regionCount=0, userLoad=false                                                                        2010-10-21 07:32:30,523 INFO org.apache.hadoop.hbase.zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sv2borg184,60020,1287645693960]                                                    2010-10-21 07:32:30,523 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=sv2borg184,60020,1287645693960 to dead servers, submitted shutdown handler to be executed                                                        2010-10-21 07:32:36,254 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=sv2borg184,60020,1287646355951, regionCount=0, userLoad=false                                                                        2010-10-21 07:32:39,567 INFO org.apache.hadoop.hbase.zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sv2borg185,60020,1287645693959]                                                    2010-10-21 07:32:39,567 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=sv2borg185,60020,1287645693959 to dead servers, submitted shutdown handler to be executed                                                        2010-10-21 07:32:45,614 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=sv2borg185,60020,1287646365304, regionCount=0, userLoad=false                                                                        2010-10-21 07:32:48,652 INFO org.apache.hadoop.hbase.zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sv2borg186,60020,1287645693962]                                                    2010-10-21 07:32:48,652 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=sv2borg186,60020,1287645693962 to dead servers, submitted shutdown handler to be executed                                                        2010-10-21 07:32:50,097 INFO org.apache.hadoop.hbase.master.ServerManager: regionservers=8, averageload=93.38, deadservers=[sv2borg185,60020,1287645693959, sv2borg183,60020,1287645693959, sv2borg182,60020,1287645693959,        sv2borg184,60020,1287645693960, sv2borg186,60020,1287645693962]
....
{code}

We're supposed to have a thread of 5 executors to handle server shutdowns.  I see an executor stuck waiting on .META. but I dont see any others running.  Odd.  Trying to figure why executors are 1 only.

{code}
""MASTER_SERVER_OPERATIONS-sv2borg180:60000-1"" daemon prio=10 tid=0x0000000041dc7000 nid=0x50a4 in Object.wait() [0x00007f285d537000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:324)
    - locked <0x00007f286d150ce8> (a java.util.concurrent.atomic.AtomicBoolean)
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:359)
    at org.apache.hadoop.hbase.catalog.MetaReader.getServerUserRegions(MetaReader.java:487)
    at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:115)
    at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:150)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
{code}",stack,stack,Major,Closed,Fixed,21/Oct/10 08:30,20/Nov/15 12:40
Bug,HBASE-3140,12477984,Rest schema modification throw null pointer exception,"2010-10-21 14:41:38,462 ERROR org.mortbay.log: /node_table_modify/schema
java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.Bytes.toBytes(Bytes.java:400)
	at org.apache.hadoop.hbase.client.HBaseAdmin.addColumn(HBaseAdmin.java:597)
	at org.apache.hadoop.hbase.rest.SchemaResource.update(SchemaResource.java:153)
	at org.apache.hadoop.hbase.rest.SchemaResource.update(SchemaResource.java:177)
	at org.apache.hadoop.hbase.rest.SchemaResource.post(SchemaResource.java:204)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:187)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:70)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:279)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:121)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:136)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:121)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:136)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:86)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:136)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:74)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1357)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1289)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1239)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1229)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:420)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:497)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:684)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:390)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:943)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:843)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",d.worms,d.worms,Major,Closed,Fixed,21/Oct/10 12:44,20/Nov/15 12:43
Bug,HBASE-3141,12478016,Master RPC server needs to be started before an RS can check in,"Starting up an RPC server is done in two steps.  In the constructor, we instantiate the RPC server.  Then in startServiceThreads() we start() it.

If someone RPCs in between the instantiation and the start(), it seems that bad things can happen.  We need to make sure this can't happen and there aren't any races here.",ryanobjc,streamy,Critical,Closed,Fixed,21/Oct/10 18:50,20/Nov/15 12:43
Bug,HBASE-3143,12478021,Adding the tests' hbase-site.xml to the jar breaks some clients,Since our move to maven we started including the src/test/resources/hbase-site.xml to our test jar. This breaks the standalone clients that include it like Hive as depicted in HIVE-1597.,jdcryans,jdcryans,Critical,Closed,Fixed,21/Oct/10 19:25,20/Nov/15 12:40
Bug,HBASE-3145,12478111,hBase importtsv fails when the line contains no data.,"I've tried to import tsv data by using importtsv tools. But the task failed with the following errors.

10/10/23 02:56:52 INFO mapred.JobClient: Task Id : attempt_201010222300_0036_m_000016_2, Status : FAILED
java.lang.IllegalArgumentException: No columns to insert
        at org.apache.hadoop.hbase.client.HTable.validatePut(HTable.java:682)
        at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:544)
        at org.apache.hadoop.hbase.client.HTable.put(HTable.java:535)
        at org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter.write(TableOutputFormat.java:104)
        at org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter.write(TableOutputFormat.java:65)
        at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:523)
        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
        at org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvImporter.map(ImportTsv.java:241)
        at org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvImporter.map(ImportTsv.java:184)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:639)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:315)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:217)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)
        at org.apache.hadoop.mapred.Child.main(Child.java:211)

If the line contains invalid data, the parser should throw BadTsvLineException. But unfortunately, the codepath throws IllegalArgumentException for the empty line, and that wasn't caught in the map() function.
",kzk,kzk,Major,Closed,Fixed,22/Oct/10 19:56,12/Jun/22 00:30
Bug,HBASE-3146,12478129,"document feature ""hbase.regionserver.codecs""","a new feature ""hbase.regionserver.codecs"" allows you to poison pill your regionservers if any of the codecs listed do not test operational on server startup.  

the format of the value is ""lzo,gz"" with strings separated by commas.",stack,ryanobjc,Major,Closed,Fixed,23/Oct/10 00:37,20/Nov/15 12:40
Bug,HBASE-3147,12478198,"Regions stuck in transition after rolling restart, perpetual timeout handling but nothing happens","The rolling restart script is great for bringing on the weird stuff.  On my little loaded cluster if I run it, it horks the cluster and it doesn't recover.  I notice two issues that need fixing:

1. We'll miss noticing that a server was carrying .META. and it never gets assigned -- the shutdown handlers get stuck in perpetual wait on a .META. assign that will never happen.
2. Perpetual cycling of the this sequence per region not succesfully assigned:

{code}
 2010-10-23 21:37:57,404 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  usertable,user510588360,1287547556587.7f2d92497d2d03917afd574ea2aca55b. state=PENDING_OPEN,                       ts=1287869814294  45154 2010-10-23 21:37:57,404 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN or OPENING for too long, reassigning region=usertable,user510588360,1287547556587.                                     7f2d92497d2d03917afd574ea2aca55b.  45155 2010-10-23 21:37:57,404 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2bd57d1475046a Attempting to transition node 7f2d92497d2d03917afd574ea2aca55b from RS_ZK_REGION_OPENING to M_ZK_REGION_OFFLINE  45156 2010-10-23 21:37:57,404 WARN org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2bd57d1475046a Attempt to transition the unassigned node for 7f2d92497d2d03917afd574ea2aca55b from RS_ZK_REGION_OPENING to                 M_ZK_REGION_OFFLINE failed, the node existed but was in the state M_ZK_REGION_OFFLINE  45157 2010-10-23 21:37:57,404 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region transitioned OPENING to OFFLINE so skipping timeout, region=usertable,user510588360,1287547556587.7f2d92497d2d03917afd574ea2aca55b.  
,,,
{code}

Timeout period again elapses an then same sequence.

This is what I've been working on.",stack,stack,Major,Closed,Fixed,24/Oct/10 22:22,20/Nov/15 12:43
Bug,HBASE-3151,12478299,NPE when trying to read regioninfo from .META.,"This is an old issue perhaps in a new guise.  From the list, Sebastien Bauer reports:

{code}
> 2010-10-25 08:13:01,690 ERROR
> org.apache.hadoop.hbase.master.CatalogJanitor: Caught exception
> java.lang.NullPointerException
> 2010-10-25 08:13:24,385 INFO
> org.apache.hadoop.hbase.master.ServerManager: regionservers=2,
> averageload=2538


> 2010-10-23 20:16:17,890 DEBUG
> org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation:
> Cached location for .META.,,1.1028785192 is
> db2a.goldenline.pl:60020
> 2010-10-23 20:16:18,432 FATAL org.apache.hadoop.hbase.master.HMaster:
> Unhandled exception. Starting
> shutdown.
>
> java.lang.NullPointerException
>
>    at
> org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:75)
>
>    at
> org.apache.hadoop.hbase.util.Writables.getHRegionInfo(Writables.java:119)
>
>    at
> org.apache.hadoop.hbase.client.MetaScanner$1.processRow(MetaScanner.java:188)
>
>    at
> org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:157)
>
>    at
> org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:69)
>
>    at
> org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:54)
>
>    at
> org.apache.hadoop.hbase.client.MetaScanner.listAllRegions(MetaScanner.java:195)
>
>    at
> org.apache.hadoop.hbase.master.AssignmentManager.assignAllUserRegions(AssignmentManager.java:1048)
>
>    at
> org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:379)
>
>    at
> org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:265)
>
> 2010-10-23 20:16:18,433 INFO org.apache.hadoop.hbase.master.HMaster:
> Aborting
>
> 2010-10-23 20:16:18,433 DEBUG org.apache.hadoop.hbase.master.HMaster:
> Stopping service threads
{code}


I think he has an old master... checking.",stack,stack,Major,Closed,Fixed,26/Oct/10 04:49,20/Nov/15 12:43
Bug,HBASE-3155,12478380,HFile.appendMetaBlock() uses wrong comparator,"We hit this exception last night...

2010-10-26 01:20:20,056 INFO org.apache.hadoop.hbase.regionserver.StoreFile: Bloom added to HFile (...): 18752B, 13012/13601 (96%)
2010-10-26 01:20:20,056 INFO org.apache.hadoop.hbase.regionserver.HRegion: aborted compaction on region 04,04c84c80,1286302852528.77d461b19c7f410041f1d03f4823ef8b. after 20mins, 43sec
2010-10-26 01:20:20,056 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region 04,04c84c80,1286302852528.77d461b19c7f410041f1d03f4823ef8b.
java.lang.ArrayIndexOutOfBoundsException: 17
at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:860)
at org.apache.hadoop.hbase.KeyValue$KeyComparator.compareRows(KeyValue.java:1888)
at org.apache.hadoop.hbase.KeyValue$KeyComparator.compare(KeyValue.java:1822)
at org.apache.hadoop.hbase.io.hfile.HFile$Writer.appendMetaBlock(HFile.java:476)
at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.close(StoreFile.java:862)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:896)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:687)
at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:858)
at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:807)

The problem is that appendMetaBlock() is using the wrong comparator. although the variable is called 'rawComparator', it's actually a normal comparator (KeyComparator) that defaults to RawComparator if not specified. All meta sorting needs to be done using the actual Bytes.RAW_COMPARATOR.  This happened because >=2 things were inserted into meta. ",nspiegelberg,nspiegelberg,Critical,Closed,Fixed,26/Oct/10 18:46,20/Nov/15 12:42
Bug,HBASE-3158,12478407,Bloom File Writes Broken if keySize is large,"Yesterday, on our cluster, a region compact() kept crashing at giving this stack trace

2010-10-25 08:48:28,330 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region MailBox_dark_launch_2010_10-04,4b64b600,1286302852538.9183a3b91ebd289bab7724d028cffa69.
java.lang.IllegalArgumentException: maxValue must be > 0
at org.apache.hadoop.hbase.util.ByteBloomFilter.sanityCheck(ByteBloomFilter.java:170)
at org.apache.hadoop.hbase.util.ByteBloomFilter.<init>(ByteBloomFilter.java:156)
at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:707)
at org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(StoreFile.java:566)
at org.apache.hadoop.hbase.regionserver.Store.createWriterInTmp(Store.java:504)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:817)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:678)
at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:842)
at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:793)
at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:103)

The problem is that we are oveflowing ""int"" for ""bitSize"". The number of keys is about 272M, and we are using about 11 bit per key. So, bitSize ends up being > 2G..",nspiegelberg,nspiegelberg,Blocker,Closed,Fixed,26/Oct/10 23:50,20/Nov/15 12:43
Bug,HBASE-3159,12478421,Double play of OpenedRegionHandler for a single region; fails second time through and aborts Master,"Here is master log with annotations: http://people.apache.org/~stack/master.txt

Region in question is:

b8827a67a9d446f345095d25e1f375f7

The running code is doctored in that I've added in a bit of logging -- zk in particular -- and I've also removed what I thought was a provocation of this condition, reassign inside in an assign if server has gone away when we try the open rpc (Turns out we have the condition even w/o this code in place).

The log starts where the region in question timesout in RIT.

We assign it to 186.

Notice how we see 'Handling transition' for this region TWICE.  This means two OpenedRegionHandlers will be scheduled -- and so the failure to delete a znode already gone.

As best I can tell, the watcher for this region is triggered once only -- which is odd because how then the double scheduling of OpenedRegionHandler but also, why am I not seeing OPENING, OPENING, OPENED and only what I presume is an OPENED?",streamy,stack,Blocker,Closed,Fixed,27/Oct/10 05:53,20/Nov/15 12:41
Bug,HBASE-3163,12478544,"If we timeout PENDING_CLOSE and send another closeRegion RPC, need to handle NSRE from RS (comes as a RemoteException)","When we send a closeRegion RPC to an RS, we are catching NSRE but when the RS is the one throwing the NSRE, then it comes back as a RemoteException (then an NSRE) and we aren't unwrapping it properly.

We need to catch this and then deal with it appropriately.

Still tracking how this happened in the first place.",streamy,streamy,Critical,Closed,Fixed,28/Oct/10 06:10,20/Nov/15 12:41
Bug,HBASE-3164,12478596,"Handle case where we open META, ROOT has been closed but znode location not deleted yet, and try to update META location in ROOT","Carrying over from HBASE-3159, I ran into a case with TestRollingRestart with META and ROOT failing concurrently.  This is how it played out:

META is closed on an RS that has been stopped:
{noformat}
2010-10-27 22:47:27,709 DEBUG [RS_CLOSE_META-192.168.0.44,59709,1288244829038-0] handler.CloseRegionHandler(128): Closed region .META.,,1.1028785192
{noformat}

Then ROOT is closed on a different RS that has been stopped:
{noformat}
2010-10-27 22:47:28,863 DEBUG [RS_CLOSE_ROOT-192.168.0.44,59662,1288244792380-0] handler.CloseRegionHandler(128): Closed region -ROOT-,,0.70236052
{noformat}

A running RS is assigned META (the master isn't even aware yet that root has been closed, it is processing shutdown for RS 59709 but not yet received expired node for 59662):
{noformat}
2010-10-27 22:47:29,095 DEBUG [MASTER_META_SERVER_OPERATIONS-192.168.0.44:59629-0] master.AssignmentManager(908): No previous transition plan for .META.,,1.1028785192 so generated a random one; hri=.META.,,1.1028785192, src=, dest=192.168.0.44,59735,1288244843993; 3 (online=3, exclude=null) available servers
2010-10-27 22:47:29,095 DEBUG [MASTER_META_SERVER_OPERATIONS-192.168.0.44:59629-0] master.AssignmentManager(802): Assigning region .META.,,1.1028785192 to 192.168.0.44,59735,1288244843993
...
2010-10-27 22:47:29,123 DEBUG [RS_OPEN_META-192.168.0.44,59735,1288244843993-0] handler.OpenRegionHandler(69): Processing open of .META.,,1.1028785192
{noformat}

After finishing the open of META, the RS goes to update location in ROOT and gets:
{noformat}
2010-10-27 22:47:29,208 ERROR [RS_OPEN_META-192.168.0.44,59735,1288244843993-0] executor.EventHandler(154): Caught throwable while processing event M_RS_OPEN_META
java.lang.NullPointerException: No server for -ROOT-
	at org.apache.hadoop.hbase.catalog.MetaEditor.updateMetaLocation(MetaEditor.java:127)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1271)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:156)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}

This doesn't actually kill the RS, it's just a caught exception up in the generic EventHandler.  But we get left in a weird state.  Eventually master does the right thing and times-out the OPENING:
{noformat}
2010-10-27 22:48:15,694 INFO  [192.168.0.44:59629.timeoutMonitor] master.AssignmentManager$TimeoutMonitor(1444): Regions in transition timed out:  .META.,,1.1028785192 state=PENDING_OPEN, ts=1288244865700
2010-10-27 22:48:15,694 INFO  [192.168.0.44:59629.timeoutMonitor] master.AssignmentManager$TimeoutMonitor(1454): Region has been PENDING_OPEN for too long, reassigning region=.META.,,1.1028785192
{noformat}

But it chooses to assign it back to the same person because the plan is still there:
{noformat}
2010-10-27 22:48:15,702 DEBUG [192.168.0.44:59629.timeoutMonitor] master.AssignmentManager(916): Using preexisting plan=hri=.META.,,1.1028785192, src=, dest=192.168.0.44,59735,1288244843993
2010-10-27 22:48:15,702 DEBUG [192.168.0.44:59629.timeoutMonitor] master.AssignmentManager(802): Assigning region .META.,,1.1028785192 to 192.168.0.44,59735,1288244843993
{noformat}

But then the RS doesn't open it because it's actually already open on that server.  We fail the ROOT edit but then don't close the region out.
{noformat}
2010-10-27 22:48:15,805 INFO  [IPC Server handler 3 on 59735] regionserver.HRegionServer(1952): Received request to open region: .META.,,1.1028785192
2010-10-27 22:48:15,807 DEBUG [RS_OPEN_META-192.168.0.44,59735,1288244843993-0] handler.OpenRegionHandler(69): Processing open of .META.,,1.1028785192
2010-10-27 22:48:15,807 WARN  [RS_OPEN_META-192.168.0.44,59735,1288244843993-0] handler.OpenRegionHandler(84): Attempting open of .META.,,1.1028785192 but it's already online on this server
{noformat}

This continues indefinitely, once every minute.

1.  Address the race condition when we get the connection to the root server (could exist for meta too).  The blocking call thinks we have a location but then when we get the cached location and don't get one.

2.  If we do get this NPE writing to root (or maybe meta too), we should not just throw the exception all the way up to the EventHandler and log it and continue.  That just stops our META_OPEN in it's tracks.  We complete the open but just not the edit.  We don't roll-back in any way.

3.  If the master is assigning stuff out and a region says, hey, I'm already hosting this region... something must be up.  In this case, it would not have been good for the RS to tell the master that it was already hosting it because it was missing the root edit.  So maybe if this happens, the master asks the RS to close the region in question?  Dunno.

Probably more issues to think about around this :)

This seems to be extremely rare.  I have been running this TestRollingRestart script constantly and this only happens when I do a concurrent kill of the server hosting ROOT and then server hosting META, and then only sometimes, it does work more times than not.",streamy,streamy,Critical,Closed,Fixed,28/Oct/10 16:24,20/Nov/15 12:43
Bug,HBASE-3169,12478685,NPE when master joins running cluster if a RIT references a RS no longer present,,stack,stack,Major,Closed,Fixed,29/Oct/10 18:04,20/Nov/15 12:40
Bug,HBASE-3170,12478686,RegionServer confused about empty row keys,"I'm no longer sure about the expected behavior when using an empty row key (e.g. a 0-byte long byte array).  I assumed that this was a legitimate row key, just like having an empty column qualifier is allowed.  But it seems that the RegionServer considers the empty row key to be whatever the first row key is.
{code}
Version: 0.89.20100830, r0da2890b242584a8a5648d83532742ca7243346b, Sat Sep 18 15:30:09 PDT 2010

hbase(main):001:0> scan 'tsdb-uid', {LIMIT => 1}
ROW                           COLUMN+CELL                                                                          
 \x00                         column=id:metrics, timestamp=1288375187699, value=foo      
 \x00                         column=id:tagk, timestamp=1287522021046, value=bar         
 \x00                         column=id:tagv, timestamp=1288111387685, value=qux      
1 row(s) in 0.4610 seconds

hbase(main):002:0> get 'tsdb-uid', ''
COLUMN                        CELL                                                                                 
 id:metrics                   timestamp=1288375187699, value=foo                         
 id:tagk                      timestamp=1287522021046, value=bar                         
 id:tagv                      timestamp=1288111387685, value=qux                      
3 row(s) in 0.0910 seconds

hbase(main):003:0> get 'tsdb-uid', ""\000""
COLUMN                        CELL                                                                                 
 id:metrics                   timestamp=1288375187699, value=foo                         
 id:tagk                      timestamp=1287522021046, value=bar                         
 id:tagv                      timestamp=1288111387685, value=qux                      
3 row(s) in 0.0550 seconds
{code}

This isn't a parsing problem with the command-line of the shell.  I can reproduce this behavior both with plain Java code and with my asynchbase client.

Since I don't actually have a row with an empty row key, I expected that the first {{get}} would return nothing.",ddas,tsuna,Critical,Closed,Fixed,29/Oct/10 18:23,23/Sep/13 18:30
Bug,HBASE-3172,12478691,Reverse order of AssignmentManager and MetaNodeTracker in ZooKeeperWatcher,"Over in HBASE-3159, I reported a similar double-opened issue but for META.  This is because of the fact that the META unassigned znode has two different consumers {{MetaNodeTracker}} in use by the CatalogTracker and {{AssignmentManager}} which uses it for normal regions-in-transition stuff.

The AssignmentManager needs to go first, otherwise we can get double handling of state transitions.",streamy,streamy,Critical,Closed,Fixed,29/Oct/10 19:05,20/Nov/15 12:44
Bug,HBASE-3173,12478701,HBase 2984 breaks ability to specify BLOOMFILTER & COMPRESSION via shell,HBase 2984 breaks ability to specify BLOOMFILTER & COMPRESSION via shell,kannanm,kannanm,Minor,Closed,Fixed,29/Oct/10 21:06,20/Nov/15 12:40
Bug,HBASE-3175,12478710,Commit of HBASE-3160 broke TestPriorityCompactionQueue up on hudson,Not sure what's up but this is failing up on hudson.  Assigning nicolas so he takes a look.,nspiegelberg,streamy,Critical,Closed,Fixed,30/Oct/10 05:00,20/Nov/15 12:41
Bug,HBASE-3179,12478766,"Enable ReplicationLogsCleaner only if replication is, and fix its test","I'm seeing logs like this on a trunk master:

{noformat}
2010-10-31 11:21:13,793 DEBUG org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner: Didn't find this log in ZK, deleting: 1.1.1.1%3A60020.1288545516340
2010-10-31 11:21:13,793 WARN org.apache.hadoop.hdfs.DFSClient: File /HBASE/.oldlogs/1.1.1.1%3A60020.1288545516340 is beng deleted only through Trash org.apache.hadoop.fs.FsShell.delete because all deletes must go through Trash.
2010-10-31 11:21:13,798 DEBUG org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner: Didn't find this log in ZK, deleting: 1.1.1.1%3A60020.1288545899208
2010-10-31 11:21:13,798 WARN org.apache.hadoop.hdfs.DFSClient: File /HBASE/.oldlogs/1.1.1.1%3A60020.1288545899208 is beng deleted only through Trash org.apache.hadoop.fs.FsShell.delete because all deletes must go through Trash.
{noformat}

There are like 50 of these showing up once a minute.  I'm unsure if this is more valuable when replication is turned on, but should this be running when replication is not on in this cluster?  At the least there should be less logging, like one line per run.",jdcryans,streamy,Critical,Closed,Fixed,31/Oct/10 18:30,20/Nov/15 12:42
Bug,HBASE-3182,12478789,"If server hosting META dies or is stopping while processing another server shutdown, IOE accessing META stop shutdown handler from finishing","In TestRollingRestart, there is a test which kills server hosting ROOT then immediately kills server hosting META.  In a recent run this turned up a small race condition if the server hosting META is closing while we process shutdown of server hosting ROOT.

{noformat}
2010-10-31 20:41:34,621 ERROR [MASTER_META_SERVER_OPERATIONS-dev692.sf2p.facebook.com:54989-0] executor.EventHandler(154): Caught throwable while processing event M_META_SERVER_SHUTDOWN
org.apache.hadoop.ipc.RemoteException: java.io.IOException: Server not running
        at org.apache.hadoop.hbase.regionserver.HRegionServer.checkOpen(HRegionServer.java:2216)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1652)
        at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:561)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1025)

        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:749)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:255)
        at $Proxy8.openScanner(Unknown Source)
        at org.apache.hadoop.hbase.catalog.MetaReader.getServerUserRegions(MetaReader.java:495)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:125)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
{noformat}
",streamy,streamy,Critical,Closed,Fixed,01/Nov/10 04:04,20/Nov/15 12:41
Bug,HBASE-3183,12478791,Commits of HBASE-3102 and HBASE-3160 broke TestHeapSize,Both JIRAs added class members to Store.,streamy,streamy,Major,Closed,Fixed,01/Nov/10 06:39,20/Nov/15 12:44
Bug,HBASE-3184,12478825,Xmx setting in pom to use for tests/surefire does not appear to work,"I was running intense version of TestRollingRestart and changing the Xmx setting in our pom file wasn't changing anything.

What we have now is:

{noformat}
            <argLine>-enableassertions</argLine>
            <argLine>-Xmx1400m</argLine>
{noformat}

But on process listing (and through experimentation with my tests), only -enableassertions is used.

However, changing to below and it worked:

{noformat}
            <argLine>-enableassertions -Xmx1400m</argLine>
{noformat}

Just wanted to open a jira and see if i'm missing something from someone with more maven experience.",streamy,streamy,Major,Closed,Fixed,01/Nov/10 18:57,20/Nov/15 12:43
Bug,HBASE-3185,12478831,User-triggered compactions are triggering splits!,"Doh... This came in with original commit of master rewrite, not sure why it's in there.

compactRegion is calling region.shouldSplit(true);",streamy,streamy,Critical,Closed,Fixed,01/Nov/10 19:51,20/Nov/15 12:41
Bug,HBASE-3189,12478938,Stagger Major Compactions,"For pre-split regions, we can get into a case where the oldest HFile in a Store is pretty large and will not encounter a compaction within the 24hr major compact window.  If that's the case, we don't want multiple multi-GB major compactions being triggered at the same time.  Add ability to stagger the major compaction expiration window.",nspiegelberg,nspiegelberg,Minor,Closed,Fixed,02/Nov/10 21:22,20/Nov/15 12:43
Bug,HBASE-3191,12479027,FilterList with MUST_PASS_ONE and SCVF isn't working,"In a special case the FilterList with MUST_PASS_ONE operator doesn't work correctly:
- a filter in the list is a SingleColumValueFilter with filterIfMissing=true
- FilterList.filterKeyValue(KeyValue) is called
- SingleColumValueFilter.filterKeyValue(KeyValue) is called
- SingleColumValueFilter.filterKeyValue(KeyValue) returns ReturnCode.INCLUDE if the KeyValue doesn't match a column (to support filterIfMissing)
- FilterList.filterKeyValue(KeyValue) immediately returns ReturnCode.INCLUDE, remaining filters in the list aren't evaluated.

However it is required to evaluate remaining filters, otherwise filterRow() filters out rows in case the filter's filterKeyValue() saves state that is used by filterRow(). (SingleColumValueFilter, SkipFilter, WhileMatchFilter do so)
",,seelmann,Minor,Closed,Fixed,03/Nov/10 20:40,12/Oct/12 06:17
Bug,HBASE-3194,12479053,HBase should run on both secure and vanilla versions of Hadoop 0.20,"There have been a couple cases recently of folks trying to run HBase trunk (or 0.89 DRs) on CDH3b3 or secure Hadoop.    While HBase security is in the works, it currently only runs on secure Hadoop versions.  Meanwhile HBase trunk won't compile on secure Hadoop due to backward incompatible changes in org.apache.hadoop.security.UserGroupInformation.

This issue is to work out the minimal set of changes necessary to allow HBase to build and run on both secure and non-secure versions of Hadoop.  Though, with secure Hadoop, I don't even think it's important to target running with HDFS security enabled (and krb authentication).  Just allow HBase to build and run in both versions.

I think mainly this amounts to abstracting usage of UserGroupInformation and UnixUserGroupInformation.",stack,ghelmling,Major,Closed,Fixed,03/Nov/10 23:23,20/Nov/15 12:41
Bug,HBASE-3195,12479063,Fix the new TestTransform breakage up on hudson,"This new test has been failing up on hudson since it was introduce at #1606.  I took a look.  It looks reasonable but its failing in an odd way -- can't find blocks in  hdfs.

I'm moving it aside for now till test gets some loving.  Breakage lasted till at least #1613.",apurtell,stack,Major,Closed,Fixed,04/Nov/10 03:48,20/Nov/15 12:43
Bug,HBASE-3198,12479237,Log rolling archives files prematurely,"From the mailing list, Erdem Agaoglu found a case where when an HLog gets rolled from the periodic log roller and it gets archived even tho the region (ROOT) still has edits in the MemStore. I did an experiment on a local empty machine and it does look broken:

{noformat}
org.apache.hadoop.hbase.regionserver.LogRoller: Hlog roll period 6000ms elapsed
org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-200
org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase-89-su/.logs/hbasedev,60020,1288977933643/10.10.1.177%3A60020.1288977933829, entries=1,
 filesize=295. New hlog /hbase-89-su/.logs/hbasedev,60020,1288977933643/10.10.1.177%3A60020.1288977943913
org.apache.hadoop.hbase.regionserver.wal.HLog: Found 1 hlogs to remove  out of total 1; oldest outstanding sequenceid is 270055 from region -ROOT-,,0
org.apache.hadoop.hbase.regionserver.wal.HLog: moving old hlog file /hbase-89-su/.logs/hbasedev,60020,1288977933643/10.10.1.177%3A60020.1288977933829
 whose highest sequenceid is 270054 to /hbase-89-su/.oldlogs/10.10.1.177%3A60020.1288977933829
{noformat}

Marking as Blocker and taking a deeper look.",jdcryans,jdcryans,Blocker,Closed,Fixed,05/Nov/10 17:40,20/Nov/15 12:41
Bug,HBASE-3199,12479245,large response handling: some fixups and cleanups,"This may not be common for many use cases, but it might be good to put a couple of safety nets as well as logging to protect against large responses.

(i) Aravind and I were trying to track down why JVM memory usage was oscillating so much when dealing with very large buffers rather than OOM'ing or hitting some Index out of bound type exception, and this is what we found.

java.io.ByteArrayOutputStream graduates its internal buffers by doubling them. Also, it is supposed to be able to handle ""int"" sized buffers (2G). The code which handles ""write"" (in jdk 1.6) is along the lines of:

{code}
   public synchronized void write(byte b[], int off, int len) {
	if ((off < 0) || (off > b.length) || (len < 0) ||
            ((off + len) > b.length) || ((off + len) < 0)) {
	    throw new IndexOutOfBoundsException();
	} else if (len == 0) {
	    return;
	}
        int newcount = count + len;
        if (newcount > buf.length) {
            buf = Arrays.copyOf(buf, Math.max(buf.length << 1, newcount));
        }
        System.arraycopy(b, off, buf, count, len);
        count = newcount;
    }
{code}

The ""buf.length << 1"" will start producing -ve values when buf.length reaches 1G, and ""newcount"" will instead dictate the size of the buffer allocated. At this point, all attempts to write to the buffer will grow linearly, and the buffer will be resized by only the required amount on each write. Effectively, each write will allocate a new 1G buffer + reqd size buffer, copy the contents, and so on. This will put the process in heavy GC mode (with jvm heap oscillating by several GBs rapidly), and render it practically unusable.

(ii) When serializing a Result, the writeArray method doesn't assert that the resultant size does not overflow an ""int"".

{code}
    int bufLen = 0;
    for(Result result : results) {
      bufLen += Bytes.SIZEOF_INT;
      if(result == null || result.isEmpty()) {
        continue;
      }
      for(KeyValue key : result.raw()) {
        bufLen += key.getLength() + Bytes.SIZEOF_INT;
      }
    }
{code}

We should do the math in ""long"" and assert on bufLen values > Integer.MAX_VALUE.

(iii) In HBaseServer.java on RPC responses, we could add some logging on responses above a certain thresholds.

(iv) Increase buffer size threshold for buffers that are reused by RPC handlers. And make this configurable. Currently, any response buffer about 16k is not reused on next response. (HBaseServer.java).



",ryanobjc,kannanm,Major,Closed,Fixed,05/Nov/10 18:35,20/Nov/15 12:43
Bug,HBASE-3201,12479262,Add accounting of empty regioninfo_qualifier rows in meta to hbasefsck.,Make an accounting of empty HREGION_QUALIFIER rows in .META.  Emit count and details in hbase fsck too.,stack,stack,Major,Closed,Fixed,05/Nov/10 21:21,20/Nov/15 12:40
Bug,HBASE-3202,12479266,"Closing a region, if we get a ConnectException, handle it rather than abort","Got this testing:
{code}
2010-11-05 21:58:41,259 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region usertable,user1117790996,1288993795362.0d5d2055ef9d7ee5f11097d58461211f. (offlining)
2010-11-05 21:58:41,261 FATAL org.apache.hadoop.hbase.master.HMaster: Remote unexpected exception
java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
    at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:311)
    at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:865)
    at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:732)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
    at $Proxy1.closeRegion(Unknown Source)
    at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:564)
    at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1019)
    at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:966)
    at org.apache.hadoop.hbase.master.handler.DisableTableHandler.handleDisableTable(DisableTableHandler.java:84)
    at org.apache.hadoop.hbase.master.handler.DisableTableHandler.process(DisableTableHandler.java:64)
    at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:803)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1025)
2010-11-05 21:58:41,263 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{code}",stack,stack,Major,Closed,Fixed,05/Nov/10 22:10,20/Nov/15 12:40
Bug,HBASE-3203,12479269,We can get an order to open a region while shutting down and it'll hold up regionserver shutdown,"Starting and stopping clusters I see that an open directive can come in while we are shutting down all the user space regions on a particular regionserver.  Regionservers will only shut themselves down after all user space regions have closed.  We queue up all the closes at a particular time.  If an open comes in while we are in the closing condition, then the regionserver won't go down.  The region open needs to be blocked.",,stack,Major,Closed,Fixed,05/Nov/10 22:45,20/Nov/15 12:41
Bug,HBASE-3204,12479271,Reenable deferred log flush,"Deferred log flush was disabled a few months ago, reenable it and make it false by default.",jdcryans,jdcryans,Major,Closed,Fixed,05/Nov/10 23:19,20/Nov/15 12:42
Bug,HBASE-3205,12479416,TableRecordReaderImpl.restart NPEs when first next is restarted,"We got this pretty interesting NPE out of TableRecordReaderImpl.restart on a job that was filtering more than 99% of the data from a very huge table with caching set to 10k:

{noformat}
2010-11-08 13:08:22,344 DEBUG org.apache.hadoop.hbase.mapreduce.TableRecordReader:
 recovered from org.apache.hadoop.hbase.client.ScannerTimeoutException:
 61521ms passed since the last invocation, timeout is currently set to 60000
	at org.apache.hadoop.hbase.client.HTable$ClientScanner.next(HTable.java:956)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:132)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReader.nextKeyValue(TableRecordReader.java:142)
...
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
Caused by: org.apache.hadoop.hbase.UnknownScannerException: org.apache.hadoop.hbase.UnknownScannerException:
 Scanner was closed (timed out?) after we renewed it. Could be caused by a very slow scanner or a lengthy garbage collection
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2233)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2260)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1933)
...
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:81)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:37)
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getRegionServerWithRetries(HConnectionManager.java:1138)
	at org.apache.hadoop.hbase.client.HTable$ClientScanner.next(HTable.java:942)
	... 8 more

2010-11-08 13:08:22,347 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.Bytes.toStringBinary(Bytes.java:301)
	at org.apache.hadoop.hbase.client.HTable$ClientScanner.<init>(HTable.java:803)
	at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:484)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.restart(TableRecordReaderImpl.java:58)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.nextKeyValue(TableRecordReaderImpl.java:135)
...
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
2010-11-08 13:08:22,349 INFO org.apache.hadoop.mapred.TaskRunner
{noformat}

This is because the last row key we saw is set to null, since we haven't seen any yet :)",jdcryans,jdcryans,Critical,Closed,Fixed,08/Nov/10 21:16,12/Oct/12 06:17
Bug,HBASE-3207,12479433,"If we get IOException when closing a region, we should still remove it from online regions and complete the close in ZK","Ran into issue on cluster where HDFS was taken out from under it.  RS eventually tried to shut itself down.  As regions were being closed, they got IOException ""Filesystem closed"".  In the CloseRegionHandlers, this was causing the close operation to not finish (in ZK and in the online region list in RS).  That, in turn, held up the waitOnAllRegionsToClose() so the RS never shut down.

If we get an IOException during a close, which can happen if fatal error doing flush, this is not recoverable so we should complete the region close in ZK and by removing from map of online regions on that RS.",streamy,streamy,Major,Closed,Fixed,08/Nov/10 23:15,20/Nov/15 12:40
Bug,HBASE-3208,12479435,HLog.findMemstoresWithEditsOlderThan needs to look for edits that are equal to too,"Follow up to HBASE-3198, now that we are doing the right thing it seems that TestWALObserver is throwing NPEs. Digging in more, I figured that HLog.findMemstoresWithEditsOlderThan actually needs to be looking at edits that are equal or older rather than just older since logs with only 1 edit won't get any regions to flush.",jdcryans,jdcryans,Blocker,Closed,Fixed,08/Nov/10 23:39,20/Nov/15 12:43
Bug,HBASE-3212,12479541,More testing of enable/disable uncovered base condition not in place; i.e. that only one enable/disable runs at a time,"Testing, uncovered fact that master has 3 handlers currently for enable/disable/delete and modify when hbase-3112 was built on supposition that there was only one.",stack,stack,Major,Closed,Fixed,10/Nov/10 00:06,20/Nov/15 12:43
Bug,HBASE-3213,12479550,If do abort of backup master will get NPE instead of graceful abort,"Doesn't really matter because it's aborting the server anyways, but I've seen it on TestMasterFailover.  Simple fix.",streamy,streamy,Minor,Closed,Fixed,10/Nov/10 01:57,20/Nov/15 12:42
Bug,HBASE-3214,12479551,TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS is failing,Failing on hudson and locally,ghelmling,streamy,Major,Closed,Fixed,10/Nov/10 02:05,20/Nov/15 12:40
Bug,HBASE-3215,12479554,TestRollingRestart failing on hudson,Seems unrelated to HBASE-3214.  I think it must be in the one sleep thing I'm doing in this unit test.,streamy,streamy,Major,Closed,Fixed,10/Nov/10 02:48,20/Nov/15 12:41
Bug,HBASE-3219,12479653,Split parents are reassigned on restart and on disable/enable,J-D found this nice bug testing.,stack,stack,Major,Closed,Fixed,10/Nov/10 23:02,20/Nov/15 12:44
Bug,HBASE-3221,12479663,Race between splitting and disabling,"There's a race when you disable a table, if one of the region servers started the split before the call and it reports after the master scanned the .META. regions, then you're not disabling the daughter regions. I see this in the master log:

{noformat}
2010-11-10 15:29:35,990 INFO org.apache.hadoop.hbase.master.handler.DisableTableHandler: Attemping to disable table TestTable
2010-11-10 15:29:35,996 INFO org.apache.hadoop.hbase.master.handler.DisableTableHandler: Offlining 2 regions.
...
2010-11-10 15:29:39,014 INFO org.apache.hadoop.hbase.master.handler.DisableTableHandler: Disabled table is done=true
2010-11-10 15:29:39,105 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,,1289431761746.9de9305168be83098273e083f24ea5d8.:
 Daughters; TestTable,,1289431775593.a08b127a61b89e268129aa022fd18ce1.,
 TestTable,0001037720,1289431775593.4a5f831723ffbdb859d45510742d9926. from hbasedev,60020,1289431673756
...
{noformat}",jdcryans,jdcryans,Critical,Closed,Fixed,10/Nov/10 23:51,20/Nov/15 12:42
Bug,HBASE-3222,12479667,Regionserver region listing in UI is no longer ordered.,J-D spotted this one.,stack,stack,Major,Closed,Fixed,11/Nov/10 00:37,20/Nov/15 12:42
Bug,HBASE-3224,12479669,NPE in KeyValue$KVComparator.compare when compacting,"While testing normal insertion via PE, I got this recurrent NPE coming out of KeyValue$KVComparator.compare while it's compacting. So far I saw 2 different stack traces:

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.KeyValue$KVComparator.compare(KeyValue.java:1356)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:250)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:385)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:291)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:324)
	at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:926)
	at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:734)
{noformat}

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.KeyValue$KVComparator.compare(KeyValue.java:1375)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:180)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:156)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:146)
	at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:594)
	at java.util.PriorityQueue.siftUp(PriorityQueue.java:572)
	at java.util.PriorityQueue.offer(PriorityQueue.java:274)
	at java.util.PriorityQueue.add(PriorityQueue.java:251)
	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:258)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:385)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:291)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:324)
	at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:926)

	at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:734)
{noformat}",ryanobjc,jdcryans,Critical,Closed,Fixed,11/Nov/10 01:00,20/Nov/15 12:41
Bug,HBASE-3229,12479744,"Table creation, though using ""async"" call to master, can actually run for a while and cause RPC timeout","Our create table methods in HBaseAdmin are synchronous from client POV.  However, underneath, we're using an ""async"" create and then looping waiting for table availability.  Because the create is async and we loop instead of block on RPC, we don't expect RPC timeouts.

However, when creating a table with lots of initial regions, the ""async"" create can actually take a long time (more than 30 seconds in this case) which causes the client to timeout and gives impression something failed.

We should make the create truly async so that this can't happen.  And rather than doing one-off, inline assignment as it is today, we should reuse the fancy enable/disable code stack just added to make this faster and more optimal.",mingma,streamy,Critical,Closed,Fixed,11/Nov/10 19:49,20/Nov/15 12:42
Bug,HBASE-3230,12479822,Refresh our hadoop jar and update zookeeper to just-released 3.3.2.,,stack,stack,Major,Closed,Fixed,12/Nov/10 19:34,20/Nov/15 12:42
Bug,HBASE-3232,12479849,Fix KeyOnlyFilter + Add Value Length,"HBASE-3211 altered filter code to mutate KeyValues.  What could go wrong?  Well, your scan could mess up because the KVHeap compare functions don't work properly.  If we're going to soft mutate KVs in filter code, we also need to soft copy the KV before filtering.  This was found while adding the ability to have KeyOnlyFilter have the option to return the Value's length.  This is useful for grouping your reduce tasks into equal-sized blocks.",nspiegelberg,nspiegelberg,Blocker,Closed,Fixed,13/Nov/10 04:41,20/Nov/15 12:41
Bug,HBASE-3233,12479853,Fix Long Running Stats,"HBASE-3102 has a small bug.  Once long running stats are reset, the reset flag is never cleared.  This is a one-line fix for this issue. Verified on our test clusters.",nspiegelberg,nspiegelberg,Minor,Closed,Fixed,13/Nov/10 06:25,20/Nov/15 12:42
Bug,HBASE-3234,12479898,"hdfs-724 ""breaks"" TestHBaseTestingUtility multiClusters","We upgraded our hadoop jar in TRUNK to latest on 0.20-append branch.  TestHBaseTestingUtility started failing reliably.  If I back out hdfs-724, the test passes again.  This issue is about figuring whats up here.",hairong,stack,Critical,Closed,Fixed,14/Nov/10 06:51,20/Nov/15 12:43
Bug,HBASE-3235,12479902,Intermittent incrementColumnValue failure in TestHRegion,"I first saw this in a Hudson build, but can reproduce locally with enough test runs (5-10 times):

{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hbase.regionserver.TestHRegion
-------------------------------------------------------------------------------
Tests run: 51, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 39.413 sec <<< FAILURE!
testIncrementColumnValue_UpdatingInPlace(org.apache.hadoop.hbase.regionserver.TestHRegion)  Time elapsed: 0.079 sec  <<< FAILURE!
junit.framework.AssertionFailedError: expected:<1> but was:<2>
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.failNotEquals(Assert.java:283)
        at junit.framework.Assert.assertEquals(Assert.java:64)
        at junit.framework.Assert.assertEquals(Assert.java:195)
        at junit.framework.Assert.assertEquals(Assert.java:201)
        at org.apache.hadoop.hbase.regionserver.TestHRegion.testIncrementColumnValue_UpdatingInPlace(TestHRegion.java:1889)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}

Alternately, the failure can also show up in testIncrementColumnValue_UpdatingInPlace_Negative():
{noformat}
testIncrementColumnValue_UpdatingInPlace_Negative(org.apache.hadoop.hbase.regionserver.TestHRegion)  Time elapsed: 0.03 sec  <<< FAILURE!
junit.framework.AssertionFailedError: expected:<2> but was:<3>
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.failNotEquals(Assert.java:283)
        at junit.framework.Assert.assertEquals(Assert.java:64)
        at junit.framework.Assert.assertEquals(Assert.java:130)
        at junit.framework.Assert.assertEquals(Assert.java:136)
        at
org.apache.hadoop.hbase.regionserver.TestHRegion.assertICV(TestHRegion.java:2081)
        at
org.apache.hadoop.hbase.regionserver.TestHRegion.testIncrementColumnValue_UpdatingInPlace_Negative(TestHRegion.java:1990)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}",ghelmling,ghelmling,Blocker,Closed,Fixed,14/Nov/10 08:30,20/Nov/15 12:40
Bug,HBASE-3237,12480035,Split request accepted -- BUT CURRENTLY A NOOP,"The ""split"" button from the web UI displays this message and indeed seems to do nothing.",jdcryans,tlipcon,Blocker,Closed,Fixed,16/Nov/10 07:13,20/Nov/15 12:42
Bug,HBASE-3238,12480107,HBase needs to have the CREATE permission on the parent of its ZooKeeper parent znode,"Upon startup, HBase attempts to create its zookeeper.parent.znode in ZooKeeper, it does so using ZKUtil.createAndFailSilent which as its name seems to imply will fail silent if the znode exists. But if HBase does not have the CREATE permission on its zookeeper.parent.znode parent znode then the create attempt will fail with a org.apache.zookeeper.KeeperException$NoAuthException and will terminate the process.

In a production environment where ZooKeeper has a managed namespace it is not possible to give HBase CREATE permission on the parent of its parent znode.

ZKUtil.createAndFailSilent should therefore be modified to check that the znode exists using ZooKeeper.exist prior to attempting to create it.",posix4e,herberts,Blocker,Closed,Fixed,16/Nov/10 12:48,20/Nov/15 12:43
Bug,HBASE-3239,12480142,Handle null regions to flush in HLog.cleanOldLogs,"Note from Kannan

findMemstoresWithEditsEqualOrOlderThan() can return NULL it seems like. And we don't check NULL, before ""region.length"".

      regions = findMemstoresWithEditsEqualOrOlderThan(this.outputfiles.firstKey(),
        this.lastSeqWritten);
      StringBuilder sb = new StringBuilder();
      for (int i = 0; i < regions.length; i++) {

===


Stack Trace

2010-11-15 19:19:54,258 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: LRU Stats: total=6.1 GB, free=1.71 GB, max=7.81 GB, blocks=385740, accesses=7020255, hits=6329399, hitRatio=90.15%%, cachingAccesses=6765050, cachingHits=6329399, cachingHitsRatio=93.56%%, evictions=1, evicted=49911, evictedPerRun=49911.0
2010-11-15 19:21:05,204 INFO org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-200
2010-11-15 19:21:05,211 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /PUMAHBASE001-SNC5-HBASE/.logs/pumahbase042.snc5.facebook.com,60020,1289856892583/10.38.28.57%3A60020.1289877154987, entries=649004, filesize=255069060. New hlog /PUMAHBASE001-SNC5-HBASE/.logs/pumahbase042.snc5.facebook.com,60020,1289856892583/10.38.28.57%3A60020.1289877665062
2010-11-15 19:21:05,222 ERROR org.apache.hadoop.hbase.regionserver.LogRoller: Log rolling failed
java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanOldLogs(HLog.java:648)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:528)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:94)
2010-11-15 19:21:05,226 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server serverName=pumahbase042.snc5.facebook.com,60020,1289856892583, load=(requests=3476, regions=40, usedHeap=8388, maxHeap=15987): Log rolling failed
java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanOldLogs(HLog.java:648)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:528)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:94)
2010-11-15 19:21:05,227 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Dump of metrics: request=1264.5834, regions=40, stores=70, storefiles=98, storefileIndexSize=35, memstoreSize=83, compactionQueueSize=0, usedHeap=8370, maxHeap=15987, blockCacheSize=6593768536, blockCacheFree=1788154792, blockCacheCount=388283, blockCacheHitRatio=90, blockCacheHitCachingRatio=93
2010-11-15 19:21:05,227 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: STOPPED: Log rolling failed
2010-11-15 19:21:05,227 INFO org.apache.hadoop.hbase.regionserver.LogRoller: LogRoller exiting.
2010-11-15 19:21:07,255 INFO org.apache.hadoop.ipc.HBaseServer: Stopping server on 60020


===


",kannanm,khemani,Blocker,Closed,Fixed,16/Nov/10 20:22,20/Nov/15 12:42
Bug,HBASE-3241,12480164,check to see if we exceeded hbase.regionserver.maxlogs limit is incorrect,"In HLog.java:cleanOldLogs(), the number of logs left after archiving of old logs is computed as:

{code}
    int logCount = this.outputfiles.size() - logsToRemove;
{code}

However, the archival itself already removes the files that were archived from the ""this.outputfiles"" map. 


So shouldn't the above logic simply be the following?

{code}
  int logCount = this.outputfiles.size();
{code}

",kannanm,kannanm,Blocker,Closed,Fixed,17/Nov/10 00:18,20/Nov/15 12:41
Bug,HBASE-3249,12480394,Typing 'help shutdown' in the shell shouldn't shutdown the cluster,"_hp_ on IRC found out the bad way that typing 'help shutdown' actually gives you the full help... and shuts down the cluster. I don't really understand why we process both commands, putting against 0.90.0 if anyone has an idea.",stack,jdcryans,Major,Closed,Fixed,19/Nov/10 00:00,20/Nov/15 12:41
Bug,HBASE-3252,12480446,TestZooKeeperNodeTracker sometimes fails due to a race condition in test notification,"TestZooKeeperNodeTracker sometimes fails with errors like the following:

{noformat}
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.738 sec <<< FAILURE!
testNodeTracker(org.apache.hadoop.hbase.zookeeper.TestZooKeeperNodeTracker)  Time elapsed: 0.17 sec  <<< FAILURE!
java.lang.AssertionError: 
        at org.junit.Assert.fail(Assert.java:91)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.junit.Assert.assertTrue(Assert.java:54)
        at org.apache.hadoop.hbase.zookeeper.TestZooKeeperNodeTracker.testNodeTracker(TestZooKeeperNodeTracker.java:203)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}

Running the test locally this can happen as much as 25-50% of the time.

It looks like this is due to a basic race condition in the way the test is structured.  The test code uses:

{code}
    // Wait for zk event to be processed
    zkListener.waitForDataChange();
{code}

But, since zkListener is instantiated (and registered with ZooKeeperWatcher) prior to secondTracker (which is always the source of the failure), zkListener will be notified first of the change and there is a race condition between the subsequent test assertions and the secondTracker notification.

Attaching a patch with a simple fix of just instantiating secondTracker prior to zkListener so that it's registered (and notified) first.",,ghelmling,Minor,Closed,Fixed,19/Nov/10 20:31,20/Nov/15 12:41
Bug,HBASE-3253,12480466,Thrift's missing from all the repositories in pom.xml,"Follow up to what I said on the mailing list:

{quote}
Currently both trunk and 0.90's pom.xml are incomplete, we were
relying on Ryan's repo have the thrift pom but now that it was changed
to Stack's new comers cannot compile the project since that pom is
missing. Reported by kzk9 on IRC.

So either we had Ryan's repo back in the pom, or Stack copies the
files to his own repo, or we add a FB's repo that has it.
{quote}

I'm going to do it quickly by adding back Ryan's repo, it's pretty harmless.",jdcryans,jdcryans,Blocker,Closed,Fixed,19/Nov/10 23:25,20/Nov/15 12:43
Bug,HBASE-3258,12480600,EOF when version file is empty,"I somehow was able to get an empty hbase.version file on a test machine and when I start HBase I see:

{noformat}
starting master, logging to /data/jdcryans/git/hbase/bin/../logs/hbase-jdcryans-master-hbasedev.out
Exception in thread ""master-hbasedev:60000"" java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.HMaster.stopServiceThreads(HMaster.java:559)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:286)
{noformat}

And in the master's log:

{noformat}
2010-11-22 10:08:43,003 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.io.EOFException
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
        at java.io.DataInputStream.readUTF(DataInputStream.java:572)
        at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:151)
        at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:170)
        at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:226)
        at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:104)
        at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:89)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:337)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:273)
2010-11-22 10:08:43,006 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{noformat}

I thought that that kind of issue was solved a long time ago, but somehow it's there again. I'll fix by handling the EOF and also will look at that ugly NPE.",jdcryans,jdcryans,Blocker,Closed,Fixed,22/Nov/10 18:14,20/Nov/15 12:44
Bug,HBASE-3259,12480603,Can't kill the region servers when they wait on the master or the cluster state znode,"With a situation like HBASE-3258, it's easy to have the region servers stuck on waiting for either the master or the cluster state znode since it has no timeout. You have to kill -9 them to have them shutting down. This is very bad for usability.",jdcryans,jdcryans,Blocker,Closed,Fixed,22/Nov/10 18:51,20/Nov/15 12:41
Bug,HBASE-3261,12480621,NPE out of HRS.run at startup when clock is out of sync,"This is what I get when I start a region server that's not properly sync'ed:

{noformat}
Exception in thread ""regionserver60020"" java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:603)
	at java.lang.Thread.run(Thread.java:637)
{noformat}

I this case the line was:
{noformat}
hlogRoller.interruptIfNecessary();
{noformat}

I guess we could add a bunch of other null checks.

The end result is the same, the RS dies, but I think it's misleading.",jdcryans,jdcryans,Major,Closed,Fixed,22/Nov/10 23:31,20/Nov/15 12:44
Bug,HBASE-3262,12480632,TestHMasterRPCException uses non-ephemeral port for master,TestHMasterRPCException instantiates an HMaster but doesn't use an ephemeral port which can cause the test to fail if port already in use.,streamy,streamy,Major,Closed,Fixed,23/Nov/10 03:35,20/Nov/15 12:43
Bug,HBASE-3263,12480646,Stack overflow in AssignmentManager,"My test cluster experienced a switch outage earlier this week which threw the master into a really bad state. In the catch clause of AssignmentManager.assign, we recurse, and if all of the region servers are inaccessible, we do so until we get a stack overflow.",stack,tlipcon,Blocker,Closed,Fixed,23/Nov/10 07:59,20/Nov/15 12:42
Bug,HBASE-3264,12480647,Remove unnecessary Guava Dependency,"Currently, TableMapReduceUtil uses Guava for trivial functionality and addDependencyJars() currently adds Guava by default.  However, this jar is only necessary for the ImportTsv MR job.  This is annoying when naively bundling hbase jar with a MR job because you now need a second dependency jar.  Should default bundle with only critical dependencies and have jobs that need fancy Guava functionality explicitly include them.",nspiegelberg,nspiegelberg,Minor,Closed,Fixed,23/Nov/10 08:03,20/Nov/15 12:44
Bug,HBASE-3265,12480650,Regionservers waiting for ROOT while Master waiting for RegionServers,"After a cluster disastrophe due to a disconnected switch, I ended up in a state where the master was up with no region servers (see HBASE-3263). When I brought the RS back up, because of the aforementioned bug, the master didn't get itself into a happy state (internal datastructure had some null in it). So I killed the master and started it again. Now, the master is in ""Waiting for region servers to check in"" mode, and the region servers are in the following stack:

        - locked <0x00002aaab1bda5d0> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRoot(CatalogTracker.java:177)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:537)
        at java.lang.Thread.run(Thread.java:619)

I imagine what happened is that the RS got through ""tryReportForDuty"" with the old master, but the old master was unable to assign anything due to bad state. So, when it crashed, all the RS were stuck in waitForRoot(), and when I brought the new one up, no one was reporting for duty.",stack,tlipcon,Critical,Closed,Fixed,23/Nov/10 08:18,20/Nov/15 12:41
Bug,HBASE-3267,12480652,close_region shell command breaks region,"It used to be that you could use the close_region command from the shell to close a region on one server and have the master reassign it elsewhere. Now if you close a region, you get the following errors in the master log:

2010-11-23 00:46:34,090 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSING for region ffaa7999e909dbd6544688cc8ab303bd from server haus01.sf.cloudera.com,12020,1290501789693 but region was in  the state null and not in expected PENDI
2010-11-23 00:46:34,530 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: master:60000-0x12c537d84e10062 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/ffaa7999e909dbd6544688cc8ab303bd
2010-11-23 00:46:34,531 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x12c537d84e10062 Retrieved 128 byte(s) of data from znode /hbase/unassigned/ffaa7999e909dbd6544688cc8ab303bd and set watcher; region=usertable,user1951957302,1290501969
2010-11-23 00:46:34,531 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_CLOSED, server=haus01.sf.cloudera.com,12020,1290501789693, region=ffaa7999e909dbd6544688cc8ab303bd
2010-11-23 00:46:34,531 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region ffaa7999e909dbd6544688cc8ab303bd from server haus01.sf.cloudera.com,12020,1290501789693 but region was in  the state null and not in expected PENDIN

and the region just gets stuck closed",stack,tlipcon,Critical,Closed,Fixed,23/Nov/10 08:49,20/Nov/15 12:43
Bug,HBASE-3269,12480714,"HBase table truncate semantics seems broken as ""disable"" table is now async by default.","The new async design for disable table seems to have caused a side effect on the truncate command. (IRC chat with jdcryans)

Apparent Cause: 
""Disable"" is now async by default. When truncate is called, the disable operation returns immediately and when the drop is called, the disable operation is still not completed. This results in HMaster.checkTableModifiable() throwing a TableNotDisabledException.

With earlier versions, disable returned only after Table was disabled.",stack,svarma,Critical,Closed,Fixed,23/Nov/10 19:13,18/Sep/16 07:40
Bug,HBASE-3272,12480745,Remove no longer used options,"From Lars George list up on hbase-dev:

{code}
Hi,

I went through the config values as per the defaults XML file (still
going through it again now based on what is actually in the code, i.e.
those not in defaults). Here is what I found:

hbase.master.balancer.period - Only used in hbase-default.xml?

hbase.regions.percheckin, hbase.regions.slop - Some tests still have
it but not used anywhere else

zookeeper.pause, zookeeper.retries - Never used? Only in hbase-defaults.xml


And then there are differences between hardcoded and XML based defaults:

hbase.client.pause - XML: 1000, hardcoded: 2000 (HBaseClient) and 30 *
1000 (HBaseAdmin)

hbase.client.retries.number - XML: 10, hardcoded 5 (HBaseAdmin) and 2 (HMaster)

hbase.hstore.blockingStoreFiles - XML: 7, hardcoded: -1

hbase.hstore.compactionThreshold - XML: 3, hardcoded: 2

hbase.regionserver.global.memstore.lowerLimit - XML: 0.35, hardcoded: 0.25

hbase.regionserver.handler.count - XML: 25, hardcoded: 10

hbase.regionserver.msginterval - XML: 3000, hardcoded: 1000

hbase.rest.port - XML: 8080, hardcoded: 9090

hfile.block.cache.size - XML: 0.2, hardcoded: 0.0


Finally, some keys are already in HConstants, some are in local
classes and others used as literals. There is an issue open to fix
this though. Just saying.

Thoughts?
{code}",stack,stack,Major,Closed,Fixed,23/Nov/10 22:11,20/Nov/15 12:43
Bug,HBASE-3273,12480750,Set the ZK default timeout to 3 minutes,"Following HBASE-3272, Stack suggested that we up the ZK timeout and proposed that we set it to 3 minutes (he said that last part in person). This should cover most of the big GC pauses out there.",jdcryans,jdcryans,Major,Closed,Fixed,23/Nov/10 23:26,20/Nov/15 12:41
Bug,HBASE-3275,12480839,[rest] No gzip/deflate content encoding support,The REST gateway does not support gzip or deflate content encoding. This is a bug because it is a very common performance optimization and Jetty trivially supports it. ,apurtell,apurtell,Major,Closed,Fixed,24/Nov/10 18:04,20/Nov/15 12:43
Bug,HBASE-3277,12480864,HBase Shell zk_dump command broken,"HBase shell zk_dump command is broken. Typing ""zk_dump"" in the shell gives: 
hbase(main):002:0> zk_dump
ERROR: undefined method `dump' for #<Java::OrgApacheHadoopHbaseZookeeper::ZooKeeperWatcher:0xd6ee28>

Here is some help for this command:
Dump status of HBase cluster as seen by ZooKeeper.
",svarma,svarma,Major,Closed,Fixed,24/Nov/10 22:11,20/Nov/15 12:43
Bug,HBASE-3278,12480880,AssertionError in LoadBalancer,"While running PE with low splitting configuration, I got this:

{noformat}

2010-11-24 23:23:24,508 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,0002485653,...
2010-11-24 23:23:26,129 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,0004309306,...
2010-11-24 23:23:26,132 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,0001281491...
2010-11-24 23:23:26,162 FATAL org.apache.hadoop.hbase.master.HMaster$1: sv2borg180:61000-BalancerChoreerror
java.lang.AssertionError
	at org.apache.hadoop.hbase.master.LoadBalancer.balanceCluster(LoadBalancer.java:296)
	at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:679)
	at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:578)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
2010-11-24 23:23:26,163 INFO org.apache.hadoop.hbase.master.HMaster$1: sv2borg180:61000-BalancerChore exiting
2010-11-24 23:23:26,236 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,0000921369,...
{noformat}

The thread dies but the master survives. There's nothing specific before that in the log, just regions splitting.

The line in LoadBalancer is:

{code}
    assert(regionidx == regionsToMove.size());
{code}",streamy,jdcryans,Major,Closed,Fixed,24/Nov/10 23:53,20/Nov/15 12:43
Bug,HBASE-3280,12491551,YouAreDeadException being swallowed in HRS getMaster(),"In the HRS, when we lose our connection to the master, we enter into a loop where we keep trying to get the new master location in ZK and attempt to send our heartbeat.  Within tryRegionServerReport() we could get a YouAreDeadException, but we won't let it out.  This leads to the RS continuously heartbeating in to the master although the master keeps telling it to kill itself.",streamy,streamy,Major,Closed,Fixed,29/Nov/10 15:54,20/Nov/15 12:41
Bug,HBASE-3282,12491565,Need to retain DeadServers to ensure we don't allow previously expired RS instances to rejoin cluster,"Currently we clear a server from the deadserver set once we finish processing it's shutdown.  However, certain circumstances (network partitions, race conditions) could lead to the RS not doing a check-in until after the shutdown has been processed.  As-is, this RS will now be let back in to the cluster rather than rejected with YouAreDeadException.

We should hang on to the dead servers so we always reject them.

One concern is that the set will grow indefinitely.  One recommendation by stack is to use SoftReferences.",streamy,streamy,Major,Closed,Fixed,29/Nov/10 18:11,20/Nov/15 12:42
Bug,HBASE-3283,12491580,NPE in AssignmentManager if processing shutdown of RS who doesn't have any regions assigned to it,"When doing server shutdown, we do the following:

{noformat}
deadRegions = new TreeSet<HRegionInfo>(this.servers.remove(hsi));
{noformat}

This removes the list of regions currently assigned to the specified server from the master in-memory state.  But it's possible that the remove returns null (have seen it in running production cluster).

We should check if this is null and just return if so.",streamy,streamy,Major,Closed,Fixed,29/Nov/10 20:59,20/Nov/15 12:43
Bug,HBASE-3285,12491584,Hlog recovery takes too much time,"Currently HBase uses append to trigger the close of HLog during Hlog split. Append is a very expensive operation, which involves not only NameNode operations but creating a writing pipeline. If one of datanodes on the pipeline has a problem, this recovery may takes minutes. I'd like implement a lightweight NameNode operation to trigger lease recovery and make HBase to use this instead.",hairong,hairong,Major,Closed,Fixed,29/Nov/10 21:28,20/Nov/15 12:43
Bug,HBASE-3286,12491590,Master passes IP and not hostname back to region server,"Starting my little test cluster on the latest from 0.90, I see:

{noformat}
2010-11-29 23:21:34,131 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 1024 region(s) across 9 server(s), retainAssignment=true
2010-11-29 23:21:34,134 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 22 region(s) to sv2borg181,61020,1291072886282
2010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 24 region(s) to sv2borg182,61020,1291072885473
2010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 37 region(s) to sv2borg183,61020,1291072885646
2010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 25 region(s) to sv2borg184,61020,1291072886734
2010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 26 region(s) to sv2borg185,61020,1291072886606
2010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 70 region(s) to sv2borg186,61020,1291072885486
2010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 30 region(s) to sv2borg187,61020,1291072886355
2010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 89 region(s) to sv2borg188,61020,1291072885926
2010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 701 region(s) to sv2borg189,61020,1291072886739
{noformat}

After another restart:

{noformat}
2010-11-30 00:03:38,100 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 1024 region(s) across 9 server(s), retainAssignment=true
2010-11-30 00:03:38,103 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 71 region(s) to sv2borg181,61020,1291075409984
2010-11-30 00:03:38,103 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 82 region(s) to sv2borg182,61020,1291075409956
2010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 61 region(s) to sv2borg183,61020,1291075409952
2010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 122 region(s) to sv2borg184,61020,1291075409957
2010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 59 region(s) to sv2borg185,61020,1291075409955
2010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 71 region(s) to sv2borg186,61020,1291075409963
2010-11-30 00:03:38,105 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 52 region(s) to sv2borg187,61020,1291075411049
2010-11-30 00:03:38,105 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 254 region(s) to sv2borg188,61020,1291075410360
2010-11-30 00:03:38,105 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 252 region(s) to sv2borg189,61020,1291075409959
{noformat}

I also saw one time where everything was assigned to 189.",jdcryans,jdcryans,Major,Closed,Fixed,30/Nov/10 00:05,20/Nov/15 12:42
Bug,HBASE-3291,12491715,"If split happens while regionserver is going down, we can stick open.","J-D found this one testing.  He found that if a split comes in during shutdown of a regionserver, then the regionserver can stick open... and won't go down.

We fixed a similar problem in the past where if balancer cut in during shutdown and assigned a regionserver an region during shutdown, we'd open it and it'd cause us again to stick open.  We fixed that by introducing the 'closing' state.

Fix for the issue j-d found is to do closing check when onlining daughters.",stack,stack,Major,Closed,Fixed,01/Dec/10 00:15,20/Nov/15 12:43
Bug,HBASE-3294,12491802,WARN org.apache.hadoop.hbase.regionserver.Store: Not in set (double-remove?) org.apache.hadoop.hbase.regionserver.StoreScanner@76607d3d,"We see this ugly message in running hbase.  Its a little disorientating.  I added stack traces around the call to close.  Here are the two I see (The WARN message is the bad one... the INFO is the good case).  Looks like compaction is the one that triggers the WARN.

{code}
2010-12-01 18:20:28,307 WARN org.apache.hadoop.hbase.regionserver.Store: Not in set (double-remove?) org.apache.hadoop.hbase.regionserver.StoreScanner@68faedc7
java.io.IOException: WHY
    at org.apache.hadoop.hbase.regionserver.Store.deleteChangedReaderObserver(Store.java:583)
    at org.apache.hadoop.hbase.regionserver.StoreScanner.close(StoreScanner.java:204)
    at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:242)
    at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:326)
    at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:939)
    at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:748)
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:753)
    at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:698)
    at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:81)

2010-12-01 18:21:18,103 INFO org.apache.hadoop.hbase.regionserver.Store: CLOSE org.apache.hadoop.hbase.regionserver.StoreScanner@3dca256e
java.io.IOException: WHY
    at org.apache.hadoop.hbase.regionserver.Store.deleteChangedReaderObserver(Store.java:592)
    at org.apache.hadoop.hbase.regionserver.StoreScanner.close(StoreScanner.java:204)
    at org.apache.hadoop.hbase.regionserver.KeyValueHeap.close(KeyValueHeap.java:192)
    at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.close(HRegion.java:2361)
    at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2945)
    at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2844)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1566)
    at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1036)
{code}",stack,stack,Major,Closed,Fixed,01/Dec/10 19:19,20/Nov/15 12:42
Bug,HBASE-3295,12491803,Dropping a 1k+ regions table likely ends in a client socket timeout and it's very confusing,"I tried truncating a 1.6k regions table from the shell and, after the usual disabling timeout, I then got a socket timeout on the second invocation while it was dropping. It looked like this:

{noformat}
ERROR: java.net.SocketTimeoutException: Call to sv2borg180/10.20.20.180:61000 failed on socket timeout exception:
 java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch :
 java.nio.channels.SocketChannel[connected local=/10.20.20.180:59153 remote=sv2borg180/10.20.20.180:61000]
{noformat}

At first I thought that was coming from the master because HDFS was somehow slow, but then understood that it was my socket that timed out meaning that the master was still dropping the table. Calling truncate again, I got:

{noformat}
ERROR: Unknown table TestTable!
{noformat}

Which means that the table would be deleted... I learned later that it wasn't totally deleted after I shut down the cluster. So it leaves me in a situation where I have to manually delete the files on the FS and the remaining .META. entries.

Since I expect a few people will hit this issue rather soon, for 0.90.0, I propose we just set the socket timeout really high in the shell. For 0.90.1, or 0.92, we should do for drop what we do for disabling.",stack,jdcryans,Major,Closed,Fixed,01/Dec/10 19:29,20/Nov/15 12:42
Bug,HBASE-3296,12491806,Newly created table ends up disabled instead of assigned,"Something that was seen by someone on the channel yesterday and by me this morning, it's possible to create a table that ends up disabled and the 'create' calls times out. The master log looks like:

{noformat}
2010-12-01 19:32:52,350 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true, flushlogentries=1, optionallogflushinternal=1000ms
2010-12-01 19:32:52,450 INFO org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-200
2010-12-01 19:32:52,451 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: New hlog /hbase/TestTable/2e2099cd5fce907e670ce8596d9b2368/.logs/hlog.1291231972350
2010-12-01 19:32:52,451 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Using getNumCurrentReplicas--HDFS-826
2010-12-01 19:32:52,452 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.
2010-12-01 19:32:52,645 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.; next sequenceid=1
2010-12-01 19:32:52,784 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Added region TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368. to META
2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.: disabling compactions & flushes
2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.
2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.Store: closed info
2010-12-01 19:32:52,784 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.
2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: IPC Server handler 4 on 61000.logSyncer interrupted while waiting for sync requests
2010-12-01 19:32:52,784 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: IPC Server handler 4 on 61000.logSyncer exiting
2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: closing hlog writer in hdfs://sv2borg180:9100/hbase/TestTable/2e2099cd5fce907e670ce8596d9b2368/.logs
2010-12-01 19:32:52,908 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Moved 1 log files to /hbase/TestTable/2e2099cd5fce907e670ce8596d9b2368/.oldlogs
2010-12-01 19:32:52,966 INFO org.apache.hadoop.hbase.master.AssignmentManager: Table TestTable disabled; skipping assign of TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.
2010-12-01 19:32:52,967 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Table being disabled so deleting ZK node
 and removing from regions in transition, skipping assignment of region TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.
2010-12-01 19:32:52,967 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:61000-0x12c84725b4b00b6
 Deleting existing unassigned node for 2e2099cd5fce907e670ce8596d9b2368 that is in expected state RS_ZK_REGION_CLOSED
2010-12-01 19:32:52,968 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Tried to delete closed node for REGION =>
 {NAME => 'TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.', STARTKEY => '', ENDKEY => '', ENCODED =>
 2e2099cd5fce907e670ce8596d9b2368, TABLE => {{NAME => 'TestTable', FAMILIES => [{NAME => 'info', BLOOMFILTER
 => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE =>
 '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}} but it does not exist so just offlining
{noformat}",jdcryans,jdcryans,Blocker,Closed,Fixed,01/Dec/10 19:46,20/Nov/15 12:42
Bug,HBASE-3297,12491813,"If rows in .META. with no HRegionInfo cell, then hbck fails read of .META.","hbck was aborting read of .META. because adding empty row to TreeSet, TreeSet complained Result is not Comparable -- which it is not.

Here is a fix:

{code}
@@ -81,7 +83,7 @@ public class HBaseFsck {
   private boolean rerun = false; // if we tried to fix something rerun hbck
   private static boolean summary = false; // if we want to print less output
   // Empty regioninfo qualifiers in .META.
-  private TreeSet<Result> emptyRegionInfoQualifiers = new TreeSet<Result>();
+  private Set<Result> emptyRegionInfoQualifiers = new HashSet<Result>();
{code}
",stack,stack,Major,Closed,Fixed,01/Dec/10 20:55,20/Nov/15 12:42
Bug,HBASE-3298,12491817,Regionserver can close during a split causing double assignment,"A regionserver got a close message during a split, which seemed to cause the split to fail, and also caused a double assignment and orphaned region.

The log:

2010-11-30 18:29:24,310 INFO org.apache.hadoop.hbase.regionserver.HRegion: completed compaction on region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0
f01. after 40sec
2010-11-30 18:29:24,310 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd
5f0f01.
2010-11-30 18:29:24,312 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.: disabling compac
tions & flushes
2010-11-30 18:29:24,312 INFO org.apache.hadoop.hbase.regionserver.HRegion: Running close preflush of usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:24,312 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01
., current region memstore size 256.1m
2010-11-30 18:29:24,312 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2010-11-30 18:29:24,700 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd
5f0f01., flushing=false, writesEnabled=false
2010-11-30 18:29:27,291 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:27,291 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd
5f0f01., flushing=false, writesEnabled=false
2010-11-30 18:29:27,961 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01
/.tmp/1673662210031989562 to hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/1156800948892817282
2010-11-30 18:29:27,966 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/1156800948892
817282, entries=1157300, sequenceid=14819, memsize=256.1m, filesize=59.4m
2010-11-30 18:29:27,966 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f
01. 'IPC Server handler 1 on 60020'
2010-11-30 18:29:27,966 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 0 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 9 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 4 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 8 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 5 on 60020'
2010-11-30 18:29:27,966 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 6 on 60020'
2010-11-30 18:29:27,966 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~256.1m for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. in 3654ms, sequenceid=14819, compaction requested=true
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 7 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 3 on 60020'
2010-11-30 18:29:27,967 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. 'IPC Server handler 2 on 60020'
2010-11-30 18:29:28,005 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:28,011 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:28,012 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x2c765e0da906eb Creating unassigned node for 6e260cb69fda466a97f650debd5f0f01 in a CLOSING state
2010-11-30 18:29:28,074 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:28,074 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01., current region memstore size 4.8m
2010-11-30 18:29:28,074 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2010-11-30 18:29:28,079 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.: disabling compactions & flushes
2010-11-30 18:29:28,332 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/.tmp/1412284505922212951 to hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/3835216797592840515
2010-11-30 18:29:28,345 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/3835216797592840515, entries=21600, sequenceid=14835, memsize=4.8m, filesize=1.1m
2010-11-30 18:29:28,365 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~4.8m for region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01. in 291ms, sequenceid=14835, compaction requested=true
2010-11-30 18:29:28,381 DEBUG org.apache.hadoop.hbase.regionserver.Store: closed data
2010-11-30 18:29:28,381 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:28,382 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x2c765e0da906eb Attempting to transition node 6e260cb69fda466a97f650debd5f0f01 from RS_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2010-11-30 18:29:28,466 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x2c765e0da906eb Successfully transitioned node 6e260cb69fda466a97f650debd5f0f01 from RS_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2010-11-30 18:29:28,466 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Closed region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.
2010-11-30 18:29:29,000 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: usertable,user1476676931,1291170472014.829c72aa4c47a31bd71b735da7e33883.
2010-11-30 18:29:29,001 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of usertable,user1476676931,1291170472014.829c72aa4c47a31bd71b735da7e33883.
2010-11-30 18:29:29,013 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x2c765e0da906eb Creating unassigned node for 829c72aa4c47a31bd71b735da7e33883 in a CLOSING state
2010-11-30 18:29:29,079 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Running rollback of failed split of usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.; org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/splits/019cf676872050ee05c15a6a37655e8a/data/5976204784865149447.6e260cb69fda466a97f650debd5f0f01 File does not exist. [Lease.  Holder: DFSClient_hb_rs_sv4borg226,60020,1291168733786_1291168755622, pendingcreates: 1]
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1378)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1369)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:1424)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:1412)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.complete(NameNode.java:491)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:512)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:968)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:964)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:962)

2010-11-30 18:29:29,107 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing usertable,user1476676931,1291170472014.829c72aa4c47a31bd71b735da7e33883.: disabling compactions & flushes
2010-11-30 18:29:29,107 INFO org.apache.hadoop.hbase.regionserver.HRegion: Running close preflush of usertable,user1476676931,1291170472014.829c72aa4c47a31bd71b735da7e33883.
2010-11-30 18:29:29,108 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for usertable,user1476676931,1291170472014.829c72aa4c47a31bd71b735da7e33883., current region memstore size 63.4m
2010-11-30 18:29:29,108 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2010-11-30 18:29:29,118 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/1156800948892817282, isReference=false, isBulkLoadResult=false, seqid=14819, majorCompaction=false
2010-11-30 18:29:29,163 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/1485319185163827758, isReference=false, isBulkLoadResult=false, seqid=12873, majorCompaction=false
2010-11-30 18:29:29,179 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/2866498733799966821, isReference=false, isBulkLoadResult=false, seqid=12619, majorCompaction=false
2010-11-30 18:29:29,219 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/3835216797592840515, isReference=false, isBulkLoadResult=false, seqid=14835, majorCompaction=false
2010-11-30 18:29:29,237 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/444462617223715350, isReference=false, isBulkLoadResult=false, seqid=12005, majorCompaction=false
2010-11-30 18:29:29,247 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/5536858845826091476, isReference=false, isBulkLoadResult=false, seqid=13173, majorCompaction=false
2010-11-30 18:29:29,254 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/5976204784865149447, isReference=false, isBulkLoadResult=false, seqid=13427, majorCompaction=false
2010-11-30 18:29:29,275 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/8614929289191582955, isReference=false, isBulkLoadResult=false, seqid=6111, majorCompaction=false
2010-11-30 18:29:29,281 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://sv4borg235:9000/hbase-rc1/usertable/6e260cb69fda466a97f650debd5f0f01/data/9150811729171627958, isReference=false, isBulkLoadResult=false, seqid=12265, majorCompaction=false
2010-11-30 18:29:29,283 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.; next sequenceid=14836
2010-11-30 18:29:29,284 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Successful rollback of failed split of usertable,user877196232,1291170014467.6e260cb69f
da466a97f650debd5f0f01.
2010-11-30 18:29:29,284 INFO org.apache.hadoop.hbase.regionserver.HRegion: NOT compacting region usertable,user877196232,1291170014467.6e260cb69fda466a97f650debd5f0f01.: compacting=false, writesEnabled=false


",ryanobjc,ryanobjc,Blocker,Closed,Fixed,01/Dec/10 21:38,20/Nov/15 12:43
Bug,HBASE-3299,12491831,"If failed open, we don't output the IOE","Jon found this one.... Here's fix:

{code}
Index: src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java   (revision 1040359)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java   (working copy)
@@ -111,8 +111,8 @@
           }
         });
     } catch (IOException e) {
-      LOG.error(""IOException instantiating region for "" + regionInfo +
-        ""; resetting state of transition node from OPENING to OFFLINE"");
+      LOG.error(""Failed open of "" + regionInfo +
+        ""; resetting state of transition node from OPENING to OFFLINE"", e);
       try {
         // TODO: We should rely on the master timing out OPENING instead of this
         // TODO: What if this was a split open?  The RS made the OFFLINE
@@ -216,4 +216,4 @@
     }
     return openingVersion;
   }
-}
\ No newline at end of file
+}
{code}",,stack,Major,Closed,Fixed,01/Dec/10 22:55,20/Nov/15 12:41
Bug,HBASE-3301,12491851,Treat java.net.SocketTimeoutException same as ConnectException assigning/unassigning regions.,I saw java.net.SocketTimeoutException on my cluster this evening.,stack,stack,Major,Closed,Fixed,02/Dec/10 05:56,20/Nov/15 12:40
Bug,HBASE-3304,12491951,Get spurious master fails during bootup,"the log says:


2010-12-01 20:42:21,115 WARN
org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation:
Remove exception connecting to RS
org.apache.hadoop.ipc.RemoteException:
org.apache.hadoop.hbase.ipc.ServerNotRunningException: Server is not
running yet
       at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1035)

       at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:753)
       at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
       at $Proxy6.getProtocolVersion(Unknown Source)
       at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:419)
       at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:393)
       at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:444)
       at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:349)
       at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:953)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:384)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRootServerConnection(CatalogTracker.java:210)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRootRegionLocation(CatalogTracker.java:453)
       at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:421)
       at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:379)
       at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:274)
2010-12-01 20:42:21,118 FATAL org.apache.hadoop.hbase.master.HMaster:
Unhandled exception. Starting shutdown.
org.apache.hadoop.hbase.ipc.ServerNotRunningException:
org.apache.hadoop.hbase.ipc.ServerNotRunningException: Server is not
running yet
       at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1035)

       at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
       at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
       at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
       at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
       at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:96)
       at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:959)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:384)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRootServerConnection(CatalogTracker.java:210)
       at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRootRegionLocation(CatalogTracker.java:453)
       at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:421)
       at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:379)
       at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:274)
2010-12-01 20:42:21,119 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2010-12-01 20:42:21,119 DEBUG org.apache.hadoop.hbase.master.HMaster:
Stopping service threads


then the master exits.  the cluster doesn't start.",ryanobjc,ryanobjc,Blocker,Closed,Fixed,03/Dec/10 01:34,20/Nov/15 12:40
Bug,HBASE-3309,12492050,""" Not running balancer because dead regionserver processing"" is a lie","When running the balancer I see the message:

{noformat}
 Not running balancer because dead regionserver processing
{noformat}

But that's not true, it does run, because the check is wrong:

{noformat}
if (!this.serverManager.areDeadServersInProgress()) {
  LOG.debug(""Not running balancer because dead regionserver processing"");
}
{noformat}

Also it doesn't return false like it should.",jdcryans,jdcryans,Major,Closed,Fixed,04/Dec/10 00:23,20/Nov/15 12:41
Bug,HBASE-3310,12492077,Failing creating/altering table with compression agrument from the HBase shell,"HColumnDescriptor setCompressionType takes Compression.Algorithm and not String

hbase(main):007:0> create 't1', { NAME => 'f', COMPRESSION => 'lzo'}

ERROR: cannot convert instance of class org.jruby.RubyString to class org.apache.hadoop.hbase.io.hfile.Compression$Algorithm

",iranitov,iranitov,Major,Closed,Fixed,04/Dec/10 22:08,20/Nov/15 12:43
Bug,HBASE-3311,12492089,Addendum patch on HBASE-3298 broke buid,"Our addendum patch on HBASE-3298 removed reassign if an unassign failed; it makes no sense doing an assign on failed unassign -- it makes for double-assign (at least, reasoning through the possible failures, i can't find case where we'd want to assign on failed unassign).  Well, TestMasterFailover depended on this behavior.  Its the test that exercises all of this zk stuff.  It tries its best to manufacture messy scenarios.  One scenario adds a PENDING_CLOSE but it doesn't have an associated server.  A null server was returning a 'false' when we tried to do the close rpc which would fall into the since removed reassign code -- which would pick up a server, and the test would go on to succeed.

Removing reassign on unassign broke this.",,stack,Major,Closed,Fixed,05/Dec/10 04:44,20/Nov/15 12:43
Bug,HBASE-3313,12492528,Table name isn't checked in isTableEnabled/isTableDisabled,"Currently when we enable or disable a table in the shell, since we don't verify the table name with HTableDescriptor.isLegalTableName in isTableEnabled and isTableDisabled, we get the following exception:

{noformat}
ERROR: java.lang.IllegalArgumentException: Invalid path string ""/jdcryans180/table/TestTable
"" caused by invalid charater @28
{noformat}

This is coming out of ZooKeeper. Instead we should check the table name ourselves in order to give a better feedback.",yuzhihong@gmail.com,jdcryans,Major,Closed,Fixed,06/Dec/10 18:56,20/Nov/15 12:41
Bug,HBASE-3314,12492535,[shell] 'move' is broken,"I cannot use 'move' in the shell. throws a casting error. It's a small fix, and I will commit at the same time a change for assign and unassign to make it more like the rest of the code that already exists.",jdcryans,jdcryans,Major,Closed,Fixed,06/Dec/10 19:39,20/Nov/15 12:40
Bug,HBASE-3315,12492560,Add debug output for when balancer makes bad balance,Balancer had assertions at end of the balanceCluster method.  These assertions trigger on occasion -- just did for me and did previously for j-d -- only there's no data to analyze when it fails.  Add logging data on balancer input and summary.,stack,stack,Major,Closed,Fixed,07/Dec/10 00:00,20/Nov/15 12:43
Bug,HBASE-3317,12492671,Javadoc and Throws Declaration for Bytes.incrementBytes() is Wrong,"The throws declaration for Bytes.incrementBytes() states that an IOException is thrown by the method, and javadocs suggest that this is expected if the byte array's size is larger than SIZEOF_LONG.

The code actually uses an IllegalArgumentException, which is probably more appropriate anyways. This should be changed to simplify the code that uses this method.",ekohlwey,ekohlwey,Trivial,Closed,Fixed,07/Dec/10 22:09,20/Nov/15 12:40
Bug,HBASE-3318,12492684,Split rollback leaves parent with writesEnabled=false,"I saw a split rollback today, and it left the region in a state where it was able to take writes, but wasn't able to flush or compact. It's printing this message every few milliseconds:

{noformat}
NOT flushing memstore for region xxx., flushing=false, writesEnabled=false
{noformat}

I see why, writesEnabled is never set back in HRegion.initialize:

{code}
// See if region is meant to run read-only.
if (this.regionInfo.getTableDesc().isReadOnly()) {
  this.writestate.setReadOnly(true);
}
{code}

Instead it needs to pass isReadOnly into the setReadOnly method to work correctly.

I think it should go in 0.90.0 if there's a new RC.",jdcryans,jdcryans,Critical,Closed,Fixed,07/Dec/10 23:52,20/Nov/15 12:44
Bug,HBASE-3321,12492786,Replication.join shouldn't clear the logs znode,Replication.join calls zkHelper.deleteOwnRSZNode and it shouldn't since ReplicationSourceManager does it already with a check. Currently you can lose replication data if there where still stuff to replicate.,jdcryans,jdcryans,Critical,Closed,Fixed,09/Dec/10 01:29,20/Nov/15 12:43
Bug,HBASE-3323,12492801,OOME in master splitting logs,"In testing a RS failure under heavy increment workload I ran into an OOME when the master was splitting the logs.

In this test case, I have exactly 136 bytes per log entry in all the logs, and the logs are all around 66-74MB). With a batch size of 3 logs, this means the master is loading about 500K-600K edits per log file. Each edit ends up creating 3 byte[] objects, the references for which are each 8 bytes of RAM, so we have 160 (136+8*3) bytes per edit used by the byte[]. For each edit we also allocate a bunch of other objects: one HLog$Entry, one WALEdit, one ArrayList, one LinkedList$Entry, one HLogKey, and one KeyValue. Overall this works out to 400 bytes of overhead per edit. So, with the default settings on this fairly average workload, the 1.5M log entries takes about 770MB of RAM. Since I had a few log files that were a bit larger (around 90MB) it exceeded 1GB of RAM and I got an OOME.

For one, the 400 bytes per edit overhead is pretty bad, and we could probably be a lot more efficient. For two, we should actually account this rather than simply having a configurable ""batch size"" in the master.

I think this is a blocker because I'm running with fairly default configs here and just killing one RS made the cluster fall over due to master OOME.",tlipcon,tlipcon,Blocker,Closed,Fixed,09/Dec/10 08:02,20/Nov/15 12:41
Bug,HBASE-3326,12492869,Replication state's znode should be created else it defaults to false,"When you start replication from a fresh new 0.90, the replication state isn't written so it defaults to false (eg nothing gets replicated). That's very confusing, should be created by the first thread that sees it.",jdcryans,jdcryans,Major,Closed,Fixed,09/Dec/10 19:06,20/Nov/15 12:40
Bug,HBASE-3330,12492907,Add publish of a snapshot to apache repo to our pom,See here http://www.apache.org/dev/publishing-maven-artifacts.html#publish-snapshot,stack,stack,Major,Closed,Fixed,10/Dec/10 05:19,20/Nov/15 12:44
Bug,HBASE-3332,12492967,Regions stuck in transition after RS failure,"Testing 0.90rc2 I ran into this issue. The test scenario was to kill -9 the server hosting ROOT and META, and before it had been detected, run ""balancer"" from the shell. After logs were split and regions were reassigned, I ended up with some regions stuck in transition.",streamy,tlipcon,Blocker,Closed,Fixed,10/Dec/10 19:01,20/Nov/15 12:41
Bug,HBASE-3337,12493100,Restore HBCK fix of unassignment and dupe assignment for new master,"HBCK fixing of region unassignment and duplicate assignment was broken with the move to the new master.

We've seen it happen doing testing of 0.90 RCs like that over in HBASE-3332.

Rather than the old ""clear everything out approach"" which relied on the BaseScanner, in the new master we should just manipulate unassigned ZK nodes and let the master handle the transition.",streamy,streamy,Critical,Closed,Fixed,13/Dec/10 18:31,20/Nov/15 12:41
Bug,HBASE-3343,12493114,Server not shutting down after losing log lease,"Ran into this bug testing 0.90rc2. I kill -STOPed a server, and then -CONT it after its logs had been split. It correctly decided it should abort, but got stuck during the shutdown process.",stack,tlipcon,Critical,Closed,Fixed,13/Dec/10 19:48,20/Nov/15 12:41
Bug,HBASE-3344,12493117,Master aborts after RPC to server that was shutting down,"I was doing a rolling restart during a bunch of splits happening, and the master aborted with the following:

2010-12-13 12:24:55,536 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region usertable,user1590589031,1291843166306.dbcbe21b3447c78560802962b87fd34f. (offlining)
2010-12-13 12:24:55,537 FATAL org.apache.hadoop.hbase.master.HMaster: Remote unexpected exception
java.io.IOException: Call to haus03.sf.cloudera.com/172.29.5.34:60020 failed on local exception: java.io.EOFException
        at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:788)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:757)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
        at $Proxy6.closeRegion(Unknown Source)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:589)
        at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1085)
        at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1032)
        at org.apache.hadoop.hbase.master.AssignmentManager.balance(AssignmentManager.java:1791)
        at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:688)
        at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:579)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:521)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:459)
2010-12-13 12:24:55,541 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
",,tlipcon,Blocker,Closed,Fixed,13/Dec/10 20:27,20/Nov/15 12:42
Bug,HBASE-3347,12493120,Can't truncate/disable table that has rows in .META. that have empty info:regioninfo column,"I somehow manufactured empty info:regioninfo cells in .META. -- still trying to figure how -- but trying to drop the table I get NPE

{code}
ERROR: java.lang.NullPointerException: null
Backtrace: org/apache/hadoop/hbase/util/Writables.java:75:in `getWritable'
           org/apache/hadoop/hbase/util/Writables.java:119:in `getHRegionInfo'
           org/apache/hadoop/hbase/client/HConnectionManager.java:505:in `processRow'
           org/apache/hadoop/hbase/client/MetaScanner.java:190:in `metaScan'
           org/apache/hadoop/hbase/client/MetaScanner.java:95:in `metaScan'
           org/apache/hadoop/hbase/client/MetaScanner.java:73:in `metaScan'
           org/apache/hadoop/hbase/client/HConnectionManager.java:530:in `getHTableDescriptor'
           org/apache/hadoop/hbase/client/HTable.java:320:in `getTableDescriptor'
           /home/stack/hbase/bin/../bin/../src/main/ruby/hbase/admin.rb:205:in `truncate'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands/truncate.rb:33:in `command'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:64:in `format_simple_command'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands/truncate.rb:31:in `command'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:31:in `command_safe'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:70:in `translate_hbase_exceptions'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:31:in `command_safe'
           /home/stack/hbase/bin/../bin/../src/main/ruby/shell.rb:106:in `command'
           (eval):2:in `truncate'
           (hbase):4:in `irb_binding'
{code}",stack,stack,Major,Closed,Fixed,13/Dec/10 21:03,20/Nov/15 12:41
Bug,HBASE-3351,12493138,ReplicationZookeeper goes to ZK every time a znode is modified,"While debugging other issues, I found that ReplicationAdmin.ReplicationStatusTracker is doing a ZK request every time a znode changes. Also reading the replication state shouldn't go to ZK if we already maintain a local variable that's updated with the tracker.",jdcryans,jdcryans,Major,Closed,Fixed,13/Dec/10 23:46,20/Nov/15 12:41
Bug,HBASE-3352,12493140,enabling a non-existent table from shell prints no error,"hbase(main):001:0> enable 'testtable'
0 row(s) in 0.3120 seconds

Only thing is that I don't have a table called 'testtable'",stack,tlipcon,Major,Closed,Fixed,14/Dec/10 00:16,20/Nov/15 12:41
Bug,HBASE-3353,12493153,table.jsp doesn't handle entries in META without server info,"I have a table where one of the rows in META is missing server info. table.jsp doesn't check for this case, so it throws an NPE, which is then dumped to the .out log rather than anywhere where someone might find it. We should catch this case.",tlipcon,tlipcon,Critical,Closed,Fixed,14/Dec/10 03:35,20/Nov/15 12:42
Bug,HBASE-3355,12493229,Stopping a stopped cluster leaks an HMaster,"This is a very annoying bug, I have two clusters running on the same machine so I often stop the wrong one (both are for testing). When it happens, it leaves a HMaster running with this:

{noformat}

""main"" prio=10 tid=0x0000000041dd3800 nid=0x55d5 in Object.wait() [0x00007f3c7165a000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007f3bac94f528> (a java.lang.Object)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:382)
	- locked <0x00007f3bac94f528> (a java.lang.Object)
	at org.apache.hadoop.hbase.client.HBaseAdmin.<init>(HBaseAdmin.java:90)
	at org.apache.hadoop.hbase.master.HMasterCommandLine.stopMaster(HMasterCommandLine.java:160)
	at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:104)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
	at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1058)
{noformat}

And if I happen to restart that cluster right away, IT KILLS IT!",jdcryans,jdcryans,Major,Closed,Fixed,14/Dec/10 19:08,20/Nov/15 12:41
Bug,HBASE-3356,12493237,Add more checks in replication if RS is stopped,"In a few places in the replication code I abort the region server if I get ZK exceptions, which often happens when closing down a region server cleanly (wasn't the case before). It results in a bunch of exceptions being thrown for nothing, adding more checks should resolve this easily.",jdcryans,jdcryans,Major,Closed,Fixed,14/Dec/10 19:51,20/Nov/15 12:40
Bug,HBASE-3357,12493244,"lockID parameter not used in HRegion.get(get, lockid)","Not sure what the intention is, but the lockId is never used in HRegion's

{code}
  public Result get(final Get get, final Integer lockid) throws IOException
{code}

If we are not locking on Get's then we can drop this parameter?",,larsgeorge,Major,Closed,Fixed,14/Dec/10 21:53,12/Jun/22 00:40
Bug,HBASE-3358,12493255,Recovered replication queue wait on themselves when terminating,"When I ported the replication code on the new master I broke a few things, and it seems that the threads created for recovered queues never properly terminate because they join on themselves. Easy fix in ReplicationSource:

{code}
-      this.terminate(""Finished recovering the queue"");
+      LOG.info(""Finished recovering the queue"");
+      this.running = false;
{code}",jdcryans,jdcryans,Major,Closed,Fixed,15/Dec/10 00:14,20/Nov/15 12:43
Bug,HBASE-3359,12493256,LogRoller not added as a WAL listener when replication is enabled,"Another thing I broke when porting to the new master, LogRoller is started but never added to HLog meaning that only hourly rolls work. Easy fix in HRS:

{code}
     this.hlogRoller = new LogRoller(this, this);
     listeners.add(this.hlogRoller);
     if (this.replicationHandler != null) {
-      listeners = new ArrayList<WALObserver>();
       // Replication handler is an implementation of WALActionsListener.
       listeners.add(this.replicationHandler);
{code}",jdcryans,jdcryans,Major,Closed,Fixed,15/Dec/10 00:16,20/Nov/15 12:41
Bug,HBASE-3360,12493258,ReplicationLogCleaner is enabled by default in 0.90 -- causes NPE,"{code}
2010-12-15 00:33:17,706 ERROR org.apache.hadoop.hbase.master.LogCleaner: Caught exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.isLogDeletable(ReplicationLogCleaner.java:59)
        at org.apache.hadoop.hbase.master.LogCleaner.chore(LogCleaner.java:138)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
        at org.apache.hadoop.hbase.master.LogCleaner.run(LogCleaner.java:165)
{code}

Assigning J-D at his request.",jdcryans,stack,Major,Closed,Fixed,15/Dec/10 00:35,20/Nov/15 12:42
Bug,HBASE-3362,12493361,"If .META. offline between OPENING and OPENED, then wrong server location in .META. is possible","This is a good one.  It happened to me testing OOME in split logging.

* Balancer moves region to new location, regionservrer X.
* New location regionserver X successfully opens the region and then goes to update .META.
* At this point, the server carrying .META. crashes.
* Regionserver X is stuck waiting on .META. to come back online.  It takes so long master times out the region-in-transition
* Master assigns the region elsewhere to regionserver Y
* It opens successfully on regionserver Y and then it also parks waiting on .META. coming online
* .META. comes online
* The two servers X and Y race to update .META.

I saw case where server X edit went in after server Ys edit which means that lookups in .META. get the wrong server.  HBCK can detect this situation.

RegionServer X when it wakes up coreeclty notices that its lost control of the region but the damage is done -- where damage is .META. edit.

Chatting with Jon, he suggested that regionserver X should 'rollback' the .META. edit -- do explicit delete of what it added.  This would work I think but chatting more, I'll make a fix that keeps updating the zookeeper OPENING state while edit goes on in a separate thread.  Our continuous setting of OPENING will make it so region-in-transition does not timeout.",stack,stack,Critical,Closed,Fixed,15/Dec/10 20:50,20/Nov/15 12:42
Bug,HBASE-3364,12493375,ReplicationZookeeper.lockOtherRS doesn't return false when lock exists,"ReplicationZookeeper.lockOtherRS is missing a ""return false"" when handling the KeeperException.",jdcryans,jdcryans,Major,Closed,Fixed,15/Dec/10 23:28,20/Nov/15 12:43
Bug,HBASE-3365,12493378,EOFE contacting crashed RS causes Master abort,"Just got this testing:

{code}
2010-12-16 00:05:02,863 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region TestTable,0071897074,1292373519828.8cec43d5df41ea830b08180f688f2819. to sv2borg181,60020,1292457487454
2010-12-16 00:05:02,867 FATAL org.apache.hadoop.hbase.master.HMaster: Remote unexpected exception
java.io.IOException: Call to sv2borg185/10.20.20.185:60020 failed on local exception: java.io.EOFException
    at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:788)
    at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:757)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
    at $Proxy7.closeRegion(Unknown Source)
    at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:589)
    at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1085)
    at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1032)
    at org.apache.hadoop.hbase.master.AssignmentManager.balance(AssignmentManager.java:1791)
    at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:688)
    at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:579)
    at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
Caused by: java.io.EOFException
    at java.io.DataInputStream.readInt(DataInputStream.java:375)
    at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:521)
    at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:459)
2010-12-16 00:05:02,868 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{code}",stack,stack,Major,Closed,Fixed,16/Dec/10 00:20,20/Nov/15 12:40
Bug,HBASE-3366,12493386,WALObservers should be notified before the lock,"TestReplication failed on the last 0.90 build because the log of the machine that was killed was rolled and got a few edits in before it was put in a ZK queue.

This fix is pretty easy, the notification should be sent before the lock is acquired instead of after that. It shouldn't have been like that since the beginning but I guess I missed it when I narrowed the lock.",jdcryans,jdcryans,Major,Closed,Fixed,16/Dec/10 01:37,20/Nov/15 12:42
Bug,HBASE-3367,12493396,Failed log split not retried,"Found this running TestReplication:

{noformat}
2010-12-15 17:58:33,639 DEBUG [MASTER_SERVER_OPERATIONS-h17.sfo.stumble.net:58644-0] wal.HLogSplitter(299)
: Closed hdfs://localhost:58631/user/jdcryans/test/211477a0a924abda419b5579c7a83452/recovered.edits/0000000000000000002
2010-12-15 17:58:33,642 ERROR [MASTER_SERVER_OPERATIONS-h17.sfo.stumble.net:58644-0] master.MasterFileSystem(197):
 Failed splitting hdfs://localhost:58631/user/jdcryans/.logs/h17.sfo.stumble.net,58647,1292464631034
java.io.IOException: Discovered orphan hlog after split. Maybe HRegionServer was not dead when we started
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:290)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:151)
        at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:193)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:96)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
2010-12-15 17:58:33,686 INFO  [MASTER_SERVER_OPERATIONS-h17.sfo.stumble.net:58644-0] handler.ServerShutdownHandler(144):
 Reassigning 8 region(s) that h17.sfo.stumble.net,58647,1292464631034 was carrying (skipping 0 regions(s) that are already in transition)
{noformat}

What I see is that there was an orphan HLog, but the exception was eaten in MasterFileSystem.splitLog (it just logs as an error) and then it proceeds to reassign the regions. There is potential data loss.

Another bad side effect is that those HLogs never get archived, and stay in .logs",,jdcryans,Blocker,Closed,Fixed,16/Dec/10 05:43,20/Nov/15 12:41
Bug,HBASE-3369,12493472,We should use HBaseClient.call() that has parallelism built in ,"the HBaseClient supports a call like so:

  public Writable[] call(Writable[] params, InetSocketAddress[] addresses)

And we are able to dispatch and retrieve multiple requests without threads.  We should try to use this instead of using thread pools.",,ryanobjc,Major,Closed,Fixed,16/Dec/10 23:19,12/Jun/22 00:41
Bug,HBASE-3370,12493478,ReplicationSource.openReader fails to locate HLogs when they aren't split yet,"In ReplicationSource.openReader there's a special handling of HLogs location during a failover and it's currently broken as it will retry 10 times and if the log splitting isn't done by then then it will skip the file forever.

First issue is that it looks at the chain of failovers but not the oldest one.

Second issue is that the location is found starting from the current RS's folder, instead of the dead RS's folder.",jdcryans,jdcryans,Major,Closed,Fixed,17/Dec/10 01:06,20/Nov/15 12:43
Bug,HBASE-3371,12493561,Race in TestReplication can make it fail,I found that truncating in TestReplication.setUp would sometime fail when during testStartStop the log would be rolled and not added to ReplicationSource's queue.,jdcryans,jdcryans,Major,Closed,Fixed,17/Dec/10 23:33,20/Nov/15 12:42
Bug,HBASE-3374,12493621,Our jruby jar has *GPL jars in it; fix,"The latest JRuby's complete jar bundles *GPL jars (JNA and JFFI among others).   It looks like the functionality we depend on -- the shell in particular -- makes use of these dirty jars so they are hard to strip.  They came in because we (I!) just updated our JRuby w/o checking in on what updates contained.  JRuby has been doing this for a while now (1.1.x added the first LGPL).  You have to go all the ways back to the original HBase checkin, HBASE-487, of JRuby -- 1.0.3 -- to get a JRuby w/o *GPL jars.

Plan is to try and revert our JRuby all the ways down to 1.0.3 before shipping 0.90.0.  Thats what this issue is about.

We should also look into moving off JRuby in the medium to long-term.  Its kinda awkward sticking on an old version that is no longer supported.  I'll open an issue for that.",stack,stack,Major,Closed,Fixed,19/Dec/10 06:15,20/Nov/15 12:40
Bug,HBASE-3380,12493779,Master failover can split logs of live servers,"The reason why TestMasterFailover fails is that when it does the master failover, the new master doesn't wait long enough for all region servers to checkin so it goes ahead and split logs... which doesn't work because of the way lease timeouts work:

{noformat}
2010-12-21 07:30:36,977 DEBUG [Master:0;vesta.apache.org:33170] wal.HLogSplitter(256): Splitting hlog 1 of 1:
 hdfs://localhost:49187/user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3A38743.1292916617204, length=0
2010-12-21 07:30:36,977 DEBUG [WriterThread-1] wal.HLogSplitter$WriterThread(619): Writer thread Thread[WriterThread-1,5,main]: starting
2010-12-21 07:30:36,977 DEBUG [WriterThread-2] wal.HLogSplitter$WriterThread(619): Writer thread Thread[WriterThread-2,5,main]: starting
2010-12-21 07:30:36,977 INFO  [Master:0;vesta.apache.org:33170] util.FSUtils(625): Recovering file
 hdfs://localhost:49187/user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3A38743.1292916617204
2010-12-21 07:30:36,979 WARN  [IPC Server handler 8 on 49187] namenode.FSNamesystem(1122): DIR* NameSystem.startFile:
 failed to create file /user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3A38743.1292916617204 for
 DFSClient_hb_m_vesta.apache.org:33170_1292916630791 on client 127.0.0.1, because this file is already being created by
 DFSClient_hb_rs_vesta.apache.org,38743,1292916616340_1292916617166 on 127.0.0.1
...
2010-12-21 07:33:44,332 WARN  [Master:0;vesta.apache.org:33170] util.FSUtils(644): Waited 187354ms for lease recovery on
 hdfs://localhost:49187/user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3A38743.1292916617204:
 org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file
 /user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3A38743.1292916617204
 for DFSClient_hb_m_vesta.apache.org:33170_1292916630791 on client 127.0.0.1, because this file is already
 being created by DFSClient_hb_rs_vesta.apache.org,38743,1292916616340_1292916617166 on 127.0.0.1
{noformat}

I think that we should always check in ZK the number of live region servers before waiting for them to check in, this way we know how many we should expect during failover. There's also a case where we still want to timeout, since RS can die during that time, but we should wait a bit longer.",streamy,jdcryans,Blocker,Closed,Fixed,21/Dec/10 18:38,20/Nov/15 12:43
Bug,HBASE-3381,12493805,Interrupt of a region open comes across as a successful open,"Meta was offline when below happened:

{code}
2010-12-21 19:45:23,023 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x12d0a53c540000e Attempting to transition node 337038b50e467fbd6b031f278bbd9c22 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2010-12-21 19:45:23,046 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x12d0a53c540000e Successfully transitioned node 337038b50e467fbd6b031f278bbd9c22 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2010-12-21 19:45:26,379 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Interrupting thread Thread[PostOpenDeployTasks:337038b50e467fbd6b031f278bbd9c22,5,main]
2010-12-21 19:45:26,379 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x12d0a53c540000e Attempting to transition node 337038b50e467fbd6b031f278bbd9c22 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2010-12-21 19:45:26,381 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Exception running postOpenDeployTasks; region=337038b50e467fbd6b031f278bbd9c22
org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException: Interrupted
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:364)
    at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:146)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1331)
    at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:195)
...
{code}

So, we timed out trying to open the region but rather than close the region because edit failed, we missed seeing the InterruptedException.

Here is suggested fix:

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java b/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java
index 7bf680d..2b0078c 100644
--- a/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java
+++ b/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java
@@ -339,7 +339,7 @@ public class MetaReader {
     get.addFamily(HConstants.CATALOG_FAMILY);
     byte [] meta = getCatalogRegionNameForRegion(regionName);
     Result r = catalogTracker.waitForMetaServerConnectionDefault().get(meta, get);
-    if(r == null || r.isEmpty()) {
+    if (r == null || r.isEmpty()) {
       return null;
     }
     return metaRowToRegionPair(r);
{code}

Let me try it.

W/o this, what we see is hbck showing that region is on server X but in .META. it shows as being on Y (its pre-balance server)",stack,stack,Major,Closed,Fixed,21/Dec/10 20:52,20/Nov/15 12:42
Bug,HBASE-3383,12493837,"[0.90RC1] bin/hbase script displays ""no such file"" warning on target/cached_classpath.txt","HBase version: 0.90 release-candidate-1

bin/hbase blindly creates ""target"" directory and fails to generate ""target/cached_classpath.txt"" if it's in the binary (pre-built) distribution. 

{code}
$ bin/hbase shell
cat: ... /hbase-0.90.0/bin/../target/cached_classpath.txt: No such file or directory
HBase Shell; enter 'help<RETURN>' for list of supported commands.
{code}

{code:title=bin/hbase}

112: add_maven_deps_to_classpath() {
113:   # The maven build dir is called 'target'
114:   target=""${HBASE_HOME}/target""
115:   if [ ! -d ""${HBASE_HOME}/target"" ]
116:   then
117:     mkdir ""${target}""
118:   fi
119:   # Need to generate classpath from maven pom. This is costly so generate it
120:   # and cache it. Save the file into our target dir so a mvn clean will get
121:   # clean it up and force us create a new one.
122:   f=""${target}/cached_classpath.txt""
123:   if [ ! -f ""${f}"" ]
124:   then
125:     ${MVN} -f ""${HBASE_HOME}/pom.xml"" dependency:build-classpath -Dmdep.outputFile=""${f}"" &> /dev/null
126:   fi
127:   CLASSPATH=${CLASSPATH}:`cat ""${f}""`
{code}

Maybe we can simply skip this process if ""target"" directory doesn't exist.
",jdcryans,tatsuya6502,Trivial,Closed,Fixed,22/Dec/10 07:18,20/Nov/15 12:43
Bug,HBASE-3384,12493838,Move User-Triggered Compactions to Store,There are currently miscellaneous small race conditions or workflows that would ignore a user-triggered major compaction.  Moving the major compaction flag to the Store level and adding a couple small checks should fix these issues.,nspiegelberg,nspiegelberg,Trivial,Closed,Fixed,22/Dec/10 07:32,20/Nov/15 12:41
Bug,HBASE-3386,12493916,NPE in TableRecordReaderImpl.restart,"I was running some testing on a already busy cluster and my clients in the mappers sometimes aren't able to create an HTable because of ConnectionLoseException. The code logs that as an error but still continues with a null HTable then this happens:

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.restart(TableRecordReaderImpl.java:58)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.init(TableRecordReaderImpl.java:67)
	at org.apache.hadoop.hbase.mapreduce.TableRecordReader.init(TableRecordReader.java:57)
	at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.createRecordReader(TableInputFormatBase.java:108)
{noformat}

Needs to be prettier.",jdcryans,jdcryans,Minor,Closed,Fixed,22/Dec/10 19:22,20/Nov/15 12:41
Bug,HBASE-3387,12493917,Pair does not deep check arrays for equality. ,"Pair does not deep check arrays for equality. It merely does x.equals(y) for the sent Object. However, with any type of array this is merely going to compare the array pointers, rather than the underlying data structure.

It requires a rewriting of the private equals method in Pair to check for elements being an array, then checking the underlying elements.",jesse_yates,jesse_yates,Major,Closed,Fixed,22/Dec/10 19:26,12/Jun/22 00:41
Bug,HBASE-3388,12493922,NPE processRegionInTransition(AssignmentManager.java:264) doing rolling-restart.sh,"Trying to run rolling-restart got this:

{code}
2010-12-22 20:16:48,579 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.lang.NullPointerException
    at org.apache.hadoop.hbase.master.AssignmentManager.processRegionInTransition(AssignmentManager.java:264)
    at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:223)
    at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:395)
    at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:277)
2010-12-22 20:16:48,581 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2010-12-22 20:16:48,581 DEBUG org.apache.hadoop.hbase.master.HMaster: Stopping service threads
{code}",,stack,Major,Closed,Fixed,22/Dec/10 20:24,20/Nov/15 12:43
Bug,HBASE-3392,12494051,Update backport of InputSampler,"We have a backport copy of o.a.h.mapreduce.lib.partition.InputSampler in HBase which contains a bug, MAPREDUCE-1820. This JIRA is to update our copy to the version in MR trunk.",tlipcon,tlipcon,Major,Closed,Fixed,25/Dec/10 04:41,20/Nov/15 12:41
Bug,HBASE-3400,12494280,Coprocessor Support for Generic Interfaces,"Coprocessors currently do not support generic interfaces because type erasure makes their generic parameters appear as Objects to Invocation.java.

This can be overcome by writing out the parameters using their own types (rather than the type parameters), and then separately writing the class names for the type parameters. While it would be ideal to implement this in Invocation.java, some other code seems to be relying on its write order and doing so breaks other RPC code. The modification can, however, be implemented in Exec.java instead.

The included patch modifies Invocation.java's fields to that they are protected scope, and fully implements the read and write methods for Exec rather than using the parent method for the parent fields. ExecResult is also modified to accommodate generic returns in the same way.",ekohlwey,ekohlwey,Major,Closed,Fixed,29/Dec/10 23:59,20/Nov/15 12:43
Bug,HBASE-3401,12494320,Region IPC operations should be high priority,"I manufactured an imbalanced cluster so one region server had 300 regions and the others had very few. I then ran balancer while hitting the high-load region server with YCSB. I observed that the rate of load shedding was VERY slow since the closeRegion IPCs were getting stuck at the back of the IPC queue.

All of these important master->RS RPC calls should be set to high priority.",tlipcon,tlipcon,Major,Closed,Fixed,30/Dec/10 19:55,20/Nov/15 12:43
Bug,HBASE-3402,12494332,Web UI shows two META regions,"Running 0.90@r1052112 I see two regions for META on the same server. Both have start key '-' and end key '-'.

Things seem to work OK, but it's very strange.",stack,tlipcon,Critical,Closed,Fixed,30/Dec/10 22:51,20/Nov/15 12:42
Bug,HBASE-3403,12494333,Region orphaned after failure during split,"ERROR: Region hdfs://haus01.sf.cloudera.com:11020/hbase-normal/usertable/2ad8df700eea55f70e02ea89178a65a2 on HDFS, but not listed in META or deployed on any region server.
ERROR: Found inconsistency in table usertable

Not sure how I got into this state, will look through logs.",stack,tlipcon,Blocker,Closed,Fixed,30/Dec/10 22:54,20/Nov/15 12:42
Bug,GROOVY-3965,12814994,Inconsistent numbering - deploying groovy artifacts to Maven repositories,"Raised by Russel on the groovy dev mailing list (reproducing here)

---------------------------------------------------
It appears that the trunk Groovy build system creates groovy jars with
the number 1.8.0-beta-1-SNAPSHOT but installs to the Maven repository
using the artifactId 1.8-beta-1-SNAPSHOT.  This inconsistency is
seriously irritating for doing certain scripting hacks.
---------------------------------------------------",roshandawrani,roshandawrani,Major,Closed,Fixed,01/Jan/10 08:23,22/Jul/10 17:33
Bug,GROOVY-3966,12814997,GroovyConsole window loses focus when script is run,"When I run a script in Groovy Console, the window often (not always) loses focus. One effect is that I cannot run the same script again with Cmd+R without first clicking the window.",roshandawrani,pniederw,Major,Closed,Fixed,02/Jan/10 07:50,07/Apr/10 23:46
Bug,GROOVY-3971,12815032,adding method to class in ASTBuilder not working,"The following code produces class MyClass OK, but running the class produces error:
*Exception in thread ""main"" java.lang.NoSuchMethodError: main*

Here's the ASTBuilder code I used:
{noformat}
import static org.objectweb.asm.Opcodes.ACC_PUBLIC
import static org.objectweb.asm.Opcodes.ACC_STATIC
import org.codehaus.groovy.ast.builder.AstBuilder
import org.codehaus.groovy.control.CompilerConfiguration
import org.codehaus.groovy.control.CompilationUnit

def classes= new AstBuilder().buildFromSpec{
  classNode 'MyClass', ACC_PUBLIC, {
    classNode Object //superclass
    interfaces{
      classNode GroovyObject
    }
    mixins{}
    genericsTypes{}
    method('main', ACC_PUBLIC & ACC_STATIC, Void.class) {
      parameters{
        parameter 'args': String[].class
      }
      exceptions{}
      block{
        methodCall {
          variable ""this""
          constant ""println""
          argumentList {
            constant ""Hello, world!""
          }
        }
      }
    }
  }
}

def gcl= new GroovyClassLoader()
def sec= null
def ccfg= CompilerConfiguration.DEFAULT
def cu= new CompilationUnit(ccfg, sec, gcl)
cu.addClassNode(classes[0])
cu.compile() 
{noformat}

Also tried this but same result:
{noformat}
import static org.objectweb.asm.Opcodes.ACC_PUBLIC
import static org.objectweb.asm.Opcodes.ACC_STATIC
import org.codehaus.groovy.ast.*
import java.security.CodeSource
import org.codehaus.groovy.ast.builder.AstBuilder
import org.codehaus.groovy.control.CompilerConfiguration
import org.codehaus.groovy.control.CompilationUnit
import org.codehaus.groovy.control.CompilePhase

def classes= new AstBuilder().buildFromString(CompilePhase.CONVERSION, """"""\
public class MyClass{
  public static void main(String[] args){
    println ""Hello, world!""
  }
}
"""""")

def gcl= new GroovyClassLoader()
def sec= null //new CodeSource()
def ccfg= CompilerConfiguration.DEFAULT
def cu= new CompilationUnit(ccfg, sec, gcl)
cu.addClassNode(classes[1])
cu.compile()
{noformat}
",paulk,gavingrover,Major,Closed,Fixed,03/Jan/10 21:22,11/Jun/14 14:25
Bug,GROOVY-3975,12815022,No generic type information for closure parameters in generated closure class,"cl = { List<String> s -> }

println cl.getClass().getMethod(""call"", List).genericParameterTypes[0]   // interface java.util.List (would have expected List<String>) 
println cl.getClass().getMethod(""doCall"", List).genericParameterTypes[0] // interface java.util.List (would have expected List<String>)

According to Jochen, this is a (probably easy to fix) bug.
",roshandawrani,pniederw,Major,Closed,Fixed,06/Jan/10 11:55,07/Apr/10 23:46
Bug,GROOVY-3977,12814995,Auto-conversion inconsistency in BigDecimal treatment in method calls - Double vs Float,"As reported by Peter on groovy user mailing list - :

{code}
class Foo {
 double myDouble
 float myFloat
}

def foo = new Foo()

// OK
foo.setMyDouble 0.1

/* groovy.lang.MissingMethodException: No signature of method: Foo.setMyFloat() 
is applicable for argument types: (java.math.BigDecimal) values: 0.1] */
foo.setMyFloat 0.1  
{code}",roshandawrani,roshandawrani,Major,Closed,Fixed,06/Jan/10 22:57,07/Apr/10 23:47
Bug,GROOVY-3978,12814969,Error in as Int[] conversion,"Running the next scritp will fail on second line when it should parse the numbers correctly.

def numbers = ['100','200','123']
def integers = numbers as Integer[]

Exception thrown: org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object '100' with class 'java.lang.String' to class 'java.lang.Integer'

org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object '100' with class 'java.lang.String' to class 'java.lang.Integer'
	at Script1.run(Script1:2)",paulk,edcruz,Major,Closed,Fixed,07/Jan/10 08:58,03/Feb/11 10:51
Bug,GROOVY-3979,12815015,Enum inside a class,"Iteration of enum defined inside a class fails with the following exception:

Caught: groovy.lang.MissingMethodException: No signature of method:
static EnumTest2$Direction.values() is applicable for argument types: ()
values: []

The following example demonstrates the problem   

class EnumTest2 {  

  enum Direction { North, East, South, West } 

  static void main(args) { 
    for (d in Direction) { 
      println d
    }

  }

}",roshandawrani,xjia,Minor,Closed,Fixed,07/Jan/10 11:42,07/Apr/10 23:46
Bug,GROOVY-3980,12811995,GroovyScriptingEngine doesn't recognize changes in the source of checked class only when dependencies got changed,"Expected behaviour from my point of view it that GroovyScriptEngine should refresh class everytime its source code or source code of any of classes this class references changes. Such behaviour was implemented in 1.6.5. When upgrading to 1.7.0 our test began to fail - class is refreshed only in case of referenced classes source code gets changed, but when I directly change the source of the class I am directly loading, I receive old version -  no refresh occurs.

I can prove this by the set of tests that pass on 1.6.5 but fails on 1.7.0. Precisely the proving test is: 
com.fg.scripting.groovy.GroovyScriptingEngineReloadSupportTest#testReloadGroovyClass

Before running tests, please inspect com.fg.scripting.groovy.AbstractGroovyTest#setUp and modify the path.",roshandawrani,novoj,Major,Closed,Fixed,07/Jan/10 14:25,05/Apr/15 14:43
Bug,GROOVY-3985,12815016,"error in instance initializer for Enum element, without special method","{code}
enum Color {
    RED {
      { println(""foo""); } // Instance initalizer
    },GREEN,BLUE
}
{code}

makes an error like:
{code}
$ groovy  enumtest.groovy
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
/Users/uehaj/src/groovy/enumtest.groovy: -1: You are not allowed to overwrite the final method $INIT([Ljava.lang.Object;) from class 'Color'.
 @ line -1, column -1.
1 error
{code}
following is OK.
{code}
num Color {
    RED {
      String toString() {}
      { println(""foo""); } // Instance initalizer
    },GREEN,BLUE
}
{code}

",roshandawrani,uehaj,Major,Closed,Fixed,10/Jan/10 20:20,07/Apr/10 23:46
Bug,GROOVY-3986,12811794,can't omit toplevel parentheses in enum element method,"{code}
enum Color {
    RED {
      String toString() { println ""foo"" }
    },GREEN,BLUE
}
{code}
produces an error:
{code}
$ groovy enumtest2.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
/Users/uehaj/src/groovy/enumtest2.groovy: 2: unexpected token: RED @ line 2, column 5.
       RED {
       ^

1 error
{code}

following is OK:
{code}
enum Color {
    RED {
      String toString() { println(""foo"") }
    },GREEN,BLUE
}
{code}
",paulk,uehaj,Major,Closed,Fixed,10/Jan/10 20:27,23/Jun/10 15:45
Bug,GROOVY-3987,12815006,Cannot copy the text selected in output pane of GroovyConsole,"While the text can be selected in the output pane of GroovyConsole, it cannot be copied using Ctrl-C or menu option Edit -> Copy.

It will be useful if it can work.",roshandawrani,roshandawrani,Minor,Closed,Fixed,10/Jan/10 23:02,07/Apr/10 23:47
Bug,GROOVY-3989,12814845,Groovy compiler allows overriding final methods but class loading fails with java.lang.VerifyError,"The following script compiles but when run, class loading fails saying ""java.lang.VerifyError: class B overrides final method""

{code}
class A {
    def foo() {}
    final def bar() {}
}
class B extends A {
    def foo() {}
    def bar() {}
}
B
{code}

If I swap the order of method definitions in class B as:
{code}
class A {
    def foo() {}
    final def bar() {}
}
class B extends A {
    def bar() {}
    def foo() {}
}
B
{code}
then it correctly gives the error message ""You are not allowed to overwrite the final method bar() from class 'A'.""",roshandawrani,roshandawrani,Critical,Closed,Fixed,11/Jan/10 02:48,07/Apr/10 23:47
Bug,GROOVY-3991,12815025,Exponantiation,"On command line:

$ groovy -e 'println 2**63' #=> 9223372036854775807
$ ruby -e 'puts 2**63' #=> 9223372036854775808

Ruby is right, Groovy is wrong.

Special thanks to @datenreisender: http://twitter.com/datenreisender/status/6996674701

Ruined my kata: http://berndschiffer.blogspot.com/2009/12/prime-factors-kata-first-try.html :(",roshandawrani,berndschiffer,Major,Closed,Fixed,11/Jan/10 13:26,09/Nov/11 05:30
Bug,GROOVY-3993,12818047,NPE when trying to access member of a static array from an inner static enum,"In the attached test case, I can read the size of the static array (3) but whenever I try to access a property of one of the array elements, I get the following result :

hello
a
3

java.lang.NullPointerException: Cannot invoke method name() on null object
	at org.codehaus.groovy.runtime.NullObject.invokeMethod(NullObject.java:77)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:45)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.NullCallSite.call(NullCallSite.java:17)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:121)
	at TestInnerEnum.testPrint(TestInnerEnum.groovy:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:130)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:94)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:165)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:110)


Process finished with exit code 255",roshandawrani,xsautejeau,Major,Closed,Fixed,12/Jan/10 07:48,07/Apr/10 23:47
Bug,GROOVY-3994,12811728,ClassCastException when trying to instantiat an EnumSet from an inner enum,"When trying to run the attached test case, which does noting much more than trying to instantiate an EnumSet from an inner enum, I get the following exception :

java.lang.ClassCastException: class TestInnerEnum$MyEnum not an enum
	at java.util.EnumSet.noneOf(EnumSet.java:93)
	at java.util.EnumSet.allOf(EnumSet.java:110)
	at java_util_EnumSet$allOf.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
	at TestInnerEnum.testPrint(TestInnerEnum.groovy:20)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:130)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:94)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:165)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:110)",roshandawrani,xsautejeau,Major,Closed,Fixed,12/Jan/10 08:57,07/Apr/10 23:47
Bug,GROOVY-3995,12815004,An inner enum class cannot resolve itself in its own body,"The compilation of 
{code}
class Test {
	enum Color {
		  R, G, B
		  public static Color[] ALL_COLORS = [R, G, B];
	}
}
{code}
fails with
{noformat}
unable to resolve class Color[] 
 @ line 4, column 5.
   		  public static Color[] ALL_COLORS = [R, G, B];
       ^

1 error
{noformat}

However the compilation of following code goes through (top-level enum):
{code}
enum Color {
	  R, G, B
	  public static Color[] ALL_COLORS = [R, G, B];
}
{code}",roshandawrani,roshandawrani,Major,Closed,Fixed,13/Jan/10 03:53,07/Apr/10 23:50
Bug,GROOVY-3996,12815013,Enum static field initialization inconsistency,"Output of the code
{code}
enum Color {
	R, G, B
	static Color[] ALL_COLORS = [R, G, B];
}

println Color.ALL_COLORS
{code}
is correct - [R, G, B]

But if I just make the static field public then the code fails with output as [null, null, null]
{code}
enum Color {
	R, G, B
	public static Color[] ALL_COLORS = [R, G, B];
}

println Color.ALL_COLORS
{code}",roshandawrani,roshandawrani,Major,Closed,Fixed,13/Jan/10 04:15,07/Apr/10 23:47
Bug,GROOVY-4001,12815049,Grapes/Grab spending time checking remote repositories every 3rd run?,"This is a performance issue with grapes. I have two groovy files: 

* A httpServer.groovy (attached) which runs an embedded jetty server and responds to http post requests
* A httpClient.groovy (attached) which runs HttpBuilder and does a http post to the server in the other script

both groovy files download dependencies using @Grab annotations and they both work. 

Weirdness ensues in the following scenario:

# I have network connectivity enabled
# I delete my grapes cache
# I start the server script and wait of it to come up
# I run an infinite bash while loop which executes the client script and collects timings: 'while true ; do time groovy -d httpClient.groovy ; done'

\\
Console logs of both client and server runs attached. It turns out that the client script takes about 4 seconds to run two times and then every third time it takes 25-30 seconds to run. I've verified this over hundreds of runs and it does not seem like a random occurrence of say network congestion or cpu load. In the attached console logs we can see that the server takes 1-5ms to handle requests, but the client script trundles along in a sequence of 4s, 4s, 25s, 4s, 4s, 25s etc. 

If I turn off my nic the timings become deterministic: 4s, 4s, 4s

I have also tried running against the RC2 release of the http-builder package with identical results. As I believe the snapshot and RC2 versions are served by different remote repos, this would lower the probability that this is a remote repo issue. I also removed my ~/.groovy/grapeConfig.xml before the test. 

So a couple of questions: 

# what is going on here 
# is there any way to force debug logging for grapes during script execution to find out what it is actually doing

\\
I suspect this issue might be caused by the check-for-new-version-every-time snapshot jar handling logic. If so, maybe it would be a good idea to have an extra parameter to the grab annotation: 
{noformat} 
  @Grab(..., checkmodified=false)

{noformat} 
just as we do for repositories in the grapeConfig.xml file. This would basically instruct the resolver to _not_ check the remote repos for a new version of this particular jar once one matching the version is in the local cache...even if the version has 'SNAPSHOT' or 'RC2' at the end. As we don't always have control over 3rd party versioning (http-builder latest stable is for example RC2 which I suspect is treated the same way as a snapshot release), it would be nice to be able to force your script to run in 4 seconds insted of 30. Grapes gurus, you'll probably have a better solution, but figured I'd throw it out there. 

In summary, the above behavior makes calling the http-builder from groovy using grapes useless for me as the 30s delay is not acceptable in my current context. Any ideas or suggestions would be most welcome.   
",blackdrag,mbjarland,Major,Closed,Fixed,16/Jan/10 10:12,26/Jul/12 16:49
Bug,GROOVY-4002,12815005,deadlock in GroovyClassLoader,"GroovyClassLoader may deadlock when AbstractScriptEngine.eval() is called from different threads. The problem seems to be the locks on the sourceCache Map that both GroovyClassLoader.parseClass() and GroovyClassLoader.loadClass() acquire.
There is a discussion about a similar situation (most likely the same bug) in here: http://markmail.org/message/pfnv546io5yvsqan

Here is a stack dump from my application:

""Thread-1"":
  waiting to lock monitor 0x09f8be04 (object 0x9c7c4a00, a java.util.HashMap),
  which is held by ""Thread-2""
""Thread-2"":
  waiting to lock monitor 0x09f8bd3c (object 0x8562eed8, a java.lang.Class),
  which is held by ""Thread-1""


Thread-1:
----------------------------
at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:711)
 - waiting to lock <0x9c7c4a00> (a java.util.HashMap)
 at groovy.lang.GroovyClassLoader$InnerLoader.loadClass(GroovyClassLoader.java:425)
 at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:772)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
 at java.beans.Introspector.instantiate(Introspector.java:1438)
 at java.beans.Introspector.findExplicitBeanInfo(Introspector.java:410)
 - locked <0x8562eed8> (a java.lang.Class for java.beans.Introspector)
 at java.beans.Introspector.<init>(Introspector.java:359)
 at java.beans.Introspector.getBeanInfo(Introspector.java:159)
 at groovy.lang.MetaClassImpl$15.run(MetaClassImpl.java:2924)
 at java.security.AccessController.doPrivileged(Native Method)
 at groovy.lang.MetaClassImpl.addProperties(MetaClassImpl.java:2922)
 at groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:2905)
 - locked <0x9cb7ceb8> (a groovy.lang.MetaClassImpl)
 at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:164)
 at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:180)
 at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:195)
 at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:701)
 at groovy.lang.GroovyObjectSupport.<init>(GroovyObjectSupport.java:32)
 at groovy.lang.Script.<init>(Script.java:40)
 at groovy.lang.Script.<init>(Script.java:37)
 at Script4.<init>(Script4.groovy)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
 at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
 at java.lang.Class.newInstance0(Class.java:355)
 at java.lang.Class.newInstance(Class.java:308)
 at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:259)
 at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:112)
 at javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:216)


Thread-2:
----------------------------
""http-8210-1"":

 at java.beans.Introspector.findExplicitBeanInfo(Introspector.java:408)
 - waiting to lock <0x8562eed8> (a java.lang.Class for java.beans.Introspector)
 at java.beans.Introspector.<init>(Introspector.java:359)
 at java.beans.Introspector.getBeanInfo(Introspector.java:159)
 at groovy.lang.MetaClassImpl$15.run(MetaClassImpl.java:2924)
 at java.security.AccessController.doPrivileged(Native Method)
 at groovy.lang.MetaClassImpl.addProperties(MetaClassImpl.java:2922)
 at groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:2905)
 - locked <0x9cb96308> (a groovy.lang.MetaClassImpl)
 at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:164)
 at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:193)
 at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:199)
 at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:697)
 at org.codehaus.groovy.runtime.callsite.CallSiteArray.createPojoSite(CallSiteArray.java:107)
 at org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallSite(CallSiteArray.java:148)
 at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
 at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:54)
 at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
 at org.codehaus.groovy.ast.builder.AstBuilderInvocationTrap.visitMethodCallExpression(AstBuilderTransformation.groovy:177)
 at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:63)
 at org.codehaus.groovy.ast.CodeVisitorSupport.visitBinaryExpression(CodeVisitorSupport.java:144)
 at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
 at org.codehaus.groovy.ast.CodeVisitorSupport.visitBinaryExpression(CodeVisitorSupport.java:144)
 at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
 at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:69)
 at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
 at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
 at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:51)
 at sun.reflect.GeneratedMethodAccessor918.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoCachedMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:229)
 at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:52)
 at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
 at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callSafe(AbstractCallSite.java:90)
 at org.codehaus.groovy.ast.builder.AstBuilderTransformation.visit(AstBuilderTransformation.groovy:57)
 at org.codehaus.groovy.transform.ASTTransformationVisitor$3.call(ASTTransformationVisitor.java:303)
 at org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:831)
 at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:519)
 at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:495)
 at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:472)
 at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:291)
 - locked <0x9c7c4a00> (a java.util.HashMap)
 at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:262)
 at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:206)
 at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.getScriptClass(GroovyScriptEngineImpl.java:337)
 at org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:110)
 at javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:216)
 at com.sustain.util.script.ScriptUtils.executeScript(ScriptUtils.java:73)
 ",guillaume,vakopian,Major,Closed,Fixed,19/Jan/10 18:24,02/Jun/10 08:03
Bug,GROOVY-4004,12815036,stub generator creates wrong code for inner and nested classes,"class Foo {
  def foo

  class Bar { // could also be static
  }
}

Look at the generated Java source code. All property accessors in Foo are replicated in Bar. An easy way to show this is to define a property with the same name in both classes. Javac will fail with a duplicate declaration error.",roshandawrani,pniederw,Blocker,Closed,Fixed,20/Jan/10 12:25,07/Apr/10 23:46
Bug,GROOVY-4005,12815037,Stub generator creates invalid Java code for anonymous inner classes,"Stub generator should generate no code at all for anonymous inner classes. Instead it generates code that produces numerous Java compile errors. Therefore, if a project contains just a single AIC, joint compilation breaks.",roshandawrani,pniederw,Blocker,Closed,Fixed,20/Jan/10 12:30,07/Apr/10 23:46
Bug,GROOVY-4009,12815040,Avoiding repeated resolve() calls in case of failure to resolve the type.,"When I run the code
{code}
class Test {
    static main(args) {
        Int x = 1
        def y = x + x
    }
}
{code}
I get the errors
{noformat}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Try.groovy: 3: unable to resolve class Int 
 @ line 3, column 13.
           Int x = 1
               ^

Try.groovy: 4: unable to resolve class Int 
 @ line 4, column 17.
           def y = x + x
                   ^

Try.groovy: 4: unable to resolve class Int 
 @ line 4, column 21.
           def y = x + x
                       ^

3 errors
{noformat}

If the declaration statement has determined that the type couldn't be resolved, I am not sure that calling resolve() again for each use of that variable is of any use. It results in the same error coming multiple times (as many times as that variable is used) and also makes the compilation slower due to these unnecessary resolve() calls (even though the performance is hit only in case of errors, when resolve was unsuccessful.)",roshandawrani,roshandawrani,Minor,Closed,Fixed,23/Jan/10 13:13,07/Apr/10 23:46
Bug,GROOVY-4010,12815014,XmlParser wraps nested elements in extra ArrayList,"When using XmlParser we are experienceing different behaviour within Grails 1.2 than directly using Groovy 1.6.7. The issue manifests itself when accessing nested elements in a document.

example, this works fine from within GroovyConsole 1.6.7:

{code}
def XML = '''<channel>
  <item id=""1""/>
  <item id=""2""/>
  <item id=""3""/>
</channel>'''
assert 3 == new XmlParser().parseText(XML).item.size()
assert 3 == new XmlParser().parseText(""<rss>$XML</rss>"").channel.item.size()
{code}

Here is a test case that passed under Grails 1.1.1 but fails under 1.2:
{code}
class XMLUnitTests extends GroovyTestCase {
    def XML = '''
        <channel>
          <item id=""1""/>
          <item id=""2""/>
          <item id=""3""/>
        </channel>
    '''

    void testOneLevel() {
        def records = new XmlParser().parseText(XML)
        assertEquals 3, records.item.size()
    }

    void testTwoLevels() {
        def records = new XmlParser().parseText(""<rss>$XML</rss>"")
        assertEquals 3, records.channel.item.size()
    }
}
{code}

",blackdrag,james.cookie,Critical,Closed,Fixed,05/Jan/10 04:34,19/Feb/10 07:12
Bug,GROOVY-4012,12815071,java.lang.ArrayStoreException thrown on trying to execute a list,"The script:
{code}
directory = new File ( 'tmp' )
println ( directory.path )
[ 'sh' , '-c' , ""${directory.path} && ls"" ].execute ( ).waitFor ( )
{code}
casues the output:
{code}
tmp
Exception thrown
26-Jan-2010 15:23:41 org.codehaus.groovy.runtime.StackTraceUtils sanitize
WARNING: Sanitizing stacktrace:
java.lang.ArrayStoreException
	at java.lang.System.arraycopy(Native Method)
	at java.util.ArrayList.toArray(ArrayList.java:328)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.execute(DefaultGroovyMethods.java:7511)
	at org.codehaus.groovy.runtime.dgm$163.invoke(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:270)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:52)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:121)
	at ConsoleScript1.run(ConsoleScript1:3)
	at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:264)
	at groovy.lang.GroovyShell.run(GroovyShell.java:513)
	at groovy.lang.GroovyShell.run(GroovyShell.java:170)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:857)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:151)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:276)
	at groovy.lang.Closure.call(Closure.java:271)
	at groovy.lang.Closure.run(Closure.java:354)
	at java.lang.Thread.run(Thread.java:636)

java.lang.ArrayStoreException
	at ConsoleScript1.run(ConsoleScript1:3)
{code}
which seems a bit mean.",paulk,russel,Blocker,Closed,Fixed,26/Jan/10 09:26,07/Apr/15 19:12
Bug,GROOVY-4013,12815047,GroovyScriptEngine.loadScriptByName returns anonymous inner classes on second invocation/reload,"When using GroovyScriptEngine to create classes, invoking loadScriptByName twice on the same source file will return an anonymous inner classes on the second invocation (eg, reloading classes).


class MySwingPanel extends FrameworkPanel
{
   def initPanel()
   {
      def b = new Button('click me')
      b.addActionListener( new ActionListener(){
         // implementation code...
      })
   }
}


GroovyScriptEngine gse ...
gse.loadScriptByName('myswingpanel.groovy') // returns MySwingPanel

// now reload everything

gse.getGroovyClassLoader().clearCache();
gse.loadScriptByName('myswingpanel.groovy') // returns MySwingPanel$1


For now the workaround in my project is to just create a new instance of GroovyScriptEngine.


http://old.nabble.com/loading-multiple-classes-with-loadScriptByName----bug-or-feature--td27252081.html",roshandawrani,mpriatel,Minor,Closed,Fixed,26/Jan/10 22:18,07/Apr/10 23:46
Bug,GROOVY-4016,12815048,Single super constructor argument is casted to array if super constructor has vararg parameter,"{code:Java}
class Foo {
  def Foo(String... s) { }
}
class ImplOneParameter extends Foo {
  def ImplOneParameter(String s) {
    super(s)
  }
}
class ImplArray extends Foo {
  def ImplArray(String[] s) {
    super(s)
  }
}


new ImplArray(""String"")
new ImplOneParameter(""String"") //fails with CCE
{code}
The same dispatch as for usual method calls should apply.",blackdrag,gromopetr,Major,Closed,Fixed,27/Jan/10 12:18,08/Jul/12 04:51
Bug,GROOVY-4020,12813602,"ClassFormatError when using the name including ""-"" sign","I'm not sure if there any constraint with the name parameter before passing to GroovyCodeSource but it is raising an exception if the name includes ""-"" sign :

GroovyCodeSource gcs = new GroovyCodeSource(inputStream, ""file-name.gtmpl"", ""/groovy/shell"");

Here is the exception :

Caused by: java.lang.ClassFormatError: Illegal class name ""File-name"" in class file File-name
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:616)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:124)
        at groovy.lang.GroovyClassLoader.access$200(GroovyClassLoader.java:54)
        at groovy.lang.GroovyClassLoader$ClassCollector.createClass(GroovyClassLoader.java:482)
        at groovy.lang.GroovyClassLoader$ClassCollector.onClassNode(GroovyClassLoader.java:499)
        at groovy.lang.GroovyClassLoader$ClassCollector.call(GroovyClassLoader.java:503)
        at org.codehaus.groovy.control.CompilationUnit$10.call(CompilationUnit.java:728)
        at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:925)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:462)
        at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:278)

It seems that the name parameter is used for createing Class in Java, so it must be a binary name as defined by the Java Language Specification. If so, the name would be validated and converted to good one ? Or I must to take care of that myself before using GroovyCodeSource ?",emilles,trongtt,Major,Resolved,Fixed,29/Jan/10 06:36,22/May/22 19:43
Bug,GROOVY-4025,12815059,Groovy interfaces allow methods with full body?,"The compilation of the following code goes through
{code}
interface ITest {
    def foo(a, b) {
        return a + b
    }
}
{code}

The bytecode of the interface, however, has the correct abstract method
{code}
public abstract Object foo(Object paramObject1, Object paramObject2);
{code}

It should not allow the full methods only to reject their bodies later.",roshandawrani,roshandawrani,Major,Closed,Fixed,31/Jan/10 11:45,14/Aug/13 03:57
Bug,GROOVY-4028,12813640,Inner classes - Constructor calls with named arguments don't get implicit this reference passed as 1st argument,"Groovy doesn't yet support setting properties on (non-static) inner classes using named arguments, as in code below
{code}
class MyOuterClass {
    def foo() {
        def ic = new MyInnerClass(fName: 'Roshan', lName: 'Dawrani')
    }
    class MyInnerClass {
        def fName
        def lName
    }
}
def oc = new MyOuterClass()
oc.foo()
{code}
The code above fails with the message
{noformat}
failed to invoke constructor: public MyOuterClass$MyInnerClass(MyOuterClass) with arguments: [] reason: java.lang.IllegalArgumentException: wrong number of arguments
{noformat}
indicating that the implicit ""this"" reference is not being passed as the first argument.

Supporting above usage may require significant changes possibly. 

The bit related to ""this"" not being passed implicitly can be corrected in the meantime - so that at least the following works fine
{code}
class MyOuterClass {
    def foo() {
        def ic = new MyInnerClass(fName: 'Roshan', lName: 'Dawrani')
    }
    class MyInnerClass {
        Map propMap
        def MyInnerClass(Map propMap) {
            this.propMap = propMap
        }
    }
}
def oc = new MyOuterClass()
oc.foo()
{code}",roshandawrani,roshandawrani,Major,Closed,Fixed,01/Feb/10 07:06,14/Aug/13 03:57
Bug,GROOVY-4029,12815072,putAt and  put behave different on Maps,"
Map m = [:]
//this is ok as LinkedHashMap supports null as key and value
m.put(null, null)

//this will throw an exception
m[null] = null
",roshandawrani,kmuntau,Major,Closed,Fixed,02/Feb/10 02:00,07/Apr/10 23:46
Bug,GROOVY-4034,12815069,Mixin's getClass() method replaces original getClass() implementation,"{code}
class M {
    def doIt() {
        assert metaClass.owner.class == M // passes but shouldn't, should be T
    }
}

class T {}
T.mixin(M)
new T().doIt()
{code}",roshandawrani,ldaley,Major,Closed,Fixed,03/Feb/10 05:56,16/Jun/10 03:50
Bug,GROOVY-4035,12815052,Super method calls from anonymous classes sometimes fail,"{code}
class Foo {
  def foo(msg) {
  }
}

def z = new Foo() {

  def foo(Object msg) {
    return super.foo(msg);
  }

}

z.foo(""42"")
{code}

Caught: groovy.lang.MissingMethodException: No signature of method: test.foo() is applicable for argument types: (java.lang.String) values: [42]
Possible solutions: run(), run(), any(), any(groovy.lang.Closure), is(java.lang.Object), find(groovy.lang.Closure)
	at test.this$dist$invoke$4(test.groovy)
	at test$1.methodMissing(test.groovy)
	at test$1.foo(test.groovy:9)
	at test.run(test.groovy:14)
",roshandawrani,gromopetr,Major,Closed,Fixed,03/Feb/10 13:53,07/Apr/10 23:47
Bug,GROOVY-4036,12815046,null pointer exception from createissue() function,,,siddh89,Minor,Closed,Fixed,04/Feb/10 02:30,07/Apr/10 23:52
Bug,GROOVY-4037,12815034,GroovyClassLoader leaves file handles open,"GroovyClassLoader leaves file handles open.

We use Groovy scripts within an application server. On systems that protect open files with locks (e.g. Windows) this bug prevents Groovy scripts from being redeployed.

The following code shows the problem:

{code}
public void test() throws Exception
{
  final File              l_fileScript;
  final GroovyClassLoader l_ldr;
  final GroovyCodeSource  l_cs;
  final Class<?>          l_classScript;

  l_fileScript = new File(""path/to/script1944A6CADCE5EBA2BE66D50EA1D8AE36562C645E.groovy"");

  l_ldr = new GroovyClassLoader();

  l_cs = new GroovyCodeSource(l_fileScript);

  l_cs.setCachable(false);

  l_classScript = l_ldr.parseClass(l_cs);

  return; // set breakpoint here
}
{code}

At the breakpoint the handle of the script file is still open.
",blackdrag,veita,Blocker,Closed,Fixed,04/Feb/10 06:21,17/Feb/10 13:01
Bug,GROOVY-4038,12815065,"Closure meta class respondsTo(closure, ""doCall"") does not work","When running the following code:

Closure test = {String x -> println x};
println test.metaClass.respondsTo(test, ""doCall"");

The resulting output is: []

Which is incorrect, since the MOP for respondsTo(Object, String) states:
[...] if the implementing MetaClass responds to a method with the given name *regardless* of arguments.
But the Closure does respond to doCall(String), which can easily be verified using this code:

println test.metaClass.respondsTo(test, ""doCall"", ""Hello"");

which will output something similar to: [public MyClass$_myMethod_closure1.doCall(java.lang.String)]
which tells us, that the closure does respond to doCall(String).

Hence, this meta method, and all other meta methods named doCall - *regardless* of their parameters - aswell, should also be included in the list returned by respondsTo(Object,String).
",roshandawrani,misterd,Major,Closed,Fixed,04/Feb/10 06:42,07/Apr/10 23:45
Bug,GROOVY-4040,12811586,"DGM#flatten(Collection, Closure) isn't using closure properly","DGM#flatten(Collection items, Closure flattenUsing) calls the flattenUsing closure but then adds the original item back into the result rather than the flattened item",paulk,paulk,Minor,Closed,Fixed,06/Feb/10 17:36,07/Apr/15 19:13
Bug,GROOVY-4043,12815028,Error in resolving inner class,"Originally reported by Alex on the groovy dev mailing list.

The following code fails to compile (Error: unable to resolve class B):

{code}
class A {
	static class B {}
}

class C extends A {
	B b
}
{code}",roshandawrani,roshandawrani,Major,Closed,Fixed,08/Feb/10 20:01,16/Aug/12 10:07
Bug,GROOVY-4046,12815075,1 == new Object() throws ClassCastException,"
Example 1:

assertFalse(new Object() == 1) // this is ok
assertFalse(1 == new Object()) // this throws a ClassCastException, because Groovy redirects the call to java.lang.Integer.compareTo(Integer i)



Example 2:

enum MyEnum { A, B, C }

assertFalse(new Object() == MyEnum.A) // this is ok
assertFalse(MyEnum.A == new Object()) // this throws a ClassCastException, because Groovy redirects the call to java.lang.Enum.compareTo(E e) where E extends java.lang.Enum<E>

",roshandawrani,cptnuss,Critical,Closed,Fixed,10/Feb/10 08:05,19/Jul/16 11:34
Bug,GROOVY-4048,12812191,waitForProcessOutput should also waitFor internally,"Related to GROOVY-3796, there is a race condition where someone could use waitForProcessOutput() and then call exitValue and get an 'java.lang.IllegalThreadStateException: process hasn't exited' error, since the actual process itself hasn't terminated. This can be worked around with a process.waitForProcessOutput(); process.waitFor(), but this is confusing since waitForProcessOutput also implies that we wait for *everything*.

This patch adds a specific waitFor() to waitForProcessOutput()",paulk,jimjag,Major,Closed,Fixed,11/Feb/10 08:32,07/Apr/15 19:13
Bug,GROOVY-4049,12815029,1.7.x's faster DGM loading system causes issues on Google App Engine,"The new system for improving faster startup of Groovy which loads DGM methods actually causes issue on Google App Engine, as it tries to load classes (referenced in DefaultGroovyMethods) which are not white listed on GAE.",guillaume,guillaume,Major,Closed,Fixed,11/Feb/10 10:15,16/Aug/12 10:07
Bug,GROOVY-4051,12815045,Power assert doesn't pretty-print groovy.lang.Reference content inside a closure inside an anonymous class,"{code}def y = [null]
def o = new Object() {
  def foo() {
    def c = {
      assert y[0]
    }
    c()
  }
}
o.foo(){code}

gives

{code}Caught: Assertion failed: 

assert y[0]
       ||
       |null
       groovy.lang.Reference@99d12cc
{code}",blackdrag,gromopetr,Blocker,Closed,Fixed,11/Feb/10 13:29,17/Feb/10 12:04
Bug,GROOVY-4058,12814851,Unexpected compilation error with MapEntryExpression usage - groovy grammar issue?,"The following code compiles fine:
{code}
import java.util.concurrent.*
class Test {
    def bar () {
        ExecutorService pool = null
        pool.submit(( Runnable ) [ run : { ""some statement""; 
            //println ""fishy print""
        }])
    }
}
{code}

But if I uncomment the print statement from inside the closure, like
{code}
import java.util.concurrent.*
class Test {
    def bar () {
        ExecutorService pool = null
        pool.submit(( Runnable ) [ run : { ""some statement""; 
            println ""fishy print""
        }])
    }
}
{code}
I get the following compilation error:
{noformat}
...\Try.groovy: 5: You tried to use a map entry for an index operation, this is not allowed. Maybe something should be set in parentheses or a comma is missing?
 @ line 5, column 36.
           pool.submit(( Runnable ) [ run : { ""some statement""; 
{noformat}

Here is the AST formed in 2 cases (2nd case has the wrong AST)

Case 1: (Successful compilation)
{noformat}
ArgumentListExpression[
    CastExpression[
        (java.lang.Runnable) 
            MapExpression[
                MapEntryExpression(key: ConstantExpression[run], value: ClosureExpression[]{...}
                )
            ]
    ]
]
{noformat}

Case 2: (Failing compilation)
{noformat}
ArgumentListExpression[
    BinaryExpression[
        left = ClassExpression[type: java.lang.Runnable]
        operation = ""[""
        right = MapEntryExpression(key: ConstantExpression[run], value: ClosureExpression[]{...})]]
{noformat}",ait,roshandawrani,Major,Closed,Fixed,14/Feb/10 21:19,07/Apr/10 23:45
Bug,GROOVY-4061,12815076,Error trying to configure GroovyServlet to point to single groovy file for all URLs,"When trying to configure the GroovyServlet to direct all request to a single groovy script to be able to have dynamic RESTful URLs, a java.lang.StackOverflowError is thrown. What seems to be happening is something is recursively calling the getResource() method on the AbstractHttpServlet over and over always appending BeanInfo onto the name of the file. I've attached a simple war file that demonstrates this. I've also included the log file as well.",roshandawrani,tcomer,Major,Closed,Fixed,15/Feb/10 15:09,07/Apr/10 23:47
Bug,GROOVY-4065,12814896,groovyc not compiling correctly when Grape.grab is used,"This is my first Jira on codehaus. Please let me know if I'm filing this the right way. Thank You!

Example class Foo:
{code}
import groovy.grape.*
class Foo {
  public static void main(String[] args){
    Grape.grab(group:'org.apache.ant', module:'ant', version:'1.8.0');
    System.out.println(""Grape.grab seems to work"");
  }
}
{code}
Compilation:
{noformat}
> groovy Foo.groovy 
Grape.grab seems to work

> groovyc Foo.groovy 

> java -cp $PATH:/opt/groovy/embeddable/groovy-all-1.7.0.jar:ivy-2.1.0.jar:. Foo 
Exception in thread ""main"" java.lang.NullPointerException: Cannot invoke method removeAll() on null object
	at org.codehaus.groovy.runtime.NullObject.invokeMethod(NullObject.java:77)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:45)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.NullCallSite.call(NullCallSite.java:17)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
	at groovy.grape.GrapeIvy.grab(GrapeIvy.groovy:241)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSite.invoke(PogoMetaMethodSite.java:225)
	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:51)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:44)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:143)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:155)
	at groovy.grape.GrapeIvy.grab(GrapeIvy.groovy:216)
	at groovy.grape.Grape.grab(Grape.java:131)
	at groovy.grape.Grape$grab.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
	at Foo.main(Foo.groovy:4)
{noformat}",,mattmcq,Major,Closed,Fixed,17/Feb/10 17:18,14/Oct/13 16:53
Bug,GROOVY-4066,12815747,"Groovy clibuilder doesn't take the parameter when the longOpt name ends with character ""s""","When the clibuilder creates a command line option whose longOpt ends with character ""s"", such as ""seconds"", the parser can't get the parameter, it is null. 

Here's a simple groovy script that demonstrates the problem.  Note that the long option ending with an 's' gets enabled when the non-s option is given.



$> ./cli.groovy -s
options.s evaluates to true
options.seconds evaluates to false
options.e evaluates to false
options.second evaluates to false


$> ./cli.groovy -e
options.s evaluates to false
options.seconds evaluates to true
options.e evaluates to true
options.second evaluates to true



def cli = new CliBuilder()
cli.s longOpt:'seconds', 'a long arg that ends with an ""s""'
cli.e longOpt:'second', 'a long arg that does not end with an ""s""'

def options = cli.parse( args )
if( null == options ) 
{ 
    return
}

if( args.length == 0 || options.h ) 
{
    cli.usage()
}

println ""options.s evaluates to "" + (options.s as boolean)
println ""options.seconds evaluates to "" + (options.seconds as boolean)
println ""options.e evaluates to "" + (options.e as boolean)
println ""options.second evaluates to "" + (options.second as boolean)
 ",roshandawrani,miwang,Major,Closed,Fixed,18/Feb/10 17:10,07/Apr/10 23:45
Bug,GROOVY-4069,12815060,Custom constructors added via metaclass are sometimes not cleaned up,"There seems to be an issue with constructors added via metaclass and cleanup:

{code}
class MetaClassMagicTests extends GroovyTestCase {
    void testIt() {
        ExpandoMetaClass.enableGlobally()
        
        // Child.metaClass // uncomment and everything works
        
        // Save the old meta class
        def oldMetaClass = Parent.metaClass
        
        // Install a new one
        def emc = new ExpandoMetaClass(Parent, true, true)
        emc.initialize()
        GroovySystem.metaClassRegistry.setMetaClass(Parent, emc)
        
        // Install a map based constructor
        emc.constructor = { Map m -> Parent.newInstance() }
        
        // Parent constructor is used, all good
        assert new Child([:]) instanceof Parent
        
        // Reinstate the old meta class
        GroovySystem.metaClassRegistry.removeMetaClass(Parent) 
        GroovySystem.metaClassRegistry.setMetaClass(Parent, oldMetaClass)

        // This fails, this calls the custom constructor from above and returns an instance of Parent
        assert new Child([:]) instanceof Child
    }
    
}

class Parent { def a }
class Child extends Parent { def b }
{code}",roshandawrani,ldaley,Major,Closed,Fixed,20/Feb/10 18:49,07/Apr/10 23:45
Bug,GROOVY-4070,12816348,"Wrong value returned for ""Attribute.name()"" when iterating over GPath Attributes","When iterating over a collection of GPath Attributes I'm seeing the wrong value being returned for the ""Attribute.name()"" method.  The issue seems to be in the implementation of the ""groovy.util.slurpersupport.Attributes.iterator()"" method when constructing a new Attribute.  It uses the ""attributeName"" field which holds the name of the attribute minus the '@' character.  The ""Attribute.name()"" method is expecting the first character in the name to be '@' as well and throws it away.  This leads to the first character in the attribute name going missing.

I've attached a patch which shows one possible fix to the issue in Attributes.java.  It also modifies GpathSyntaxTestSupport.groovy to add some Unit Tests for this issue.

The following code demonstrates the problem I'm seeing.

{code}
def xml = '''
<people>
  <person age=""20"">John</person>
  <person age=""25"">Jane</person>
</people>
'''

def p = new XmlSlurper().parseText(xml)
p.person.each {
  println it.@age.name()
}
p.person.@age.each {
  println it.name()
}
{code}

Running the above gives the following output:
age
age
ge
ge
",paulk,frayerm,Major,Closed,Fixed,21/Feb/10 13:18,07/Apr/15 19:12
Bug,GROOVY-4071,12815094,IndexOutOfBoundsException for labeled assertion statement,"Try to compile the following code (or run it in GroovyConsole):
{code}
mylabel:
assert true == true
{code}

Result:
{noformat}
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.codehaus.groovy.transform.powerassert.SourceText.getNormalizedColumn(SourceText.java:103)
	at org.codehaus.groovy.classgen.AsmClassGenerator.record(AsmClassGenerator.java:1661)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBinaryExpression(AsmClassGenerator.java:1625)
	at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBooleanExpression(AsmClassGenerator.java:1901)
	at org.codehaus.groovy.ast.expr.BooleanExpression.visit(BooleanExpression.java:40)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitAssertStatement(AsmClassGenerator.java:978)
	at org.codehaus.groovy.ast.stmt.AssertStatement.visit(AssertStatement.java:47)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:161)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:707)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:51)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:161)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:707)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:51)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:97)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:108)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:586)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:562)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:119)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:664)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1044)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:275)
	at org.codehaus.groovy.control.CompilationUnit$11.call(CompilationUnit.java:758)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:973)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:520)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:498)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:475)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:292)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:727)
	at groovy.lang.GroovyShell.run(GroovyShell.java:512)
	at groovy.lang.GroovyShell.run(GroovyShell.java:170)
	at groovy.lang.GroovyShell$run$1.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:848)
	at sun.reflect.GeneratedMethodAccessor423.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:151)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.GeneratedMethodAccessor422.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:276)
	at groovy.lang.Closure.call(Closure.java:271)
	at groovy.lang.Closure.run(Closure.java:354)
	at java.lang.Thread.run(Thread.java:637)
{noformat}",pniederw,pniederw,Major,Closed,Fixed,21/Feb/10 14:23,09/Mar/10 21:48
Bug,GROOVY-4072,12815064,finally block not called if exception rethrown in catch block,"Blah.groovy:
{code}
try {
  throw new Exception()
} catch (Exception e) {
  println ""e: ${e}""
  throw e
} finally {
  println ""finally""
}
{code}

~/Downloads/groovy-1.6.8/bin/groovy Blah =>

{code}
e: java.lang.Exception
Caught: java.lang.Exception
	at Blah.run(Blah.groovy:2)
{code}

~/Downloads/groovy-1.6.7/bin/groovy Blah =>

{code}
e: java.lang.Exception
finally
Caught: java.lang.Exception
	at Blah.run(Blah.groovy:2)
{code}",blackdrag,mstudman,Blocker,Closed,Fixed,21/Feb/10 21:19,06/Apr/10 12:08
Bug,GROOVY-4075,12815477,shouldFailWithCause no longer works for unchecked exceptions,"So we upgraded from 1.6.3 to Groovy 1.7.0 and realized that one of our tests started failing... it seems that in 1.7.0 unchecked exceptions are simply rethrown (groovy.lang.Closure#throwRuntimeException(Throwable) called from #call(Object[])). Which in groovy.util.GroovyTestCase#shouldFailWithCause(Class,Closure), the exception falls to the catch Throwable branch and the cause is not attempted to be determined beyond that, which causes the test to fail.

Example test showing a successful checked and unsuccessful unchecked exception:
{code}
package dummy;

import groovy.util.GroovyTestCase;

import java.io.IOException;

import org.junit.Test;

/**
 * Demonstrates cases of should fail with cause.
 * @author Charlie Huggard-Lee 
 */
class DummyTest extends GroovyTestCase {
    
    public static void failChecked() throws Exception {
        throw new Exception(new IOException());
    }
    
    public static void failUnchecked() {
        throw new RuntimeException(new IOException());
    }
    
    
    @Test
    public void testcheckedFailure() {
        //Hurray!
        shouldFailWithCause(IOException.class, {DummyTest.failChecked()})    
    }
    
    @Test
    public void testuncheckedFailure() {
        //BOO!!!
        shouldFailWithCause(IOException.class, {DummyTest.failUnchecked()})    
    }
    
}
{code}",roshandawrani,achuggardlee,Minor,Closed,Fixed,23/Feb/10 11:29,07/Apr/10 23:45
Bug,GROOVY-4078,12815114,Number.step incorrectly believes there is an infinite loop,"
recently i noticed that an exception is thrown with something like
0.step 0, 10, {}

i checked the dgm (version 1.7.0 i think) and found that self and to equality is never considered as a possibility.

i believe it should just do nothing like, say
(0..<0).step 10, {}
or
for (int i=0; i<0; i++) {}",roshandawrani,jpertino,Major,Closed,Fixed,24/Feb/10 18:03,07/Apr/10 23:45
Bug,GROOVY-4079,12815085,@Grab fails with ClassCastException,"{code}
@GrabResolver(
  name = 'm2repo.spockframework.org',
  root = 'http://m2repo.spockframework.org/snapshots')
@Grab('org.groovyext:groovyext:0.1-SNAPSHOT')
import java.lang.annotation.Retention
import java.lang.annotation.RetentionPolicy

@Retention(RetentionPolicy.RUNTIME)
@interface Require {
  Class value()
}


class Validator {
  def isValid(pogo) {
    pogo.getClass().declaredFields.every {
      isValidField(pogo, it)
    }
  }

  def isValidField(pogo, field) {
    def annotation = field.getAnnotation(Require)
    !annotation || meetsConstraint(pogo, field, annotation.value())
  }

  def meetsConstraint(pogo, field, constraint) {
    def closure = constraint.newInstance(null, null)
    field.setAccessible(true)
    closure.call(field.get(pogo))
  }
}

class Person {
  @Require({ it ==~ /[a-z A-Z]+/ })
  String name
  @Require({ it in (0..130) })
  int age
}

def validator = new Validator()

def fred = new Person(name: ""Fred Flintstone"", age: 43)
assert validator.isValid(fred)

def barney = new Person(name: ""!!!Barney Rubble!!!"", age: 37)
assert !validator.isValid(barney)

def dino = new Person(name: ""Dino"", age: 176)
assert !validator.isValid(dino)
{code}

Running this code in GroovyConsole gives:
{noformat}
java.lang.ClassCastException: org.codehaus.groovy.ast.stmt.BlockStatement cannot be cast to org.codehaus.groovy.ast.stmt.ReturnStatement
	at org.codehaus.groovy.classgen.ExtendedVerifier.visitConstructorOrMethod(ExtendedVerifier.java:92)
	at org.codehaus.groovy.classgen.ExtendedVerifier.visitMethod(ExtendedVerifier.java:71)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1044)
	at org.codehaus.groovy.classgen.ExtendedVerifier.visitClass(ExtendedVerifier.java:59)
	at org.codehaus.groovy.control.CompilationUnit$11.call(CompilationUnit.java:736)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:971)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:519)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:497)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:474)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:292)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:727)
	at groovy.lang.GroovyShell.run(GroovyShell.java:512)
	at groovy.lang.GroovyShell.run(GroovyShell.java:170)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:133)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:857)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:44)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:143)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:151)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:276)
	at groovy.lang.Closure.call(Closure.java:271)
	at groovy.lang.Closure.run(Closure.java:354)
	at java.lang.Thread.run(Thread.java:637)
{noformat}

Problem seems to be related to where I put @Grab/@GrabResolver. If I put it on the annotation type, I get the same exception as shown above. If I put it on Validator, the script succeeds.

General question: Is it relevant where I put a @Grab? There is no logical place for the @Grab in this code, it ""just"" pulls in a global AST transform.

Maybe the @Grab transform and my transform are incompatible, but I doubt it. My transform only rewrites the arguments of the @Require annotations (not the annotation type).",roshandawrani,pniederw,Major,Closed,Fixed,25/Feb/10 18:32,01/Mar/10 19:07
Bug,GROOVY-4080,12815118,Invalid class generated for an annotation when @Grab is used with it.,"{code}
@Grab(group='commons-primitives', module='commons-primitives', version='1.0')
import java.lang.annotation.*

@Retention(RetentionPolicy.RUNTIME)
@interface Require {
    String value()
}
{code}

The code above produces a class that is invalid and whose loading fails with the error below:
{noformat}
Caught: java.lang.VerifyError: (class: Require, method: <clinit> signature: ()V) Empty code
{noformat}

It seems that the static initializer method (<clinit>) gets written in bytecode without its body.",roshandawrani,roshandawrani,Major,Closed,Fixed,26/Feb/10 06:43,07/Apr/10 23:45
Bug,GROOVY-4081,12815087,Compiler should not allow enum constructor calls from outside the enum,"Currently groovy compiler lets enum constructor invocations from outside the enum class (and its sub-classes) get compiled. Such calls fail at runtime because enum constructors are transformed to add 2 additional parameters in front of the user-provided parameters - 1) identifier, 2) ordinal.

{code}
enum Alphabet {
	A(1), Z(26)
	private int m_pos;

	public Alphabet( int pos ) {
		m_pos = pos;
	}
}

/* compiler should reject this invocation as enum constructor are only for internal purposes */
println new Alphabet(2) 
{code}

In the code above, the call ""new Alphabet(2)"" fails at runtime because including the 2 internally added parameters, the constructor is <(String,II)V> and not <(I)V>.",roshandawrani,roshandawrani,Major,Closed,Fixed,26/Feb/10 10:28,07/Apr/10 23:45
Bug,GROOVY-4086,12815024,ClassCastException on every() ,"The following code throws a ClassCastException 

int[] a = [ 2, 2, 2, 2, 2]

println (1..4.every { a[it] == a[0] } )",paulk,xjia,Major,Closed,Fixed,04/Mar/10 10:08,07/Apr/15 19:12
Bug,GROOVY-4095,12815026,Labeled statements have wrong source position,"Example:

{code}
mylabel:
foo()
{code}

Source position of this statement is [1,8]-[1,9], which is the position of the ':' token. Same for all other labeled statements I've checked.",daniel_sun,pniederw,Major,Closed,Fixed,09/Mar/10 16:07,06/Mar/18 23:25
Bug,GROOVY-4096,12811639,GroovyConsole cosmetic annoyances,GroovyConsole has some issues with linespacing in the output window. This is particularly noticeable when usign large fonts as per the attached screenshot.,paulk,paulk,Minor,Closed,Fixed,10/Mar/10 06:31,07/Apr/15 19:12
Bug,GROOVY-4097,12815078,MOP implementation does not handle method calls in relation with class objects properly!,"In a few places, the Meta Object Protocoll implementation uses parameter objects classes to look up the matching meta methods. But if one of these arguments already is of type Class, instead of using Class<Class<?>>, the original class parameter is used. This leads to a couple of bugs.

My attached TestCase includes two methods, each demonstrating on of these bugs.

h4. First example:

The first method, testClassArguments(), presents a major issue.
The TestClass has two methods: getSomething(String) and getSomething(Class).

Using the MetaObjectProtocol, i first retrieve the MetaMethod of getSomething (line 23) for an argument set [String] (line 22). MetaClassImpl looks up this method by taking the classes of [String] (namely  [Class<String>]), and looking for a method with a signature, that matches [Class<String>]. This works fine, the result printed (line 24) is the correct method, and the assertion (line 25) passes.

Now i do the same thing again, but this time using an argument set [Class<String>] (line 27). getMetaMethod returns a method (line 28). But, as one can see in the output (line 29), the method is once again getSomething(String): Since MetaClassImpl internally looks up the method, by taking the parameters classes, but in the process _skipping_ class transformation for class arguments, it once again looks for a method with signature [Class<String>] instead of [Class<Class<String>>]. Now, invoking getSomething(String) with Class<String> as parameter of course leads to an IllegalArgumentException, thereby failing the test.

h4. Second example

This second example, method testInvokeStaticMethod(), demonstrates, how a call to getSuperclass, a normal public instance method for Class<T>, is wrongly executed as a static method call on T.

This bug does not occur, when using a default MetaClass: since all default MetaClasses are subclasses of MetaClassImpl, and the class org.codehaus.groovy.runtime.callsite.CallSiteArray has an instanceof test at the third line of it's method createCallStaticSite(CallSite,Class,Object[]), this does not happen for any of MetaClassImpl subclasses.

So the first thing this test does is, to replace its MetaClass with a DelegatingMetaClass, which does nothing except forward all calls to the original MetaClassImpl, but it is not a subclass of MetaClassImpl, so the mentioned instanceof test does not work.

Firstly, the test now retrieves it's own Class object (line 38), which works as expected.

Now, the tests asserts, that its own superclass is GroovyTestCase (line 39), but this assertion fails. The actually returned value of the clazz.getSuperclass() call is ""This was the wrong method!"", which is returned by the test cases static getSuperclass() method (lines 42 to 44).

So what happened here? That is pretty simple:

A normal static call to the getSuperclass() method at lines 42 to 44 would be translated to invokeStaticMethod(GroovyTestCase.class, ""getSuperclass"", []) on the MetaClass of GroovyTestCase.

The superclass call at line 39 *should* have been translated to invokeMethod(clazz, ""getSuperclass"", []) on the MetaClass of Class<T>.

But what happened instead is, since clazz == GroovyTestCase.clazz, the call site was simply ""mistaken"" for a static call site, and delegated to the wrong MetaClass.

h5. some final words

though the displayed test cases may seem a little artificially constructed, i actually stumbled over both of these bugs in my project, after renaming a few java files to groovy. So please note that these are real issues, the examples only may seem that surreal because i reduced them to a minimum. This stuff does  regularly break code when doing some heavy MOP related work.",emilles,misterd,Critical,Resolved,Fixed,10/Mar/10 09:37,15/Apr/23 15:45
Bug,GROOVY-4098,12817491,setter and getter destroy meta properties,"When a class offers a getter or setter (but not both) for a property, which is also accessible without that getter or setter breaks, the MOP's according MetaProperty is broken in terms of access permission: A (visible) property with additional setter is marked ""write only"", and one with additional getter ""read only"".

I have attached a test case which shows all four possible variants, two of which are broken:
For each variant, first, read and write access to the property are done by simply accessing the property. This proves, that both read and write access should be possible to that property.
Then, read and write access are done again, this time using the properties MetaProperty.

h4. No getter, no setter.

for propertyOne (line 6), neither a getter, nor a setter is defined. The according test (line 27 to 34) passes: The normal access and the MetaProperty can be read and written. It is neither marked read-only, nor write-only, just as it is supposed to be.

h4. Setter only.

for propertyTwo(line 7), only a setter is defined (line 11 to 13). The according test (line 36 to 43) fails: The normal access works fine, also setting the MetaProperty is possible, but when trying to read the MetaProperty, a GroovyRuntimeException is thrown: ""Cannot read write-only property: propertyTwo"", although it should be possible to read it directly without a setter (as done in line 38), but the MOP does not respect that.

h4. Getter only.
for propertyThree(line 8), only a getter is defined (line 15 to 17). The according test (line 45 to 52) fails just like the previous one, except this time read access is possible and write access fails, and the exception states, this was a read-only property, which it is not. Direct write access without a setter (as done in line 46) should be possible.

h4. Getter and Setter
for propertyFour(line 9), both a getter (lines 19 to 21) and a setter (lines 23 to 25) are defined. The according test (lines 54 to 61) passes just like the first test, where neither were defined. Again, the meta property is neither marked write-only or read-only, as it is supposed to be.

h5. few final words
This bug prevents MetaClasses, from implementing get/setProperty(object, name[, value]) by forwarding it to getMeta/hasProperty(name).get/setProperty(object[, value]). This is very disturbing if you redefined getMetaProperty or hasProperty (where's the difference in these methods by the way?) to perform additional searches to find properties, and you simply want to invoke the returned meta property instead of first determining, if you should ignore the returned meta property and call super.get/setProperty(..) because the meta property invocation might fail with such a misplaced access violation.",paulk,misterd,Critical,Closed,Fixed,10/Mar/10 10:30,07/Apr/15 19:13
Bug,GROOVY-4099,12815141,@Immutable annotation does not allow untyped static fields,"I cannot compile a class with @Immutable if it has a static field declared with def. I don't think the transform should touch static fields.

{code}
~$ cat ImmutableTest.groovy
@Immutable                                 
class ImmutableTest {                      
  static foo = {}                          
}                                          
marcus@better:~$ groovyc ImmutableTest.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
General error during canonicalization: @Immutable processor doesn't know how to handle field 'foo' of type 'java.lang.Object or def' while compiling class ImmutableTest.                                                                         
@Immutable classes currently only support properties with known immutable types or types where special handling achieves immutable behavior, including:                                                                                           
- Strings, primitive types, wrapper types, BigInteger and BigDecimal                                                     
- enums, other @Immutable classes and known immutables (java.awt.Color)                                                  
- Cloneable classes, collections, maps and arrays, and other classes with special handling (java.util.Date)              
Other restrictions apply, please see the groovydoc for @Immutable for further details                                    

java.lang.RuntimeException: @Immutable processor doesn't know how to handle field 'foo' of type 'java.lang.Object or def' while compiling class ImmutableTest.                                                                                    
@Immutable classes currently only support properties with known immutable types or types where special handling achieves immutable behavior, including:                                                                                           
- Strings, primitive types, wrapper types, BigInteger and BigDecimal
- enums, other @Immutable classes and known immutables (java.awt.Color)
- Cloneable classes, collections, maps and arrays, and other classes with special handling (java.util.Date)
Other restrictions apply, please see the groovydoc for @Immutable for further details
        at org.codehaus.groovy.transform.ImmutableASTTransformation.createConstructorStatement(ImmutableASTTransformation.java:373)
        at org.codehaus.groovy.transform.ImmutableASTTransformation.createConstructorMap(ImmutableASTTransformation.java:324)
        at org.codehaus.groovy.transform.ImmutableASTTransformation.createConstructor(ImmutableASTTransformation.java:310)
        at org.codehaus.groovy.transform.ImmutableASTTransformation.visit(ImmutableASTTransformation.java:113)
        at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:129)
        at org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:173)
        at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:957)
        at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:517)
        at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:495)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:472)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:456)
        at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:57)
        at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:170)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:138)
        at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:152)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:108)
        at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:130)

1 error
{code}
",paulk,marcusb,Major,Closed,Fixed,11/Mar/10 10:42,07/Apr/10 23:45
Bug,GROOVY-4100,12815760,clearTime() fails for times in the afternoon,"Calendar.clearTime() fails if the time part of the date is in the afternoon.

The gdk seems to be doing:
{code}
self.clear(Calendar.HOUR_OF_DAY);
self.clear(Calendar.HOUR);
{code}

I think that second line is unnecessary and breaking things.

Some test code:
{code}
// clearTime works for times in the morning
def morning = GregorianCalendar.getInstance()
morning.set(Calendar.AM_PM, Calendar.AM)
morning.clearTime()
assert morning.get(Calendar.HOUR_OF_DAY) == 0

// clearTime fails for times in the afternoon
def afternoon = GregorianCalendar.getInstance()
afternoon.set(Calendar.AM_PM, Calendar.PM)
afternoon.clearTime()
assert afternoon.get(Calendar.HOUR_OF_DAY) == 0  // fails in Groovy 1.7.1
{code}
",paulk,gconboy,Minor,Closed,Fixed,12/Mar/10 08:11,07/Apr/15 19:12
Bug,GROOVY-4102,12815134,remove timestamp fields for better hotswap,"While debugging I try to hot swap a groovy class.
It always fails with messages ""Schema change not implemented. Operation not supported by VM"".

With Java classes there is no such problem - hot swap works as long as I don't add/remove any field or method or change a method signature.

I had hoped to gradually convert my Java app to groovy, but this made me refrain from using groovy for anything more complex than ""java"" beans.

Looking at the bytecode with javap, it is obvious where the problem is:
There is an artifical  field that changes its name on each compilation.
Initial:
   public static java.lang.Long __timeStamp__239_neverHappen1268432715287;
After recompilation:
   public static java.lang.Long __timeStamp__239_neverHappen1268433121889;

What is this field used for? Can't we get rid of it?
All other fields/methods look the same after recompilation, so I am pretty sure that hot swap will work without this field change.
",melix,stephen.friedrich,Major,Closed,Fixed,12/Mar/10 16:42,17/Dec/14 13:25
Bug,GROOVY-4104,12815136,MethodClosure for a protected method in a super class not working properly,"The below java code outputs 1 and 0

{code:title=A.java}
public class A {
    public MethodClosure createMethodClosure() {
        return new MethodClosure(this, ""someMethod"");
    }

    protected void someMethod(int someParameter) {}
}
{code}
{code:title=B.java}
public class B extends A {
}
{code}
{code:title=Test.java}
public class Test {
    public static void main(String[] args) {
        System.out.println(new A().createMethodClosure().getMaximumNumberOfParameters());
        System.out.println(new B().createMethodClosure().getMaximumNumberOfParameters());
    }
}
{code}",roshandawrani,cdanielw,Major,Closed,Fixed,16/Mar/10 04:34,07/Apr/10 23:45
Bug,GROOVY-4106,12815130,ExpandoMetaClassCreationHandle does infinite recursion for ExpandoMetaClass,"in ExpandoMetaClassCreationHandle line 49 (else clause in createNormalMetaClass(Class,MetaClassRegistry)), there is a call
{code}super.create(theClass, registry){code}
which should be replaced by a call
{code}super.createNormalMetaClass(theClass, registry){code}
since, if createNormalMetaClass is invoked, this call came from super.create. Thus the call to super.create(..) creates infinite recursion.

This occurs for example when executing code like this:
{code}
MetaClassCreationHandle handle = new ExpandoMetaClassCreationHandle();
println handle.create(ExpandoMetaClass, GroovySystem.getMetaClassRegistry());
{code}
",roshandawrani,misterd,Trivial,Closed,Fixed,16/Mar/10 14:38,05/Apr/15 14:44
Bug,GROOVY-4107,12815139,ResolveVisitor falsely resolves an import against itself,"ResolveVisitor seems to be resolving an unqualified import against itself by mistake.

The following code should fail with a ""Unable to resolve class Test"" error, but due to the ResolveVisitor bug, it reaches ACG and then fails.

{code}
import Test

Test.foo()
{code}

It is failing with the following message:
{noformat}
Caught: BUG! exception in phase 'class generation' in source unit 'D:\Roshan\GroovyDevSetup\Workspace18x\Try18X\src\Try.groovy' ClassNode#getTypeClass for Roshan is called before the type class is set 
{noformat}",roshandawrani,roshandawrani,Major,Closed,Fixed,17/Mar/10 11:58,14/Aug/13 03:57
Bug,GROOVY-4108,12815132,Possible NPE in Groovy Ant task,In Groovy#createClasspathParts() there is a call to getSysProperties().getVariables() which according to the Ant JavaDoc can return null but we don't check for that case.,paulk,paulk,Major,Closed,Fixed,17/Mar/10 23:29,07/Apr/15 19:12
Bug,GROOVY-4112,12815030,Joint compilation fails if Groovy method with array parameter is called in vararg style from Java,"Foo.groovy:
{code}
class Foo {
  static foo(String[] args) {}
}
{code}

Bar.java:
{code}
public class Bar {
  public static void main(String[] args) {
    Foo.foo(""one"", ""two"", ""three"");
  }
}
{code}

This compiles fine separately, but with joint compilation I get:

{noformat}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Compile error during compilation with javac.
Bar.java:3: foo(java.lang.String[]) in Foo cannot be applied to (java.lang.String,java.lang.String,java.lang.String)
		Foo.foo(""a"", ""b"", ""c"");
		       ^
1 error
{noformat}

I suppose stub generation is to blame.
",paulk,pniederw,Major,Closed,Fixed,19/Mar/10 16:49,21/Jul/11 19:06
Bug,GROOVY-4116,12815031,"Call to an interface method results in IllegalAccessError when not implemented as ""public""","{code}
class C implements I {
    protected foo() {}
}

interface I {
    def foo()
}

def c = new C()
c.foo()
{code}

The code above results in the error below
{noformat}
Caught: java.lang.IllegalAccessError: C.foo()Ljava/lang/Object;
	at Try.run(Try.groovy:10)
{noformat}

If the interface methods are not implemented with public visibility, the compiler should crib.",roshandawrani,roshandawrani,Minor,Closed,Fixed,20/Mar/10 07:31,07/Apr/10 23:45
Bug,GROOVY-4117,12815140,org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl#registerMethods should not call System.exit,"After an unexpected exit from our app, I saw that org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl#registerMethods has an System.exit routine.

We are using groovy from within our Java application. It actually killed our app due to a configuration issue.

The groovy engine should throw an exception not System.exit.

I suggest to review all System.exit calls and replace them with throwing an exception.",roshandawrani,hucmuc,Blocker,Closed,Fixed,20/Mar/10 11:14,07/Apr/10 23:45
Bug,GROOVY-4118,12818392,JavaStubGenerator doesn't generate annotations available in Groovy code,"Looking at org.codehaus.groovy.tools.javac.JavaStubGenerator (http://goo.gl/ob23) I see it doesn't generate annotations that are originally available in Groovy sources.

I think it causes those issues later:
* http://jira.codehaus.org/browse/GMAVEN-68 - ""GMaven: generateStubs generates stubs without original Javadocs or annotations""
* http://jira.codehaus.org/browse/GMAVEN-4  - ""GMaven: Stub generation should generate annotations""

I'm trying to use AnnoMojo annotations (http://goo.gl/rbRw) when developing my MOJOs in Groovy.
GMaven's ""generateStubs"" goal doesn't produce Java sources with original AnnoMojo annotations",paulk,genie,Major,Closed,Fixed,20/Mar/10 15:48,06/Sep/10 08:49
Bug,GROOVY-4119,12815062,Implicit construtor call fails with NoSuchMethodError when an inner class extends another,"{code}
class Test {
    static main(args) {
        new Test().foo()
    }
    void foo() {
        new B()
    }
    class A {}
    class B extends A {}
}
{code}

Running the above code results in the following error
{noformat}
Caught: java.lang.NoSuchMethodError: Test$A: method <init>()V not found
	at Test$B.<init>(Try.groovy)
	at Test.foo(Try.groovy:6)
	at Test.main(Try.groovy:3)
{noformat}",roshandawrani,roshandawrani,Minor,Closed,Fixed,21/Mar/10 11:24,23/Jun/10 15:45
Bug,GROOVY-4120,12815023,Non static inner class usage fails with MethodSelectionException,"{code}
class Test {
    static main(args) {
        new A()
    }
    class A {
        def A() {}
        def A(num){}
    }
}
{code}

The above code fails with the following error
{noformat}
Caught: org.codehaus.groovy.runtime.metaclass.MethodSelectionException: Could not find which method <init>() to invoke from this list:
  public Test$A#<init>(Test, java.lang.Object)
  public Test$A#<init>(Test)
	at Test.main(Try.groovy:3)
{noformat}

I think it is happening because the inner class is non-static and no enclosing ""this"" instance is being passed to it. The compiler should enforce it at compile time instead of letting it get compiled and fail at runtime.",roshandawrani,roshandawrani,Major,Closed,Fixed,22/Mar/10 03:11,07/Apr/10 23:45
Bug,GROOVY-4121,12815055,Compiler fails to do assignment-to-final-fields checks on fields made final by @Immutable (was: @Immutable does not make fields immutable),"@Immutable annotation does not prevent changing fields:
{code}
@Immutable
class Account {
    BigDecimal balance
    String customer
    
    Account deposit(amount) {
        balance = balance + amount
        this
    }
    
    String toString() {
        ""Account[balance: $balance, customer: $customer]""   
    }
}

def acc = new Account(0.0, ""Test"")
acc.deposit(3.1) //should raise exception, shouldn't it?
assert 3.1 == acc.balance 
{code}

Maybe I got the description in http://groovy.codehaus.org/api/groovy/lang/Immutable.html wrong:
""Properties automatically have private, final backing fields with getters. Attempts to update the property will result in a ReadOnlyPropertyException."" ?",roshandawrani,johanneslink,Critical,Closed,Fixed,22/Mar/10 05:55,07/Apr/10 23:45
Bug,GROOVY-4127,12815125,deadlock in concurrent calls of GroovyShell.parse method against the same GroovyShell object,"Two threads calling GroovyShell.parse from the same GroovyShell instance enter in deadlock (see the stack below).
I'm not sure whether this multi-threaded use is supposed to be supported or not, so feel free to reject it in case GroovyShell is supposed to be used only from one thread or from multiple threads but with explicit synchronization.
Anyway this seems similar to GROOVY-4050



Java stack information for the threads listed above:
===================================================
""pool-4-thread-2"":
	at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:712)
	- waiting to lock <0x00002aaab42d2a50> (a java.util.HashMap)
	at groovy.lang.GroovyClassLoader$InnerLoader.loadClass(GroovyClassLoader.java:426)
	at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:773)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
	at java.beans.Introspector.instantiate(Introspector.java:1453)
	at java.beans.Introspector.findExplicitBeanInfo(Introspector.java:425)
	- locked <0x00002aaaaecd26c0> (a java.lang.Class for java.beans.Introspector)
	at java.beans.Introspector.<init>(Introspector.java:374)
	at java.beans.Introspector.getBeanInfo(Introspector.java:168)
	at groovy.lang.MetaClassImpl$15.run(MetaClassImpl.java:2931)
	at java.security.AccessController.doPrivileged(Native Method)
	at groovy.lang.MetaClassImpl.addProperties(MetaClassImpl.java:2929)
	at groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:2912)
	- locked <0x00002aaad57f7700> (a groovy.lang.MetaClassImpl)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:166)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:182)
	at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:211)
	at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:701)
	at groovy.lang.GroovyObjectSupport.<init>(GroovyObjectSupport.java:32)
	at groovy.lang.Script.<init>(Script.java:40)
	at groovy.lang.Script.<init>(Script.java:37)
	at Script288.<init>(Script288.groovy)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at java.lang.Class.newInstance0(Class.java:355)
	at java.lang.Class.newInstance(Class.java:308)
	at org.codehaus.groovy.runtime.InvokerHelper.createScript(InvokerHelper.java:400)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:739)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:766)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:757)
	at com.iontrading.punit.itdb.c.<init>(SupplyProcessorUnit.java:68)
	at com.iontrading.punit.itdb.a.<init>(SupplyProcessorUnit.java:162)
	at com.iontrading.punit.itdb.SupplyProcessorUnit.processUpdate(SupplyProcessorUnit.java:266)
	at com.iontrading.punit.core.PUnitBase.notifySinks(PUnitBase.java:83)
	at com.iontrading.punit.jdbc.JDBCQueryUnit.notifySinks(JDBCQueryUnit.java:744)
	at com.iontrading.punit.jdbc.JDBCQueryUnit$a.run(JDBCQueryUnit.java:493)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
""pool-6-thread-1"":
	at java.beans.Introspector.getPublicDeclaredMethods(Introspector.java:1271)
	- waiting to lock <0x00002aaaaecd26c0> (a java.lang.Class for java.beans.Introspector)
	at java.beans.Introspector.internalFindMethod(Introspector.java:1321)
	at java.beans.Introspector.findMethod(Introspector.java:1392)
	at java.beans.Introspector.findMethod(Introspector.java:1372)
	at java.beans.PropertyDescriptor.getReadMethod(PropertyDescriptor.java:179)
	- locked <0x00002aaab4279358> (a java.beans.PropertyDescriptor)
	at groovy.lang.MetaClassImpl.applyPropertyDescriptors(MetaClassImpl.java:2209)
	at groovy.lang.MetaClassImpl.setupProperties(MetaClassImpl.java:1993)
	at groovy.lang.MetaClassImpl.addProperties(MetaClassImpl.java:2941)
	at groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:2912)
	- locked <0x00002aaad58d5290> (a groovy.lang.MetaClassImpl)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:166)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:195)
	at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:215)
	at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:697)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.createPojoSite(CallSiteArray.java:107)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallSite(CallSiteArray.java:148)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:54)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
	at org.codehaus.groovy.ast.builder.AstBuilderInvocationTrap.visitMethodCallExpression(AstBuilderTransformation.groovy:177)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:67)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitExpressionStatement(CodeVisitorSupport.java:69)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoCachedMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:229)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:52)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callSafe(AbstractCallSite.java:90)
	at org.codehaus.groovy.ast.builder.AstBuilderTransformation.visit(AstBuilderTransformation.groovy:57)
	at org.codehaus.groovy.transform.ASTTransformationVisitor$3.call(ASTTransformationVisitor.java:303)
	at org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:845)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:521)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:497)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:474)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:292)
	- locked <0x00002aaab42d2a50> (a java.util.HashMap)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:727)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:739)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:766)
	at groovy.lang.GroovyShell.parse(GroovyShell.java:757)
	at com.iontrading.punit.itdb.c.<init>(SupplyProcessorUnit.java:68)
	at com.iontrading.punit.itdb.a.<init>(SupplyProcessorUnit.java:162)
	at com.iontrading.punit.itdb.SupplyProcessorUnit.processUpdate(SupplyProcessorUnit.java:266)
	at com.iontrading.punit.core.PUnitBase.notifySinks(PUnitBase.java:83)
	at com.iontrading.punit.jdbc.JDBCQueryUnit.notifySinks(JDBCQueryUnit.java:744)
	at com.iontrading.punit.jdbc.JDBCQueryUnit$a.run(JDBCQueryUnit.java:493)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)",guillaume,giamby,Major,Closed,Fixed,23/Mar/10 08:51,01/Jun/10 09:49
Bug,GROOVY-4129,12815086,Implicit 'this' reference not being passed if inner class instance is created in outer class instance initializer,"{code}
class Test {
    {
        new A()
    }
    class A {}
}
{code}

The code above fails with the following error indicating that the implicit 'this' is not being passed. It should be.
{noformat}
No enclosing instance passed in constructor call of a non-static inner class
{noformat}",roshandawrani,roshandawrani,Major,Closed,Fixed,24/Mar/10 05:48,07/Apr/10 23:45
Bug,GROOVY-4130,12815123,Using categories results in multiple thread locals being retained that Tomcat has to forcibly clear,See GRAILS-5823 basically we need a way for frameworks like Grails to clear out the thread locals Groovy uses otherwise Tomcat will forcibly clear them resulting in errors,ait,graemerocher,Major,Closed,Fixed,25/Mar/10 05:13,21/Jan/16 22:23
Bug,GROOVY-4131,12815058,Compiler allows 2 main methods in a script,"The code below works fine with groovy. Compiler picks up the 1st main() and ignores the 2nd one. The code should not compiled because it has repetitive methods/

{code}
static main(args) {
    println ""Bar""
}
static main(args) {
    println ""Foo""
}
{code}",roshandawrani,roshandawrani,Minor,Closed,Fixed,26/Mar/10 01:08,07/Apr/10 23:45
Bug,GROOVY-4133,12815056,@Delegate does not handle native methods correctly,"{code}
class MyString {
    @Delegate String str
}

assert MyString != null
{code}

The code above fails with the error below
{noformat}
Caught: java.lang.ClassFormatError: Code attribute in native or abstract methods in class file MyString
{noformat}

It happens because String class has a native intern(). The delegate method that @Delegate adds to MyString continues to be marked native and now has code body added to it, which causes the ClassFormatError.",roshandawrani,roshandawrani,Minor,Closed,Fixed,27/Mar/10 04:43,07/Apr/10 23:45
Bug,GROOVY-4134,12815152,Closure default values cannot be static constants,"I'd like to use a default value in a closure argument, but at the moment,
it's not working, and I was wondering if the thing I'm trying is not
supported...

This is my code:

Sheet.metaClass.dump = { Integer maxRows = Integer.MAX_VALUE ->

}

Code from Tim Yates:

This works:

def max = Integer.MAX_VALUE
String.metaClass.dump = { Integer maxRows = max ->
  delegate.substring( 0, delegate.length() < maxRows ? delegate.length() : maxRows )
}
assert 'hello'.dump( 2 ) == 'he'
assert 'hello'.dump() == 'hello'

but as you say, putting the Integer.MAX_VALUE in the closure definition throws the MissingPropertyException.
",roshandawrani,bodiam,Major,Closed,Fixed,31/Mar/10 03:28,07/Apr/10 23:45
Bug,GROOVY-4137,12815115,Default constructor added by groovy does not have source information set on it,"If no constructor is provided, groovy compiler adds a default constructor, which is public / non-synthetic. This constructor does not have source information set on it.

Java sets the source information of the class on such constructors - e.g., class starting line number.",roshandawrani,roshandawrani,Minor,Closed,Fixed,01/Apr/10 10:21,07/Apr/10 23:45
Bug,GROOVY-4138,12815121,Groovy JDK File.eachFileMatch insufficient documentation to use the method.,The documentation in the File.eachFilematch entry of the Groovy JDK does not give enough information to make use of the method quickly and easily.,paulk,russel,Major,Closed,Fixed,02/Apr/10 03:09,07/Apr/15 19:13
Bug,GROOVY-4139,12815126,Assigning with empty String as hash key (i.e. a['']=t) yields StringIndexOutOfBoundsException,"This is a bizarre one - and I can't always get it to replicate; but where it fails, it ALWAYS fails.  (e.g. in my IDE, it works fine.  In a standalone Tomcat instance, it reliably seems to bomb on two different machines - even though the IDE is using the exact same version of Tomcat.  Seems OK when I run it on the command line - no failure.

The line looks about like this :

topics[id] = t
where id.getClass() is java.lang.String and id==''
t is a complex class, with various fields.
i.e. topics[''] = t

It's the '' that seems to throw it.  (other assignments using non-empty strings work fine).

At the bottom is the stack trace, with the topics['']=t line on the very bottom.  (it goes down further, and I will gladly provide it if needed, but I don't think that part is probably useful).

A few other bizarre details: 

1) after this error occurs (if I catch it and keep going) 'topics' will contain the key/value pair, but groovy will not be able to fetch them by key.  However, by casting 'topics' as a map and going from java (or jsp) I can use topics.get("""") to successfully retrieve the correct value.

2) at this same point (after the error), java can see the entry by enumerating the values; but groovy cannot see the '' when it enumerates the keys (using keySet())

My question is, regarding the below stack trace, why the hell is doing all this crap just to put a value into a hash?  I count 15 stack frames, most of which involve groovy classes.  Shouldn't it just be map.put(key,value) ???

Whatever.  It bombed.

If I find out more, I'll post it here.


[from catalina.out]:------------------------------------

id is ''
GroovyServlet Error:  script: '/index.groovy':  Script processing failed.String index out of range: -1java.lang.String.substring(String.java:1938)
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(String.java:1938)
        at java.lang.String.substring(String.java:1905)
        at org.codehaus.groovy.runtime.MetaClassHelper.capitalize(MetaClassHelper.java:453)
        at groovy.lang.MetaClassImpl.setProperty(MetaClassImpl.java:2312)
        at groovy.lang.MetaClassImpl.setProperty(MetaClassImpl.java:3306)
        at org.codehaus.groovy.runtime.InvokerHelper.setProperty(InvokerHelper.java:183)
        at org.codehaus.groovy.runtime.DefaultGroovyMethods.putAt(DefaultGroovyMethods.java:185)
        at org.codehaus.groovy.runtime.dgm$504.doMethodInvoke(Unknown Source)
        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1058)
        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
        at org.codehaus.groovy.runtime.callsite.PojoMetaClassSite.call(PojoMetaClassSite.java:44)
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:129)
        at kb.TopList$_refresh_closure1.doCall(TopList.groovy:58)

",roshandawrani,mlzarathustra,Major,Closed,Fixed,02/Apr/10 03:17,07/Apr/10 23:45
Bug,GROOVY-4140,12815135,Groovy JDK documentation has mismatch between signatures and explanation,The Groovy JDK documentation shows the signatures of the methods but without any variable names for the parameters.  The documentation uses only the variable names so there is no connect between the variables to be provided to teh call and the explanation.  cf the String page of the Groovy JDK documentation.,paulk,russel,Major,Closed,Fixed,02/Apr/10 05:18,07/Apr/15 19:13
Bug,GROOVY-4145,12815091,Statically imported fields & properties,"Create class Foo
{code}
class Foo {
  static foo=2
  static getFoo(){
    return 3
  } 
}
{code}

and a script 
{code}
import static Foo.foo

print foo
{code}

This script calls getter getFoo and prints ""3"".
But if you delete declaration ""static foo=2"" from the class the script fails with MissingPropertyException. 
IMHO the script should call getter in both cases.",roshandawrani,mxm-groovy,Major,Closed,Fixed,03/Apr/10 03:14,17/Jun/15 20:09
Bug,GROOVY-4147,12815092,Timecategory strange format (too many '-' signs) when printing 'negative' date,"{code}
use(groovy.time.TimeCategory) {
  def first = Calendar.instance.time
  Thread.sleep 4200
  def second = Calendar.instance.time
  
  println first - second
}
{code}

When run in GroovyConsole this gives gives:

{code}
-4.-201 seconds
{code}
",paulk,alpheratz,Minor,Closed,Fixed,03/Apr/10 18:23,07/Apr/15 19:13
Bug,GROOVY-4151,12815122,"println ""$this"" fails in inner class, but println ''+this works","{code}
class C {
  class S {
   //  S() { println ""$this"" } // compilation error: unexpected token: this
    S() { println ''+this } // OK
  }
}
{code}",roshandawrani,mlzarathustra,Major,Closed,Fixed,05/Apr/10 12:58,17/May/18 16:55
Bug,GROOVY-4158,12815159,cannot mix rcurry and ncurry together,"Groovy is not letting me mix ncurry and rcurry.

def operation = { int x, Closure f, int y -> f(x, y) }
def divider = operation.ncurry(1) { a, b -> a / b }
def halver = divider.rcurry(2)
assert 50 == halver(100)

the last line fails with the parameters out of order, even though it
should not:

Caught: groovy.lang.MissingMethodException: No signature of method:
NewInGroovy_1_7_2$_run_closure4.doCall() is applicable for argument
types: (NewInGroovy_1_7_2$_run_closure5, java.lang.Integer,
java.lang.Integer) values: [NewInGroovy_1_7_2$_run_closure5@6179d854,
100, 2]
Possible solutions: doCall(int, groovy.lang.Closure, int)
       at NewInGroovy_1_7_2.run(NewInGroovy_1_7_2.groovy:73)",paulk,hamletdrc,Major,Closed,Fixed,09/Apr/10 00:41,23/Jun/10 15:45
Bug,GROOVY-4159,12811641,hang on exception in synchronized block,"Following script hangs on 1.7.2:

obj = 1
synchronized(obj) {
    obj.e
}",blackdrag,san,Blocker,Closed,Fixed,09/Apr/10 03:55,10/Jun/10 09:11
Bug,GROOVY-4161,12815138,Source locations for added default constructors are incorrect,"Groovy Eclipse has just upgraded to Groovy 1.7.2 from 1.7.0 and there is a change to Verifier.addDefaultConstructors() that is breaking us.

The code is now:

{code}
BlockStatement empty = new BlockStatement();
empty.setSourcePosition(node); 
ConstructorNode constructor = new ConstructorNode(ACC_PUBLIC, empty);
constructor.setSourcePosition(node);
{code}

Is there a reason why the source location for the empty blockstatement and its containing constructor is set to be the source location of the entire class?

This is breaking content assist in Groovy-Eclipse.  We would prefer to have these kinds of synthetic methods not have any source location at all.

I propose to change it to this:

{code}
BlockStatement empty = new BlockStatement();
ConstructorNode constructor = new ConstructorNode(ACC_PUBLIC, empty);
{code}

(As above, but just remove the calls to ""setSourcePosition"".)",roshandawrani,werdna,Major,Closed,Fixed,09/Apr/10 16:43,23/Jun/10 15:45
Bug,GROOVY-4163,12815155,Groovyc is unable to compile a class which implements interface and uses @Delegate annotation,"Groovy source file is attached.

I am trying to compile this file with the command:
groovy Temp.groovy

Compiler's output is as follows:

org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Temp.groovy: 4: Can't have an abstract method in a non-abstract class. The class 'Temp' must be declared abstract or the method 'void run()' must be implemented.
 @ line 4, column 1.
   class Temp implements Runnable {
   ^

1 error
",paulk,pdolega,Major,Closed,Fixed,11/Apr/10 19:27,22/Jul/10 17:32
Bug,GROOVY-4168,12815096,MapWithDefault doesn't have correct equals functionality,"In this example, the last line fails but shouldn't:
{code}
def m = [:].withDefault {k -> k * 2}
m[1] = 3
assert m[1] == 3
assert m[2] == 4
assert [1:3, 2:4] == m
assert m == [1:3, 2:4]
{code}
",paulk,paulk,Major,Closed,Fixed,13/Apr/10 05:56,07/Apr/15 19:06
Bug,GROOVY-4169,12815144,Compile error on annotations for closure parameters,"I get a compile error on annotations which are defined for closure parameters. This is a minimal and complete example where the problem occurs:


public @interface description {
   String value()
}

newClassImpl = { @description(""Whether the class is abstract"") boolean abstr ->
}


The compile error in this case says ""class description is not an annotation in @description"".",roshandawrani,herrmama,Major,Closed,Fixed,13/Apr/10 10:49,23/Jun/10 15:45
Bug,GROOVY-4170,12818397,Currying private methods,"Private methods cannot be curried any more under 1.7.2. I'm pretty sure the code worked fine under earlier Groovy versions.

The code:
{code}
class CurryFoo {
    protected void foo1(String s, int i) {
        println 'Hurray! Foo1 can be curried.'
    }

    private void foo2(String s, int i) {
        println 'Hurray! Foo2 can be curried.'
    }

    public void bar() {
        this.&foo1.curry('anything', 1).call()
        this.&foo2.curry('anything', 1).call()
    }
}

new CurryFoo().bar()
{code}

outputs:

{noformat}
Hurray! Foo1 can be curried.
Caught: java.lang.IllegalArgumentException: To curry 2 argument(s) expect index range 0..-2 but found 0
    at groovyx.gpars.samples.safe.CurryFoo.bar(CurrySample.groovy:30)
    at groovyx.gpars.samples.safe.CurrySample.run(CurrySample.groovy:34)

Process finished with exit code 1 
{noformat}

Roshan Dawrani commented on the mailing list:
True. Doesn't work under 1.7.3 as reported and works on 1.6.9.
This seems to have been caused (unintentionally) by some recent changes in closure currying (rcurry / ncurry).
",roshandawrani,roller_vaclav,Major,Closed,Fixed,13/Apr/10 13:25,23/Jun/10 15:45
Bug,GROOVY-4177,12811922,Groovy categories cause tomcat memory leak and prevent tomcat shutting down,"Any Groovy application (including Grails apps) that deploy to Tomcat 6.0.26 and above has this issue because basically Tomcat clears thread locals not manually shutdown by the application to prevent memory leaks.

This can lead to problems starting and stopping Groovy based application with errors like the following occuring:

{code}
    [java] Exception in thread ""Thread-3"" java.lang.NullPointerException
     [java] 	at org.codehaus.groovy.runtime.GroovyCategorySupport.hasCategoryInCurrentThread(GroovyCategorySupport.java:216)
     [java] 	at groovy.lang.MetaClassImpl.getMethodWithCaching(MetaClassImpl.java:1128)
     [java] 	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:904)
     [java] 	at groovy.lang.ExpandoMetaClass.invokeMethod(ExpandoMetaClass.java:915)
     [java] 	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
     [java] 	at groovy.lang.Closure.call(Closure.java:276)
     [java] 	at groovy.lang.Closure.call(Closure.java:271)
     [java] 	at groovy.lang.Closure.run(Closure.java:354)
{code}

Groovy needs to include the ability to manually shutdown the any categories that exist in any threads and also re-instate the categories if they are nulled (like what happens with Tomcat)",jwagenleitner,graemerocher,Critical,Closed,Fixed,20/Apr/10 06:45,04/Mar/16 03:13
Bug,GROOVY-4180,12818053,groovydoc appears to be broken with java.util.NoSuchElementException,"groovydoc in grails 1.3 (groovy 1.7.2) applications is broken with the following error occuring:

{code}
java.util.NoSuchElementException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)
	at java.util.HashMap$EntryIterator.next(HashMap.java:834)
	at java.util.HashMap$EntryIterator.next(HashMap.java:832)
	at org.codehaus.groovy.tools.groovydoc.GroovyRootDocBuilder.processFile(GroovyRootDocBuilder.java:209)
	at org.codehaus.groovy.tools.groovydoc.GroovyRootDocBuilder.buildTree(GroovyRootDocBuilder.java:163)
	at org.codehaus.groovy.tools.groovydoc.GroovyDocTool.add(GroovyDocTool.java:65)
	at org.codehaus.groovy.ant.Groovydoc.execute(Groovydoc.java:391)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:288)
	at sun.reflect.GeneratedMethodAccessor81.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at 
{code}",paulk,graemerocher,Critical,Closed,Fixed,23/Apr/10 04:13,23/Jun/10 15:45
Bug,GROOVY-4182,12815099,very bad performance in GroovyCategorySupport because of static AtomicInteger use,"Profiler shows average 1ms per call in our multi-threaded (20 threads) application in most of the calls to PojoMetaClassSite.call()
because of static atomic Inreger is used (categoriesInUse) in GroovyCategorySupport.hasCategoryInCurrentThread():

    public static boolean hasCategoryInCurrentThread() {
        return categoriesInUse.get() != 0 && threadInfo.getInfo().level != 0;
    }
",blackdrag,vkorehov,Critical,Closed,Fixed,28/Apr/10 06:25,14/Dec/10 10:43
Bug,GROOVY-4187,12815173,"GDK should make the File method  setText(String text, String charset)    an alias for   write(String text, String charset)","Because GDK makes the File method setText(String text) an alias for  write(text),
and because it's possible to say both  getText()  and getText(String charset),
it feels clunky that there is no  setText(String text, String charset).

In short, the following is annoying:
{code}
myfile.getText(charset)   
...
myfile.write(text,charset)     // Hm... I wish   myfile.setText(text,charset) just worked here too.
{code}

It seems like adding this alias would not break anybody's code,
and would remove a silly-thing-you-gotta-remember.

",paulk,jcox,Minor,Closed,Fixed,03/May/10 14:02,23/Jun/10 15:44
Bug,GROOVY-4188,12815150,"NoSuchFieldError when defining a field named ""metaClass""","The following script causes a NoSuchFieldError. It would be better if all Groovy-internal members started with ""$"", or at least caused a sensible compile error.

{code}
class Foo {
  private String metaClass
}
  
  
new Foo()
{code}",roshandawrani,pniederw,Major,Closed,Fixed,03/May/10 14:44,23/Jun/10 15:44
Bug,GROOVY-4190,12815182,Static generic method invocation with explicitely stated type doesn't compile,"Cannot compile statements like SomeClass.<Integer>create(). Example code:
{code:title=Test.groovy}
class Test<E> {
        E someVariable;

        protected Test() {
        }

        public static <E> Test<E> create() {
                return new Test<E>();
        }

        public static void main(String[] args) {
                Test<Integer> t = Test.<Integer>create();
        }
}
{code}
The error is:
{noformat}
$ groovy Test.groovy
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
/home/aless/Test.groovy: 12: Unknown type: TYPE_ARGUMENTS at line: 12 column: 26. File: /home/aless/Test.groovy @ line 12, column 26.
                Test<Integer> t = Test.<Integer>create();
                            ^

1 error
{noformat}
Exactly the same file (with a .java extension) compiles with javac.
",roshandawrani,aless,Major,Closed,Fixed,04/May/10 10:23,23/Jun/10 15:45
Bug,GROOVY-4191,12818049, java.lang.ArrayStoreException on attempt to assign number to an element of a string array for a second time,"The following code demonstrates the problem:

{code}
def recreateProblem() {
    val = ""a;2"".split("";"");
    val[1] = 1;
}

recreateProblem();

// this second call will throw java.lang.ArrayStoreException
//
recreateProblem();
{code}

When I run this in the console I get this:

{code}

Exception thrown
5-May-2010 1:49:44 PM org.codehaus.groovy.runtime.StackTraceUtils sanitize
WARNING: Sanitizing stacktrace:
java.lang.ArrayStoreException: java.lang.Integer
	at org.codehaus.groovy.runtime.dgmimpl.arrays.ObjectArrayPutAtMetaMethod$MyPojoMetaMethodSite.call(ObjectArrayPutAtMetaMethod.java:81)
	at ConsoleScript6.recreateProblem(ConsoleScript6:4)
	at ConsoleScript6$recreateProblem.callCurrent(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:44)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:143)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:147)
	at ConsoleScript6.run(ConsoleScript6:12)
	at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:264)
	at groovy.lang.GroovyShell.run(GroovyShell.java:513)
	at groovy.lang.GroovyShell.run(GroovyShell.java:170)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:857)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:151)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:276)
	at groovy.lang.Closure.call(Closure.java:271)
	at groovy.lang.Closure.run(Closure.java:354)
	at java.lang.Thread.run(Thread.java:619)

java.lang.ArrayStoreException: java.lang.Integer
	at ConsoleScript6.recreateProblem(ConsoleScript6:4)
	at ConsoleScript6$recreateProblem.callCurrent(Unknown Source)
	at ConsoleScript6.run(ConsoleScript6:12)
{code}



",roshandawrani,boriskob,Major,Closed,Fixed,05/May/10 12:54,23/Jun/10 15:45
Bug,GROOVY-4192,12815169,Class ReleaseInfo isn't thread-safe,Current implementation of class ReleaseInfo doesn't look like it's thread-safe.,roshandawrani,pniederw,Major,Closed,Fixed,06/May/10 08:15,14/Aug/13 03:57
Bug,GROOVY-4193,12815189,ImportNode.getClassName() throws NPE when type is null,"ImportNode has the following method:

{code}
    public String getClassName() {
    	return type.getName();
    }
{code}

When type is null, this throws an NPE.  I think the following is preferable (i.e., the equivalent of Groovy's safe dereferencing operator):


{code}
    public String getClassName() {
    	return type == null ? null : type.getName();
    }
{code}
",roshandawrani,werdna,Major,Closed,Fixed,06/May/10 21:28,23/Jun/10 15:45
Bug,GROOVY-4196,12811778,Source locations not correct for some return and block statements created in ReturnAdder class,"ReturnAdder.addReturnsIfNeeded() does not add source locations in the following places (in Groovy 1.7.2):

line 126:

{code}
            return new BlockStatement(list,block.getVariableScope());
{code}

Should be:
{code}
            BlockStatement newBlock = new BlockStatement(list,block.getVariableScope());
            newBlock.setSourcePosition(block);
            return newBlock;
{code}


line 135:

{code}
            return new BlockStatement(list,new VariableScope(scope));
{code}

Should be:
{code}
            BlockStatement newBlock = new BlockStatement(list,new VariableScope(scope));
            newBlock.setSourcePosition(statement);
            return newBlock;
{code}

Apologies for not sending a proper patch file, but I am working off the source jar, rather than the svn project.  If you prefer, I can send a proper patch.  Thanks.",guillaume,werdna,Major,Closed,Fixed,10/May/10 14:41,23/Jun/10 15:45
Bug,GROOVY-4201,12815191,@Immutable cannot handle List,"This doesn't compile:
{code}
@Immutable
class Person {
    String name
    List likes
}
{code}
If I declare the {{likes}} property as {{Collection}} instead, the compilation works. Huh?",paulk,pledbrook,Major,Closed,Fixed,12/May/10 12:06,23/Jun/10 15:45
Bug,GROOVY-4202,12815156,successive metaclass modification for class then instance fails.,"define a groovy class (an empty class declaration will do).

class TestClass0 {}

Muck with the metaclass using the class and an instance. the second closure assignment throws an exception (java.lang.ClassCastException: groovy.lang.MetaClassImpl cannot be cast to groovy.lang.GroovyObject at  org.codehaus.groovy.runtime.HandleMetaClass.replaceDelegate(HandleMetaClass.java:71)).

	void testSomething1() {
		def inst = new TestClass0();
		TestClass0.metaClass.addedMethod0 = { return 'hello0'}
		inst.metaClass.addedMethod1 = { return 'hello1'}
	}

note that simply changing the order of the closure assignments executes without throwing exceptions.
",roshandawrani,rovrevik,Major,Closed,Fixed,12/May/10 13:06,23/Jun/10 15:45
Bug,GROOVY-4206,12815145,Inconsistent handling of boolean properties,"Boolean getters with ""is"" prefix can be accessed with property syntax:

{code}
class Foo {
  boolean isValid() { true }
}
new Foo().valid
{code}

However, this doesn't work for static properties:

{code}
class Bar {
  static boolean isValid() { true }
}
Bar.valid // groovy.lang.MissingPropertyException: No such property: valid for class: Bar
{code}",paulk,pniederw,Major,Closed,Fixed,15/May/10 13:42,16/Jun/10 03:50
Bug,GROOVY-4215,12811780,exception trying to define method in closure,"{code}
{->
    def say(String msg) {
      println(msg)
    }
}()
{code}

throws:
{noformat}

1 compilation error:

Unknown type: METHOD_DEF at line: 2 column: 5. File: predef.groovy at Exception in thread ""AWT-EventQueue-0"" 
groovy.lang.MissingPropertyException: No such property: column for class: org.codehaus.groovy.antlr.ASTParserException
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:49)
	at org.codehaus.groovy.runtime.callsite.GetEffectivePojoPropertySite.getProperty(GetEffectivePojoPropertySite.java:63)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callGetProperty(AbstractCallSite.java:237)
	at groovy.ui.Console$_finishException_closure9.doCall(Console.groovy:636)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:276)
	at groovy.lang.Closure.call(Closure.java:289)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:1176)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:1152)
	at org.codehaus.groovy.runtime.dgm$111.invoke(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:270)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:52)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
	at groovy.ui.Console.finishException(Console.groovy:619)
	at sun.reflect.GeneratedMethodAccessor441.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:361)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:151)
	at groovy.ui.Console$_runScriptImpl_closure16_closure29.doCall(Console.groovy:868)
	at sun.reflect.GeneratedMethodAccessor655.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:151)
	at groovy.ui.Console$_runScriptImpl_closure16_closure29.doCall(Console.groovy)
	at sun.reflect.GeneratedMethodAccessor654.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:276)
	at groovy.lang.Closure.call(Closure.java:271)
	at groovy.lang.Closure.run(Closure.java:354)
	at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:209)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:597)
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:269)
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:184)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:174)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:169)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:161)
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:122)
{noformat}",paulk,ittayd,Major,Closed,Fixed,21/May/10 00:46,13/Apr/11 14:33
Bug,GROOVY-4218,12815183,Causes application to crash due to too many files being open,"I am executing several scripts within a web application. In a moment, the following exception occur:

groovy.util.ResourceException: Cannot open URL: file:/home/jboss-deploy/app/pharmania/scripts/criticas/pharmania/validaUsandoCpf.groovy
	at groovy.util.GroovyScriptEngine.getResourceConnection(GroovyScriptEngine.java:298)
	at groovy.util.GroovyScriptEngine.loadScriptByName(GroovyScriptEngine.java:407)
	at groovy.util.GroovyScriptEngine.createScript(GroovyScriptEngine.java:481)
	at groovy.util.GroovyScriptEngine.run(GroovyScriptEngine.java:468)
	at com.prana.critica.CriticaScript.execute(CriticaScript.java:167)
	at com.prana.critica.CriticaRegistro.valida(CriticaRegistro.java:51)
	at com.prana.critica.CriticaManager.valida(CriticaManager.java:156)
	at com.prana.critica.CriticaManager.getLotesCarga(CriticaManager.java:200)
	at com.prana.pharmania.session.CargaPontuacaoAction.analisa(CargaPontuacaoAction.java:217)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.jboss.seam.util.Reflections.invoke(Reflections.java:22)
	at org.jboss.seam.intercept.RootInvocationContext.proceed(RootInvocationContext.java:32)
	at org.jboss.seam.intercept.SeamInvocationContext.proceed(SeamInvocationContext.java:56)
	at org.jboss.seam.transaction.RollbackInterceptor.aroundInvoke(RollbackInterceptor.java:28)
	at org.jboss.seam.intercept.SeamInvocationContext.proceed(SeamInvocationContext.java:68)
	at org.jboss.seam.core.BijectionInterceptor.aroundInvoke(BijectionInterceptor.java:77)
	at org.jboss.seam.intercept.SeamInvocationContext.proceed(SeamInvocationContext.java:68)
	at org.jboss.seam.transaction.TransactionInterceptor$1.work(TransactionInterceptor.java:97)
	at org.jboss.seam.util.Work.workInTransaction(Work.java:47)
	at org.jboss.seam.transaction.TransactionInterceptor.aroundInvoke(TransactionInterceptor.java:91)
	at org.jboss.seam.intercept.SeamInvocationContext.proceed(SeamInvocationContext.java:68)
	at org.jboss.seam.core.MethodContextInterceptor.aroundInvoke(MethodContextInterceptor.java:44)
	at org.jboss.seam.intercept.SeamInvocationContext.proceed(SeamInvocationContext.java:68)
	at org.jboss.seam.intercept.RootInterceptor.invoke(RootInterceptor.java:107)
	at org.jboss.seam.intercept.JavaBeanInterceptor.interceptInvocation(JavaBeanInterceptor.java:185)
	at org.jboss.seam.intercept.JavaBeanInterceptor.invoke(JavaBeanInterceptor.java:103)
	at com.prana.pharmania.session.CargaPontuacaoAction_$$_javassist_seam_6.analisa(CargaPontuacaoAction_$$_javassist_seam_6.java)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.jboss.el.util.ReflectionUtil.invokeMethod(ReflectionUtil.java:335)
	at org.jboss.el.util.ReflectionUtil.invokeMethod(ReflectionUtil.java:348)
	at org.jboss.el.parser.AstPropertySuffix.invoke(AstPropertySuffix.java:58)
	at org.jboss.el.parser.AstValue.invoke(AstValue.java:96)
	at org.jboss.el.MethodExpressionImpl.invoke(MethodExpressionImpl.java:276)
	at com.sun.facelets.el.TagMethodExpression.invoke(TagMethodExpression.java:68)
	at javax.faces.component.MethodBindingMethodExpressionAdapter.invoke(MethodBindingMethodExpressionAdapter.java:88)
	at com.sun.faces.application.ActionListenerImpl.processAction(ActionListenerImpl.java:102)
	at javax.faces.component.UICommand.broadcast(UICommand.java:387)
	at org.ajax4jsf.component.AjaxViewRoot.processEvents(AjaxViewRoot.java:329)
	at org.ajax4jsf.component.AjaxViewRoot.broadcastEventsForPhase(AjaxViewRoot.java:304)
	at org.ajax4jsf.component.AjaxViewRoot.processPhase(AjaxViewRoot.java:261)
	at org.ajax4jsf.component.AjaxViewRoot.processApplication(AjaxViewRoot.java:474)
	at com.sun.faces.lifecycle.InvokeApplicationPhase.execute(InvokeApplicationPhase.java:82)
	at com.sun.faces.lifecycle.Phase.doPhase(Phase.java:100)
	at com.sun.faces.lifecycle.LifecycleImpl.execute(LifecycleImpl.java:118)
	at javax.faces.webapp.FacesServlet.service(FacesServlet.java:265)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
	at org.jboss.seam.servlet.SeamFilter$FilterChainImpl.doFilter(SeamFilter.java:83)
	at org.jboss.seam.web.IdentityFilter.doFilter(IdentityFilter.java:40)
	at org.jboss.seam.servlet.SeamFilter$FilterChainImpl.doFilter(SeamFilter.java:69)
	at org.jboss.seam.web.MultipartFilter.doFilter(MultipartFilter.java:86)
	at org.jboss.seam.servlet.SeamFilter$FilterChainImpl.doFilter(SeamFilter.java:69)
	at org.jboss.seam.web.ExceptionFilter.doFilter(ExceptionFilter.java:64)
	at org.jboss.seam.servlet.SeamFilter$FilterChainImpl.doFilter(SeamFilter.java:69)
	at org.jboss.seam.web.RedirectFilter.doFilter(RedirectFilter.java:45)
	at org.jboss.seam.servlet.SeamFilter$FilterChainImpl.doFilter(SeamFilter.java:69)
	at org.ajax4jsf.webapp.BaseXMLFilter.doXmlFilter(BaseXMLFilter.java:206)
	at org.ajax4jsf.webapp.BaseFilter.handleRequest(BaseFilter.java:290)
	at org.ajax4jsf.webapp.BaseFilter.processUploadsAndHandleRequest(BaseFilter.java:388)
	at org.ajax4jsf.webapp.BaseFilter.doFilter(BaseFilter.java:515)
	at org.jboss.seam.web.Ajax4jsfFilter.doFilter(Ajax4jsfFilter.java:56)
	at org.jboss.seam.servlet.SeamFilter$FilterChainImpl.doFilter(SeamFilter.java:69)
	at org.jboss.seam.web.LoggingFilter.doFilter(LoggingFilter.java:60)
	at org.jboss.seam.servlet.SeamFilter$FilterChainImpl.doFilter(SeamFilter.java:69)
	at org.jboss.seam.servlet.SeamFilter.doFilter(SeamFilter.java:158)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
	at org.jboss.web.tomcat.filters.ReplyHeaderFilter.doFilter(ReplyHeaderFilter.java:96)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:235)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
	at org.jboss.web.tomcat.security.SecurityAssociationValve.invoke(SecurityAssociationValve.java:190)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:433)
	at org.jboss.web.tomcat.security.JaccContextValve.invoke(JaccContextValve.java:92)
	at org.jboss.web.tomcat.security.SecurityContextEstablishmentValve.process(SecurityContextEstablishmentValve.java:126)
	at org.jboss.web.tomcat.security.SecurityContextEstablishmentValve.invoke(SecurityContextEstablishmentValve.java:70)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)
	at org.jboss.web.tomcat.service.jca.CachedConnectionValve.invoke(CachedConnectionValve.java:158)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:330)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:829)
	at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:601)
	at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)
	at java.lang.Thread.run(Thread.java:636)
Caused by: groovy.util.ResourceException: Cannot open URL: file:/home/jboss-deploy/app/pharmania/scripts/criticas/dbmax/validaUsandoCpf.groovy
	at groovy.util.GroovyScriptEngine.getResourceConnection(GroovyScriptEngine.java:296)
	... 91 more

And after, the server throw the following exception:

2010-05-21 16:09:41,476 ERROR [org.apache.catalina.core.ContainerBase.[jboss.web].[localhost].[/].[default]] (http-0.0.0.0-80-3) Servlet.service() for servlet default threw exception
java.io.FileNotFoundException: /var/lib/jboss-5.0.1.GA/server/default/deploy/ROOT.war/favicon.ico (Too many open files)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:137)
	at org.apache.naming.resources.FileDirContext$FileResource.streamContent(FileDirContext.java:921)
	at org.apache.catalina.servlets.DefaultServlet.copy(DefaultServlet.java:1703)
	at org.apache.catalina.servlets.DefaultServlet.serveResource(DefaultServlet.java:800)
	at org.apache.catalina.servlets.DefaultServlet.doGet(DefaultServlet.java:332)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:617)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
	at org.jboss.web.tomcat.filters.ReplyHeaderFilter.doFilter(ReplyHeaderFilter.java:96)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:235)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
	at org.jboss.web.tomcat.security.SecurityAssociationValve.invoke(SecurityAssociationValve.java:190)
	at org.jboss.web.tomcat.security.JaccContextValve.invoke(JaccContextValve.java:92)
	at org.jboss.web.tomcat.security.SecurityContextEstablishmentValve.process(SecurityContextEstablishmentValve.java:126)
	at org.jboss.web.tomcat.security.SecurityContextEstablishmentValve.invoke(SecurityContextEstablishmentValve.java:70)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)
	at org.jboss.web.tomcat.service.jca.CachedConnectionValve.invoke(CachedConnectionValve.java:158)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:330)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:829)
	at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:601)
	at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)
	at java.lang.Thread.run(Thread.java:636)

This problem is similar to the issue GROOVY-3570 reported to version 1.6.3. When I compare the code between the version 1.6.8, that works fine in the same environment, and the version 1.7.2 of the class GroovyScriptEngine, I realize that the patch applied to the version 1.6.8 is not present on 1.7.2.
The solution applied in the version 1.6.8 was close the input stream retrieved by URLConnection.
To complement this issue, I testing run my web application in Windows XP and it works fine there with both versions (1.6.8 and 1.7.2).
This can be fixed?",,gustavo.gomesc@gmail.com,Critical,Closed,Fixed,21/May/10 15:23,09/Nov/11 05:14
Bug,GROOVY-4219,12815188,Error during class generation for simple java/groovy program,"Reported against groovy eclipse but appears to be a groovy problem.  Similar stack to GROOVY-3613, but wasn't clear what was happening in that bug.

Zip is attached.  Compile with: 
groovyc -j de\brazzy\nikki\Texts.java de\brazzy\nikki\model\Image.groovy de\brazzy\nikki\model\ImageSortField.groovy

produces

{code}
>>> a serious error occurred: BUG! exception in phase 'class generation' in source unit 'de\brazzy\nikki\model\ImageSortField.groovy
' MapEntryExpression should not be visited here
>>> stacktrace:
BUG! exception in phase 'class generation' in source unit 'de\brazzy\nikki\model\ImageSortField.groovy' MapEntryExpression should no
t be visited here
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitMapEntryExpression(AsmClassGenerator.java:3123)
        at org.codehaus.groovy.ast.expr.MapEntryExpression.visit(MapEntryExpression.java:37)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitAndAutoboxBoolean(AsmClassGenerator.java:4078)
        at org.codehaus.groovy.classgen.AsmClassGenerator.makeCallSite(AsmClassGenerator.java:2148)
        at org.codehaus.groovy.classgen.AsmClassGenerator.makeCall(AsmClassGenerator.java:1982)
        at org.codehaus.groovy.classgen.AsmClassGenerator.makeCall(AsmClassGenerator.java:1968)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitStaticMethodCallExpression(AsmClassGenerator.java:2376)
        at org.codehaus.groovy.ast.expr.StaticMethodCallExpression.visit(StaticMethodCallExpression.java:43)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitAndAutoboxBoolean(AsmClassGenerator.java:4078)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitCastExpression(AsmClassGenerator.java:1891)
        at org.codehaus.groovy.classgen.AsmClassGenerator.assignmentCastAndVisit(AsmClassGenerator.java:4017)
        at org.codehaus.groovy.classgen.AsmClassGenerator.evaluateEqual(AsmClassGenerator.java:3969)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitBinaryExpression(AsmClassGenerator.java:1446)
        at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitAndAutoboxBoolean(AsmClassGenerator.java:4078)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:1427)
        at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
        at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:165)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:700)
        at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:51)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:587)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:562)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:657)
        at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1039)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:275)
        at org.codehaus.groovy.control.CompilationUnit$11.call(CompilationUnit.java:751)
        at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:958)
        at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:517)
        at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:495)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:472)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:456)
        at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:57)
        at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:170)
        at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:138)
        at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:152)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:108)
        at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:130)
{code}",blackdrag,aclement,Major,Closed,Fixed,21/May/10 16:58,14/Dec/10 10:03
Bug,GROOVY-4220,12815203,Groovy-all-jdk14-1.6.8 fails on Java 1.4 (java/util/concurrent/locks/AbstractQueuedSynchronizer),"groovy-all-jdk14-1.6.8.jar fails on Java 1.4.

The following Java code shows the problem:

{code}
GroovyClassLoader loader = new GroovyClassLoader();
loader.parseClass("""");
{code}

This produces the following stack trace:
{code}
java.lang.NoClassDefFoundError: java/util/concurrent/locks/AbstractQueuedSynchronizer
	at java.lang.ClassLoader.defineClass0(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:539)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:123)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:251)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:55)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:194)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:187)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:289)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:274)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:235)
	at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:302)
	at java.lang.ClassLoader.defineClass0(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:539)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:123)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:251)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:55)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:194)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:187)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:289)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:274)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:235)
	at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:302)
	at groovyjarjarbackport.java.util.concurrent.ConcurrentHashMap.<init>(ConcurrentHashMap.java:602)
	at groovyjarjarbackport.java.util.concurrent.ConcurrentHashMap.<init>(ConcurrentHashMap.java:653)
	at groovyjarjarretroruntime.impl.WeakIdentityTable.<init>(WeakIdentityTable.java:42)
	at groovyjarjarretroruntime.java.lang.Enum_$1.<init>(Enum_.java:49)
	at groovyjarjarretroruntime.java.lang.Enum_.<clinit>(Enum_.java:47)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.addPhaseOperations(ASTTransformationVisitor.java:160)
	at org.codehaus.groovy.control.CompilationUnit.<init>(CompilationUnit.java:180)
	at org.codehaus.groovy.control.CompilationUnit.<init>(CompilationUnit.java:117)
	at groovy.lang.GroovyClassLoader.createCompilationUnit(GroovyClassLoader.java:444)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:267)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:250)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:245)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:206)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:216)
{code}",paulk,sebb,Blocker,Closed,Fixed,22/May/10 07:53,07/Apr/15 18:54
Bug,GROOVY-4222,12811683,@Lazy has incorrect behavior for static fields,"The fragment:
{code}
@Lazy static Type myField
{code}
is transformed into a static private field {{$myField}} but an instance method {{getMyField()}}. The method should be static and in the case of synchronization, would need to synch on the class not '{{this}}'. But there is also a question as to whether double checked locking is best for static, perhaps holder class idiom is better.",paulk,paulk,Major,Closed,Fixed,24/May/10 08:08,07/Apr/15 19:07
Bug,GROOVY-4225,12815193,Wrong generation of import statements in stubs,"Single-type import declarations in Groovy files are collected/converted into type-on-demand import declarations by the stub compiler -- in other words,
{quote}
{{import my.package.Type1}}
{{import my.package.Type2}}
{quote}
becomes
{quote}
{{import my.package.*;}}
{quote}

This breaks the type shadowing mechanisms described in Section 7.5.1 of the Java Language Specification (3rd edition) and leads to Java compiler errors like ""cannot decide which of the classes to use"".

Moreover, current implementations is based on an unordered set, thus destroying the order of import declarations.

Attached patch solves these issues by preserving both the form and the order of original import declarations.",paulk,rnd,Major,Closed,Fixed,26/May/10 10:53,05/Apr/15 14:44
Bug,GROOVY-4227,12818050,Grails 1.3.1 groovysh render incorrect char encoding in prompt and messages,"Hi, the issue is related to appearance. In the output there are chars in a incorrect encoding i think:

&#8592;[32mGroovy Shell&#8592;[m (1.7.2, JVM: 1.6.0_18)
Type '&#8592;[1mhelp&#8592;[m' or '&#8592;[1m\h&#8592;[m' for help.
-------------------------------------------------------------------------------
&#8592;[1mgroovy:&#8592;[m000&#8592;[1m>&#8592;[m help

For information about &#8592;[32mGroovy&#8592;[m, visit:
    &#8592;[36mhttp://groovy.codehaus.org&#8592;[m

Available commands:
  &#8592;[1mhelp    &#8592;[m  (&#8592;[1m\h &#8592;[m) Display this help message
  &#8592;[1m?       &#8592;[m  (&#8592;[1m\? &#8592;[m) Alias to: &#8592;[1mhelp&#8592;[m
  &#8592;[1mexit    &#8592;[m  (&#8592;[1m\x &#8592;[m) Exit the shell
  &#8592;[1mquit    &#8592;[m  (&#8592;[1m\q &#8592;[m) Alias to: &#8592;[1mexit&#8592;[m
  &#8592;[1mimport  &#8592;[m  (&#8592;[1m\i &#8592;[m) Import a class into the namespace
  &#8592;[1mdisplay &#8592;[m  (&#8592;[1m\d &#8592;[m) Display the current buffer
  &#8592;[1mclear   &#8592;[m  (&#8592;[1m\c &#8592;[m) Clear the buffer
  &#8592;[1mshow    &#8592;[m  (&#8592;[1m\S &#8592;[m) Show variables, classes or imports
  &#8592;[1minspect &#8592;[m  (&#8592;[1m\n &#8592;[m) Inspect a variable or the last result with the G
UI object browser
  &#8592;[1mpurge   &#8592;[m  (&#8592;[1m\p &#8592;[m) Purge variables, classes, imports or preferences

  &#8592;[1medit    &#8592;[m  (&#8592;[1m\e &#8592;[m) Edit the current buffer
  &#8592;[1mload    &#8592;[m  (&#8592;[1m\l &#8592;[m) Load a file or URL into the buffer
  &#8592;[1m.       &#8592;[m  (&#8592;[1m\. &#8592;[m) Alias to: &#8592;[1mload&#8592;[m
  &#8592;[1msave    &#8592;[m  (&#8592;[1m\s &#8592;[m) Save the current buffer to a file
  &#8592;[1mrecord  &#8592;[m  (&#8592;[1m\r &#8592;[m) Record the current session to a file
  &#8592;[1mhistory &#8592;[m  (&#8592;[1m\H &#8592;[m) Display, manage and recall edit-line history
  &#8592;[1malias   &#8592;[m  (&#8592;[1m\a &#8592;[m) Create an alias
  &#8592;[1mset     &#8592;[m  (&#8592;[1m\= &#8592;[m) Set (or list) preferences
  &#8592;[1mregister&#8592;[m  (&#8592;[1m\rc&#8592;[m) Registers a new command with the shell

For help on a specific command type:
    help &#8592;[1mcommand&#8592;[m

-Some system properties could be util:

grails.version=1.3.1
sun.boot.library.path=C:\Program Files\Java\jdk1.6.0_18\jre\bin
java.vm.version=16.0-b13
grails.env=development
java.vm.name=Java HotSpot(TM) Client VM
file.encoding.pkg=sun.io
user.country=ES
sun.os.patch.level=Service Pack 2
java.runtime.version=1.6.0_18-b07
grails.env.set=true
user.variant=
os.name=Windows Vista
tools.jar=C:\Program Files\Java\jdk1.6.0_18\lib\tools.jar
sun.jnu.encoding=Cp1252
disable.grails.plugin.transform=false
os.version=6.0
user.timezone=Europe/Paris
file.encoding=UTF-8
java.specification.version=1.6
grails.env.default=true
java.vm.specification.version=1.0
sun.arch.data.model=32
user.language=es
java.specification.vendor=Sun Microsystems Inc.
awt.toolkit=sun.awt.windows.WToolkit
java.version=1.6.0_18
sun.io.unicode.encoding=UnicodeLittle
groovy.starter.conf=\Users\atreyu\dev\java\grails\grails-1.3.1\conf\groovy-start
er.conf
",roshandawrani,jneira,Major,Closed,Fixed,30/May/10 16:19,23/Jun/10 15:44
Bug,GROOVY-4231,12815226,DOMCategory.setValue is unable to handle newly created elements,"If a newly created element has no children then a NullPointerException is thrown.

IMO in these cases the method should create a new TextElement and add the contents to it.

How to reproduce:
{code}
import groovy.xml.dom.DOMCategory

use(DOMCategory) {
    def prop = doc.createElement('property')
    prop.setAttribute('name', 'cache.class')
    prop.setValue('true')
}
{code}",paulk,ragedog,Major,Closed,Fixed,01/Jun/10 06:06,02/Apr/12 04:43
Bug,GROOVY-4235,12815220,"Closure returned from static method cannot access class properties through ""this""","Given this code:

class Foo {
    static prop = ""sadfs""
    static foo() {
        return { -> println this.prop }
    }
}

Foo.foo().call()


A ClassCastException will be thrown even though printing out ""this"" inside of the closure prints out ""class Foo"".",roshandawrani,jcoleman,Critical,Closed,Fixed,02/Jun/10 13:44,16/Jun/10 03:50
Bug,GROOVY-4237,12815171,LocalVariableTable index attributes aren't consecutive,"The background: [media.io|http://media.io] is built on Groovy and Pico Web Remoting (PWR). I was observing in one case that PicoContainer would not pick up the URL parameters correctly, i.e. you would have
{code}
class Keks {
void blorb(String rofl) {
...
{code}
and calling {{http://bla.bla/Keks/blorb?rofl=copter}} would result in PicoContainer not finding the right method.
PicoContainer uses [Paranamer|http://paranamer.codehaus.org/] which in turn is based on ASM. I found that the generated class files have a LocalVariableTable with non-consecutive index values.
Paranamer expects consecutive index values in the LocalVariableTable and so is unable to find them for the method.
I'm attaching a screenshot of what this looks like in jclasslib.
I don't really know if this is legal as per the class file spec -- if it is, close this.
I'll try to get a patch done for Paranamer, too.",blackdrag,johann,Major,Closed,Fixed,04/Jun/10 14:42,14/Dec/10 08:37
Bug,GROOVY-4241,12815164,Problems with the 'as' operator,"The following code reproduce the error:

class Foo {}

Foo.metaClass.invokeMethod = { String name, args ->
    for (arg in args) {
        arg.getClass()
    }
}

def f = new Foo()

f.echo(f as Foo)",roshandawrani,poiati,Major,Closed,Fixed,05/Jun/10 19:03,16/Jun/10 03:50
Bug,GROOVY-4243,12818051,Incorrect super class for script containing an anonymous inner class,"When a script contains an anonymous inner class, then the compiler seems to ignore the {{scriptBaseClass}} property of {{CompilerConfiguration}}. Here's a test case which (hopefully) demonstrates this problem. The test case fails on the last assert statement.

{code}
class ScriptTest {
    @Test
    public void scriptWithAnonymousInnerClass() {
        def configuration = new CompilerConfiguration()
        configuration.scriptBaseClass = TestScript.name
        def classLoader = new GroovyClassLoader(getClass().classLoader, configuration)

        // This works
        def scriptClass = classLoader.parseClass('''
            def r = new TestRunnable()
            class TestRunnable implements Runnable {
                public void run() {}
            }
''')
        assert TestScript.isAssignableFrom(scriptClass)

        // This does not work
        scriptClass = classLoader.parseClass('''
            def r = new Runnable() {
                public void run() { }
            }
''')
        assert Script.isAssignableFrom(scriptClass)
        assert TestScript.isAssignableFrom(scriptClass) // <-- fails here    
    }
}

abstract class TestScript extends Script {

}
{code}",roshandawrani,adammurdoch,Major,Closed,Fixed,08/Jun/10 18:45,23/Jun/10 15:45
Bug,GROOVY-4244,12815172,Properties cannot be set through a @Delegate,"When accessing a non-final property through a @Delegate as below, a ReadOnlyPropertyException is thrown.

{code}
class DelegateTests extends GroovyTestCase {
    void testNonFinalPropertiesCanBeSet() {
        def foo = new Foo()
        foo.baz = ""baz""
        assert foo.baz == ""baz""
    }
}

class Foo {
    @Delegate Bar bar = new Bar()
}

class Bar {
    String baz
}
{code}

{noformat}
groovy.lang.ReadOnlyPropertyException: Cannot set readonly property: baz for class: test.Foo
	at groovy.lang.MetaClassImpl.setProperty(MetaClassImpl.java:2406)
	at groovy.lang.MetaClassImpl.setProperty(MetaClassImpl.java:3307)
	at test.Foo.setProperty(DelegateTests.groovy)
	at org.codehaus.groovy.runtime.InvokerHelper.setProperty(InvokerHelper.java:179)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.setProperty(ScriptBytecodeAdapter.java:483)
	at test.DelegateTests.testNonFinalPropertiesCanBeSet(DelegateTests.groovy:11){noformat}",roshandawrani,cdanielw,Major,Closed,Fixed,09/Jun/10 03:15,23/Jun/10 15:45
Bug,GROOVY-4245,12815215,GroovyScriptEngine breaks with UnsupportedClassVersionError using JDK 1.4,"Exception:

java.lang.UnsupportedClassVersionError: groovy/util/GroovyScriptEngine (Unsupported major.minor version 49.0)
        at java.lang.ClassLoader.defineClass0(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java(Compiled Code))
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java(Co
mpiled Code))
        at com.ibm.ws.classloader.CompoundClassLoader._defineClass(CompoundClass
Loader.java(Compiled Code))
        at com.ibm.ws.classloader.CompoundClassLoader.findClass(CompoundClassLoa
der.java(Compiled Code))
        at com.ibm.ws.classloader.CompoundClassLoader.loadClass(CompoundClassLoa
der.java(Compiled Code))
        at java.lang.ClassLoader.loadClass(ClassLoader.java(Compiled Code))
",roshandawrani,wrschneider99,Major,Closed,Fixed,09/Jun/10 10:17,23/Jun/10 15:51
Bug,GROOVY-4246,12815222,Issue with incrementing array when using Random number generation,"I have found an issue with array incrementation, when using randomly generated number as array index.
The idea was to randomly increment one of the indexes.
For array of size = 3 and 10 incrementations I can see array was modified like this [example from system.out]:
[0, 0, 1]
[0, 0, 1]
[2, 0, 1] should be [1,0,1]
[2, 3, 1] should be [1,1,1]
[3, 3, 1] should be [2,1,1]
[3, 2, 1] now I'm puzzled as the digit is decremented!
[3, 2, 1] what was incremented? no one knows! ;]
[3, 2, 2] 
[3, 2, 3]
[3, 3, 3]

See the code attached.

Additional info:
When the randomiser invocation is moved to assign value to a variable, and then variable is used as array index it works fine.",roshandawrani,krstn,Major,Closed,Fixed,11/Jun/10 05:24,22/Jul/10 17:32
Bug,GROOVY-4248,12811776,CLONE -JavaStubGenerator doesn't generate annotations available in Groovy code - More Tests required,"We made some progress on the original issue GROOVY-4118 and so we closed it for the 1.7.3 release. This issue captures remaining work, primarily some additional test cases.
{quote}
Looking at org.codehaus.groovy.tools.javac.JavaStubGenerator (http://goo.gl/ob23) I see it doesn't generate annotations that are originally available in Groovy sources.

I think it causes those issues later:
* http://jira.codehaus.org/browse/GMAVEN-68 - ""GMaven: generateStubs generates stubs without original Javadocs or annotations""
* http://jira.codehaus.org/browse/GMAVEN-4  - ""GMaven: Stub generation should generate annotations""

I'm trying to use AnnoMojo annotations (http://goo.gl/rbRw) when developing my MOJOs in Groovy.
GMaven's ""generateStubs"" goal doesn't produce Java sources with original AnnoMojo annotations
{quote}
",paulk,genie,Major,Closed,Fixed,14/Jun/10 13:13,08/Jul/14 15:28
Bug,GROOVY-4250,12815217,Power assert doesn't pretty-print empty String,"expected:
{code}
$ groovy -e 'assert new String() == ""xxx"";'
Caught: Assertion failed: 

assert new String() == ""xxx""
       |            |
       |            false
       """"
{code}

but actual:
{code}
$ groovy -e 'assert new String() == ""xxx"";'
Caught: Assertion failed: 

assert new String() == ""xxx""
       |            |
       |            false
       java.lang.String@45c3987
{code}",roshandawrani,nobeans,Major,Closed,Fixed,16/Jun/10 20:22,22/Jul/10 17:32
Bug,GROOVY-4252,12815201,Invalid code crashes the compiler,"This code:

{code}
class Crasher {
	public void m() {
		def fields = [1,2,3]
		def expectedFieldNames = [""patentnumber"", ""status""].
		for (int i=0; i<fields.size(); i++) {
			Object f = fields[i] 
			System.out.println(f); 
		}
	}
}
{code}

is not valid (I believe?).  There is an extraneous '.' on the end of the second 'def' line.  Instead of a nice error message about having done something wrong, we get this stack on compilation:

{code}
>>> a serious error occurred: BUG! exception in phase 'class generation' in source unit 'Crasher.groovy' tried to get a variable wit
h the name i as stack variable, but a variable with this name was not created
>>> stacktrace:
BUG! exception in phase 'class generation' in source unit 'Crasher.groovy' tried to get a variable with the name i as stack variable
, but a variable with this name was not created
        at org.codehaus.groovy.classgen.CompileStack.getVariable(CompileStack.java:263)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitClosureExpression(AsmClassGenerator.java:1759)
        at org.codehaus.groovy.ast.expr.ClosureExpression.visit(ClosureExpression.java:46)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitAndAutoboxBoolean(AsmClassGenerator.java:4071)
        at org.codehaus.groovy.classgen.AsmClassGenerator.makeCallSite(AsmClassGenerator.java:2147)
        at org.codehaus.groovy.classgen.AsmClassGenerator.makeCall(AsmClassGenerator.java:1984)
        at org.codehaus.groovy.classgen.AsmClassGenerator.makeCall(AsmClassGenerator.java:1970)
        at org.codehaus.groovy.classgen.AsmClassGenerator.makeInvokeMethodCall(AsmClassGenerator.java:1953)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:2300)
        at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:72)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitAndAutoboxBoolean(AsmClassGenerator.java:4071)
        at org.codehaus.groovy.classgen.AsmClassGenerator.assignmentCastAndVisit(AsmClassGenerator.java:4012)
        at org.codehaus.groovy.classgen.AsmClassGenerator.evaluateEqual(AsmClassGenerator.java:3962)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitDeclarationExpression(AsmClassGenerator.java:1442)
        at org.codehaus.groovy.ast.expr.DeclarationExpression.visit(DeclarationExpression.java:53)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitAndAutoboxBoolean(AsmClassGenerator.java:4071)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:1430)
        at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
{code}",roshandawrani,aclement,Minor,Closed,Fixed,17/Jun/10 19:01,22/Jul/10 17:32
Bug,GROOVY-4254,12815195,Compiler wrongly compiles a ClosureListExpression that results in NoSuchMethodError at runtime,"r19740 removed the following constructor from CurriedClosure class
{code}
public CurriedClosure(Closure uncurriedClosure, int i) {
    this(uncurriedClosure, new Object[]{Integer.valueOf(i)});
}
{code}

ACG still calls this non-existent constructor in the bytecode it generates for ClosureListExpression and the compiled code results in the following error at runtime
{noformat}
java.lang.NoSuchMethodError: org.codehaus.groovy.runtime.CurriedClosure.<init>(Lgroovy/lang/Closure;I)V
{noformat}

Here is the test to reproduce it:
{code}
import org.codehaus.groovy.runtime.CurriedClosure

class GroovyXXXXBug extends GroovyTestCase {
    void testClosureListExpressionUsage() {
        def clList = (1;2)
        assert clList[0] instanceof CurriedClosure
        assert clList[1] instanceof CurriedClosure
    }
}
{code}",roshandawrani,roshandawrani,Major,Closed,Fixed,18/Jun/10 04:37,22/Jul/10 17:33
Bug,GROOVY-4255,12815178,BUG! exception in phase 'class generation' ... SpreadExpression should not be visited here,"this code produces the error:
println [*it.albumId.text().split("","")]&#8203;

BUG! exception in phase 'class generation' in source unit 'Script1.groovy' SpreadExpression should not be visited here",paulk,livelock,Major,Closed,Fixed,19/Jun/10 16:57,02/May/17 02:03
Bug,GROOVY-4257,12811795,Collection.unique() method might remove not duplicate items,"list=[[3035, 26972], [2795, 34412]]
list.unique()
println list

Result: [[3035, 26972]]

list=[[3035, 26972], [2795, 34412]]
list.unique{it}
println list

Result: [[3035, 26972]]

Because the unique method use NumberAwareComparator<T> to compare elements in the collection, and NumberAwareComparator<T> will use object's hashCode to compare two object when the object doesn't have a compareTo method, then the two list [3035, 26972], [2795, 34412] are considered to be duplicate(because they have the same hashCode).

Will it be better to use equals method to decide whether two elements are duplicate? 
As the document say:
""A convenience method for making a collection unique using a Closure to determine duplicate (equal) items. If the closure takes a single parameter, the argument passed will be each element, and the closure should return a value used for comparison (either using {@link Comparable#compareTo(Object)} or {@link Object#equals(Object)}). If the closure takes two parameters, two items from the collection will be passed as arguments, and the closure should return an int value (with 0 indicating the items are not unique).""
",paulk,fangyixun,Major,Closed,Fixed,22/Jun/10 01:54,22/Jul/10 17:33
Bug,GROOVY-4258,12815246,wiki-snapshot.pdf is not searchable. Text can't be copied.,"The wiki-snapshot.pdf is only searchable on the first few pages. On most of the pages it is neither possible to search or to copy text. The latter produces unreadable and unusable text. E.g on page 5 marking and copying the text ""class A"" leads to ""-- "". Probably there is something wrong with embedded fonts?",guillaume,ladermann@hbg.dpa.de,Minor,Closed,Fixed,23/Jun/10 03:44,10/Jul/13 04:42
Bug,GROOVY-4259,12815240,Groovyc Ant task hard-codes,"The ant task for Groovy hard-codes the glob pattern ""*.groovy"" when searching for source files. This obviously doesn't work for those of us who use a different extension (like "".g""). I've attached a patch that adds a property called ""scriptExtension"" to the task, which parallels GMaven's defaultScriptExtension property. This makes the extension configurable. It defaults to "".groovy"" but can be changed if one uses something different.",roshandawrani,jonbrisbin,Minor,Closed,Fixed,23/Jun/10 16:50,05/Apr/15 14:44
Bug,GROOVY-4265,12815205,@Delegate's dealing with static methods seems incorrect,"@Delegate's dealing with static methods seems incorrect

In the code below, shouldn't the call from B#main() go to C#foo()?

If for a delegate method (which can only be an instance method), if the owner class has a static method, the delegate method should be the one used.

{code:title=A.groovy}
class A {
     static foo(){println ""A->foo()""}
}
{code}
{code:title=B.groovy}
class B extends A {
     @Delegate C c = new C()
     static main(args){
        new B().foo()
     }
}
{code}
{code:title=C.groovy}
class C {
     def foo(){println ""C->foo()""}
}
{code}

The above code outputs
{noformat}
A->foo()
{noformat}
but it should output
{noformat}
C->foo()
{noformat}
",paulk,roshandawrani,Major,Closed,Fixed,26/Jun/10 08:50,22/Jul/10 17:32
Bug,GROOVY-4266,12815231,CLONE -Wrong generation of import statements in stubs,"NOTE: Below is fixed, this issue is for tracking additional impots added at end of other imports, e.g. {{import java.io.*;}}

----

Single-type import declarations in Groovy files are collected/converted into type-on-demand import declarations by the stub compiler -- in other words,
{code}
import my.package.Type1
import my.package.Type2
{code}
becomes
{code}
import my.package.*;
{code}

This breaks the type shadowing mechanisms described in Section 7.5.1 of the Java Language Specification (3rd edition) and leads to Java compiler errors like ""cannot decide which of the classes to use"".

Moreover, current implementations is based on an unordered set, thus destroying the order of import declarations.

Attached patch solves these issues by preserving both the form and the order of original import declarations.",emilles,rnd,Major,Closed,Fixed,27/Jun/10 02:45,03/Feb/22 22:35
Bug,GROOVY-4267,12811798,Unable to import static for static inner classes,"This issue tracks partial implementation of this functionality. See note at end of description.

----

Given this Java class (note the _static_ inner class):
{code}
package pkg;

public class Outer {
    public static class Inner {}
}
{code}
The inner class can be printed from Java using this:
{code}
import pkg.Outer.Inner;
//import static pkg.Outer.Inner;

public class Main {
    public static void main(String[] args) {
        System.out.println(Inner.class.getName());
    }
}
{code}
Note that both the non-static or static version (shown commented out) will work.

For Groovy, the non-static version works:
{code}
import pkg.Outer.Inner

println Inner.class.name
{code}

but the static version doesn't:
{code}
import static pkg.Outer.Inner

println Inner.class.name
{code}
instead, it fails with:
{noformat}
Caught: groovy.lang.MissingPropertyException: No such property: Inner for class: TestStaticImport
{noformat}

NOTE: The initial implementation of functionality for this issue works for external jars/classes on the classpath and for classes included in the same file but doesn't work for the critical case of external groovy files. As that case also doesn't work for attempts to import using a non-static variation of import, I think it is worthy of its own issue and so GROOVY-4287 has been created to track remaining work.",paulk,paulk,Major,Closed,Fixed,28/Jun/10 06:58,07/Apr/15 19:07
Bug,GROOVY-4268,12815219,Breaking change in enum syntax,"The following code compiles in 1.7.2, but not in 1.7.3:

{code}
private enum EnumWithToString {
  VALUE
  String toString() { ""I'm a value"" }
}
{code}

groovyc output:

{noformat}
unexpected token: VALUE @ line 337, column 3.
     VALUE
     ^

1 error
{noformat}

The compile error goes away when adding a semicolon after ""VALUE"".
",paulk,pniederw,Major,Closed,Fixed,29/Jun/10 17:44,22/Jul/10 17:32
Bug,GROOVY-4269,12815211,metaClosure not called when using named parameters in Sql.rows,"Running the following code shows the problem:
{code}
import groovy.sql.*

def sql = Sql.newInstance([...])

def metaClosure = { meta ->
  meta.each { col ->
    println col.columnName
  }
}

println ""Named:""

sql.rows(""SELECT COUNT(*) AS Number FROM Table WHERE Id BETWEEN :start AND :end"", [start: 10000, end: 11000], metaClosure).each { res ->
  println res[0]
}

println ""Ordinal:""

sql.rows(""SELECT COUNT(*) AS Number FROM Table WHERE Id BETWEEN ? AND ?"", [10000, 11000], metaClosure).each { res ->
  println res[0]
}
{code}

Result on my computer:
{code}
Named:
994
Ordinal:
Number
994
{code}

Showing the metaClosure that prints the column name is not called when using named parameters.",paulk,aperry,Minor,Closed,Fixed,30/Jun/10 03:36,13/May/12 03:30
Bug,GROOVY-4272,12815242,Constant integer and double expressions in AstBuilder.buildFromCode { } cause NoSuchFieldErrors at runtime,"the following blocks of code cause a NoSuchFieldError at runtime in the transformed class due to failure to look up a constant:

new AstBuilder.buildFromCode{ return 1 }
new AstBuilder.buildFromCode{ return 1.0 }
new AstBuilder.buildFromString(""return 1"")

I've attached an example and the test that shows the failure.

The current workaround is to use the String constructor of their java.lang.Number equivalents:

new AstBuilder.buildFromCode{ return new Integer(""1"") }
new AstBuilder.buildFromCode{ return new Double(""1.0"") }

My suspicion is that the compiler is optimizing the inline constants away, but that the reference to those constants are in some intermediary generated class which is thrown away later instead of the intended target classes.
",roshandawrani,bcarr,Minor,Closed,Fixed,30/Jun/10 12:42,22/Jul/10 17:32
Bug,GROOVY-4285,12815229,XmlUtil.serialize doesn't seem to pick right method for GPathResult,"The following gives an error:

{code}
import groovy.xml.XmlUtil
import groovy.xml.StreamingMarkupBuilder

def xmlStr = """"""<?xml version=""1.0"" encoding=""UTF-8""?>
<stuff ver=""1.0"">
  <properties>
    <foo>bar</foo>
  </properties>
</stuff>""""""

def gpr = new XmlSlurper().parseText( xmlStr )
println XmlUtil.serialize( gpr )
{code}

Gives the following error:

{code}
[Fatal Error] :1:1: Content is not allowed in prolog.
ERROR:  'Content is not allowed in prolog.'
<?xml version=""1.0"" encoding=""UTF-8""?>
{code}

However if we change the println to:

{code}
println XmlUtil.serialize( (groovy.util.slurpersupport.GPathResult)gpr )
{code}

We get the correct output:

{code}
<?xml version=""1.0"" encoding=""UTF-8""?>
<stuff ver=""1.0"">
  <properties>
    <foo>bar</foo>
  </properties>
</stuff>
{code}

Looks like Groovy is picking the wrong method somewhere... (though I can't see how)",roshandawrani,tim_yates,Minor,Closed,Fixed,01/Jul/10 02:33,22/Jul/10 17:32
Bug,GROOVY-4286,12811856,GroovyScriptEngine doesn't recompile updateted script,"If you run the following java code and alter the executed script while running, it is unpredictable when GroovyScriptEngine recompiles the script. Sometimes it works, sometimes not.

{code:title=GroovyScriptEngineTest.java|borderStyle=solid}
public class GroovyScriptEngineTest {

	public static void main(String[] args) throws Exception {

		GroovyScriptEngine groovyScriptEngine = new GroovyScriptEngine(""/home/peddn/groovy"");
		
		groovyScriptEngine.getConfig().setMinimumRecompilationInterval(0);
		
		while (true) {

			Class<?> groovyClass = groovyScriptEngine.loadScriptByName(""TestPrint.groovy"");

			GroovyObject groovyObject = (GroovyObject) groovyClass.newInstance();

			Object[] groovyArgs = {};

			groovyObject.invokeMethod(""testPrint"", groovyArgs);

                        //check for recompile every half second
			Thread.sleep(500L);

		}

	}

}
{code}

{code:titel=TestPrint.groovy|borderStyle=solid}
class TestPrint {

	public void testPrint() {
		
		System.out.println(""hello World"");
		
	}
	
}
{code}

here some suggestions from a [discussion|http://groovy.329449.n5.nabble.com/GroovyScriptEngine-doesn-t-recompile-updateted-script-tp511841p511841.html] at the mailing list:


I fact I think its kind of bug in GroovyScriptEngine here is code 
snippet: 

{code}
    protected boolean isSourceNewer(ScriptCacheEntry entry) throws 
ResourceException  { 
        if (entry == null) return true; 
        long time = System.currentTimeMillis(); 

        ScriptCacheEntry newEntry = new 
ScriptCacheEntry(depEntry.scriptClass, time, depEntry.dependencies); 
        scriptCache.put(scriptName, newEntry); 
    } 
{code}
I think more right solution is using aomthing like this: 
{code}
    protected boolean isSourceNewer(ScriptCacheEntry entry) throws 
ResourceException  { 
        if (entry == null) return true; 
        long time = long time = <get entry file lastModified 
timestamp> 

        ScriptCacheEntry newEntry = new 
ScriptCacheEntry(depEntry.scriptClass, time, depEntry.dependencies); 
        scriptCache.put(scriptName, newEntry); 
    }
{code}
",guillaume,peddn,Major,Closed,Fixed,01/Jul/10 03:16,22/Jul/10 17:32
Bug,GROOVY-4288,12815239,GroovyScriptEngine can't refresh class from url other than file,"the class GroovyScriptEngine loaded from a url other than file will never be updated. I made a patch for it.

exam:
gse = new groovy.util.GroovyScriptEngine(new String[] { ""scripts"",
				""http://localhost/"" });
gse.getConfig().setMinimumRecompilationInterval(0);

the class from http://localhost/ will never refresh.

I found this when made a URL type for JDBC.",guillaume,wonder365,Major,Closed,Fixed,02/Jul/10 01:45,05/Apr/15 14:43
Bug,GROOVY-4292,12815224,ClassHelper.ClassHelperCache#classCache is not thread-safe,"When parsing Groovy scripts from multiple threads at a time, the call to WeakHashMap.get() may never return due to internal corruption due to the concurrent access. Since IntelliJ IDEA does its GDSL parsing in parallel threads, this blocks it completely.",blackdrag,gromopetr,Critical,Closed,Fixed,05/Jul/10 12:48,15/Jun/12 22:56
Bug,GROOVY-4293,12811855,BUG! exception in phase 'parsing' in source unit 'Script1.groovy' null,"I am encountering another parser ""bug""

does not work:
{code}
def params = new Expando()
params.newValue = 23
def trendMap = ['worsening': (-2), 'slightly worsening':(-1), 'neutral':0 ,'slightly improving': 1,'improving':2]

def obj = []
def el = [obj]
if (!el[1]) el[1] = ""1""

Eval.x(el[0],""x[1]=""+ el[2] ? trendMap[params.newValue.toInteger()] : params.newValue.toInteger() )
{code}
changing the last line to include brackets makes it work
{code}
Eval.x(el[0],""x[1]=""+ (el[2] ? trendMap[params.newValue.toInteger()] : params.newValue.toInteger()) )
{code}

Exception thrown

BUG! exception in phase 'parsing' in source unit 'Script1.groovy' null

	at groovy.util.Eval.me(Eval.java:50)

	at groovy.util.Eval.x(Eval.java:60)

	at groovy.util.Eval$x.call(Unknown Source)

	at ConsoleScript24.run(ConsoleScript24:9)

Caused by: java.lang.NullPointerException

	at java.io.StringReader.<init>(StringReader.java:33)

	... 4 more
Result: 23 
Exception thrown

BUG! exception in phase 'parsing' in source unit 'Script1.groovy' null

	at groovy.util.Eval.me(Eval.java:50)

	at groovy.util.Eval.x(Eval.java:60)

	at groovy.util.Eval$x.call(Unknown Source)

	at ConsoleScript26.run(ConsoleScript26:9)

Caused by: java.lang.NullPointerException

	at java.io.StringReader.<init>(StringReader.java:33)

	... 4 more

",roshandawrani,livelock,Minor,Closed,Fixed,06/Jul/10 07:43,22/Jul/10 17:32
Bug,GROOVY-4295,12815254,Joint compilation fails for interfaces with primitive fields,"Try to joint-compile the following Groovy interface:

{code}
interface Bar {
  int bar = 42
}
{code}

Output:
{noformat}
Compile error during compilation with javac.
/var/folders/ak/akL02VQpE-qytx-wR1FH3++++TI/-Tmp-/groovy-generated-7418703305781250498-java-source/Bar.java:10: incompatible types
found   : <nulltype>
required: int
int bar = null;
          ^
1 error
{noformat}",,pniederw,Major,Closed,Fixed,07/Jul/10 07:01,22/Jul/10 17:32
Bug,GROOVY-4299,12815261,Bug on compiler when using ==~ operator,"A correct expression can't be evaluated by compiler when it ends with \\ (when using ==~ operator),
forcing the user of Pattern.compile()
The code below should run normally and matches strings that starts whith an \ and ends with an \


package h.gr.pat

import java.util.regex.Pattern;
import java.io.BufferedReader;
import groovy.ui.Console;


class TestClass {
	static main(args) {
		println(""start"");
		String str = new BufferedReader(new InputStreamReader(System.in)).readLine();		
		if( str ==~ /\\.*\\/ )
			println(""ok"");
		else
			println(""oops"");
	}

}
",,hoodwgs,Major,Closed,Fixed,09/Jul/10 19:12,09/Jul/10 20:56
Bug,GROOVY-4304,12818054,"OptimizerVisitor may run twice, corrupting constants","The issue came up when Paul King showed me a Spock spec behaving in strange ways:

TestSpock.groovy:
{code}
@Grab('org.spockframework:spock-core:0.4-groovy-1.7')
class TestSpock extends spock.lang.Specification {
  def convert = new Converter()

  def 'important scenarios'() {
    expect:
    c == convert.toCelsius(f)

    where:
    c   | f   | scenario
    0   | 32  | 'Freezing'
    20  | 68  | 'Garden party conditions'
    35  | 95  | 'Beach conditions'
    100 | 212 | 'Boiling'
  }
}
{code}

Converter.groovy:
{code}
class Converter {
  def toCelsius (fahrenheit) { (fahrenheit - 32) * 5 / 9 }
}
{code}

If you run this with 'groovy TestSpock.groovy', you will notice that the values for 'c' and 'f' get mixed up. After debugging groovyc, I came to the following conclusion:

Compilation of SpockTest advances until end of phase ""semantic analysis"", then CompilationUnit.dequeued() sets the phase back to ""initialization"". On the second pass, SourceUnitOperation's that have already run are skipped due to this check in CompilationUnit.applyToSourceUnits():

{code}
if ((source.phase < phase) || (source.phase == phase &&
!source.phaseComplete)) ...
{code}

However, Compilation.applyToPrimaryClassNodes() has a slightly different check:

{code}
if (context == null || context.phase <= phase) ...
{code}

source.phaseComplete is not checked here, hence OptimizerVisitor (among others) runs a second time. This leads to wrong results, at least in the case where an AST transform has added additional constants. The TODO in OptimizerVisitor.setConstField() is on the spot.

Adding the source.phaseComplete check to Compilation.applyToPrimaryClassNodes() seemed to solve the problem for Paul. I haven't investigated further if this is the correct approach.",,pniederw,Critical,Closed,Fixed,14/Jul/10 10:15,22/Jul/10 17:32
Bug,GROOVY-4306,12815117,Groovy Console Icon for Mac OS X broken,The icns file includes in groovy-1.7.3 seems to be broken. Find mine enclosed,guillaume,stefanscheidt,Trivial,Closed,Fixed,15/Jul/10 08:39,21/Jul/11 19:06
Bug,GROOVY-4307,12815253,Grapes doesn't handle properties correctly used in the organisation field of a POM.,"It would appear that properties can't be used in the organisation field (potentially others) of a POM, which is certainly valid for Maven projects.  I'm not sure how many other things relating to parent projects (as in my example).

Here's the top lines of the stack trace I get from depending on a project that in turn depends on Guava:
General error during conversion: Error grabbing Grapes -- [unresolved dependency: com.google.guava#guava-collections;r03: java.text.ParseException: inconsistent module descriptor file found in 'file:/home/sean/.m2/repository/com/google/guava/guava-collections/r03/guava-collections-r03.pom': bad organisation: expected='com.google.guava' found='${parent.groupId}'; ]

java.lang.RuntimeException: Error grabbing Grapes -- [unresolved dependency: com.google.guava#guava-collections;r03: java.text.ParseException: inconsistent module descriptor file found in 'file:/home/sean/.m2/repository/com/google/guava/guava-collections/r03/guava-collections-r03.pom': bad organisation: expected='com.google.guava' found='${parent.groupId}'; ]
",paulk,seanparsons,Major,Closed,Fixed,18/Jul/10 15:55,14/Sep/10 07:39
Bug,GROOVY-4309,12818055,"printf not bound to ""out"" in groovlet context as in print/println","In groovlet print and println are bound to the response.Writer() or out variable so lone print ... and println ... calls are written th the response output Writer as is calls to out.

However, printf calls in goovlet context is still bound to System.out so calls to printf write to System.out not out.

printf method getting invoked is following:
{code}
package org.codehaus.groovy.runtime.DefaultGroovyMethods

    public static void printf(Object self, String format, Object arg) {
        if (self instanceof PrintStream) {
            printf((PrintStream) self, format, arg);
        else
            printf(System.out, format, arg); **** this branch is getting invoked
    }
{code}
Object self is Groovlet class itself which is not instance of PrintStream so System.out is getting used.

Work-around is not using ""printf"" short-cut but explicitly call print/println and String.format() instead.

Test output:
{code}
groovy script output:
1. println test
2. this is a test
3. this is a test

groovlet response:
1. println test
3. this is a test
4. test out.println
{code}

Groovlet test.gsp
{code}
try {
 response.contentType = 'text/plain'
 groovylet = true
} catch (RuntimeException rte) {
 groovylet = false
}

// output to groovlet response
println ""1. println test""

//************************************************************************
// in groovyet context this gets output to System.out not out which is bound to response.getWriter()
printf ""2. this is a %s%n"", 'test' // this is the problem case in groovlet mode
//************************************************************************

print String.format(""3. this is a %s%n"", 'test')

if (groovylet) {
  out.println ""4. test out.println""
}
{code}",guillaume,gjmathews1,Major,Closed,Fixed,19/Jul/10 12:11,22/Jul/10 17:32
Bug,GROOVY-4312,12815119,Empty primitive arrays evaluate to true,"In contrast to other empty collections, empty primitive arrays evaluate to true:

{code}
def x = [] as int[]
assert !x // fails!
{code}

Empty non-primitive arrays behave as expected:

{code}
def x = [] as String[]
assert !x // passes
{code}",guillaume,pniederw,Critical,Closed,Fixed,20/Jul/10 11:33,22/Jul/10 17:32
Bug,GROOVY-4316,12815256,Add test/coverage (for CLONE of Joint compilation fails for interfaces with primitive fields),"Try to joint-compile the following Groovy interface:

{code}
interface Bar {
  int bar = 42
}
{code}

Output:
{noformat}
Compile error during compilation with javac.
/var/folders/ak/akL02VQpE-qytx-wR1FH3++++TI/-Tmp-/groovy-generated-7418703305781250498-java-source/Bar.java:10: incompatible types
found   : <nulltype>
required: int
int bar = null;
          ^
1 error
{noformat}",guillaume,pniederw,Major,Closed,Fixed,22/Jul/10 07:37,15/Dec/10 22:19
Bug,GROOVY-4320,12811716,Joint compiler + @Delegate fail to recognize method signature with default value,"Spotted this problem when upgrading Griffon to Groovy 1.7.4.

Start with a base contract defined in java

{code}
import java.util.List;
public interface IApp {
    void event(String name);
    void event(String name, List params);
}
{code}

Implement said contract in Groovy
{code}
class BaseApp implements IApp {
    private IApp appDelegate
    BaseApp(IApp app) {
        this.appDelegate = app
    }
    void event(String name, List params = []) {
        // empty
    }
}
{code}

Now create a Groovy class where the previous will be used as delegate

{code}
class RealApp {
    @Delegate BaseApp _base
    RealApp() {
        _base = new BaseApp(this)
    }
}
{code}

Compile the code

{code}
groovyc IApp.java BaseApp.groovy RealApp.groovy
{code}

With Groovy 1.7.3 you get the following method signatures

{code}
$ javap BaseApp | grep event
    public void event(java.lang.String, java.util.List);
    public void event(java.lang.String);
$ javap RealApp | grep event
    public void event(java.lang.String);
    public void event(java.lang.String, java.util.List);
{code}

However 1.7.4 fails to recognize that the second argument to event() has a default value

{code}
$ groovy -version
Groovy Version: 1.7.4 JVM: 1.6.0_17
$ groovyc IApp.java BaseApp.groovy RealApp.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
RealApp.groovy: 1: Can't have an abstract method in a non-abstract class. The class 'RealApp' must be declared abstract or the method 'void event(java.lang.String)' must be implemented.
 @ line 1, column 1.
   class RealApp {
   ^

1 error
{code}",emilles,aalmiray,Major,Closed,Fixed,23/Jul/10 09:09,22/Feb/22 03:12
Bug,GROOVY-4325,12815106,Compiler does not mind multiple default cases for a switch statement,"The following code compiles, when it shouldn't. Compiler does not mind any number of default cases.

{code}
def foo() {
    def x = 1
    switch(x) {
       case 1: println '1'
       default: println 'default 1'
       default: println 'default 2'
       default: println 'default 3'
       default: println 'default 4'
       }
}

foo()        
{code}",roshandawrani,roshandawrani,Major,Closed,Fixed,25/Jul/10 10:49,15/Dec/10 22:19
Bug,GROOVY-4329,12815271,Stub generator exhibits a synthetic method in the stubs it should not,"The stub generator generates the following method:
{code}
MetaClass $getStaticMetaClass()
{code}
which is an internal method not intended to be called directly, so should not be present in the stubs.",guillaume,guillaume,Minor,Closed,Fixed,26/Jul/10 09:23,15/Dec/10 22:19
Bug,GROOVY-4330,12818056,Groovy 1.7.4 wont build from source,"I have just downloaded the Groovy 1.7.4 source and tried to compile on Windows 7 with Sun/Oracle JDK 1.6.0_21.  The build failed while performing the -testAll phase:

    [junit] Running UberTestCaseBugs
    [junit] Tests run: 400, Failures: 0, Errors: 0, Time elapsed: 4.977 sec
    [junit] Running UberTestCaseGroovySourceCodehausPackages
    [junit] Tests run: 429, Failures: 0, Errors: 0, Time elapsed: 54.605 sec
    [junit] Running UberTestCaseGroovySourceCodehausPackages_VM6
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.074 sec
    [junit] Running UberTestCaseGroovySourceRootPackage
    [junit] Tests run: 1000, Failures: 0, Errors: 0, Time elapsed: 27.002 sec
    [junit] Running UberTestCaseGroovySourceSubPackages
    [junit] Tests run: 1187, Failures: 0, Errors: 1, Time elapsed: 66.116 sec
    [junit] Test UberTestCaseGroovySourceSubPackages FAILED

From groovy-1.7.4/target/reports/junit/all-tests.html

Class: UberTestCaseGroovySourceSubPackages
Name: testReloadingInterval
Status: Error
Type:

assert gse.run(scriptName, binding) == 3 | | | | | | 23 | | false | | groovy.lang.Binding@1769afe | gse6921152879800224663.groovy groovy.util.GroovyScriptEngine@1b9caa8

Assertion failed:

assert gse.run(scriptName, binding) == 3
| | | | |
| 23 | | false
| | groovy.lang.Binding@1769afe
| gse6921152879800224663.groovy
groovy.util.GroovyScriptEngine@1b9caa8

at org.codehaus.groovy.runtime.InvokerHelper.assertFailed(InvokerHelper.java:378)
at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.assertFailed(ScriptBytecodeAdapter.java:662)
at groovy.util.GroovyScriptEngineTest.testReloadingInterval(GroovyScriptEngineTest.groovy:246)


I had a quick look at groovy.util.GroovyScriptEngineTest.testReloadingInterval(GroovyScriptEngineTest.groovy:246).  Looks as if the test that is failing was introduced to test previous issues (GROOVY-2811 and GROOVY-4286) - maybe still an issue?

Kind regards,
Mike

",guillaume,mikevines,Major,Closed,Fixed,26/Jul/10 11:27,15/Dec/10 22:19
Bug,GROOVY-4336,12815290,EMC DSL is broken for some methods on Closure,"The Groovy EMC DSL breaks for method names within the Closure class:
{code}
def o = new Object()
o.metaClass {
  foo { 'foo' }
  curry { 'curry' }
  rcurry { 'rcurry' }
}
println o.foo()
//println o.curry()   // MissingMethodException
//println o.rcurry()  // MissingMethodException
{code}
",paulk,paulk,Minor,Closed,Fixed,28/Jul/10 05:59,07/Apr/15 19:06
Bug,GROOVY-4337,12815244,Generated createCallSiteArray is too big for JVM,"We are using groovy as a DSL engine and we have a script that is 4000 lines long.  We've split it up into several methods, but we still receive an error about 'Invalid method Code length 66090 in class file script1280242156093619159538'.  I analyzed the generated code and the issue is that the groovy generated method $createCallSiteArray is too big.  This is happening because we have over 8000 entries in our call site array.  I've implemented a fix where the initialization of the String array for the callsite array is split up over several methods if it would generate code that is too big. (Patch attached)  In our preliminary testing, this has resolved our issue.",guillaume,tschneider,Critical,Closed,Fixed,28/Jul/10 11:41,05/Apr/15 14:44
Bug,GROOVY-4340,12818396,Curry method fails when using it in a chain with IllegalArgumentException,"Curry method did not work like stated on http://groovy.codehaus.org/JN2515-Closures. The feature described there to chain curry on a closure to generate more specific ones did not work.

I tried to use these code example:

{code}
def c = { arg, Object[] extras -> arg + ', ' + extras.join(', ') }
def d = c.curry( 1 ) //curry first param only
assert d( 2, 3, 4 ) == '1, 2, 3, 4'
def e = c.curry( 1, 3 ) //curry part of Object[] also
assert e( 5 ) == '1, 3, 5'
def f = e.curry( 5, 7, 9, 11 ) //currying continues on Object
assert f( 13, 15 ) == '1, 3, 5, 7, 9, 11, 13, 15'
{code}

The following code example shows what works and what did not:

{code}
def a = { arg, Object[] extras -> arg + ', ' + extras.join(', ') }
//def b = a.curry( 1,2,3 ) //did not work
def c = a.curry( 1,[ 2,3 ] ) //works
def d = a.curry( 1,2 ) //works
//def e = d.curry( 3 ) //did not work
//def e = d.curry( [3] ) //did not work
{code}

The exception thrown is:

{code}
java.lang.IllegalArgumentException: To curry 4 argument(s) expect index range 0..-4 but found 0
	at org.codehaus.groovy.runtime.CurriedClosure.<init>(CurriedClosure.java:60)
	at org.codehaus.groovy.runtime.CurriedClosure.<init>(CurriedClosure.java:66)
	at groovy.lang.Closure.curry(Closure.java:370)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSite.invoke(PogoMetaMethodSite.java:225)
	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.call(PogoMetaMethodSite.java:63)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:137)
	at ConsoleScript0.run(ConsoleScript0:6)
	at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:264)
	at groovy.lang.GroovyShell.run(GroovyShell.java:513)
	at groovy.lang.GroovyShell.run(GroovyShell.java:170)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:133)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:865)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:44)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:143)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:151)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:276)
	at groovy.lang.Closure.call(Closure.java:271)
	at groovy.lang.Closure.run(Closure.java:354)
	at java.lang.Thread.run(Thread.java:619)
{code}",paulk,mszalbach,Major,Closed,Fixed,30/Jul/10 05:41,15/Dec/10 22:19
Bug,GROOVY-4347,12815273,"DGM#consumeProcessOutput(Process, StringBuffer, StringBuffer) accepts only StringBuffers",This is too restrictive. Appendable would be better.,paulk,veita,Minor,Closed,Fixed,31/Jul/10 08:02,15/Dec/10 22:19
Bug,GROOVY-4348,12815270,Cannot get a closure's resolve strategy with property syntax,"Closure.getProperty() gives no special treatment to ""resolveStrategy"", hence ""closure.resolveStrategy"" is simply forwarded to delegate/owner.",pniederw,pniederw,Major,Closed,Fixed,02/Aug/10 10:08,15/Dec/10 22:19
Bug,GROOVY-4356,12815291,Static members should not be able to access class-level generic types,"The following code compiles, but it shouldn't because T is a non-static generic type, and difference instances of Outer will have different concrete representations of T. Hence it makes no sense that static inner classes should be able to use such generic type variables.
{code}
class Outer<T> {
    T ofoo
    static class Inner {
        T ifoo
    }
}
{code}

Similarly, all the cases below should not compile as well:
{code}
class Test1<T> {
    static T f1
}

class Test2<T> {
    static foo(T param1) {}
}

class Test3<T> {
    static foo() {
        T localVar1
    }
}
{code}",paulk,roshandawrani,Major,Closed,Fixed,08/Aug/10 13:45,02/May/17 02:03
Bug,GROOVY-4359,12815576,Generic types are incorrectly shared/resolved within the module,"The following code should not compile, as class A is defined as a generic class and not class B, but type variables' resolution is spilling over from A to other non-related classes in the same module.
{code:title=Test.groovy}
class A<T>{}

class B {
    T f
}
{code}


However, the following should continue to work
{code}
class A<T>{
    class B {
        T f
    }
}
{code}",blackdrag,roshandawrani,Major,Closed,Fixed,10/Aug/10 13:27,17/Dec/14 13:25
Bug,GROOVY-4362,12815293,Node.text() returns empty string for non-String or Collection values,"The text() method of groovy.util.Node is coded to return an empty string ("""") if you set a value that is not a string or a collection. This means if you set the value as a number you get back an empty string.

Should this method return value.toString() in all cases where value is not a collection?

For instance -
{code}
def xml = ""<root><node1/></root>""
def root = new XmlParser().parseText(xml)
root.node1[0].replaceNode() {
    node2() {
       childN(5)
       childS('5')
    }
}

assert root.node2[0].childS[0].text() == '5'
assert root.node2[0].childN[0].text() == '5'
{code}",paulk,jspeakman,Minor,Closed,Fixed,12/Aug/10 07:15,01/Apr/12 21:41
Bug,GROOVY-4363,12818059,"@Immutable class, failure when trying to define private static members","Attempt to add 'private static' field to a class that has been
annotated as @Immutable causes an error.

Groovy Shell (1.7.4, JVM: 1.6.0_20)
Type 'help' or '\h' for help.
------------------------------------------------------------------------------------------------------------------
groovy:000> @Immutable class AClass {
groovy:001>    static final String FOO = 'bar'
groovy:002> }
===> true
groovy:000> @Immutable class BClass {
groovy:001>    private static final String FOO = 'bar'
groovy:002> }
ERROR org.codehaus.groovy.control.MultipleCompilationErrorsException:
startup failed:
groovysh_evaluate: -1: cannot modify static final field 'FOO' outside
of static initialization block.
 @ line -1, column -1.
groovysh_evaluate: -1: cannot modify static final field 'FOO' outside
of static initialization block.
 @ line -1, column -1.
2 errors
...

In Groovy 1.7.3 it does not cause an error.
Groovy Shell (1.7.3, JVM: 1.6.0_20)
Type 'help' or '\h' for help.
------------------------------------------------------------------------------------------------------------------
groovy:000> @Immutable class AClass  {
groovy:001>    static final String FOO = 'bar'
groovy:002> }
===> true
groovy:000> @Immutable class BClass {
groovy:001>    private static final String FOO = 'bar'
groovy:002> }
===> true
groovy:000>
...",paulk,jlamsa,Major,Closed,Fixed,12/Aug/10 11:06,02/Feb/23 22:46
Bug,GROOVY-4364,12815277,DGM.eachLine() returns wrong result,"DGM.eachLine() should return the last value returned by the closure, but actually returns the last line. For trunk the code change has already been made as part of GROOVY-4361, but we also need a fix for the 1.7 branch.",pniederw,pniederw,Major,Closed,Fixed,12/Aug/10 13:08,15/Dec/10 22:19
Bug,GROOVY-4365,12815276,Compiler does not do access check at classgen time and invalid class fails with IllegalAccessError later,"{code}
class MyHashMap extends HashMap {
    static class MyEntry extends HashMap.Entry { }
}

println MyHashMap.MyEntry
{code}

Code above fails with
{noformat}
Caught: java.lang.IllegalAccessError: class MyHashMap$MyEntry cannot access its superclass java.util.HashMap$Entry
{noformat}",emilles,roshandawrani,Major,Closed,Fixed,13/Aug/10 05:05,08/Mar/22 12:16
Bug,GROOVY-4366,12815289,Compilation error in using inheritance and generics together,"The following code does not compile on 1.7.5, but compiles successfully on 1.8-beta-2.

{code}
class Factory {
    abstract static class Machine<E> {
        abstract void make(E what)
    }
}

class PizzaFactory {
    static class PizzaMachine<E extends SimplePizza> extends Factory.Machine<E> {
        void make(E what) {}
    }
}

class SimplePizza {}
{code}

On 1.7.5, it fails with the following error
{noformat}
...\PizzaMaking.groovy: 8: Can't have an abstract method in a non-abstract class. The class 'PizzaFactory$PizzaMachine' must be declared abstract or the method 'void make(java.lang.Object)' must be implemented.
 @ line 8, column 5.
       static class PizzaMachine<E extends SimplePizza> extends Factory.Machine<E> {
       ^
{noformat}
",roshandawrani,roshandawrani,Major,Closed,Fixed,13/Aug/10 05:42,15/Dec/10 22:31
Bug,GROOVY-4367,12815202,Implicit this fails to get passed correctly from one non-static inner class to another,"{code}
class A {
    void foo() {
        new B().r()
    }

    class B {
        def r() {
            new C("""")
        }
    }

    class C {
        C(s) {}
    }
}

new A().foo()
{code}

The code above fails with the following error
{noformat}
Caught: groovy.lang.GroovyRuntimeException: Could not find matching constructor for: A$C(java.lang.String)
	at A$B.r(TryGroovy.groovy:8)
	at A.foo(TryGroovy.groovy:3)
	at TryGroovy.run(TryGroovy.groovy:17)
{noformat}",roshandawrani,roshandawrani,Major,Closed,Fixed,13/Aug/10 06:45,15/Dec/10 22:31
Bug,GROOVY-4373,12815297,"Groovlet generated error ""The requested resource () is not available.""","Groovy Script is in 
WEB-INF/groovy/start.groovy


URL: http://localhost:8080/mframev200
First call run fine

If I call the script again the following error is displayed:

The requested resource () is not available.

On the console:

GroovyServlet Error:  script: '/start.groovy':  Script not found, sending 404.

",blackdrag,wnagy,Major,Closed,Fixed,19/Aug/10 09:18,22/Dec/12 01:10
Bug,GROOVY-4374,12811679,Java stubs should never need to explicitly include alias imports,"Given that an import with an alias always has the fully qualified path we should always be able to substitute the fqn and hence don't need an explicit import. Java code has no visibility to the alias.

See: http://groovy.markmail.org/thread/z2prm3z33a5olb2y
",,paulk,Major,Closed,Fixed,20/Aug/10 06:58,08/Mar/22 12:16
Bug,GROOVY-4376,12815209,deprecated @groovy.lang.Immutable feature broken on current trunk (1.8-beta-2),"The following code is not supposed to go through, as it defeats the whole purpose of @Immutable. It correctly fails with ReadOnlyPropertyException on 1.7.5, but on trunk it works.
{code}
@Immutable final class Person {
     boolean married
}
  
person = new Person(married:false)

/* should not be allowed as it is mutating the object after it is formed */
person.married = true 
{code}",paulk,roshandawrani,Critical,Closed,Fixed,21/Aug/10 15:44,15/Dec/10 22:19
Bug,GROOVY-4378,12815315,Incorrect handling of unicode escape sequences when calculating source locations,"When compiling this code with Groovy 1.7.3:


{code}
class One {
	 /*\u00E9*/ Stack<F> plates;
}
{code}

I get the following error message:


{code}
$ groovy One.groovy
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
/private/tmp/One.groovy: 2: unable to resolve class F 
 @ line 2, column 15.
   	 /*\u00E9*/ Stack<F> plates;
                 ^

1 error
{code}
I would expect the ^ to be under the 'F'.  It appears that the unicode sequence is being considered a single character, rather than 6 characters.
",blackdrag,werdna,Major,Closed,Fixed,23/Aug/10 15:59,03/Jun/17 15:43
Bug,GROOVY-4380,12811966,Round-tripping XML which contains a newline character reference in an attribute's value removes the character reference,"Given an XML file which contains a newline character reference (&#10;) in the value of an attribute, the following groovy code incorrectly outputs the actual newline character instead of the original character reference. This is a significant problem because the XML standard dictates that newlines found in the value of an attribute must be treated as a space. Thus, if any XML parser reads the output of this groovy snippet, it is as if the newline is gone.

newline.xml:
<option name=""LINE_SEPARATOR"" value=""&#10;""/>

groovy:

def fileWriter = new FileWriter('./newline-roundtrip.xml')
def input = new XmlParser().parse('./newline.xml')
PrintWriter printWriter = new PrintWriter(fileWriter)
new XmlNodePrinter(printWriter).print(input)
printWriter.flush()

I've attached a junit test which demonstrates the problem.",paulk,aogail,Critical,Closed,Fixed,25/Aug/10 19:43,05/Apr/15 14:43
Bug,GROOVY-4385,12812171,Response already committed on Google App Engine when serving blobs,"When using the BlobstoreService from Google App Engine to serve blob content from a Groovlet, whether through direct streaming or through getting an input stream / reader over the blob content, the GAE server would complain that the response is already committed.
What ""commits"" the response is the calls by GroovyServlet to response.setStatusCode() and response.flushBuffer().
The former sets the default 200 status code, which is usually set by default at 200 anyway by servlet containers.
The latter is also done by servlet containers when the servlet terminates.
At least, this is what I gathered from glancing through Tomcat and Jetty sources.
So the suggested fix is to remove the call to these two methods which ""commit"" the response too early.",guillaume,guillaume,Minor,Closed,Fixed,31/Aug/10 17:27,15/Dec/10 22:19
Bug,GROOVY-4386,12812172,static import does not work in Groovy 1.7.4,"Static import does not work from Groovy scripts.

Example:

foo/Constants.groovy:

{noformat}
package foo

class Constants {
    static final PI = 3.14
}
{noformat}

foo/Test.groovy

{noformat}
package foo
import static foo.Constants.PI

class Test {
    static main(args) {
        println(""pi="" + PI)
    }
}
{noformat}

When you try to run this (without compling with groovyc first), you get:

{noformat}
groovy -cp . foo\Test.groovy
Caught: groovy.lang.MissingPropertyException: No such property: PI for class: foo.Test
        at foo.Test.main(Test.groovy:6)
{noformat}

If you first run ""groovyc foo\Constants.groovy"" the above command succeeds.




",paulk,frodesto,Major,Closed,Fixed,01/Sep/10 01:21,07/Apr/15 19:13
Bug,GROOVY-4391,12815295,Groovy build fails test target using Java 1.7.0,"These two tests fail and error respectively when running the Ant test target using Java 1.7.0-ea-b108:

org.codehaus.groovy.ant.GroovycTest.testGroovyc_Joint_NoFork_NestedCompilerArg_WithGroovyClasspath
org.codehaus.groovy.transform.DelegateTransformTest.testLock",roshandawrani,malbery,Minor,Closed,Fixed,04/Sep/10 23:28,15/Dec/10 16:56
Bug,GROOVY-4392,12815287,asymmetry of closure parameter type among String#replaceAll/String#eachMatch/Pattern#each,"The closure which passed to String#replaceAll() takes arguments in two patterns:

1) Object[]
2) Arguments correspond to matcher's group(0), group(1), group(2) ...

but the closure for Pattern#match(Closure) and String#eachMatch(Closure) takes:

1) List
2) Arguments correspond to matcher's group(0), group(1), group(2) ...

So there is an asymmetry about 1).

If this issue should be fixed, IMHO, to change replaceAll() to take a list like
match/eachMatch is better. Because you can write simply:
{code}
assert ""ABDE"" == ""abcdef"".replaceAll(/(..)(.)/) { it[1].toUpperCase() } // (A)
{code}
rather than:
{code}
assert ""ABDE"" == ""abcdef"".replaceAll(/(..)(.)/) { Object[] it -> it[1].toUpperCase() } //(B)
{code}
(A) is same as: 
{code}
assert ""ABDE"" == ""abcdef"".replaceAll(/(..)(.)/) { List it -> it[1].toUpperCase() } // (C)
{code}

I attach small patch for 1.8-trunk and some test cases for this.
This patch leads small backward-incompatibility so that (B) would fail because
of the difference of parameter type of the closure. But current test cases
don't include this.",guillaume,uehaj,Major,Closed,Fixed,05/Sep/10 09:25,15/Dec/10 16:56
Bug,GROOVY-4394,12811696,JavaStubGenerator doesn't properly generate annotations with properties of array type,"Annotation properties of array type are generated with null walue in java stubs. Subsequently, stub compilation fail.

For example : 

{code}
// ArrayAnnotation.java
public @interface ArrayAnnotation {
String[] value() default {};
}

// AnnotatedClass.groovy
@ArrayAnnotation([""foo"", ""bar""])
class AnnotatedClass { }
{code}

generates the stub : 

{code}
@ArrayAnnotation(value=null)
public class AnnotatedClass extends java.lang.Object implements groovy.lang.GroovyObject {
// irrelevent stuff removed
}
{code}

Compilation fails with the message : 
{code}
Compilation failure: \...\target\generated-sources\groovy-stubs\main\groovystubs\AnnotatedClass.java:[4,23] attribute value must be constant
{code}",paulk,gfouquet,Major,Closed,Fixed,06/Sep/10 08:45,15/Dec/10 22:19
Bug,GROOVY-4397,12815316,@Delegate broken in Groovy 1.8.0-beta-2-SNAPSHOT,"Running this with Groovy pre 1.8.0 works:

{code}
class ListDelegate {
  @Delegate List aList = []
}

def list = new ListDelegate() << 'a' << 'b' << 'c'
{code}

to give:

{code}
Result: [""a"", ""b"", ""c""]
{code}

However, on the GroovyWebConsole (running Groovy 1.8.0-beta-2-SNAPSHOT), it gives the following exception:

{code}
java.lang.NullPointerException
	at com.sun.beans.TypeResolver.resolve(TypeResolver.java:321)
	at com.sun.beans.TypeResolver.resolve(TypeResolver.java:303)
	at com.sun.beans.TypeResolver.resolve(TypeResolver.java:351)
	at com.sun.beans.TypeResolver.resolve(TypeResolver.java:172)
	at com.sun.beans.TypeResolver.resolveInClass(TypeResolver.java:93)
	at Script1.run(Script1.groovy:5)
{code}",roshandawrani,tim_yates,Major,Closed,Fixed,07/Sep/10 03:22,15/Dec/10 22:31
Bug,GROOVY-4404,12815308,Variable scope of BlockStatement getting lost inside ReturnAdder,"Inside of of ReturnAdder, there is the code:

{code}
BlockStatement newBlock = new BlockStatement();
newBlock.setVariableScope(node.getVariableScope());
{code}

This is meant to ensure that the variable scope of the new block statement has the appropriate variable scope.  However, node is of type MethodNode.  The code block should be as follows:


{code}
BlockStatement newBlock = new BlockStatement();
Statement code = node.getCode();
if (code instanceof BlockStatement) {
  newBlock.setVariableScope(((BlockStatement) code).getVariableScope());
}
{code}

If code is not of type BlockStatement, then we don't need to set the variable scope.",guillaume,werdna,Major,Closed,Fixed,07/Sep/10 23:06,15/Dec/10 22:19
Bug,GROOVY-4406,12815300,Script extension set on groovyc does not flow down to the CompilerConfiguration,"Groovyc supports an attribute called scriptExtension, that is used to compile the files with that particular extension instead of standard *.groovy extension.

It seems like Groovyc doesn't pass this information to CompilerConfiguration - in forked or non-forked compilation both, which continues to see as ""*.groovy"" always and it may cause to compilation inconsistencies.",roshandawrani,roshandawrani,Major,Closed,Fixed,08/Sep/10 21:00,15/Dec/10 16:57
Bug,GROOVY-4408,12815335,Groovy truth of groovy.util.slurpersupport.NoChildren should be false,"Add a ""asBoolean"" method to NoChildren so that it's groovy truth will evaluate as false.
",paulk,lhotari,Major,Closed,Fixed,10/Sep/10 05:24,15/Dec/10 22:19
Bug,GROOVY-4409,12815299,Groovy snapshot source archives for 1.8 beta-1 and beta-2 are having everything packaged twice,"The source archives at for 1.8.0 beta-1 and beta-2 both are having all the files duplicated.
* http://snapshots.repository.codehaus.org/org/codehaus/groovy/groovy-all/1.8.0-beta-1-SNAPSHOT/groovy-all-1.8.0-beta-1-SNAPSHOT-sources.jar
* http://snapshots.repository.codehaus.org/org/codehaus/groovy/groovy-all/1.8.0-beta-2-SNAPSHOT/groovy-all-1.8.0-beta-2-SNAPSHOT-sources.jar

The source archive for 1.7.5 is correct (for cross-checking)
",roshandawrani,roshandawrani,Major,Closed,Fixed,10/Sep/10 08:56,15/Dec/10 22:20
Bug,GROOVY-4410,12815328,ClassFormatError extending generic interface that has a method with an Array argument returning the generic type,"Real world example where I run into this... javax.ws.rs.ext.MessageBodyReader<T>

Basically what happens is that a duplicate method marked ""public bridge synthetic"" method (according to jclasslib bytecode viewer) is generated after the <clinit> with an few extras in the method signature:

(L[Ljava/lang/String;;)Ljava/lang/Object;

The first (correct generated method) has the proper signature:

([Ljava/lang/String;)Ljava/lang/Object;

But changing the return type from the overridden generic type, to def or Object eliminates this extra method (and thus the issue disappears as well). hence the minor classification.

Attached:
Producer.java (simple interface)
StringProducer.groovy (bad food)
HackProducer.groovy (good food)
MainClass.java (test case)",roshandawrani,achuggardlee,Minor,Closed,Fixed,10/Sep/10 18:40,15/Dec/10 22:19
Bug,GROOVY-4412,12815292,FindBugs errors for new Integer() vs. Integer.valueOf() like calls,"Groovy compiler seems to be generating calls like {{ new Integer() }} and {{ new Long() }} (also for other number wrapper types), which FindBugs tool calls out as performance issues and suggests that the equivalent calls {{ Integer.valueOf() }} and {{ Long.valueOf() }} be used.

Class generation needs to change a bit to support it to reduce the errors reported by FindBugs for groovy classes.",roshandawrani,roshandawrani,Major,Closed,Fixed,11/Sep/10 13:06,15/Dec/10 22:19
Bug,GROOVY-4414,12815341,unaryMinus fails for Short and Byte,"The code:
{code}
Short s = 1
println(-s)
{code}
fails with the following exception:
{{groovy.lang.MissingMethodException: No signature of method: java.lang.Short.negative() is applicable for argument types: () values: []}}

This happens because {{org.codehaus.groovy.runtime.InvokerHelper#unaryMinus}} doesn't negate {{Short}} and {{Byte}} directly, but falls through to a call to non-existent function {{negative}} on those objects.

Please find attached a patch for a test case ({{InvokerHelperTest}}), and a patch for a proposed fix.

The patches are done with git, hope this is OK. If it isn't, please let me know for future reference.",roshandawrani,dsrkoc,Minor,Closed,Fixed,11/Sep/10 16:24,05/Apr/15 14:44
Bug,GROOVY-4415,12815332,Error implementing a Java or Groovy generic interface,"Hello.

  I have an issue creating a Groovy class implementing an interface coded in Java with Generics:
Here is the code:
{code}
/* Java interface */
public interface Entity<T> {
	T getId();	
}

/* Groovy class */
class Car implements Entity<Long> {
      Long id;
}
{code}
Car groovy class has a Long getId() method already, but it doesn't recognize it's enough to implement the interface method:
""Can't have an abstract method in a non-abstract class. The class 'Car' must be declared abstract or the method 'java.lang.Object getId()' must be implemented.""

The same happens when the interface is coded in Groovy instead of Java.

I believe it's an important thing to support.

I am using Groovy 1.7.3.

Thank you.",melix,lucianogreiner,Major,Closed,Fixed,12/Sep/10 20:44,24/Dec/11 03:08
Bug,GROOVY-4416,12815319,Accessing outer properties/methods from inner classes results in NPE,"The outer property access and method invocations from inner class seem to be resulting in NPE, as shown by examples below:

Case 1: Outer property access from inner class
{code}
class ReflectionToolboxTest {
   static final PROPERTY_VALUE = ""property_value""
   class Bean {
       String property = PROPERTY_VALUE
   }

   def bean = new Bean()
}

def test = new ReflectionToolboxTest()
{code}
results in
{noformat}
java.lang.NullPointerException
    at ReflectionToolboxTest$Bean.propertyMissing(Script1.groovy)
    at ReflectionToolboxTest$Bean.<init>(Script1.groovy)
    at ReflectionToolboxTest.<init>(Script1.groovy:7)
    at Script1.run(Script1.groovy:10)
{noformat}
Case 2: Outer method access from inner class
{code}
class Dummy {
   def foo(){}
   class Bean {
       String property = foo()
    }
   def bean = new Bean()
}
new Dummy()
{code}
results in
{noformat}
java.lang.NullPointerException
    at Dummy$Bean.methodMissing(Script1.groovy)
    at Dummy$Bean.<init>(Script1.groovy:4)
    at Dummy.<init>(Script1.groovy:6)
    at Script1.run(Script1.groovy:8)
{noformat}
",roshandawrani,roshandawrani,Major,Closed,Fixed,13/Sep/10 04:11,15/Dec/10 22:19
Bug,GROOVY-4418,12815309,Unqualified reference in subclass static method to superclass static fields fails to compile,"{code:title=Base.groovy}
public class Base {
    
    public static String field;

}
{code}

{code:title=Subclass.groovy}
public class Subclass extends Base {
    
    static void method() {
        System.out.println(field);
    }

}
{code}

I get a compile error stating:

Groovy:Apparent variable 'field' was found in a static scope but doesn't refer to a local variable, static field or class. Possible causes:	... Subclass.groovy	line 8

If I rename Subclass.groovy to Subclass.java, it compiles fine. If I fully qualify field with Base.field or this.field, it compiles fine. However, unqualified access, as above, produces an error.",paulk,astral,Minor,Closed,Fixed,14/Sep/10 09:06,15/Dec/10 22:19
Bug,GROOVY-4422,12815302,An Interface nested within a class fails to parse.,"Attempting to compile/evalute the following groovy code
{code:java}
 public class Foo {
   static public class Bar {}
   static public interface Baz {}
 }
{code}
produces
{quote}
1 compilation error:
 Unknown type: INTERFACE_DEF at line: 3 column: 3. File: ConsoleScript8 at line: 3, column: 3
{quote}",roshandawrani,jwadamson,Major,Closed,Fixed,17/Sep/10 13:48,15/Dec/10 16:56
Bug,GROOVY-4430,12815329,Verify Error when using wrong enum syntax,"I've tried to declare an enum and made the mistake to put the constructor before the values:

{code}
enum Foo {
  Foo(String str) {}, // first I tried with ; but this got me some other error
  ONE(""one""), TWO(""two"")
}
{code}

What got me distracted is that Groovy throws a VerifyError here:

java.lang.VerifyError: (class: Foo, method: <clinit> signature: ()V) Unable to pop operand off an empty stack",,pniederw,Minor,Closed,Fixed,23/Sep/10 07:03,22/Sep/13 03:50
Bug,GROOVY-4431,12815338,Groovy Console binding's output-transform missing yields a stacktrace when context is cleared,"The Groovy Console uses a mechanism that users can use to define visual representations of outputs in the console (for instance displaying a map in the form of a JTable).

Such ""output transforms"" are stored in a special variable in the binding.

When calling the ""clear context"" action, a new binding is created, but the output transforms variable is not added back to the binding. And when you execute a script which will yield some resulting output, Groovy Console won't find the output transforms variable, and will throw a stacktrace complaining about that.",guillaume,guillaume,Major,Closed,Fixed,23/Sep/10 08:37,15/Dec/10 16:56
Bug,GROOVY-4434,12815337,Java stub generator doesn't use FQN for annotation values,"After upgrading our app from 1.7.4 to 1.7.5, we got several (joint) compile errors for seemingly simple Groovy classes. Looking at the generated stub code, I found that imports for annotations were always missing. After debugging groovyc, I found the following:

AntlrParserPlugin:333: addImport() is called with alias==null, which is correct since no import alias is used
ASTHelper:131: if(aliasName==null) aliasName=name // now alias is non-null
JavaStubGenerator:679: if (imp.getAlias() == null) imports.add(...); // because getAlias() is non-null, import isn't added
 
Code example:

{code}
import foo.Foo // details of Foo don't matter
   
@Foo
class Bar {} 
{code}

For GMaven users this means that if just one Groovy class in the whole project contains an annotation, the project cannot be compiled.",roshandawrani,pniederw,Blocker,Closed,Fixed,23/Sep/10 17:37,04/Jan/12 19:32
Bug,GROOVY-4435,12815282,Inconsistent handling of final fields,"Snippets v1 and v2 below are equivalent but v1 compiles and v2 does not. v1 should also not compile.

{code:title=v1|borderStyle=dashed|titleBGColor=#FFFFCE}
class Test {
    private final String prop = """"
    void setProp(val) {
        this.prop = val /* compiles, when it shouldn't */
    }
}
{code}

{code:title=v2|borderStyle=dashed|titleBGColor=#F7D6C1}
class Test {
    private final String prop = """"
    void setProp(val) {
        prop = val /* correctly fails to compile saying ""cannot modify final field 'prop'"" */
    }
}
{code}
",roshandawrani,roshandawrani,Major,Closed,Fixed,24/Sep/10 12:56,15/Dec/10 16:56
Bug,GROOVY-4437,12815323,Static import don't work in scripts in default packages,"{code:title=Test.groovy}
/* both the .* or .foo/.bar ways of static importing don't work
import static Helper.*

//import static Helper.foo
//import static Helper.bar
 
println foo() // fails MissingMethodException
println bar // fails MissingPropertyException
{code}
{code:title=Helper.groovy}
class Helper {
    static foo(){
        println 'foo called'
    }
    static bar = 'bar'
}
{code}

But the following static import, which uses a class under a package, works:
{code}
import static java.lang.Integer.*
println MIN_VALUE
{code}",paulk,roshandawrani,Major,Closed,Fixed,25/Sep/10 23:59,26/Dec/10 21:40
Bug,GROOVY-4438,12815303,Use of an inner enum causes compilation to go into infinite loop,"The following piece of code causes the compilation to go into infinite loop

{code:title=Groovy Compiler - To infinity and beyond..|titleBGColor=#FF6666}
enum Outer {
    A, B
    enum Inner{X, Y}
}
{code}",daniel_sun,roshandawrani,Critical,Closed,Fixed,26/Sep/10 00:40,03/Jul/19 06:09
Bug,GROOVY-4440,12815321,Groovy Console looking for wrong LAF on Snow Leopard,The Groovy Console is looking for the wrong LAF on Snow Leopard and not using the OS/X LAF as a result. The code is checking for 'apple.laf.AquaLookAndFeel' but the LAF was moved to 'com.apple.laf.AquaLookAndFeel'. The switch statement is in groovy.ui.ConsoleView.groovy but I assume groovy.ui.view.MacOSXDefaults.groovy and groovy.ui.view.MacOSXMenuBar.groovy will also need to be updated.,,ctwise,Major,Closed,Fixed,27/Sep/10 09:49,14/Aug/13 03:57
Bug,GROOVY-4441,12815357,method File.eachFile is not returning full directory content,"Hi,

We have got a weird issue. A directory contains several files having extended chars in their filenames due to some changes made on the server back and forth.

When trying to loop of those files with the method File.eachFile(Closure), we will get only the files with names made of ascii chars only, not the others.

We have found a workaround with File.traverse which gives us the full content of the directory.

Can you confirm there is a bug with File.eachFile(Closure) ?

Thx",guillaume,gorisis,Major,Closed,Fixed,27/Sep/10 10:02,15/Dec/10 16:57
Bug,GROOVY-4443,12815325,null.asBoolean() should be false,"As per the thread [groovy-user] Groovy truth and null, null.asBoolean() should return false, not true.",guillaume,guillaume,Major,Closed,Fixed,28/Sep/10 09:13,15/Dec/10 16:56
Bug,GROOVY-4444,12815352,Constructor chaining in enums doesn't work,"The following code...
{code}
enum Foo {
  ONE(1), TWO(1, 2)
  
  int i
  int j
  
  Foo(int i) {
    this(i, 0) // if we set i and j manually here, all is fine
  }
  
  Foo(int i, int j) {
    this.i = i
    this.j = j
  }
}

def foos = [Foo.ONE, Foo.TWO]
{code}

...causes this exception:
{noformat}
java.lang.ExceptionInInitializerError
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:169)
    at ConsoleScript12.class$(ConsoleScript12)
    at ConsoleScript12.$get$$class$Foo(ConsoleScript12)
    at ConsoleScript12.run(ConsoleScript12:17)
    at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:264)
    at groovy.lang.GroovyShell.run(GroovyShell.java:513)
    at groovy.lang.GroovyShell.run(GroovyShell.java:170)
    at groovy.lang.GroovyShell$run$0.call(Unknown Source)
    at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:890)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
    at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:149)
    at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
    at groovy.lang.Closure.call(Closure.java:276)
    at groovy.lang.Closure.call(Closure.java:271)
    at groovy.lang.Closure.run(Closure.java:354)
    at java.lang.Thread.run(Thread.java:637)
Caused by: groovy.lang.GroovyRuntimeException: Could not find matching constructor for: Foo(java.lang.String, java.lang.Integer, java.lang.Integer)
    at groovy.lang.MetaClassImpl.selectConstructorAndTransformArguments(MetaClassImpl.java:1415)
    at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.selectConstructorAndTransformArguments(ScriptBytecodeAdapter.java:237)
    at Foo.$INIT(ConsoleScript12)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
    at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.invoke(StaticMetaMethodSite.java:43)
    at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:99)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:48)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:165)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:181)
    at Foo.<clinit>(ConsoleScript12)
    ... 33 more
{noformat}",roshandawrani,pniederw,Major,Closed,Fixed,28/Sep/10 10:52,15/Dec/10 16:57
Bug,GROOVY-4445,12815324,ImmutableASTTransformation puts initialization of static fields into instance constructor,"{code}
@Immutable class Foo {
    private static final String BAR = 'baz'
}
{code}

For the above code, the initialization statements for the static field should be added only to <clinit> and not to <init>.

The instructions below are taken from <init> and they show the initialization statements are getting added to instance initializers also, which is wrong.
{noformat}
108 ifeq 136 (+28)
111 ldc #84 <baz>
113 aconst_null
114 invokestatic #54 <org/codehaus/groovy/runtime/ScriptBytecodeAdapter.compareEqual>
117 ifeq 123 (+6)
120 goto 133 (+13)
123 ldc #84 <baz>
125 dup
126 checkcast #86 <java/lang/String>
129 putstatic #88 <Foo.BAR>
132 pop
133 goto 154 (+21)
136 aload_2
137 ldc #89 <4>
139 aaload
140 aload_1
141 invokeinterface #69 <org/codehaus/groovy/runtime/callsite/CallSite.callGetProperty> count 2
146 dup
147 checkcast #86 <java/lang/String>
150 putstatic #88 <Foo.BAR>
{noformat}",roshandawrani,roshandawrani,Major,Closed,Fixed,29/Sep/10 10:13,15/Dec/10 16:57
Bug,GROOVY-4447,12812177,A few issues in stubgenerator testing infrastructure,"The documentation of org.codehaus.groovy.tools.stubgenerator.StringSourcesStubTestCase says that the source mappings for testing joing compilation stub generation can be given as below:
{code}
['com/foo/Bar.groovy': '''
    package com.foo
    class Bar {}
''']
{code}

However this doesn't currently work because the sub-folders for the package structure are not recursively created physically and hence writing to 'com/foo/Bar.groovy' fails with FileNotFoundException.

Another small improvement needed is an option to not-delete the temp folder created for stubs.",roshandawrani,roshandawrani,Major,Closed,Fixed,30/Sep/10 04:17,15/Dec/10 16:56
Bug,GROOVY-4448,12816630,SwingBuilder TitledBorder not recognizing justification attribute correctly,"I was attempting to move my titled border around the panel by modifying the justification attribute.

{code}
panel(border: titledBorder(border: etchedBorder(), title: 'Project', justification: 'right', position: 'bottom')) {
{code}

It seemed no matter what value I put into the justification attribute, it would stay on the left of my panel.  I looked into the source code, and it appears as though there may be a copy paste error from the section processing the position attribute.  The current version of TitledBorderFactory reads the justification attribute from the builder, and then tries to look the value up in the position map instead of the justification map.  I think a simple code change to look things up in the justification map would fix things.

i.e. change this:
{code}
def justification = attributes.remove(""justification"")
justification = positions[justification] ?: justification

{code}

to this:
{code}
def justification = attributes.remove(""justification"")
justification = justifications[justification] ?: justification
{code}",roshandawrani,solidjb,Major,Closed,Fixed,30/Sep/10 07:01,05/Apr/15 14:44
Bug,GROOVY-4449,12815320,Vararg parameter allowed in non-last position but fails at runtime,"Groovy doesn't give an error when a vararg parameter doesn't come last in a parameter list, but then fails at runtime when the method is called in vararg-style. For example, the following code...

{code}
def foo(String... strs, int i) { println i }
foo(""me"", ""you"", 42)
{code}

...produces:
{noformat}
groovy.lang.MissingMethodException: No signature of method: ConsoleScript11.foo() is applicable for argument types: (java.lang.String, java.lang.String, java.lang.Integer) values: [me, you, 42]
{noformat}

I assume the underlying reason is that groovyc currently simply treats String... as syntactic sugar for String[], without any further checks.",roshandawrani,pniederw,Major,Closed,Fixed,30/Sep/10 07:32,15/Dec/10 16:56
Bug,GROOVY-4451,12811797,Stub generation for annotation types doesn't work,"Generated stub code looks similar to a regular interface definition. Something like:

{code}
public interface MyAnno extends java.lang.Annotation { ... }
{code}

Obviously, javac doesn't like this.",roshandawrani,pniederw,Major,Closed,Fixed,30/Sep/10 07:51,15/Dec/10 16:57
Bug,GROOVY-4453,12812186,Duplicate methods added by stub generator in joint compilation when method parameters have default values.,"A new grails project.
In the src folder a groovy class like:
{code}
package de.app

class User {
    String name

    public void setName(String name, String t = """") {
          this.name = name
    }
}
{code}
and in the src folder a java class like:
{code}
package de.me;

import de.app.User;

public class App {

    public void make() {
        User u = new User();
    }
}
{code}
result:
{code}
[groovyc] /tmp/groovy-generated-5423173608487992496-java-source/de/app/User.java:22: setName(java.lang.String) is already defined in de.app.User
  [groovyc] public  void setName(java.lang.String value) { }
  [groovyc]              ^
  [groovyc] 1 error
{code}
removing the optional parameter (String t = """") works.",roshandawrani,marcel166,Major,Closed,Fixed,01/Oct/10 10:28,15/Dec/10 16:57
Bug,GROOVY-4454,12815318,PATHEXT environment var overwritten in windows installer,"After installing Groovy using groovy-1.5.7-installer.exe, I noticed problems to start my WebLogic server...
After investigation it turned out it was no longer possible to run 'java' from command line, it did work when using 'java.exe'
Some further investigation showed that the environment variable PATHEXT was now PATHEXT=;.groovy;.gy in stead of the usual value PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH

Needless to say Groovy left the building in the mean time... :-((",jbaumann,gjmathews1,Major,Closed,Fixed,01/Oct/10 16:05,15/Dec/10 22:31
Bug,GROOVY-4455,12815354,multiple assignment code makes Groovy AST Browser throw Exception,"Type this code into GroovyConsole: 
def (bar, x, bif, qux) = [1, 2, 3, 4]$
View AST Browser
There is an exception. 

This should be easy to fix. ",hamletdrc,hamletdrc,Minor,Closed,Fixed,02/Oct/10 11:13,10/Nov/11 03:06
Bug,GROOVY-4457,12812178,generic type declarations leaking across all files in a build,"Simple file, A.groovy:

{code}
class A<String> {
}

class B {
  void foo(String s) {}
}
{code}

groovyc A.groovy
javap -private B | grep foo

produces:
{code}
public void foo(java.lang.Object);
{code}

The 'String' is treated as a type parameter name.  The reference 'String' in the foo method is mapped to this type parameter (clearly it shouldn't be) and when producing the code, String is erased to its upper bound of Object, hence foo(Object) in the bytecode.

Change it to this:
{code}
class A<T> {
}

class B {
  void foo(String s) {}
}
{code}
and it produces what you expect:
{code}
 public void foo(java.lang.String);
{code}

The problem is the genericParameterNames collection in the ResolveVisitor is never cleared, only augmented with new entries.

My solution that seems to work in groovy eclipse is to clear the parameter names at the end of visiting a classnode in ResolveVisitor. So at the end of

{code}
public void visitClass(ClassNode node) {
{code}

add
{code}
genericParameterNames.clear();
{code}
",melix,aclement,Critical,Closed,Fixed,04/Oct/10 17:21,24/Dec/11 03:08
Bug,GROOVY-4458,12815355,NPE in org.codehaus.groovy.runtime.dgmimpl.NumberNumberPlus$DoubleInteger,"We enhance the NullObject to have a more graceful null handling. 

{code}
def emc = new ExpandoMetaClass( org.codehaus.groovy.runtime.NullObject.getNullObject().getClass())
emc.plus = {b -> b}
emc.initialize()
org.codehaus.groovy.runtime.NullObject.getNullObject().setMetaClass(emc)

It works fine except for some variants of array access:
Double[][] a = new Double[10][10]
for (def i = 0; i <= 9; i++ ) {
    for (def j = 0; j <= 9; j++ ) {
        println(""i=$i j=$j a[0][i]=$a[0][i]"")
        a[0][i] = a[0][i] + 1
    }
}
{code}

We get a NPE:
{noformat}
java.lang.NullPointerException
    at org.codehaus.groovy.runtime.dgmimpl.NumberNumberPlus$DoubleInteger.call(NumberNumberPlus.java:170)
    at ConsoleScript6.run(ConsoleScript6:10)
    at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:264)
    at groovy.lang.GroovyShell.run(GroovyShell.java:513)
    at groovy.lang.GroovyShell.run(GroovyShell.java:170)
    at groovy.lang.GroovyShell$run$0.call(Unknown Source)
    at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:890)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
    at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:149)
    at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
    at groovy.lang.Closure.call(Closure.java:276)
    at groovy.lang.Closure.call(Closure.java:271)
    at groovy.lang.Closure.run(Closure.java:354)
    at java.lang.Thread.run(Thread.java:619) 
{noformat}

A script to reproduce this in the GroovyConsole is attached.
",blackdrag,uwekirsch,Major,Closed,Fixed,05/Oct/10 06:54,16/Feb/13 00:47
Bug,GROOVY-4467,12811469,Groovy JDK's java.util.Date page has double method definitions,"On the java.util.Date page in Groovy JDK:
http://groovy.codehaus.org/groovy-jdk/java/util/Date.html

All of the methods appear twice: once with no comment, and again with the javadoc comment.

",paulk,jaswartz@ebay.com,Major,Closed,Fixed,08/Oct/10 15:51,15/Dec/10 16:56
Bug,GROOVY-4469,12815343,http://groovy.codehaus.org/gapi/ - empty/duplicate/missing Javadocs,"Please, see http://goo.gl/nDrx:

* Some javadocs are empty (MetaObjectProtocol, MetaClass, GroovyObject)
* Some javadocs contain duplicate methods (Date, Process)
* Some javadocs are missing (EncodingGroovyMethods)",paulk,genie,Major,Closed,Fixed,10/Oct/10 17:57,15/Dec/10 16:56
Bug,GROOVY-4470,12811532,Stub generator cannot handle multi-line strings as annotation values,"Relevant part of generated stub:

{code}
@StringAnno(val="" 
multi
line
string
"") public class StringAnnoUser
  extends java.lang.Object  implements
    groovy.lang.GroovyObject {
public StringAnnoUser
() {}
{code}",pniederw,pniederw,Major,Closed,Fixed,11/Oct/10 10:56,08/Mar/11 09:37
Bug,GROOVY-4471,12815361,NPE in property access in anonymous inner classes,"{code}
class X {
    private A a = new A()
    public B b
    public X() {
        b = new B() {
            public String getFoo() {a}
        }
    }
}

class A {}
class B {
    B(){getFoo()}
    def getFoo(){}
}

def x = new X()
{code}

The code above fails with
{noformat}
Caught: java.lang.NullPointerException
	at X$1.propertyMissing(TryGroovy.groovy)
	at X$1.getFoo(TryGroovy.groovy:6)
	at X$1.getFoo(TryGroovy.groovy)
	at B.<init>(TryGroovy.groovy:13)
	at X$1.<init>(TryGroovy.groovy)
	at X.<init>(TryGroovy.groovy:5)
	at TryGroovy.run(TryGroovy.groovy:16)
{noformat}",melix,roshandawrani,Major,Closed,Fixed,11/Oct/10 11:28,14/Oct/11 00:28
Bug,GROOVY-4475,12811702,Boolean seems to support toBoolean() but yields incorrect results,"This is curious (with groovy shipped with grails 1.3.4)

{code}
boolean b = true
def x = b.toBoolean()

println x
assert x
{code}

1) where is no-arg toBoolean() coming from
2) why does it return false for true input.
",guillaume,marc@anyware.co.uk,Critical,Closed,Fixed,12/Oct/10 11:56,15/Dec/10 16:57
Bug,GROOVY-4477,12818313,Exceptions do not get unwrapped from InvokerInvocationException,"Based on the brief exchange on the mailing list, here's the problem:

Groovy code calls a Java library passing in a Groovy instance (which was created using the ""{...} as SomeInterface"" dynamic cast) as parameter. The Java library calls the passed-in Groovy object. The Groovy object throws an exception. The Java library doesn't see the exception, as it is wrapped (because of the groovy bug) inside InvokerInvocationException, so the catch clause inside the library doesn't trigger and the library cannot react. My Groovy code gets an unexpected library-specific exception thrown up to its face.

I revealed the issue when trying to make Multiverse (STM) work with Groovy. Please find below a distilled code example reproducing the problem.

{code:title=MyFoo.java}
public interface MyFoo {
    void baz() throws MyException;
}
{code}

{code:title=MyException.java}
public class MyException extends Exception {
    public MyException(final String message) {
        super(message);
    }
}
{code}

Processor.java representing a third-party library
{code:title=Processor.java}
public class Processor {
    public static void bar(final MyFoo code) {
        try {
            code.baz();
        } catch (MyException e) {
            System.out.println(""Just swallowed the error"");
        }
    }
}
{code}

Groovy script
{code}
Processor.bar( {
    throw new MyException('test')
} as MyFoo)
{code}

The problem is probably related to the dynamic casting, since the following script works fine:

{code}
Processor.bar new MyFoo() {
    void baz() {
        throw new MyException('test')
    }
}
{code}

The failing script uses a dynamic cast.",blackdrag,roller_vaclav,Major,Closed,Fixed,13/Oct/10 14:12,15/Dec/10 10:31
Bug,GROOVY-4480,12815345,Parsing seems to have changed in 1.8,"Running this script on the webconsole used to work prior to 1.8

http://groovyconsole.appspot.com/script/192001

However, now it fails with an exception:

{code}
Script1.groovy: 83: unexpected token: else @ line 83, column 67.
   1).type == type2 ) token += 2 else false
{code}
To get the script working again, you need to change line 83 from:

{code}
    if( get(0).type == type1 && get(1).type == type2 ) token += 2 else false
{code}
to

{code}
    if( get(0).type == type1 && get(1).type == type2 ) { token += 2 } else false
{code}",blackdrag,tim_yates,Minor,Closed,Fixed,15/Oct/10 08:56,08/Mar/11 09:13
Bug,GROOVY-4481,12815375,MetaClassRegistry listener isn't triggered and iterator is not returning updated MCs,,guillaume,guillaume,Major,Closed,Fixed,16/Oct/10 11:20,15/Dec/10 16:57
Bug,GROOVY-4482,12815362,Closure.memoize fails on Google App Engine,"Running the following code on the Groovy Web Console:

{code}
cl = {a, b ->
    sleep(3000) // simulate some time consuming processing
    a + b
}
mem = cl.memoize()

def callClosure(a, b) {
    def start = System.currentTimeMillis()

    mem(a, b)

    println ""Inputs(a = $a, b = $b) - took ${System.currentTimeMillis() - start} msecs.""
}

callClosure(1, 2)
callClosure(1, 2)
callClosure(2, 3)
callClosure(2, 3)
callClosure(3, 4)
callClosure(3, 4)

callClosure(1, 2)
callClosure(2, 3)
callClosure(3, 4)
{code}
(taken from http://roshandawrani.wordpress.com/2010/10/18/groovy-new-feature-closures-can-now-memorize-their-results/)

Throws the following exception (when caught and printed to stdout)

{code}
java.security.AccessControlException: access denied (java.lang.RuntimePermission accessDeclaredMembers)
	at com.google.appengine.runtime.Request.process-<unknown>(Request.java)
	at java.security.AccessControlContext.checkPermission(AccessControlContext.java:355)
	at java.security.AccessController.checkPermission(AccessController.java:567)
	at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
	at java.lang.SecurityManager.checkMemberAccess(SecurityManager.java:1679)
	at java.lang.Class.checkMemberAccess(Class.java:2174)
	at java.lang.Class.getDeclaredMethods(Class.java:1807)
	at java.lang.Class.getEnclosingMethod(Class.java:919)
	at sun.reflect.generics.scope.ClassScope.computeEnclosingScope(ClassScope.java:50)
	at sun.reflect.generics.scope.AbstractScope.getEnclosingScope(AbstractScope.java:74)
	at sun.reflect.generics.scope.AbstractScope.lookup(AbstractScope.java:90)
	at sun.reflect.generics.factory.CoreReflectionFactory.findTypeVariable(CoreReflectionFactory.java:109)
	at sun.reflect.generics.visitor.Reifier.visitTypeVariableSignature(Reifier.java:165)
	at sun.reflect.generics.tree.TypeVariableSignature.accept(TypeVariableSignature.java:43)
	at sun.reflect.generics.visitor.Reifier.reifyTypeArguments(Reifier.java:68)
	at sun.reflect.generics.visitor.Reifier.visitClassTypeSignature(Reifier.java:138)
	at sun.reflect.generics.tree.ClassTypeSignature.accept(ClassTypeSignature.java:49)
	at sun.reflect.generics.repository.ClassRepository.getSuperclass(ClassRepository.java:84)
	at java.lang.Class.getGenericSuperclass(Class.java:694)
	at com.sun.beans.TypeResolver.prepare(TypeResolver.java:274)
	at com.sun.beans.TypeResolver.<init>(TypeResolver.java:243)
	at com.sun.beans.TypeResolver.resolve(TypeResolver.java:172)
	at com.sun.beans.TypeResolver.resolveInClass(TypeResolver.java:93)
	at java.beans.FeatureDescriptor.getParameterTypes(FeatureDescriptor.java:385)
	at java.beans.MethodDescriptor.setMethod(MethodDescriptor.java:116)
	at java.beans.MethodDescriptor.<init>(MethodDescriptor.java:74)
	at java.beans.MethodDescriptor.<init>(MethodDescriptor.java:58)
	at java.beans.Introspector.getTargetMethodInfo(Introspector.java:1181)
	at java.beans.Introspector.getBeanInfo(Introspector.java:408)
	at java.beans.Introspector.getBeanInfo(Introspector.java:180)
	at groovy.lang.MetaClassImpl$15.run(MetaClassImpl.java:2937)
	at java.security.AccessController.doPrivileged(AccessController.java:63)
	at groovy.lang.MetaClassImpl.addProperties(MetaClassImpl.java:2935)
	at groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:2918)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:166)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:182)
	at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:214)
	at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:751)
	at groovy.lang.GroovyObjectSupport.<init>(GroovyObjectSupport.java:32)
	at groovy.lang.Closure.<init>(Closure.java:106)
	at groovy.lang.Closure.<init>(Closure.java:117)
	at org.codehaus.groovy.runtime.memoize.Memoize$1.<init>(Memoize.java:55)
	at org.codehaus.groovy.runtime.memoize.Memoize.buildMemoizeFunction(Memoize.java:55)
	at groovy.lang.Closure.memoize(Closure.java:530)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:43)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:885)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:39)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:120)
	at Script1.run(Script1.groovy:7)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:576)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:614)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:585)
	at groovy.lang.GroovyShell$evaluate.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:124)
	at executor.run(executor.groovy:25)
	at groovy.util.GroovyScriptEngine.run(GroovyScriptEngine.java:515)
	at groovy.servlet.GroovyServlet$1.call(GroovyServlet.java:120)
	at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.use(GroovyCategorySupport.java:99)
	at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.access$300(GroovyCategorySupport.java:61)
	at org.codehaus.groovy.runtime.GroovyCategorySupport.use(GroovyCategorySupport.java:239)
	at groovy.servlet.GroovyServlet.service(GroovyServlet.java:129)
	at groovyx.gaelyk.GaelykServlet.super$5$service(GaelykServlet.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:43)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1055)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.invokeMethodOnSuperN(ScriptBytecodeAdapter.java:127)
	at groovyx.gaelyk.GaelykServlet$_service_closure1.doCall(GaelykServlet.groovy:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:43)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:885)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:149)
	at groovyx.gaelyk.GaelykServlet$_service_closure1.doCall(GaelykServlet.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:43)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:885)
	at groovy.lang.Closure.call(Closure.java:288)
	at groovy.lang.Closure.call(Closure.java:282)
	at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.use(GroovyCategorySupport.java:111)
	at org.codehaus.groovy.runtime.GroovyCategorySupport.use(GroovyCategorySupport.java:250)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.use(DefaultGroovyMethods.java:415)
	at org.codehaus.groovy.runtime.dgm$740.invoke(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoMetaMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:307)
	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:51)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:153)
	at groovyx.gaelyk.GaelykServlet.service(GaelykServlet.groovy:67)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:806)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:43)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoCachedMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:229)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:52)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:128)
	at groovyx.gaelyk.routes.RoutesFilter.doFilter(RoutesFilter.groovy:156)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at com.google.net.rpc.impl.BlockingApplicationHandler.handleRequest(BlockingApplicationHandler.java:24)
	at com.google.net.rpc.impl.RpcUtil.runRpcInApplication(RpcUtil.java:418)
	at com.google.net.rpc.impl.Server$RpcTask.runInContext(Server.java:572)
	at com.google.tracing.TraceContext$TraceContextRunnable$1.run(TraceContext.java:448)
	at com.google.tracing.TraceContext.runInContext(TraceContext.java:688)
	at com.google.tracing.TraceContext$AbstractTraceContextCallback.runInInheritedContextNoUnref(TraceContext.java:326)
	at com.google.tracing.TraceContext$AbstractTraceContextCallback.runInInheritedContext(TraceContext.java:318)
	at com.google.tracing.TraceContext$TraceContextRunnable.run(TraceContext.java:446)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{code}

Seems to go bang when {{GroovyObjectSupport}} calls {{InvokerHelper.getMetaClass}} in the constructor on line 32",roshandawrani,tim_yates,Major,Closed,Fixed,18/Oct/10 09:46,15/Dec/10 22:31
Bug,GROOVY-4484,12811578,changelist 20965 Broke AST Browser in Groovy Console,"changelist 20965 Broke AST Browser in Groovy Console

Open up groovyConsole, write some code in window and press Ctrl+T
You get an exception about cannot cast JComboBox to groovy Reference. 

",blackdrag,hamletdrc,Major,Closed,Fixed,20/Oct/10 06:36,21/Oct/10 04:31
Bug,GROOVY-4485,12815388,enum does not support default map constructors,"This enum should compile, but it does not.

{code}
enum ExportFormat {

    EXCEL_OOXML(mime: ""application/vnd.ms-excel"", extension: ""xlsx""),
    EXCEL_BINARY(mime: ""application/vnd.ms-excel"", extension: ""xls""),
    EXCEL_HTML(mime: ""application/vnd.ms-excel"", extension: ""xls"")
    
    String mime
    String extension

}
{code}",paulk,ddurham,Major,Closed,Fixed,20/Oct/10 09:40,10/Jul/13 04:42
Bug,GROOVY-4486,12815365,No stacktrace or message for exceptions in the main constructor,"This script should generate a complete stacktrace including the call to ""doSomething()"" and the message, but it only writes ""This script or class was runnable but cold not be run.""

{code}
public class Foo implements Runnable
{
  public Foo()
  {
     doSomething();
  }

  public void run()
  {
     println(""B"");
  }

  public doSomething()
  {
     println(""A"");
     throw new Exception(""A specific Error Message"");
  }
}
{code}
",blackdrag,ripper234,Major,Closed,Fixed,21/Oct/10 04:19,09/Nov/11 05:21
Bug,GROOVY-4489,12812641,Generics compilation error occurs when using less then (<) operator,"The following code is throwing a compilation error (the strange code is a DSL):
{code}
def FloatComparison() {
    when:
    o.PROPerty1 < o.PROPerty2
    
    then:
    result = true
}

Missing closing bracket '>' for generics types;
   solution: Please specify the missing bracket! at line: 5, column: 5
{code}

Here are some snippets that don't produce this error:
{code}
def FloatComparison() {
    when:
    o.pROPerty1 < o.PROPerty2 //the first letter of the first variable is lowercase
    
    then:
    result = true
}
{code}

{code}
def FloatComparison() {
    when:
    o.PROPerty1 < o.PROPerty2
    
//the label is removed and .x is added after result (without .x the exception occurs)
    result.x = true
}
{code}

Looking at the samples that work, this whole mess doesn't make any sense. It seems like a minor but very annoying bug in the compiler. We are working on a DSL, so we can't influence the names of the properties or anything significant that would be a proper workaround.

(This is technically not a JUnit test case, but I think it's enough because it can be simply copied into the Groovy console and run)",,asandor,Major,Closed,Fixed,25/Oct/10 03:00,02/May/21 09:22
Bug,GROOVY-4492,12812602,Bug in groovy.util.GroovyScriptEngine isSourceNewer() results in gratuitous recompilations and lost of static state in groovlets,"isSourceNewer() method in groovy.util.GroovyScriptEngine does evaluate script sources for recompilation (it will return true) in cases where 'nextPossibleRecompilationTime >= lastMod'. This is wrong and results in gratuitous recompilations of scripts.

An ugly side effect of this bug is, that groovlet state (static objects kept in a static class) is being lost between incoming requests, due to gratuitous recompilation and subsequent restart of groovlets.

The attached patch fixes the issue.
",guillaume,timur,Major,Closed,Fixed,28/Oct/10 07:33,15/Dec/10 16:57
Bug,GROOVY-4497,12815442,"IllegalAccessError when accessing base class property with ""super.propertyName""","The following code...

{code}
class Base {
  def x = 1   
}

class Derived extends Base {
  def x = 2 
  
  def foo() { println super.x }
}

new Derived().foo()
{code}

...produces the exception shown below. Problem goes away if base class declares a field rather than a property, or when derived class uses ""super.getX()"".

{noformat}
java.lang.IllegalAccessError: tried to access field Base.x from class Derived
	at Derived.foo(ConsoleScript6:8)
	at Derived$foo.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:120)
	at ConsoleScript6.run(ConsoleScript6:11)
	at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:264)
	at groovy.lang.GroovyShell.run(GroovyShell.java:513)
	at groovy.lang.GroovyShell.run(GroovyShell.java:170)
	at groovy.lang.GroovyShell$run$0.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:890)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:149)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:276)
	at groovy.lang.Closure.call(Closure.java:271)
	at groovy.lang.Closure.run(Closure.java:354)
	at java.lang.Thread.run(Thread.java:680)
{noformat}",roshandawrani,pniederw,Major,Closed,Fixed,30/Oct/10 20:25,15/Dec/10 16:56
Bug,GROOVY-4498,12815404,Only the root exception in a script is printed ,"Can be seen by running the following in a script with the `groovy` command.

{code}
throw new Exception(""root"", new Exception(""cause""))
{code}

",guillaume,ldaley,Minor,Closed,Fixed,02/Nov/10 01:46,21/Jul/11 19:06
Bug,GROOVY-4500,12815390,Issues with remote closures,"The following usages of groovy remote do not work.

{code}
remote.exec { new GregorianCalendar().time = new Date() }
remote.exec { new GregorianCalendar().""$methodName""(new Date()) }
{code}

Both of these constructs cause a NoClassDefFoundError because somehow a reference to the owner class is embedded in the closure.

See: http://permalink.gmane.org/gmane.comp.lang.groovy.user/51193",guillaume,ldaley,Minor,Closed,Fixed,03/Nov/10 06:45,15/Dec/10 16:56
Bug,GROOVY-4503,12812116,The delete key doesn't work as expected on groovysh on Linux,"The delete key is deleting backward instead of forward, simulating the behavior for MacOSX.

Follows patches for jLine for supporting both Linux and MacOSX.",pschumacher,rosenfeld,Major,Closed,Fixed,04/Nov/10 14:24,05/Apr/15 14:44
Bug,GROOVY-4504,12815382,Groovy compilation results in warnings in generated code,"The Groovy compilation is creating generated files that contain code that generates warning.  This leads to an impossible situation for those who want warning free compilation.

e.g. from the GPars code:
{code}
/tmp/groovy-generated-2342688056285823602-java-source/groovyx/gpars/dataflow/operator/DataFlowProcessor.java:18: warning: [cast] redundant cast to boolean
protected  boolean shouldBeMultiThreaded(java.util.Map channels) { return (boolean)false;}
{code}",guillaume,russel,Major,Closed,Fixed,05/Nov/10 15:18,12/Mar/11 00:47
Bug,GROOVY-4505,12815405,Rogue line number for method declarations confuses IDE debuggers,"Do a javap -l on the following code:

{code}
class Runner {
	public void printit() {
		x(""III"")
		x(""III"")
 	}
}
{code}

and the printit method will look something like this:

{code}
public void printit();
  LineNumberTable: 
   line 2: 4
   line 3: 4
   line 4: 17
  LocalVariableTable: 
   Start  Length  Slot  Name   Signature
   0      31      0    this       LRunner;
{code}

Notice that the LineNumberTable has 3 entries, but the original method has only 2 instruction lines of code.  The first, rogue line number corresponds to the method's block statement.

The problem is that there are 2 entries corresponding to the #4 instruction.  This occasionally confuses the debugger when it is trying to install the breakpoint on a running app (ie- sometimes it works and sometimes it does not).

I tracked the problem down to AsmClassGenerator.visitBlockStatement(), which calls the onLineNumber method and (incorrectly, it seems) adds a line number entry for the start of the block statement.  Simply commenting this line out (line 721 on version 1.7.5) appears to work for me.

Is there any reason why we should not go ahead with the fix?",guillaume,werdna,Major,Closed,Fixed,05/Nov/10 17:43,15/Dec/10 16:56
Bug,GROOVY-4508,12815410,Incorrect stub code generated for constructor with optional parameters,"{code}
class Base {
  Base(String str) {}
}
{code}

{code}
class Derived extends Base {
  Derived(String foo, String bar = ""bar"") {
    super(foo)
  }
} 
{code}

Generated stub has super call in Derived(String, String) body but not in Derived(String) body, resulting in a javac compile error.

{code}
public class Derived
  extends Base  implements
    groovy.lang.GroovyObject {
public Derived
(java.lang.String foo) {}
public Derived
(java.lang.String foo, java.lang.String bar) {
super ((java.lang.String)null);
}
{code}

Tested with Groovy 1.7.5 and GMaven 1.3.

",roshandawrani,pniederw,Major,Closed,Fixed,07/Nov/10 18:04,15/Dec/10 16:56
Bug,GROOVY-4510,12815379,"NPE during ""record"" command with relative path file name","After starting the interactive shell, I executed 

record start foo.txt

then shell exited with following error message

Exception in thread ""Thread-5"" java.lang.NullPointerException: Cannot invoke method flush() on null object
	at org.codehaus.groovy.runtime.NullObject.invokeMethod(NullObject.java:77)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:45)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.NullCallSite.call(NullCallSite.java:32)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:54)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:121)
	at org.codehaus.groovy.tools.shell.commands.RecordCommand$_closure2.doCall(RecordCommand.groovy:115)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        {lot more stack}",roshandawrani,rareddy,Minor,Closed,Fixed,09/Nov/10 14:26,15/Dec/10 16:56
Bug,GROOVY-4512,12815409,Method invocation syntax does not work properly with ComposedClosures,"The problem is with metaprogramming and the ComposedClosure class. Because the ComposedClosure class does not implement the doCall method and the MetaClassImpl.invokeMethod method does not provide any special handling for ComposedClosures as it does for CurriedClosures and MethodClosures, the invocation of the doCall method in the method resolution process will fail. For example:
 
class ComposedTest {
        def closure1 = {}
        def closure2 = {}
        def composed = closure1 >> closure2

}
def inst = new ComposedTest()
 
// This is fine because ComposedClosure implements the call method.
//inst.composed.call()
 
// This fails because ComposedClosure does not implement the doCall method.
inst.composed()",paulk,hendersk,Major,Closed,Fixed,14/Nov/10 16:34,15/Dec/10 22:31
Bug,GROOVY-4514,12815353,Groovydoc does not include all the implemented interfaces of a type,"There are a couple of problems with interface handling in Groovydoc:

* Only the first implemented/extended interface of a Java type is listed
* No implemented/extended interfaces for a Groovy type are listed

Attached is a patch to fix this, along with some test cases. It also fixes a couple of TODO items in the groovydoc test cases.",paulk,adammurdoch,Major,Closed,Fixed,18/Nov/10 18:17,07/Apr/15 19:13
Bug,GROOVY-4516,12815655,@Delegate does not work with default method parameters,"Hi!

I am having problems with the following code:

class Del {
public void doSomething(boolean flag = true) {
println flag
}
}

class AClass {
@Delegate Del del = new Del()
}

def a = new AClass()
a.doSomething(true) // works fine
a.doSomething() // methodMissingException

Thank you!",roshandawrani,luizzfar,Major,Closed,Fixed,19/Nov/10 11:27,15/Dec/10 16:56
Bug,GROOVY-4517,12813838,Java stub generator does not set correct value for annotations that expect a Class as a value,"This issue is related to http://jira.codehaus.org/browse/GROOVY-4434. 

If an annotation expects a class (not a classname, but a class) as its value, the java stub compiler does not seem to write out the java stub annotation value correctly. Here is some groovy code that shows an example:

<code>
import org.springframework.test.context.junit4.SpringJUnit4ClassRunner
@RunWith(SpringJUnit4ClassRunner.class)
class someClass{
...
}
</code>

The @RunWith annotation expects an actual class (not a class name) as its parameter.

In the above mentioned issue, the annotation value is at least expanded to a fully qualified class name. Using the 1.7.6-SNAPSHOT (which includes the patch from GROOVY-4434), I can confirm the classname is expanded. However, this is the output of the java file the stub compiler creates:

<code>
@org.junit.runner.RunWith(value=org.springframework.test.context.junit4.SpringJUnit4ClassRunner) 
class someClass{
...
}
</code>

The trouble is that then javac chokes since org.springframework.test.context.junit4.SpringJUnit4ClassRunner is not an actual class reference. Interestingly, groovyc handles this same code just fine, but javac expects the .class suffix to make the class reference explicit. Thus, the generated code does not compile. 

I have attached a very simple patch which seems to address this issue. I do not know, however if this might break something else. 
",roshandawrani,yoelspotts,Major,Closed,Fixed,19/Nov/10 12:02,05/Apr/15 14:44
Bug,GROOVY-4521,12815367,"Inconsistency between CompilerConfiguration, groovy -c and groovysh ?","Given the following script, saved as {{enc_test.groovy}}
{noformat}
import org.codehaus.groovy.control.CompilerConfiguration
println ""File source encoding is probably set to "" + CompilerConfiguration.DEFAULT.getSourceEncoding()
{noformat}

I would assume all three executions below to print ""UTF-8"", but they don't:
{noformat}
$ groovy -c UTF-8 enc_test.groovy
File source encoding is probably set to MacRoman

$ groovy -Dgroovy.source.encoding=UTF-8 enc_test.groovy
File source encoding is probably set to MacRoman

$ groovysh -Dgroovy.source.encoding=UTF-8
groovy:000> org.codehaus.groovy.control.CompilerConfiguration.DEFAULT.getSourceEncoding()
===> UTF-8
{noformat}

The reason I was printing this info was to track down an encoding issue I had; turns out *the -c flag actually works* for me (i.e my sources are properly decoded) (while the {{-Dgroovy.source.encoding}} option doesn't), but I was looking for a way to ensure my scripts are executed with {{-c UTF-8}}, but the above snippet obviously doesn't help.

",roshandawrani,gjoseph,Major,Closed,Fixed,22/Nov/10 06:49,15/Dec/10 16:56
Bug,GROOVY-4524,12815377,Groovy AntBuilder reset InputHandler when use <groovy> task ,When use <groovy> task in ant build file and run ant with custom InputHandler(-inputHandler parameter) all <input> tasks which executed after <groovy> task use DefaultInputHandler for input. The DefaultInputHandler setted in AntBuilder constructor. When I comment it all works fine.,roshandawrani,ice,Major,Closed,Fixed,22/Nov/10 12:35,15/Dec/10 16:56
Bug,GROOVY-4527,12815393,Groovydoc does not include a super-type which is referenced in the source using a fully-qualified name,"In the following example, neither {{SuperClass}} or {{Interface}} are included in the groovydoc output for {{SomeClass}}:
{code}
class SomeClass extends some.fully.qualified.SuperClass implements some.fully.qualified.Interface {
}
{code}

Attached is a patch for this. The patch also fixes a NullPointerException introduced in the fix for GROOVY-4514 when a fully-qualified super type name is found in the source.",guillaume,adammurdoch,Major,Closed,Fixed,24/Nov/10 02:57,05/Apr/15 14:44
Bug,GROOVY-4528,12815430,Groovydoc does not work with java 5,Attached is a patch for this problem.,roshandawrani,adammurdoch,Major,Closed,Fixed,24/Nov/10 02:59,05/Apr/15 14:44
Bug,GROOVY-4529,12812607,GroovyClassLoader script caching not working properly if scripts not in root dir,"GroovyClassLoader caches class files compiled from scripts not in root dir under keys not matching original name.
{code}
GroovyClassLoader loader = new GroovyClassLoader();
loader.addURL(myScriptRoot);

Class class1, class2;

try {
  class1 = loader.loadClass(""test/script1"");
  class2 = loader.loadClass(""test/script1"");
  assertSame(class1, class2);  // this fails
} catch (ClassNotFoundException e) {
  fail(""Class node found!"");
}
{code}
If I write as above, class is cached by GroovyClassLoader under key ""test.script1"" so looking it up under ""test/script1"" fails. If I try with loader.loadClass(""test.script1"") it will be cached under ""test.test"" which seems even worse.

Is there any workaround other than creating my own cache for compiled classes?

As a fix in the groovy code, we could move the line 
{code}
String className = name.replace('/', '.');
{code}
to the beginning of the loadClass(String, boolean, boolean, boolean) method and using it instead of name in the entire method. GroovyResourceLoader will handle dots fine and className will be proper cache key then.

This would fix looking up scripts with '/' as package separator. It is quite possible this is not recommended way of loading scripts anyway. To fix it for dot separated script paths the problem lies I believe here (GroovyClassLoader):
{code}
    protected Class recompile(URL source, String className, Class oldClass) throws CompilationFailedException, IOException {
        if (source != null) {
            // found a source, compile it if newer
            if ((oldClass != null && isSourceNewer(source, oldClass)) || (oldClass == null)) {
                sourceCache.remove(className);
                return parseClass(source.openStream(), className);
            }
        }
        return oldClass;
    }
{code}
parseClass second argument is fileName and we pass dot-separated className here. I would change it to 
return parseClass(source.openStream(), className.replace('.','/'));",guillaume,mrembisz,Major,Closed,Fixed,24/Nov/10 09:53,14/Oct/11 00:28
Bug,GROOVY-4534,12815418,Thread.join appears to not work correctly in at least one special case,"While writing test code for a networking component I ran into the following issue. I created a thread to read from a socket, started it and then wrote to a client socket. After writing called thread.join() and expected it to finish executing my server socket thread before continuing. Instead what I get is non-deterministic behavior. Sometimes the thread finishes sometimes not. Since the thread isn't doing much if I add a sleep the code works (my work around in my test code). To be clear I'm not expecting that the print statements execute in any particular order - I'm concerned that the lines won't get added to the list and thus the current thread hasn't actually waited for the spawned thread to exit.

I've attached a sample groovy class that exhibits the problem and the equivalent Java code that does not have the issue.",roshandawrani,rnimz,Critical,Closed,Fixed,24/Nov/10 14:35,15/Dec/10 16:57
Bug,GROOVY-4545,12815426,Invalid closure name generated when containing method name has spaces,"{code}
import org.junit.Test

class ClosureProblem {
   @Test
   void 'not working'() {
       def cl = { true }
       println cl.class
   }
}
{code}

The above code fails with 
{noformat}
Caught: java.lang.ClassFormatError: Illegal class name ""ClosureProblem$_not working_closure1"" in class file ClosureProblem$_not working_closure1
{noformat}",roshandawrani,roshandawrani,Minor,Closed,Fixed,30/Nov/10 21:49,15/Dec/10 16:57
Bug,GROOVY-4546,12816833,Several bugs with @Category,"In the following code:
{code}
@Category(Integer)
class MyOps {
  def multiplesUpTo4() { [this * 2, this * 3, this * 4] }
  def multiplesUpTo(num) {
    (2..num).collect{ i -> this * i }
  }
  def alsoMultiplesUpTo(num) {
    def ans = (2..num).collect{ i -> this * i }
    ans
  }
//  def twice() { 
//    def (twice, thrice, quad) = multiplesUpTo4()
//    twice
//  }
}

use(MyOps) {
  println 21.multiplesUpTo4()       // => [42, 63, 84]
  println 21.multiplesUpTo(6)       // => [42, 63, 84, 105, 126]
//  println 21.alsoMultiplesUpTo(6) // (A)
//  println 21.twice()              // (B)
}
{code}
If the line marked (A) is uncommented, then it fails at runtime with
{noformat}
groovy.lang.MissingPropertyException: No such property: i for class: java.lang.Integer
{noformat}
Looking at the transformed code for {{alsoMultiplesUpTo}} as per the AST browser, it is easy to see why we get that error:
{code}
java.lang.Object ans = (2.. num).collect({ java.lang.Object i -> $this * $this.i })
{code}
Meanwhile the code for the nearly identical earlier method {{multiplesUpTo}} is as expected:
{code}
(2.. num).collect({ java.lang.Object i -> $this * i })
{code}
If the ""twice"" method is uncommented, then we get a {{ClassCastException}} at compile time.
Also, if you create the category long hand (i.e. not using the AST transform) both problems go away.
",paulk,paulk,Major,Closed,Fixed,30/Nov/10 23:02,07/Apr/15 19:06
Bug,GROOVY-4547,12815653,A few UI issues in AST Browser,"In AST Browser UI, if I change some parameters like Show Script flag (free form/class form) or Compile Phase and then press Refresh

1) There is no proper progress indicator of the work that is happening in the background in the non-UI threads. The hourglass also appears if I move the mouse on the small area that shows the AST tree. In most of the remaining screen, mouse pointer looks normal. A progress indicator is needed.

2) As soon as the refresh is pressed, the tree and property tables can be cleared as they take time to get populated and decompiled data comes up very fast and they look out-of-sync till the whole work is completed.",hamletdrc,roshandawrani,Minor,Closed,Fixed,01/Dec/10 00:08,10/Nov/11 03:07
Bug,GROOVY-4548,12815384,@Field cannot annotate a multiple assignment expression. throws a ClassCastException,"@Field cannot annotate a multiple assignment expression. throws a ClassCastException

This was found become because DeclarationExpression.getVariableExpression() was deprecated. 

A failing test case is: 
{code}
@groovy.transform.Field 
def (awe, awe2) = [[1, 2, 3], [1, 2, 3]]

def awesum() { awe.sum() }
assert awesum() == 6
{code}",paulk,hamlet.darcy@canoo.com,Major,Closed,Fixed,01/Dec/10 03:47,15/Dec/10 22:31
Bug,GROOVY-4549,12815911,Source locations dropped inside of StaticImportVisitor,"Inside the StaticImportVisitor, I would have expected that the PropertyExpressions generated by the visitor contain proper source locations.  There are several places where an existing ConstantExpression or VariableExpression is found to be a reference to a statically imported field.  In this case, a synthetic PropertyExpression is created.  The source location for the synthetic PropertyExpression is set to be the source location for the original Constant/Variable Expression, but the expression returned by PropertyExpression.getExpression() no longer has an sloc.

For example the script:

{code}

import static javax.swing.text.html.HTML.*

NULL_ATTRIBUTE_VALUE
{code}

after the StaticImportVisitor does its magic, the NULL_ATTRIBUTE_VALUE constant expression is converted into HTML.NULL_ATTRIBUTE_VALUE PropertyExpression.  The new PropertyExpression has the same sloc as the original NULL_ATTRIBUTE_VALUE expression, but the contained expressions do not have any slocs.

Note that this may be a problem for StaticMethodCalls as well, but so far I cannot reproduce it.

Proposed solution:

In StaticImportVisitor.java, create method:

{code}
    private void setSourcePosition(Expression toSet, Expression origNode) {
        toSet.setSourcePosition(origNode);
        if (toSet instanceof PropertyExpression) {
            ((PropertyExpression) toSet).getProperty().setSourcePosition(origNode);
        }
    }
{code}

And in this file replace all calls to ASTNode.setSourcePosition(ASTNode) with a call to the new setSourcePosition method.  If the ASTNode toSet is not a property expression, then no change in behavior occurs, but if it is, then slocs are set properly.

I have made this change to Groovy-Eclipse and everything is working fine.  It would be nice to see this contributed back to groovy core.
",guillaume,werdna,Major,Closed,Fixed,01/Dec/10 18:01,15/Dec/10 16:56
Bug,GROOVY-4550,12815395,Regression with @Immutable: @Immutable fields not recognised,An @Immutable class should be allowed to have fields whose type is annotated with @Immutable. I thought this was working but there doesn't seem to be a relevant unit test. Certainly the intention and JavaDoc was for this to work.,paulk,paulk,Major,Closed,Fixed,02/Dec/10 03:02,07/Apr/15 19:06
Bug,GROOVY-4551,12812028,Method Closure.curry() returns raw type,Now Closure implements Callable and has type parameter V. But method 'curry' returns raw Closure type and loses information about V.,paulk,mxm-groovy,Major,Closed,Fixed,02/Dec/10 04:13,17/Jun/15 20:09
Bug,GROOVY-4554,12812024,JavaStubGenerator doesn't play nicely with package-info.groovy files,It thinks they are a script. It should mostly leave them alone.,paulk,paulk,Major,Closed,Fixed,02/Dec/10 16:00,22/Apr/20 02:29
Bug,GROOVY-4555,12815419,Wrong return subtypes of collectEntries{},"{code}
class A {}
class B {}
Map<A, A> m1
Map<A, B> m2 = m1.collectEntries {
	[new A(), new B()]
}
{code}
As you can see {{m2}} gets assigned a Map<A,B> value, but DefaultGroovyMethods collectEntries says it should be assigned a Map<A,A> value:
{code}
public static <K, V> Map<K, V> collectEntries(Map<K, V> self, Closure closure) {
    return collectEntries(self, createSimilarMap(self), closure);
}
{code}
This bug starts troubles with smart editors like IDEA. Here's a link to related issue on their tracker: http://youtrack.jetbrains.net/issue/IDEA-62245",paulk,mojojojo,Major,Closed,Fixed,03/Dec/10 10:04,15/Dec/10 22:31
Bug,GROOVY-4561,12815432,VerifyError when debugging Groovy core tests,"For many test classes in package org.codehaus.groovy.classgen, I get a VerifyError when trying to debug them in IDEA 10. Examples:

org.codehaus.groovy.classgen.ForTest:

{noformat}
java.lang.VerifyError: (class: org/codehaus/groovy/ast/builder/AstBuilderTransformation, method: <clinit> signature: ()V) Attempt to split long or double on the stack
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2389)
	at java.lang.Class.getConstructor0(Class.java:2699)
	at java.lang.Class.newInstance0(Class.java:326)
	at java.lang.Class.newInstance(Class.java:308)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.addPhaseOperationsForGlobalTransforms(ASTTransformationVisitor.java:299)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.doAddGlobalTransforms(ASTTransformationVisitor.java:266)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.addGlobalTransforms(ASTTransformationVisitor.java:187)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.addPhaseOperations(ASTTransformationVisitor.java:150)
	at org.codehaus.groovy.control.CompilationUnit.<init>(CompilationUnit.java:184)
	at org.codehaus.groovy.control.CompilationUnit.<init>(CompilationUnit.java:113)
	at groovy.lang.GroovyClassLoader.createCompilationUnit(GroovyClassLoader.java:480)
	at groovy.lang.GroovyClassLoader.defineClass(GroovyClassLoader.java:177)
	at org.codehaus.groovy.classgen.TestSupport.loadClass(TestSupport.java:96)
	at org.codehaus.groovy.classgen.ForTest.testNonLoop(ForTest.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.junit3.JUnit3IdeaTestRunner.doRun(JUnit3IdeaTestRunner.java:109)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:65)
{noformat}

org.codehaus.groovy.classgen.CastTest:

{noformat}
Exception in thread ""main"" java.lang.VerifyError: (class: groovy/util/GroovyShellTestCase, method: <clinit> signature: ()V) Attempt to split long or double on the stack
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:169)
	at com.intellij.junit3.TestRunnerUtil.loadTestClass(TestRunnerUtil.java:152)
	at com.intellij.junit3.TestRunnerUtil.createClassOrMethodSuite(TestRunnerUtil.java:105)
	at com.intellij.junit3.TestRunnerUtil.getTestSuite(TestRunnerUtil.java:79)
	at com.intellij.junit3.JUnit3IdeaTestRunner.startRunnerWithArgs(JUnit3IdeaTestRunner.java:41)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:196)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:65)
{noformat}",blackdrag,pniederw,Critical,Closed,Fixed,05/Dec/10 07:12,10/Dec/10 13:52
Bug,GROOVY-4566,12815383,AbstractASTTransformation should not reference ImmutableASTTransformation,AbstractASTTransformation incorrectly references ImmutableASTTransformation. The provided patch fixes that.,hamletdrc,melix,Minor,Closed,Fixed,08/Dec/10 02:42,05/Apr/15 14:44
Bug,GROOVY-4567,12815434,AST browser decompiled source is missing parens for StaticMethodCallExpressions,"AST browser decompiled source is missing parens for StaticMethodCallExpressions

Check out the toString output for: 
{code}
@groovy.transform.ToString
class Person {
    String firstName, lastName
}{code}

It has this line: 
{code}_result.append(org.codehaus.groovy.runtime.InvokerHelper.toString lastName ){code}



when it should say this: 
{code}_result.append(org.codehaus.groovy.runtime.InvokerHelper.toString(lastName)){code}

I will fix this when I get home. ",hamletdrc,hamlet.darcy@canoo.com,Major,Closed,Fixed,08/Dec/10 07:14,15/Dec/10 16:57
Bug,GROOVY-4569,12815450,AST Browser throws exception for enum classes,"This code produces a stack overflow error in the ast browser of the trunk: 
{code}
enum MyEnum {
  FOO, 
  BAR; 
}
{code}
",hamletdrc,hamlet.darcy@canoo.com,Major,Closed,Fixed,09/Dec/10 01:23,10/Nov/11 03:05
Bug,GROOVY-4570,12815447,@ToString does not work for enum types,"@ToString does not appear to work for enums. This code does not cause a toString method to be generated and it should: 
{code}
@groovy.transform.ToString 
enum MyEnum {
  FOO('foo'), 
  BAR('bar'); 
  String prop
  MyEnum(String p) {prop = p }
}

println MyEnum.FOO.toString()
{code}

THere might be a larger issue with AST Transforms on Enums in general. See thread here: http://groovy.329449.n5.nabble.com/AST-transformations-for-enum-td3295912.html",paulk,hamlet.darcy@canoo.com,Major,Closed,Fixed,09/Dec/10 01:25,21/Jul/11 19:06
Bug,GROOVY-4573,12815381,null as boolean should be false,"{code}
def i = null
assert i.asBoolean() == false
{code}

should work as a test case but fails",,codevise,Major,Closed,Fixed,09/Dec/10 15:50,10/Dec/10 01:58
Bug,GROOVY-4578,12815394,static star ImportNode has invalid constructor?,"I came across something curious in the constructor for static star {{ImportNode}}{color:black}{color}s.  This constructor {{public ImportNode(ClassNode type)}} has the following doc comment:

{code}
    /**
     * Represent a static import of a Class, i.e. import static package.Classname.*
     *
     * @param type the referenced class
     */
{code}

Yet, inside the constructor, {{this.isStatic = false;}}.  This seems like a bug to me since the generated ImportNode is obviously supposed to be static.  Is there something I'm missing here?",paulk,werdna,Major,Closed,Fixed,10/Dec/10 17:45,15/Dec/10 16:56
Bug,GROOVY-4580,12815440,vmplugin throws NPE when trying to throw a GroovyBugException,"When running unit tests in IntelliJ I regularly get the following stacktrace:
{code}
java.lang.NullPointerException
	at org.codehaus.groovy.vmplugin.v5.Java5.configureType(Java5.java:95)
	at org.codehaus.groovy.vmplugin.v5.Java5.makeClassNode(Java5.java:374)
	at org.codehaus.groovy.vmplugin.v5.Java5.configureClassNode(Java5.java:320)
	at org.codehaus.groovy.ast.ClassNode.lazyClassInit(ClassNode.java:263)
	at org.codehaus.groovy.ast.ClassNode.getInterfaces(ClassNode.java:341)
	at org.codehaus.groovy.ast.ClassNode.declaresInterface(ClassNode.java:929)
	at org.codehaus.groovy.ast.ClassNode.implementsInterface(ClassNode.java:909)
	at org.codehaus.groovy.ast.ClassNode.isDerivedFromGroovyObject(ClassNode.java:899)
	at org.codehaus.groovy.classgen.AsmClassGenerator.isGroovyObject(AsmClassGenerator.java:2741)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitPropertyExpression(AsmClassGenerator.java:2717)
	at org.codehaus.groovy.ast.expr.PropertyExpression.visit(PropertyExpression.java:55)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitAndAutoboxBoolean(AsmClassGenerator.java:4111)
	at org.codehaus.groovy.classgen.AsmClassGenerator.makeCallSite(AsmClassGenerator.java:2175)
	at org.codehaus.groovy.classgen.AsmClassGenerator.makeCall(AsmClassGenerator.java:2012)
	at org.codehaus.groovy.classgen.AsmClassGenerator.makeCall(AsmClassGenerator.java:1998)
	at org.codehaus.groovy.classgen.AsmClassGenerator.makeInvokeMethodCall(AsmClassGenerator.java:1981)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:2335)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:72)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitAndAutoboxBoolean(AsmClassGenerator.java:4111)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:1458)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:165)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:729)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:51)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:616)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:591)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:686)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1039)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:276)
	at org.codehaus.groovy.control.CompilationUnit$11.call(CompilationUnit.java:738)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:932)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:509)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:487)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:464)
	at org.jetbrains.groovy.compiler.rt.GroovyCompilerWrapper.compile(GroovyCompilerWrapper.java:43)
	at org.jetbrains.groovy.compiler.rt.GroovycRunner.main(GroovycRunner.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.intellij.rt.execution.CommandLineWrapper.main(CommandLineWrapper.java:75)
{code}
This has already been reported to JetBrains IntelliJ team, as I think the initial bug is theirs, however, there should not be a NullPointerException here, but a GroovyBugException because it can't find the correct type.

The block of code looks like this:
{code}
   83       private ClassNode configureType(Type type) {
   84           if (type instanceof WildcardType) {
   85               return configureWildcardType((WildcardType) type);
   86           } else if (type instanceof ParameterizedType) {
   87               return configureParameterizedType((ParameterizedType) type);
   88           } else if (type instanceof GenericArrayType) {
   89               return configureGenericArray((GenericArrayType) type);
   90           } else if (type instanceof TypeVariable) {
   91               return configureTypeVariableReference((TypeVariable) type);
   92           } else if (type instanceof Class) {
   93               return configureClass((Class) type);
   94           } else {
   95               throw new GroovyBugError(""unknown type: "" + type + "" := "" + type.getClass());
   96           }
   97       }
{code}
on line 95 - I believe that type.getClass() is being called when 'type' is null, hence the NPE.",blackdrag,wirah,Major,Closed,Fixed,14/Dec/10 05:27,12/Apr/11 13:43
Bug,GROOVY-4584,12815403,VerifyError with nonstatic category references in static methods,"
when an inner class annotated with @Category is referenced from a static method,
it fails at runtime with a VerifyError

without the @Category annotation it works just fine,
so perhaps its something with the transformation


attached is a simple example",paulk,jpertino,Major,Closed,Fixed,16/Dec/10 09:07,08/Feb/11 23:25
Bug,GROOVY-4587,12815448,VerifyError after int optimizations,"The test case attached shows a very small piece of code which triggers a VerifyError.
",blackdrag,melix,Blocker,Closed,Fixed,17/Dec/10 03:18,27/Dec/10 15:18
Bug,GROOVY-4590,12815437,TestNG classes do not run within groovyConsole,"Following code:

{code}
import org.testng.annotations.Test;

class A{
    @Test
    void a(){
        println ""hello""
    }

}
{code}

leads to:
{code}
org.testng.TestNGException: 
Cannot find class in classpath: A
{code}

while changing import to:
{code}
import org.junit.Test
{code}

works as expected within groovyConsole and gives:
{code}
hello
JUnit 4 Runner, Tests: 1, Failures: 0, Time: 16
Result: org.junit.runner.Result@a42c89
{code}

Should TestNG classes not also run using groovyConsole?",,dariusan,Major,Closed,Fixed,20/Dec/10 06:44,28/Oct/15 07:07
Bug,GROOVY-4599,12815431,"Exception java.lang.VerifyError ""Expecting to find object/array on stack"" when overriding a notyped method with a typed method","This script works fine:
{code:title=|borderStyle=solid}
new Child()
abstract class Super {
    abstract boolean myMethod()
}
class Child extends Super {
    boolean myMethod() { true }
}
{code}

But it fails if I remove the ""boolean"" type to myMethod() in the Super class

{code:title=|borderStyle=solid}
new Child()
abstract class Super {
    abstract myMethod()
}
class Child extends Super {
    boolean myMethod() { true }
}
{code}

And throws

{noformat} 
Caught: java.lang.VerifyError: (class: Child, method: myMethod signature: ()Ljava/lang/Object;) Expecting to find object/array on stack
	at VerifyError.class$(VerifyError.groovy)
	at VerifyError.$get$$class$Child(VerifyError.groovy)
	at VerifyError.run(VerifyError.groovy:2)
{noformat} ",blackdrag,avilches,Minor,Closed,Fixed,27/Dec/10 17:30,16/Feb/11 07:59
Bug,GROOVY-4601,12816211,Stub generator doesn't escape strings with mixed single and double quotes,"Single and double quotes when mixed in Groovy strings within annotations are not translated correctly into Java strings in generated stubs. Internal quotes are not escaped.

This issue is related to GROOVY-4470. I used the same source code to demonstrate it.

{code:title=StringAnno.groovy}
@interface StringAnno {
  String val()
}
{code}

{code:title=StringAnnoUser.groovy}
@StringAnno(val = 'single quote string with ""double quote string""')
class StringAnnoUser {}
{code}


{code:title=StringAnnoUser.java (generated stub)}
import java.lang.*;
import java.io.*;
import java.net.*;
import java.util.*;
import groovy.lang.*;
import groovy.util.*;

@StringAnno(val=""single quote string with ""double quote string"""") public class StringAnnoUser
  extends java.lang.Object  implements
    groovy.lang.GroovyObject {
public StringAnnoUser
() {}
public  groovy.lang.MetaClass getMetaClass() { return (groovy.lang.MetaClass)null;}
public  void setMetaClass(groovy.lang.MetaClass mc) { }
public  java.lang.Object invokeMethod(java.lang.String method, java.lang.Object arguments) { return null;}
public  java.lang.Object getProperty(java.lang.String property) { return null;}
public  void setProperty(java.lang.String property, java.lang.Object value) { }
}
{code}
",guillaume,mgryszko,Major,Closed,Fixed,28/Dec/10 13:59,12/Mar/11 00:47
Bug,GROOVY-4604,12815438,Stub generator doesn't escape double quotes in String annotation values,"{code}
@StringAnn('are you ""really"" sure?')
class Foo {}
{code}

Generated stub looks like this:
{code}
@StringAnn(value=""are you ""really"" sure?"")
public class Foo {}
{code}",pniederw,pniederw,Major,Closed,Fixed,30/Dec/10 08:13,08/Mar/11 09:36
Bug,GROOVY-4607,12818026,Groovy doesn't correctly check for weaker access privileges when using inheritance,"The example below is rejected by Java but currently accepted by groovy:
{code}
abstract class Super {
    abstract myMethod()
}
class Child extends Super {
    protected myMethod() { true }
}
new Child()
{code}",paulk,paulk,Major,Closed,Fixed,30/Dec/10 20:46,07/Apr/15 19:07
Bug,GROOVY-4609,12815414,Using log variable created with @Log or @Log4j causes compiler error if used in static method,"If I have a class with it and use the 'log' variable in a static method I get a compilation error. 


Log4jTest.groovy:
import groovy.util.logging.Log4j

@Log4j
class TestLog4j {

  public static void main(String[] args) {
    log.info ""Hello World""
  }
}

Error Message:
Apparent variable 'log' was found in a static scope but doesn't refer to a local variable, static field or class. Possible causes:
You attempted to reference a variable in the binding or an instance variable from a static context.
You misspelled a classname or statically imported field. Please check the spelling.
You attempted to use a method 'log' but left out brackets in a place not allowed by the grammar. ",guillaume,krush,Major,Closed,Fixed,31/Dec/10 07:43,19/Oct/11 04:04
Bug,GROOVY-4610,12815415,GroovyInterceptable (AOP) not working with closures,"I've got a grails app with service classes that implement {{GroovyInterceptable}}: 
{code:groovy}
class customerService implements GroovyInterceptable { 
    private List<Customer> customers 
    def invokeMethod(String name, args) { 
        log.debug ""=======>INVOKING method [$name] with args:$args"" 
    } 
    void foo() { 
        customers.each { doSomething(it) } 
    } 
    void doSomething(Customer cust) {
        log.debug ""doSomething invoked with $cust""
    } 
} 
{code}

If I call foo() or doSomething() *directly* from another class, the invokeMethod gets called like it is supposed to. However, when foo() calls doSomething(), that call is not intercepted in invokeMethod. 

If I change from   
{code:groovy}
customers.each { doSomething(it) }
{code}
  to   
{code:groovy}
for(Customer cust: customers) { doSomething(cust) }
{code}
then the invokeMethod gets called just fine. 

There appears to be an {{ExpandoMetaClass}} bug that prevents closures and {{GroovyInterceptable}} from working together.",emilles,blusynergy,Major,Closed,Fixed,31/Dec/10 13:55,03/Feb/22 22:34
Bug,CASSANDRA-663,12444552,Abort bootstrap if our IP is already in the token ring,Trying to bootstrap a node w/ the same IP as one that is Down but not removed from the ring should give an error.,jbellis,jbellis,Normal,Resolved,Fixed,04/Jan/10 01:31,16/Apr/19 09:33
Bug,CASSANDRA-668,12444649,long commitlog syncs can cause write pauses,"on a heavily loaded system (deliberately exacerbated by running the bonnie++ i/o benchmarking tool at the same time as cassandra -- which might not be too far off from the environment you would see on some VPS hosts), we're seeing CL sync times of 1-5s, causing write pauses.",jbellis,jbellis,Low,Resolved,Fixed,05/Jan/10 02:29,16/Apr/19 09:33
Bug,CASSANDRA-669,12444719,Handle mmapping index files greater than 2GB,"""Who would ever have an index file larger than 2GB?"" I thought.  Turns out it's not that hard with narrow rows... :)",jbellis,jbellis,Normal,Resolved,Fixed,05/Jan/10 15:57,16/Apr/19 09:33
Bug,CASSANDRA-673,12444764,bootstrapping does not work properly using multiple key space,"when use multiple key-spaces, and one key-space has no SSTable, then bootstrap may not work right.
Say nodes A, B, C, D have key spaces ""KS1"" and ""KS2"", KS1 is empty, now add empty node E into cluster.
Suppose E decide to drag data from A and B(bootstrap source), E will send range to A and B, and A, B will scan all key-spaces they got and send ack to E, which contains list of key-space name(StreamContextManager.StreamContext),
when E get ack from A and B, it scan this list, but when encounter first empty key-space, it will stop and remove node from bootstrap sources list:

StreamInitiateVerbHandler.doVerb:
......
                if (streamContexts.length == 0 && StorageService.instance().isBootstrapMode())
                {
                    if (logger.isDebugEnabled())
                        logger.debug(""no data needed from "" + message.getFrom());
                    StorageService.instance().removeBootstrapSource(message.getFrom());
                    return;
                }
......
If list of bootstrap sources is empty, E will finish bootstrapping

So, the result is: E get nothing from source A, B, even KS2 has lots of data.
",jaakko,steel_mental,Low,Resolved,Fixed,06/Jan/10 03:14,16/Apr/19 09:33
Bug,CASSANDRA-677,12444836,cassandra-cli doesn't allow hyphens in hostnames,"It's not possible to use a hostname that contains a hyphen with the ""connect"" command interactively, (the parser does not accept hostnames that contain hyphens).

Note: It is still possible to connect to such hosts by passing it on the command line using -host.",rschildmeijer,urandom,Low,Resolved,Fixed,06/Jan/10 20:40,16/Apr/19 09:33
Bug,CASSANDRA-680,12444868,hinted handoff reads all hints for a single keyspace into memory,Need to add paging to HHOM.deliverAllHints,jbellis,jbellis,Low,Resolved,Fixed,07/Jan/10 05:12,16/Apr/19 09:33
Bug,CASSANDRA-681,12444942,Error deleting files during bootstrap,"I started a 3 node cluster and proceeded to bootstrap a 4th node.  On one of the existing nodes I began to see tracebacks like this:

ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: Unable to delete /mnt/drive3/data/Keyspace1/stream/Standard1-158-Index.db after 10 tries
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:53)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.io.IOException: Unable to delete /mnt/drive3/data/Keyspace1/stream/Standard1-158-Index.db after 10 tries
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:13)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more
Caused by: java.io.IOException: Unable to delete /mnt/drive3/data/Keyspace1/stream/Standard1-158-Index.db after 10 tries
        at org.apache.cassandra.io.DeletionService$2.runMayThrow(DeletionService.java:45)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:9)
        ... 6 more

For various data, index, and filter files.",jbellis,brandon.williams,Low,Resolved,Fixed,07/Jan/10 17:44,16/Apr/19 09:33
Bug,CASSANDRA-689,12445261,Multi-get slice failing Nullpointer Exception,"Noticed this in trunk

ERROR [pool-1-thread-40] 2010-01-11 22:13:55,333 Cassandra.java (line 960) Internal error processing multiget_slice
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:510)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:375)
        at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:81)
        at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:164)
        at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:237)
        at org.apache.cassandra.service.CassandraServer.multiget_slice(CassandraServer.java:209)
        at org.apache.cassandra.service.Cassandra$Processor$multiget_slice.process(Cassandra.java:952)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:842)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:506)
        ... 11 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.filter.SliceQueryFilter.filterSuperColumn(SliceQueryFilter.java:70)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:809)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:750)
        at org.apache.cassandra.db.Table.getRow(Table.java:398)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:59)
        at org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:691)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
",jbellis,lenn0x,Normal,Resolved,Fixed,12/Jan/10 06:36,16/Apr/19 09:33
Bug,CASSANDRA-694,12445429,Failure to flush commit log,"The following exception occurs consistently on at least node (note did not occur on other same-configured nodes) during startup:

INFO - Replaying /var/lib/cassandra/commitlog/CommitLog-1262855754427.log, /var/lib/cassandra/commitlog/CommitLog-1262832689989.log, /var/lib/cassandra/commitlog/CommitLog-1262885833186.log, /var/lib/cassandra/commitlog/CommitLog-1262900845019.log, /var/lib/cassandra/commitlog/CommitLog-1262913267844.log, /var/lib/cassandra/commitlog/CommitLog-1262927898170.log, /var/lib/cassandra/commitlog/CommitLog-1262961421039.log, /var/lib/cassandra/commitlog/CommitLog-1262977175175.log, /var/lib/cassandra/commitlog/CommitLog-1262989588783.log, /var/lib/cassandra/commitlog/CommitLog-1263000573676.log, /var/lib/cassandra/commitlog/CommitLog-1263013691393.log, /var/lib/cassandra/commitlog/CommitLog-1263044706108.log, /var/lib/cassandra/commitlog/CommitLog-1263060004191.log, /var/lib/cassandra/commitlog/CommitLog-1263071446342.log, /var/lib/cassandra/commitlog/CommitLog-1263082950154.log, /var/lib/cassandra/commitlog/CommitLog-1263095400814.log, /var/lib/cassandra/commitlog/CommitLog-1263118331046.log, /var/lib/cassandra/commitlog/CommitLog-1263143402963.log, /var/lib/cassandra/commitlog/CommitLog-1263155294308.log, /var/lib/cassandra/commitlog/CommitLog-1263166154352.log, /var/lib/cassandra/commitlog/CommitLog-1263178359247.log, /var/lib/cassandra/commitlog/CommitLog-1263202112017.log, /var/lib/cassandra/commitlog/CommitLog-1263230932274.log, /var/lib/cassandra/commitlog/CommitLog-1263250726505.log, /var/lib/cassandra/commitlog/CommitLog-1263264159438.log, /var/lib/cassandra/commitlog/CommitLog-1263289964249.log, /var/lib/cassandra/commitlog/CommitLog-1263317974387.log, /var/lib/cassandra/commitlog/CommitLog-1263331989090.log, /var/lib/cassandra/commitlog/CommitLog-1263344147667.log, /var/lib/cassandra/commitlog/CommitLog-1263359751527.log, /var/lib/cassandra/commitlog/CommitLog-1263395707008.log, /var/lib/cassandra/commitlog/CommitLog-1263397833524.log, /var/lib/cassandra/commitlog/CommitLog-1263398736183.log, /var/lib/cassandra/commitlog/CommitLog-1263399753707.log, /var/lib/cassandra/commitlog/CommitLog-1263401667504.log, /var/lib/cassandra/commitlog/CommitLog-1263404640782.log, /var/lib/cassandra/commitlog/CommitLog-1263405827234.log, /var/lib/cassandra/commitlog/CommitLog-1263406901115.log
INFO - LocationInfo has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(LocationInfo)@25934689
INFO - HintsColumnFamily has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(HintsColumnFamily)@4766820
INFO - AdXRequestStatistics has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(AdXRequestStatistics)@21521158
INFO - TokenGoogleIDCF has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(TokenGoogleIDCF)@22889075
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.AssertionError: Blocking serialized executor is not yet implemented
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:84)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:1045)
        at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:395)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:448)
        at org.apache.cassandra.db.Table.flush(Table.java:464)
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:397)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:65)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:90)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:135)
        ... 5 more

And the same exception occurs intermittently on other node (running) nodes during 'nodeprobe flush':

root@domU-12-31-38-00-26-31:~# nodeprobe -host localhost -port 8080 flush Logger
Exception in thread ""main"" java.lang.AssertionError: Blocking serialized executor is not yet implemented
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:84)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:1045)
        at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:395)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:448)
        at org.apache.cassandra.service.StorageService.forceTableFlush(StorageService.java:984)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",jbellis,ryandaum,Normal,Resolved,Fixed,13/Jan/10 19:11,16/Apr/19 09:33
Bug,CASSANDRA-695,12445431,Gossiper convicts same node over and over.,"I can reproduce this in trunk or 0.5.  It is quite easy to see with logging set to DEBUG.
1.  Bring up several nodes.
2.  Kill one of them.

Gossip still continues to examine the failed node because it is in the ring, but gets convicted over and over at every check (see FailureDetector.interpret).  If this is expected, we should consider lowering the debug statement in MessagingService.convict to TRACE.",gdusbabek,gdusbabek,Low,Resolved,Fixed,13/Jan/10 19:35,16/Apr/19 09:33
Bug,CASSANDRA-696,12445444,Bootstrapping doesn't work on new clusters,"This is an edge case.

1. start a clean 3 node cluster with autobootstrap on.
2. load some data.
3. bootstrap in a 4th node.

the logs in the 4th node will indicate that data was not received.  If you restart the cluster in between steps 1 and 2, or 2 and 3, boot strapping works fine.  

I find that waiting on the table flush when making the streaming request solves the problem (see patch).",gdusbabek,gdusbabek,Low,Resolved,Fixed,13/Jan/10 22:19,16/Apr/19 09:33
Bug,CASSANDRA-703,12445593,"Insert, Delete and Insert into a column family doesnt work... ","Here is the code to reproduce the issue...

        ColumnPath colpath = new ColumnPath().setColumn_family(""VERSIONS"").setSuper_column(""123"".getBytes()).setColumn(""1234"".getBytes());
        con.insert(""WBXCDOCUMENT"", ""vijay"", colpath, ""test"".getBytes(), System.currentTimeMillis(), 2);

        ColumnPath path = new ColumnPath().setColumn_family(""VERSIONS"").setSuper_column(""123"".getBytes());
        con.remove(""WBXCDOCUMENT"", ""vijay"", path, System.currentTimeMillis(), 2);

        con.insert(""WBXCDOCUMENT"", ""vijay"", colpath, ""test"".getBytes(), System.currentTimeMillis(), 2);

        ColumnOrSuperColumn col = con.get(""WBXCDOCUMENT"", ""vijay"", path, 2);
        assertEquals(col.getSuper_column().getColumns() != null, true);

Expected result, get the column family..... but it throws notfound exception which is wrong.",jbellis,vijay2win@yahoo.com,Low,Resolved,Fixed,15/Jan/10 04:35,16/Apr/19 09:33
Bug,CASSANDRA-711,12445854,Some Thrift Exceptions not passed down to Client,"We still don't pass all exceptions down to client via Thrift. We have seen a few of these when working on our client library:

org.apache.thrift.protocol.TProtocolException: Required field 'start' was not present! Struct: SliceRange(start:null, finish:null, reversed:false, count:100)

Would be good if those exceptions were passed down, instead of 'TSocket Read 0 Bytes'.
",jbellis,lenn0x,Low,Resolved,Fixed,18/Jan/10 20:16,16/Apr/19 09:33
Bug,CASSANDRA-715,12445884,"HHOM goes into infinite loop, wasting cpu","To replicate: take a host down, cause hints to it, wait for HHOM to kick in

The issue is line 201 of HHOM:
startColumn = keyColumn.name(); // repeating the last as the first is fine since we just deleted it

That comment is false.  The column may not have been deleted, since the endpoint could still be down.  This causes HHOM to go into an infinite loop trying to deliver hints to a down host.",jbellis,brandon.williams,Normal,Resolved,Fixed,19/Jan/10 05:03,16/Apr/19 09:33
Bug,CASSANDRA-716,12445889,bootstrapping does not work properly using multiple DataFileDirectory,"I was adding a new machine A which has 2 DataFileDirectories into the ring. The A will throw exception while bootstrapping.
DEBUG [MESSAGING-SERVICE-POOL:4] 2010-01-19 11:43:32,837 ContentStreamState.java (line 88) Removing stream context /home/store0/data/pic/raw_data-tmp-1-Data.db:209833142
 WARN [MESSAGING-SERVICE-POOL:4] 2010-01-19 11:43:32,837 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local
=/10.81.37.65:7000 remote=/10.81.42.26:10418]
 WARN [MESSAGING-SERVICE-POOL:4] 2010-01-19 11:43:32,837 TcpConnection.java (line 485) Exception was generated at : 01/19/2010 11:43:32 on thread MESSAGING-SERVICE-POOL:4
java.io.IOException: rename failed of /home/store0/data/pic/raw_data-1-Filter.db
java.io.IOError: java.io.IOException: rename failed of /home/store0/data/pic/raw_data-1-Filter.db
        at org.apache.cassandra.io.SSTableWriter.rename(SSTableWriter.java:154)
        at org.apache.cassandra.io.SSTableWriter.renameAndOpen(SSTableWriter.java:162)
        at org.apache.cassandra.io.Streaming$StreamCompletionHandler.onStreamCompletion(Streaming.java:284)
        at org.apache.cassandra.net.io.ContentStreamState.handleStreamCompletion(ContentStreamState.java:108)
        at org.apache.cassandra.net.io.ContentStreamState.read(ContentStreamState.java:90)
        at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:96)
        at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:445)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: rename failed of /home/store0/data/pic/raw_data-1-Filter.db
        at org.apache.cassandra.utils.FBUtilities.renameWithConfirm(FBUtilities.java:306)
        at org.apache.cassandra.io.SSTableWriter.rename(SSTableWriter.java:150)
        ... 9 more


I traced the exception and maybe found the reason.
StreamInitiateVerbHandler::doVerb() will create 3 temporary files(index, filter, data) for each ssTable. The name for each file is generated by getNewFileNameFromOldContextAndNames(). This method will generate a file name and a path for each ssTable, but the path is generated with DatabaseDescriptor.getDataFileLocationForTable() which will return different path for ech call when we configure multi-DataFileDirectory. 
eg: the ssTable raw_data-1 may have 3 temporary files : 
/home/store0/data/pic/raw_data-tmp-1-Index.db
/home/store1/data/pic/raw_data-tmp-1-Filter.db
/home/store0/data/pic/raw_data-tmp-1-Data.db

After receiving all data, StreamCompletionHandler::onStreamCompletion() will rename all temporary files and this method think all ssTable files will have the same path as data.db file. 
            if (streamContext.getTargetFile().contains(""-Data.db""))
            {
               ......
                try
                {
                    SSTableReader sstable = SSTableWriter.renameAndOpen(streamContext.getTargetFile());
                    ......
                }
                ......
            }
Then the renameAndOpen() will throw that exception.



",gdusbabek,david.pan,Normal,Resolved,Fixed,19/Jan/10 06:55,16/Apr/19 09:33
Bug,CASSANDRA-717,12445948,register AES verbs at SS start,"the reason we do all registration in one place is it prevents bugs like this one

ERROR - Error in ThreadPoolExecutor
java.lang.AssertionError: unknown verb TREE-RESPONSE-VERB
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR - Fatal exception in thread Thread[AE-SERVICE-STAGE:1,5,main]
java.lang.AssertionError: unknown verb TREE-RESPONSE-VERB
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",jbellis,jbellis,Low,Resolved,Fixed,19/Jan/10 17:15,16/Apr/19 09:33
Bug,CASSANDRA-719,12445969,fix failing CompactionsPurgeTest,,jbellis,jbellis,Normal,Resolved,Fixed,19/Jan/10 21:28,16/Apr/19 09:33
Bug,CASSANDRA-722,12445995,batch insert failing with TokenMetadata AssertionError,"I get this during the course of an insert.

ERROR [pool-1-thread-17] 2010-01-20 03:50:40,517 Cassandra.java (line 1096) Internal error processing batch_insert
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:212)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:129)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
        at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1183)
        at org.apache.cassandra.service.StorageProxy.insert(StorageProxy.java:101)
        at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:470)
        at org.apache.cassandra.service.CassandraServer.batch_insert(CassandraServer.java:445)
        at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:1088)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)",jaakko,dispalt,Normal,Resolved,Fixed,20/Jan/10 04:38,16/Apr/19 09:33
Bug,CASSANDRA-724,12446007,Insert/Get Contention,"We tried out the socket io patch in CASSANDRA-705, tested the latest JVM of b18 for 1.6. Still seeing very strange insert times. We see this with get_slices as well but it's easy to reproduce with batch_insert. I wonder if its related to Memtable contention, it's pretty easy to see the slow times when you restart the test script attached. We are running this on a 7 node cluster, <1% cpu. Consistency Level of 1.

Results
---------------------
Slow insert test.10882 0.203548192978
Slow insert test.18005 0.203876972198
Slow insert test.21154 0.204496860504
Slow insert test.22054 0.0444049835205
Slow insert test.26445 0.201545000076",jbellis,lenn0x,Normal,Resolved,Fixed,20/Jan/10 08:26,16/Apr/19 09:33
Bug,CASSANDRA-729,12446189,Bug in count columns.,"same as thrift api (get_count).

Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
cassandra> connect localhost/9160
Connected to localhost/9160
cassandra> del Keyspace1.Standard1['1']
row removed.
cassandra> set Keyspace1.Standard1['1']['foo'] = 'foo value'
Value inserted.
cassandra> set Keyspace1.Standard1['1']['bar'] = 'bar value'
Value inserted.
cassandra> get Keyspace1.Standard1['1']
=> (column=foo, value=foo value, timestamp=1264043095206)
=> (column=bar, value=bar value, timestamp=1264043106184)
Returned 2 results.
cassandra> count Keyspace1.Standard1['1']
2 columns
cassandra> del Keyspace1.Standard1['1']['foo']
column removed.
cassandra> get Keyspace1.Standard1['1']       
=> (column=bar, value=bar value, timestamp=1264043106184)
Returned 1 results.
cassandra> count Keyspace1.Standard1['1']     
2 columns
cassandra>",,gasolwu,Low,Resolved,Fixed,21/Jan/10 03:14,16/Apr/19 09:33
Bug,CASSANDRA-731,12446281,a few insert operations failed while bootstrapping,"I inserted 10000 key/value while bootstrapping and found 2 insert operations failed.

DEBUG [pool-1-thread-63] 2010-01-20 17:01:57,033 StorageProxy.java (line 225) insert writing key 15530 to 10981@/10.81.37.65
ERROR [pool-1-thread-46] 2010-01-20 17:01:57,033 Cassandra.java (line 1064) Internal error processing insert
java.lang.AssertionError
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:157)
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1178)
at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
at org.apache.cassandra.service.CassandraServer.insert(CassandraServer.java:417)
at org.apache.cassandra.service.Cassandra$Processor$insert.process(Cassandra.java:1056)
at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
at java.lang.Thread.run(Thread.java:619)
ERROR [pool-1-thread-44] 2010-01-20 17:01:57,033 Cassandra.java (line 1064) Internal error processing insert
java.lang.AssertionError
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:157)
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1178)
at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
at org.apache.cassandra.service.CassandraServer.insert(CassandraServer.java:417)
at org.apache.cassandra.service.Cassandra$Processor$insert.process(Cassandra.java:1056)
at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
at java.lang.Thread.run(Thread.java:619)


I traced the code and found the following assertion failed :
/* org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(Collection<InetAddress>) */
assert map.size() == targets.size(); 

The following reasons caused this issue:
1) targets is a list , not a map, as a result there may be some duplicated IP.
2) The following codes are not atomic :
org.apache.cassandra.service.StorageService.handleStateNormal(InetAddress, String)
        tokenMetadata_.updateNormalToken(token, endPoint);
        calculatePendingRanges();

 That's to say the IP may be both in the naturalEndpoints and pendingRanges.

eg : 
targets is IPa, IPb, IPc, IPa; (size = 4)
then, the map will be IPa, IPb, IPc. (size = 3)
as a result, assert failed.







",jaakko,david.pan,Normal,Resolved,Fixed,22/Jan/10 02:42,16/Apr/19 09:33
Bug,CASSANDRA-734,12446409,Table.open has a broken lock in it,Table.open's lock is used around the Map#put method call but not the #get. This makes it a source of spurious bugs. The attached patch synchronizes the entire Table.open method and removes the unused createLock static.,jbellis,jmhodges,Low,Resolved,Fixed,23/Jan/10 03:52,16/Apr/19 09:33
Bug,CASSANDRA-737,12446594,Streaming code relies on sockets being bound to the correct address (InetAddress.anyLocalAddress() is bad),"I came across this while testing streaming locally.  The new streaming code makes use of the remote socket address supplied by the socket.  This means that it will return whatever address the socket is bound to, which is not necessarily the address configured for cassandra.  This confuses StreamContextManager when data comes streaming in from addresses that it doesn't recognize.

Two solutions will work.
1. bind outgoing sockets to the correct interface.
2. Include the local address in StreamContexts that get sent.

I opted for 1 since it required less code.  2 was easy enough but would have required changing the format of the message to make the source address more easily accessible (the constructor for IncomingStreamReader wants to know the source host to create the stream context at the destination).",gdusbabek,gdusbabek,Low,Resolved,Fixed,25/Jan/10 19:46,16/Apr/19 09:33
Bug,CASSANDRA-738,12446624,cassandra-cli parsing error,"Steps to reproduce:
1. Download the 0.5 release
2. Start Cassandra
3. Start cassandra-cli
4. Execute ""set foo.bar['toot']='balls'""

Expected output:
An error message telling me I'm not doing it right.

Actual output:
cassandra> set foo.bar['toot']='balls'
Exception in thread ""main"" java.lang.AssertionError: serious parsing error (this is a bug).
	at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:367)
	at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:63)
	at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:131)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:172)

Perhaps this is related to https://issues.apache.org/jira/browse/CASSANDRA-615 in a non-direct way.",urandom,johanhil,Low,Resolved,Fixed,26/Jan/10 01:21,16/Apr/19 09:33
Bug,CASSANDRA-739,12446636,multiget returns empty ColumnOrSuperColumn instead of null,"the later is more intuitive, and the former violates the rule that COSC should have exactly one of {column, super_column} set.",jbellis,jbellis,Low,Resolved,Fixed,26/Jan/10 05:03,16/Apr/19 09:33
Bug,CASSANDRA-744,12446705,[multi_]get_count should take a SlicePredicate,"both to make it more flexible, and to emphasize that counting ""everything"" is as bad as slicing it",slebresne,jbellis,Low,Resolved,Fixed,26/Jan/10 17:53,16/Apr/19 09:33
Bug,CASSANDRA-750,12447060,avoid setting up completion handler for no-op stream in non-bootstrap mode,,jbellis,jbellis,Low,Resolved,Fixed,29/Jan/10 23:05,16/Apr/19 09:33
Bug,CASSANDRA-757,12455173,FatClient removal causes ConcurrentModificationException,"After using a fatclient and killing it, I later receive this ST on all nodes:

 INFO 16:04:58,999 FatClient /10.242.4.13 has been silent for 3600000ms, removing from gossip
ERROR 16:04:58,999 Fatal exception in thread Thread[Timer-1,5,main]
java.lang.RuntimeException: java.util.ConcurrentModificationException
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:96)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)
Caused by: java.util.ConcurrentModificationException
        at java.util.Hashtable$Enumerator.next(Hashtable.java:1048)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:382)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:90)
        ... 2 more
",brandon.williams,brandon.williams,Low,Resolved,Fixed,03/Feb/10 13:33,16/Apr/19 09:33
Bug,CASSANDRA-761,12455240,Binary Memtable does not invalidate cache,"If you have RowCache turned on for your CF, and you do a BMT import, rows are not invalidated in cache until you restart the node.",jbellis,lenn0x,Normal,Resolved,Fixed,03/Feb/10 23:00,16/Apr/19 09:33
Bug,CASSANDRA-762,12455243,Load balancing does not account for the load of the moving node,"Given a node A (with load 10 gb) and a node B (with load 20 gb), running the loadbalance command against node A will:
1. Remove node A from the ring
  * Recalculates pending ranges so that node B is responsible for the entire ring
2. Pick the most loaded node
  * node B is still reporting 20 gb load, because that is all it has locally
3. Choose a token that divides the range of the most loaded node in half

Since the token calculation doesn't take into account the load that node B is 'inheriting' from node A, the token will divide node B's load in half and swap the loads. Instead, the token calculation needs to pretend that B has already inherited the 10 gb from node A, for a total of 30 gb. The token that should be chosen falls at 15 gb of the total load, or 5 gb into node B's load.",stuhood,stuhood,Low,Resolved,Fixed,03/Feb/10 23:20,16/Apr/19 09:33
Bug,CASSANDRA-763,12455276,getRangeSlice returns keys from outside the desired range,,jbellis,jbellis,Normal,Resolved,Fixed,04/Feb/10 06:35,16/Apr/19 09:33
Bug,CASSANDRA-766,12455377,"SSTableExport can not accept -f , -k options","the SSTableExport command can not accept -f , -k options correct, always said as bellow:

[root@hfdevcasda01 bin]# ./sstable2json -f out.json /opt/cassandra-wbx/data/CONTENT_HF/ChangeHistory-2-Data.db
You must supply exactly one sstable
Usage: org.apache.cassandra.tools.SSTableExport [-f outfile] <sstable> [-k key [-k key [...]]]",jbellis,santal,Low,Resolved,Fixed,05/Feb/10 01:43,16/Apr/19 09:33
Bug,CASSANDRA-770,12455490,JMX RowCache requests also include writes,"I have a CF that I only write to, unless I manually query it.  I have observed the RowCache request count increasing on this CF in line with my writes, but have never actually queried it.",jbellis,brandon.williams,Low,Resolved,Fixed,05/Feb/10 20:30,16/Apr/19 09:33
Bug,CASSANDRA-772,12455525,DeletingReferences not created for existing sstables on startup,Trunk does not initialize SSTableTracker properly.,stuhood,stuhood,Normal,Resolved,Fixed,06/Feb/10 06:30,16/Apr/19 09:33
Bug,CASSANDRA-778,12455667,Gossiper thread deadlock,"Found this while attempting to bootstrap a node with more than a trivial amount of data:

Found one Java-level deadlock:
=============================
""GMFD:1"":
  waiting to lock monitor 0x0000000100861d60 (object 0x00000001066a7ed8, a org.apache.cassandra.service.StorageService),
  which is held by ""main""
""main"":
  waiting to lock monitor 0x0000000100860710 (object 0x0000000106c7c968, a org.apache.cassandra.gms.Gossiper),
  which is held by ""GMFD:1""

Java stack information for the threads listed above:
===================================================
""GMFD:1"":
	at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:226)
	- waiting to lock <0x00000001066a7ed8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:634)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:502)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:445)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:812)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:607)
	at org.apache.cassandra.gms.Gossiper.handleNewJoin(Gossiper.java:582)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:649)
	- locked <0x0000000106c7c968> (a org.apache.cassandra.gms.Gossiper)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1061)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
""main"":
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:861)
	- waiting to lock <0x0000000106c7c968> (a org.apache.cassandra.gms.Gossiper)
	at org.apache.cassandra.service.StorageService.startBootstrap(StorageService.java:347)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:318)
	- locked <0x00000001066a7ed8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:99)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:174)

Found 1 deadlock.

main acquires SS lock and doesn't release it before attempting to acquire the Gossiper lock.  Meanwhile, the gossip stage acquires the Gossiper lock and then attempts to acquire the SS lock.

Solution is to have finer-grained locking on the resource in SS (map of replication strategies), or to move the collection to a different class (DD maybe?).  This was introduced in CASSANDRA-620.",gdusbabek,gdusbabek,Normal,Resolved,Fixed,08/Feb/10 18:49,16/Apr/19 09:33
Bug,CASSANDRA-779,12455685,Bootstrapping is not threadsafe,"The bootstrapper thread (called from the main thread which has acquired the lock for SS via SS.init) currently makes a few calls into SS that require its lock.

Those methods need to be thread-safe, but do not need the same lock required by SS.init.",gdusbabek,gdusbabek,Normal,Resolved,Fixed,08/Feb/10 21:21,16/Apr/19 09:33
Bug,CASSANDRA-781,12455694,"in a cluster, get_range_slice() does not return all the keys it should","get_range_slice() does not return the same set of keys as get_key_range() in 0.5.0 final.

I posted a program to reproduce the behavior:

http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg01474.html

Apparently, you must have more than one node to get the behavior. Also, it may depend on the locations of the nodes on the ring.. I.e., if you don't generate enough keys randomly, then by chance they could all fall on the same host and you might not see the behavior, although I was able to get it to happen using only 2 nodes and 10 keys.

Here are the other emails describing the issue:

http://www.mail-archive.com/cassandra-user@incubator.apache.org/msg02423.html
",jbellis,bjc,Normal,Resolved,Fixed,08/Feb/10 22:32,16/Apr/19 09:33
Bug,CASSANDRA-787,12456082,NPE in DatacenterShardStatergy,There is a long pending fix to contribute back... Plz find the patch.,vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,11/Feb/10 23:31,16/Apr/19 09:33
Bug,CASSANDRA-790,12456197,SSTables limited to (2^31)/15 keys,"The current BloomFilter implementation requires a BitSet of (bucket_count * num_keys) in size, and that calculation is currently performed in an integer, which causes overflow for around 140 million keys in one SSTable.

Short term fix: perform the calculation in a long, and cap the value to the maximum size of a BitSet.
Long term fix: begin partitioning BitSets, perhaps using Linear Bloom Filters.",stuhood,stuhood,Urgent,Resolved,Fixed,12/Feb/10 22:27,16/Apr/19 09:33
Bug,CASSANDRA-795,12456333,Streaming broken on windows (FileStreamTask.CHUNK_SIZE is too big).,Setting chunk size smaller addresses the problem.  We should explore setting SO_SNDBUF higher to see if that fixes the problem.,gdusbabek,gdusbabek,Low,Resolved,Fixed,15/Feb/10 15:32,16/Apr/19 09:33
Bug,CASSANDRA-796,12456366,binary artifacts need ivy for dependencies,"Currently, if you generate a release, the binary artifact is missing required dependencies and the means to obtain them. The patch that follows a) copies build.xml and ivy.xml into the bin artifact and b) makes build.xml smart enough to tell when it's being run from a binary artifact.

This would also require updating all installation and quickstart documentation as well (the patch updates README.txt).",urandom,urandom,Low,Resolved,Fixed,15/Feb/10 22:18,16/Apr/19 09:33
Bug,CASSANDRA-797,12456369,cassandra-cli.bat batch file not passing arguments onto the main class,"If you run the following command in windows the arguments will not get passed to the CliMain class.

E:\Working\apache-cassandra-incubating-trunk>bin\cassandra-cli.bat --host 127.0.0.1 --port 9160
",wolfeidau,wolfeidau,Normal,Resolved,Fixed,15/Feb/10 23:23,16/Apr/19 09:33
Bug,CASSANDRA-800,12456474,Spurious Gossip Up/Down and IO Errors,"We're seeing a lot of nodes flapping. It appears to possibly be a race condition in Gossip.

on 10.209.23.110

WARN [MESSAGING-SERVICE-POOL:2] 2010-02-13 01:18:22,976 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.209.23.110:7000 remote=/10.209.23.80:52720]
WARN [MESSAGING-SERVICE-POOL:1] 2010-02-13 01:18:22,976 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.209.23.110:7000 remote=/10.209.23.80:36128]
 WARN [MESSAGING-SERVICE-POOL:2] 2010-02-13 01:18:22,977 TcpConnection.java (line 485) Exception was generated at : 02/13/2010 01:18:22 on thread MESSAGING-SERVICE-POOL:2
Reached an EOL or something bizzare occured. Reading from: /10.209.23.80 BufferSizeRemaining: 16
java.io.IOException: Reached an EOL or something bizzare occured. Reading from: /10.209.23.80 BufferSizeRemaining: 16
    at org.apache.cassandra.net.io.StartState.doRead(StartState.java:44)
    at org.apache.cassandra.net.io.ProtocolState.read(ProtocolState.java:39)
    at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:95)
    at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:445)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)


on 10.209.23.80 about the same time


ERROR [pool-1-thread-4751] 2010-02-13 01:17:12,261 Cassandra.java (line 1096) Internal error processing batch_insert
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:848)
    at java.util.HashMap$KeyIterator.next(HashMap.java:883)
    at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
    at java.util.HashSet.<init>(HashSet.java:100)
    at org.apache.cassandra.gms.Gossiper.getLiveMembers(Gossiper.java:173)
    at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:120)
    at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:78)
    at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1186)
    at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
    at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
    at org.apache.cassandra.service.CassandraServer.batch_insert(CassandraServer.java:445)
    at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:1088)
    at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)


just before that:

INFO [Timer-1] 2010-02-13 01:17:12,070 Gossiper.java (line 194) InetAddress /10.209.21.223 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,257 Gossiper.java (line 194) InetAddress /10.209.21.217 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,257 Gossiper.java (line 194) InetAddress /10.209.21.216 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,258 Gossiper.java (line 194) InetAddress /10.209.21.215 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,258 Gossiper.java (line 194) InetAddress /10.209.23.82 is now dead.


and just after that:

INFO [Timer-1] 2010-02-13 01:17:12,261 Gossiper.java (line 194) InetAddress /10.209.23.81 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,293 Gossiper.java (line 194) InetAddress /10.209.23.79 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,304 Gossiper.java (line 194) InetAddress /10.209.21.204 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,307 Gossiper.java (line 194) InetAddress /10.209.21.197 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,308 Gossiper.java (line 194) InetAddress /10.209.21.245 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,309 Gossiper.java (line 194) InetAddress /10.209.21.242 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,310 Gossiper.java (line 194) InetAddress /10.209.23.106 is now dead.
INFO [GMFD:1] 2010-02-13 01:17:26,780 Log4jLogger.java (line 41) 02/13/2010 01:17:26 - Remaining bytes zero. Stopping deserialization in EndPointState.
INFO [GMFD:1] 2010-02-13 01:17:26,784 Gossiper.java (line 543) InetAddress /10.209.21.204 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,785 Gossiper.java (line 543) InetAddress /10.209.23.106 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,786 Gossiper.java (line 543) InetAddress /10.209.21.197 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,800 Gossiper.java (line 543) InetAddress /10.209.21.216 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,808 Gossiper.java (line 543) InetAddress /10.209.21.217 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,823 Gossiper.java (line 543) InetAddress /10.209.21.223 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,823 Gossiper.java (line 543) InetAddress /10.209.21.215 is now UP


We're on 298a0e66ba66c5d2a1e5d4a70f2f619ae3fbf72a from git.apache.org, which claims to be:

git-svn-id: https://svn.apache.org/repos/asf/incubator/cassandra/branches/cassandra-0.5@9035",,kingryan,Normal,Resolved,Fixed,16/Feb/10 19:48,16/Apr/19 09:33
Bug,CASSANDRA-804,12456535,Warn operator when there is not enough disk space for compaction,,jbellis,jbellis,Low,Resolved,Fixed,17/Feb/10 13:51,16/Apr/19 09:33
Bug,CASSANDRA-805,12456560,using Integer.MAX_VALUE for executor keepalive time defeats the purpose of the SEDA-like stage divisions,we should allow thread pools to shrink when they have excess capacity,jbellis,jbellis,Low,Resolved,Fixed,17/Feb/10 17:02,16/Apr/19 09:33
Bug,CASSANDRA-806,12456582,Unit tests log RTE to stderr even though tests still succeed.,"[junit] ------------- Standard Error -----------------
   [junit] ERROR 20:05:29,165 Error in executor futuretask
   [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: No replica strategy configured for ltable
   [junit]     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
   [junit]     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
   [junit]     at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
   [junit]     at java.lang.Thread.run(Thread.java:619)
   [junit] Caused by: java.lang.RuntimeException: No replica strategy configured for ltable
   [junit]     at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:245)
   [junit]     at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1150)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:149)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:88)
   [junit]     at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:487)
   [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
   [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
   [junit]     ... 2 more
   [junit] ERROR 20:05:31,949 Error in executor futuretask
   [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: No replica strategy configured for rtable
   [junit]     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
   [junit]     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
   [junit]     at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
   [junit]     at java.lang.Thread.run(Thread.java:619)
   [junit] Caused by: java.lang.RuntimeException: No replica strategy configured for rtable
   [junit]     at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:245)
   [junit]     at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1150)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:149)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:88)
   [junit]     at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:487)
   [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
   [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
   [junit]     ... 2 more
   [junit] ------------- ---------------- ---------------",gdusbabek,gdusbabek,Low,Resolved,Fixed,17/Feb/10 20:11,16/Apr/19 09:33
Bug,CASSANDRA-811,12456836,support starting an avro enabled node (experimental),"The start script should support a  ""-a"" argument to make it possible to start an Avro enabled node. If/when Avro becomes a suitable replacement, the -a option can be dropped in favor of a -t option (as Avro becomes the deafult), which in turn can be dropped after Thrift is removed entirely.

The ThriftAddress and ThriftPort directives should also renamed to RPCAddress and RPCPort so that they can serve both RPC mechanism without creating confusion.

Patches to follow.",urandom,urandom,Normal,Resolved,Fixed,19/Feb/10 21:10,16/Apr/19 09:33
Bug,CASSANDRA-812,12456849,refactor system tests to accommodate avro,The patches that follow refactor the existing functional tests in order to better accommodate functional tests for avro.,urandom,urandom,Normal,Resolved,Fixed,19/Feb/10 22:15,16/Apr/19 09:33
Bug,CASSANDRA-814,12456955,Compaction bucketizing is computing average size incorrectly,"in the worst case (which doesn't seem to actually affect anyone) this could prevent sstables from being compacted, incorrectly",jbellis,jbellis,Low,Resolved,Fixed,21/Feb/10 21:48,16/Apr/19 09:33
Bug,CASSANDRA-816,12457009,Wordcount contrib is not including all required jars,The wordcount contrib build process is not including the libraries downloaded using ivy in the final jar file.,johanoskarsson,johanoskarsson,Low,Resolved,Fixed,22/Feb/10 14:45,16/Apr/19 09:33
Bug,CASSANDRA-817,12457030,Wordcount contrib does not work in Hadoop distributed mode,The column name is set in a static variable in the job setup. That variable will be empty when the job has been distributed to the tasktrackers. The variable must be set via the setup method in the mapper.,johanoskarsson,johanoskarsson,Low,Resolved,Fixed,22/Feb/10 16:41,16/Apr/19 09:33
Bug,CASSANDRA-824,12457227,Unable to start Cassandra in windows if P drive exsists,"When running bin\cassandra.bat from main dir, Cassandra exits with:
Invalid parameter - P:
Starting Cassandra Server
Listening for transport dt_socket at address: 8888
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/service/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.service.CassandraDaemon
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.cassandra.service.CassandraDaemon.  Program will exit.",gdusbabek,kazw,Low,Resolved,Fixed,23/Feb/10 22:02,16/Apr/19 09:33
Bug,CASSANDRA-828,12457237,possible NPE in StorageService,"the code
 {{{

     if (endPointThatLeft.equals(FBUtilities.getLocalAddress()))
            {
                logger_.info(""Received removeToken gossip about myself. Is this node a replacement for a removed one?"");
                return;
            }
            if (logger_.isDebugEnabled())
                logger_.debug(""Token "" + token + "" removed manually (endpoint was "" + ((endPointThatLeft == null) ? ""unknown"" : endPointThatLeft) + "")"");
            if (endPointThatLeft != null)
            {
                removeEndPointLocally(endPointThatLeft);
            }
}}}

appears wrong: if it is possible for the leaving endpoint to be unknown then the first ""if"" has a possible null dereference, which can be eliminated by swapping the arguments or reordering the code.

As a side note, I believe FBUtilities.getLocalAddress should probably be synchronized (or localInetAddress made volatile) per the usual ""the java MM does not guarantee any change will ever be visible""  mantra which may or may not be considered relevant :)",riffraff,riffraff,Low,Resolved,Fixed,23/Feb/10 23:06,16/Apr/19 09:33
Bug,CASSANDRA-833,12457318,fix consistencylevel during bootstrap,"As originally designed, bootstrap nodes should *always* get *all* writes under any consistencylevel, so when bootstrap finishes the operator can run cleanup on the old nodes w/o fear that he might lose data.

but if a bootstrap operation fails or is aborted, that means all writes will fail until the ex-bootstrapping node is decommissioned.  so starting in CASSANDRA-722, we just ignore dead nodes in consistencylevel calculations.

but this breaks the original design.  CASSANDRA-822 adds a partial fix for this (just adding bootstrap targets into the RF targets and hinting normally), but this is still broken under certain conditions.  The real fix is to consider consistencylevel for two sets of nodes:

  1. the RF targets as currently existing (no pending ranges)
  2.  the RF targets as they will exist after all movement ops are done

If we satisfy CL for both sets then we will always be in good shape.

I'm not sure if we can easily calculate 2. from the current TokenMetadata, though.",jbellis,jbellis,Normal,Resolved,Fixed,24/Feb/10 17:44,16/Apr/19 09:33
Bug,CASSANDRA-834,12457340,Exception during batch_mutate,"If a batch mutation is sent with deletions referring to a SCF but no SC is specified in the Deletion object, the following traceback is generated:

ERROR 15:28:16,746 Fatal exception in thread Thread[ROW-MUTATION-STAGE:22,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.Column cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.Column cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:300)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:284)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:87)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:73)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:329)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:341)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:314)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:270)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:200)
        at org.apache.cassandra.service.StorageProxy$3.runMayThrow(StorageProxy.java:282)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
",jbellis,brandon.williams,Low,Resolved,Fixed,24/Feb/10 20:34,16/Apr/19 09:33
Bug,CASSANDRA-837,12457743,hadoop recordreader hardcodes row count,We need to use the split size instead.,jbellis,jbellis,Low,Resolved,Fixed,01/Mar/10 14:28,16/Apr/19 09:33
Bug,CASSANDRA-850,12458223,Ant release target doesn't include all jars in binary tarball,"The ant release target doesn't create a complete tarball, the jars in build/lib/jars are not included.",urandom,johanoskarsson,Normal,Resolved,Fixed,05/Mar/10 10:18,16/Apr/19 09:33
Bug,CASSANDRA-853,12458265,ConcurrentModificationException,"i'm seeing a lot of these ... any idea?

2010-03-04 18:53:21,455 ERROR [MEMTABLE-POST-FLUSHER:1] [DebuggableThreadPoolExecutor.java:94] Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:357)
        at org.apache.cassandra.db.ColumnFamilyStore$2.runMayThrow(ColumnFamilyStore.java:392)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:349)
        ... 8 more
Caused by: java.util.ConcurrentModificationException
        at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:605)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:385)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:71)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:343)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at org.apache.cassandra.db.commitlog.CommitLogExecutorService.process(CommitLogExecutorService.java:113)
        at org.apache.cassandra.db.commitlog.CommitLogExecutorService.access$200(CommitLogExecutorService.java:35)
        at org.apache.cassandra.db.commitlog.CommitLogExecutorService$1.runMayThrow(CommitLogExecutorService.java:67)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
",riffraff,btoddb,Normal,Resolved,Fixed,05/Mar/10 17:11,16/Apr/19 09:33
Bug,CASSANDRA-857,12458371,AssertionError in MappedFileDataInput.skipBytes when slicing a large number of keys,"i'm getting the following error when performing a range query that is supposed to return a large number of keys:

ERROR [ROW-READ-STAGE:9] 2010-03-07 03:54:49,672 CassandraDaemon.java (line 78) Fatal exception in thread Thread[ROW-READ-STAGE:9,5,main]
java.lang.AssertionError
	at org.apache.cassandra.io.util.MappedFileDataInput.skipBytes(MappedFileDataInput.java:104)
	at org.apache.cassandra.io.SSTableReader.getPosition(SSTableReader.java:382)
	at org.apache.cassandra.io.SSTableReader.getFileDataInput(SSTableReader.java:481)
	at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:54)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:851)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:771)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:740)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1040)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)",jbellis,edmonds,Normal,Resolved,Fixed,07/Mar/10 04:08,16/Apr/19 09:33
Bug,CASSANDRA-858,12458407,cassandra-cli.bat fails to start cli client,"When one fetch latest and greates from svn - the cassandra.bat works. (like - the server starts and get ready to rock) but the cassandra-cli.bat fails.
java claims NoClassDefFoundErrror: jline/Completor
- after joining all four braincells I noticed that %CASSANDRA_HOME%\build\lib\jars\* is not part of the classpath that is setup inside the bat file. Adding this solved the issue.
",mwn,mwn,Low,Resolved,Fixed,07/Mar/10 22:59,16/Apr/19 09:33
Bug,CASSANDRA-864,12458566,ConcurrentModificationException during QuorumResponseHandler,"using cassandra-0.6.0-beta2/


2010-03-09 09:17:26,827 ERROR [pool-1-thread-675] [Cassandra.java:1166] Internal error processing get
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:68)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:470)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:401)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:101)
        at org.apache.cassandra.thrift.CassandraServer.multigetInternal(CassandraServer.java:309)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:274)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:1156)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1114)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619) ",jbellis,btoddb,Low,Resolved,Fixed,09/Mar/10 18:20,16/Apr/19 09:33
Bug,CASSANDRA-866,12458569,AssertionError SSTableSliceIterator.java:126,"also seeing these, using cassandra-0.6.0-beta2/

2010-03-09 07:37:57,683 ERROR [ROW-READ-STAGE:77] [CassandraDaemon.java:78] Fatal exception in thread Thread[ROW-READ-STAGE:77,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:126)
        at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:59)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:851)
        at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:748)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:773)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:740)
        at org.apache.cassandra.db.Table.getRow(Table.java:381)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:56)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:80)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",jbellis,btoddb,Normal,Resolved,Fixed,09/Mar/10 18:24,16/Apr/19 09:33
Bug,CASSANDRA-867,12458579,Invalid host/port parameters to cassandra-cli leaves system in unrecoverable state,"bin\cassandra-cl.bati -host localhost -port 8880  (cassandra not running localhost/8880 ;) ) 

Starting Cassandra Client
Exception connecting to localhost/8880 - java.net.ConnectException: Connection refused: connect
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
cassandra> j
Exception null
cassandra> quit
Exception null

Problem is that main does not ensure that cliClient_ is set to an instanse of CliClient.",mwn,mwn,Low,Resolved,Fixed,09/Mar/10 20:44,16/Apr/19 09:33
Bug,CASSANDRA-882,12458946,RowWarningThreshold doesn't allow values more than about 2GB,"Using values bigger than 2048 for RowWarningThreshold makes 
Cassandra not start with the message: 
  Fatal error: Row warning threshold must be a positive integer",slebresne,slebresne,Low,Resolved,Fixed,12/Mar/10 15:08,16/Apr/19 09:33
Bug,CASSANDRA-884,12458977,get_range_slice returns multiple copies of each row for ConsistencyLevel > ONE,"I've noticed that both 0.5.1 and 0.6b2 return multiple identical copies of the data stored in my keyspace whenever I make a call to get_range_slice or get_range_slices using
ConsistencyLevel.QUORUM and ReplicationFactor is greater than one.

So with ReplicationFactor set to 2 for my application's KeySpace I get double the number of KeySlices that I expect to get. When using ConsistencyLevel.ONE I get only one KeySlice for each row.

I've seen this happen with Cassandra 0.5.1 and with 0.6 beta 2. The behavior on 0.6 beta 2 is exhibited with both get_range_slice and get_range_slices.

The attached Java program demonstrates the issue for 0.6 beta 2. The program writes a series of single-column rows into the Standard1 table, and then uses get_range_slice to receive a list of all row. The returned number of rows is consistently twice the number of rows written to the database. I wipe out the database completely before running the test.
",omerhj,omerhj,Normal,Resolved,Fixed,12/Mar/10 20:50,16/Apr/19 09:33
Bug,CASSANDRA-885,12458978,OOM on Commit log Replay,"Running 0.5.

We had a node reboot, and when it came back up, it was unable to replay the commit logs, it would OOM every time.

We upped the max heap to 6 gigs, but it didn't help.

I have a heap dump and have it opened in Eclipse MAT.

Anything specific I should pull out?

Class Name                                                                                   | Shallow Heap | Retained Heap | Percentage 
----------------------------------------------------------------------------------------------------------------------------------------
                                                                                             |              |               |            
org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor @ 0x7fad06454f48                |          112 | 2,604,583,312 |     84.35% 
|- java.util.concurrent.LinkedBlockingQueue @ 0x7fad06405b78                                 |           80 | 2,604,579,864 |     84.35% 
|- java.util.HashSet @ 0x7fad071b2110                                                        |           24 |         2,952 |      0.00% 
|- java.lang.String @ 0x7fad071b3588  org.apache.cassandra.concurrent:type=ROW-MUTATION-STAGE|           40 |           176 |      0.00% 
|- org.apache.cassandra.concurrent.NamedThreadFactory @ 0x7fad0714e998                       |           32 |            56 |      0.00% 
|- java.util.concurrent.locks.ReentrantLock$NonfairSync @ 0x7fad0722b570                     |           48 |            48 |      0.00% 
|- java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject @ 0x7fad071b3560    |           40 |            40 |      0.00% 
|- java.util.concurrent.atomic.AtomicInteger @ 0x7fad071b20e0                                |           24 |            24 |      0.00% 
|- java.util.concurrent.locks.ReentrantLock @ 0x7fad071b20f8                                 |           24 |            24 |      0.00% 
|- java.util.concurrent.ThreadPoolExecutor$CallerRunsPolicy @ 0x7fad071b2128                 |           16 |            16 |      0.00% 
'- Total: 9 entries                                                                          |              |               |            
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad063b3390                                   |          104 |    95,871,520 |      3.10% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad063b3460                                   |          104 |    82,056,536 |      2.66% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad063b3188                                   |          104 |    72,894,176 |      2.36% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad06331798                                   |          104 |    45,394,360 |      1.47% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad06331660                                   |          104 |    33,895,560 |      1.10% 
java.lang.Thread @ 0x7fad060e2320  main Native Stack, Thread                                 |          168 |    33,573,328 |      1.09% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad06287e50                                   |          104 |    33,528,680 |      1.09% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad063b3530                                   |          104 |    33,423,720 |      1.08% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad063b32c0                                   |          104 |    20,221,624 |      0.65% 
org.apache.cassandra.db.Memtable @ 0x7fad06410120                                            |           96 |     9,383,880 |      0.30% 
org.apache.cassandra.db.ColumnFamily @ 0x7fad33df3690                                        |           80 |     1,190,016 |      0.04% 
org.apache.cassandra.io.SSTableReader @ 0x7fad0653a4a0                                       |           72 |       587,008 |      0.02% 
org.apache.cassandra.io.SSTableReader @ 0x7fad06c41ef0                                       |           72 |       543,552 |      0.02% 
org.apache.cassandra.io.SSTableReader @ 0x7fad06456450                                       |           72 |       541,320 |      0.02% 
Total: 15 of 49,239 entries                                                                  |              |               |            
----------------------------------------------------------------------------------------------------------------------------------------
",jbellis,pquerna,Normal,Resolved,Fixed,12/Mar/10 20:50,16/Apr/19 09:33
Bug,CASSANDRA-895,12459070,Replace incubator site with a redirect,The incubator website is still up with broken links. We should make it redirect to the new TLP version of the website.,,johanoskarsson,Urgent,Resolved,Fixed,14/Mar/10 09:10,16/Apr/19 09:33
Bug,CASSANDRA-899,12459125,skipBytes in SSTable*Iterator is in an assert,"In SSTable Name and Slice iterator, the seek to the indexed offset is in an assert. 
As a consequence, if Cassandra is run without assertions, reads (specially for large 
rows) can be really inefficient.",slebresne,slebresne,Low,Resolved,Fixed,15/Mar/10 10:19,16/Apr/19 09:33
Bug,CASSANDRA-902,12459419,Bootstrapping might skip needed ranges.,"Bootstrapper.getRangeWithSources should return a multimap with as many keys as myRangeAddresses.  But with the way the two loops are structured, they are not guaranteed to ever examine all of myRanges.  To see why, consider a scenario where the inner-loop breaks on the first element in myRanges.  myRangeAddresses will only ever have one key in it.

Solution is to swap the order of the loops.",gdusbabek,gdusbabek,Normal,Resolved,Fixed,17/Mar/10 19:40,16/Apr/19 09:33
Bug,CASSANDRA-906,12459602,Mixed use of Stage's name. Must use public static field.,"This line in StageManger is not using the public static field to reference the Stage's name:

        stages.put(RESPONSE_STAGE, multiThreadedStage(""RESPONSE-STAGE"", Runtime.getRuntime().availableProcessors()));

It should be:

        stages.put(RESPONSE_STAGE, multiThreadedStage(RESPONSE_STAGE, Runtime.getRuntime().availableProcessors()));
",rodrigoap,rodrigoap,Low,Resolved,Fixed,19/Mar/10 02:25,16/Apr/19 09:33
Bug,CASSANDRA-907,12459667,TimeUUID comparator identify different UUID as long as they have same timestamp,Everything's in the title. As such TimeUUID behave as simple timestamp which is weird at best.,slebresne,slebresne,Low,Resolved,Fixed,19/Mar/10 15:57,16/Apr/19 09:33
Bug,CASSANDRA-911,12459899,QuorumResponseHandler sets timeout incorrectly,"We noticed that the timeout calculation seems wrong:

long timeout = System.currentTimeMillis() - startTime + DatabaseDescriptor.getRpcTimeout();

Lets propose that 3 seconds elapse (currentTime - startTime). It will take that value and add the default RpcTimeout (5 seconds) making the timeout 8 seconds.",lenn0x,lenn0x,Low,Resolved,Fixed,22/Mar/10 19:01,16/Apr/19 09:33
Bug,CASSANDRA-915,12460046,disallow column family names containing hyphens,"You cannot use use hyphens in column family names because hyphens are used as delimiters in sstable filenames (which are derived from the CF name). 

It should be an error to configure such a column family name.",,urandom,Low,Resolved,Fixed,23/Mar/10 21:33,16/Apr/19 09:33
Bug,CASSANDRA-920,12460296,Deleting and re-inserting row causes error in get_slice count parameter,"I've found that when I delete an entire row in a column family with super columns, and then re-insert values with the same row and super column keys, the count parameter to the get_slice call no longer works properly.  Its like it is still counting the deleted columns, but only returning the new columns.

The following example uses the Ruby Cassandra client (see link below), but I've seen the same behavior with the Java Thrift interface.

Test code:
--------------
require 'rubygems'
require 'cassandra'
cc = Cassandra.new('Keyspace1')
cc.insert(:Super1,'test-key1',{'bucket1' => {'1' => 'Item 1', '2' => 'Item 2', '5' => 'Item 5'}})
items = cc.get(:Super1,'test-key1','bucket1')
puts ""returned #{items.size} items, should be 3""
cc.remove(:Super1,'test-key1')
items = cc.get(:Super1,'test-key1','bucket1')
puts ""returned #{items.size} items, should be 0""
cc.insert(:Super1,'test-key1',{'bucket1' => {'3' => 'Item 3', '4' => 'Item 4', '6' => 'Item 6'}})
items = cc.get(:Super1,'test-key1','bucket1')
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 3)
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 4)
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 5)
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 6)
puts ""returned #{items.size} items, should be 3""

Output:
returned 3 items, should be 3
returned 0 items, should be 0
returned 3 items, should be 3
returned 1 items, should be 3
returned 2 items, should be 3
returned 2 items, should be 3
returned 3 items, should be 3

Ruby library link:
http://blog.evanweaver.com/files/doc/fauna/cassandra/files/README_rdoc.html",brandon.williams,bflorian,Low,Resolved,Fixed,25/Mar/10 22:52,16/Apr/19 09:33
Bug,CASSANDRA-923,12460460,Enable DEBUG file logging during unit tests,DEBUG logging to the build/test/logs directory was disabled at some point.,stuhood,stuhood,Normal,Resolved,Fixed,28/Mar/10 01:48,16/Apr/19 09:33
Bug,CASSANDRA-924,12460506,incorrect neighbor calculation in repair,"With Replicationfactor=2, if a server is brought down and its data directory wiped out, it doesn't restore its data replica after restart and nodeprobe repair.
Steps to reproduce:
1) Bring up a cluster with three servers cs1,2,3, with their initial token set to 'foo3', 'foo6', and 'foo9', respectively. ReplicationFactor is set to 2 on all 3.
2) Insert 9 columns with keys from 'foo1' to 'foo9', and flush. Now I have foo1,2,3,7,8,9 on cs1, foo1,2,3,4,5,6, on cs2, and foo4,5,6,7,8,9
on cs3. So far so good
3) Bring down cs3 and wipe out its data directory
4) Bring up cs3
5) run nodeprobe repair Keyspace1 on cs3, the flush
At this point I expect to see cs3 getting its data back. But there's nothing in its data directory. I also tried getting all columns with
ConsistencyLevel::ALL to see if that'll do a read pair. But still cs3's data directory is empty.
",stuhood,hujn,Normal,Resolved,Fixed,29/Mar/10 06:00,16/Apr/19 09:33
Bug,CASSANDRA-933,12460730,OutOfBoundException in StorageService.getAllRanges ,"this was seen on 0.6-beta3 but it appears to be in trunk too, given the same code for getAllRanges. The problem appeared while accessing a bootstraping node via nodetool, giving the following stacktrace

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.get(ArrayList.java:324)
        at java.util.Collections$UnmodifiableList.get(Collections.java:1154)
        at org.apache.cassandra.service.StorageService.getAllRanges(StorageService.java:1133)
        at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:440)
        at org.apache.cassandra.service.StorageService.getRangeToEndPointMap(StorageService.java:431)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
...


Basically, no test is performed for non-emptyness of the input token list. If such list should never be empty, I guess this should be explicit in the interface/javadoc, otherwise I'm attaching a patch & testcase (pretty silly code, but the test passes :) ) ",riffraff,riffraff,Low,Resolved,Fixed,30/Mar/10 19:07,16/Apr/19 09:33
Bug,CASSANDRA-934,12460731,NPE in sstable2json,"When sstable2json is not passed any excluded keys via -x, an NPE is raised.",brandon.williams,brandon.williams,Low,Resolved,Fixed,30/Mar/10 19:13,16/Apr/19 09:33
Bug,CASSANDRA-935,12460737,"login() request via Thrift/PHP fails with ""Unexpected authentication problem"" in cassandra log / ""Internal error processing login"" in Thrift","When issuing a login request via PHP Thrift with the following parameters:

$auth_request = new cassandra_AuthenticationRequest;
$auth_request->credentials = array (
    ""username"" => ""jsmith"",
     ""password"" => ""havebadpass"",
);
$client->login(""Keyspace1"", $auth_request);

I get an exception, with the following details

PHP Exception:
PHP Fatal error:  Uncaught exception 'TApplicationException' with message 'Internal error processing login' in /home/redsolar/html/includes/thrift/packages/cassandra/Cassandra.php:73

Cassandra log:

ERROR 13:00:53,823 Internal error processing login
java.lang.RuntimeException: Unexpected authentication problem
        at org.apache.cassandra.auth.SimpleAuthenticator.login(SimpleAuthenticator.java:113)
        at org.apache.cassandra.thrift.CassandraServer.login(CassandraServer.java:651)
        at org.apache.cassandra.thrift.Cassandra$Processor$login.process(Cassandra.java:1147)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1125)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.NullPointerException
        at java.io.FileInputStream.<init>(FileInputStream.java:133)
        at java.io.FileInputStream.<init>(FileInputStream.java:96)
        at org.apache.cassandra.auth.SimpleAuthenticator.login(SimpleAuthenticator.java:82)
        ... 7 more

File contents (all chmod 777 for testing):

""conf/access.properties""
Keyspace1=jsmith,Elvis Presley,dilbert

""conf/password.properties""
jsmith=havebadpass
Elvis\ Presley=graceland4evar
dilbert=nomoovertime",rschildmeijer,redsolar,Low,Resolved,Fixed,30/Mar/10 20:22,16/Apr/19 09:33
Bug,CASSANDRA-936,12460748,Discard Commitlog Exception,"2010-03-30_21:19:02.31041 java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
2010-03-30_21:19:02.31041 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
2010-03-30_21:19:02.31041 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-03-30_21:19:02.31041 	at java.lang.Thread.run(Thread.java:636)
2010-03-30_21:19:02.31041 Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
2010-03-30_21:19:02.31041 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2010-03-30_21:19:02.31041 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-03-30_21:19:02.31041 	... 2 more
2010-03-30_21:19:02.31041 Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:358)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.ColumnFamilyStore$1.runMayThrow(ColumnFamilyStore.java:371)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2010-03-30_21:19:02.31041 	... 6 more
2010-03-30_21:19:02.31041 Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:350)
2010-03-30_21:19:02.31041 	... 8 more
2010-03-30_21:19:02.31041 Caused by: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:378)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:72)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:344)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLogExecutorService.process(CommitLogExecutorService.java:113)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLogExecutorService.access$200(CommitLogExecutorService.java:35)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLogExecutorService$1.runMayThrow(CommitLogExecutorService.java:67)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2010-03-30_21:19:02.31041 	... 1 more",jbellis,dispalt,Normal,Resolved,Fixed,30/Mar/10 21:47,16/Apr/19 09:33
Bug,CASSANDRA-937,12460750,Invalid Response count 4,"2010-03-30_21:59:04.64973 ERROR - Error in ThreadPoolExecutor
2010-03-30_21:59:04.64973 java.lang.AssertionError: invalid response count 4
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ReadResponseResolver.<init>(ReadResponseResolver.java:54)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.doReadRepair(ConsistencyManager.java:89)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:75)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:60)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-03-30_21:59:04.64973 	at java.lang.Thread.run(Thread.java:636)
2010-03-30_21:59:04.64973 ERROR - Fatal exception in thread Thread[RESPONSE-STAGE:5,5,main]
2010-03-30_21:59:04.64973 java.lang.AssertionError: invalid response count 4
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ReadResponseResolver.<init>(ReadResponseResolver.java:54)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.doReadRepair(ConsistencyManager.java:89)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:75)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:60)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-03-30_21:59:04.64973 	at java.lang.Thread.run(Thread.java:636)
",jbellis,dispalt,Normal,Resolved,Fixed,30/Mar/10 22:00,16/Apr/19 09:33
Bug,CASSANDRA-942,12460956,Command line arguments inversion in clustertool,"The arguments (table and key) of the get_endpoints command of clustertool are in the wrong order
(the call to getNaturalEndpoints also have it's argument in the wrong order so it kinda works in the end
but the printing of the 'Key' is wrong.
",slebresne,slebresne,Low,Resolved,Fixed,01/Apr/10 16:36,16/Apr/19 09:33
Bug,CASSANDRA-943,12460982,Spelling correction: rename DatacenterShardStategy to DatacenterShardStrategy,Missing 'r' Stategy.,esigler,rodrigoap,Low,Resolved,Fixed,01/Apr/10 23:47,16/Apr/19 09:33
Bug,CASSANDRA-944,12460991,system.test_thrift_server.TestMutations.test_batch_mutate_standard_columns appears to be non deterministic,"system.test_thrift_server.TestMutations.test_batch_mutate_standard_columns appears to be non deterministic.  The first time I ran the thrift tests after a clean checkout it failed.  However, it did not fail the ~10 times after that.

{code}
mdennis@mdennis:~/c/cassandra$ nosetests test/system/test_thrift_server.py 
...........E....................................
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_batch_mutate_standard_columns
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.6/nose/case.py"", line 183, in runTest
    self.test(*self.arg)
  File ""/home/mdennis/c/cassandra/test/system/test_thrift_server.py"", line 318, in test_batch_mutate_standard_columns
    _assert_column('Keyspace1', column_family, key, 'c1', 'value1')
  File ""/home/mdennis/c/cassandra/test/system/test_thrift_server.py"", line 43, in _assert_column
    raise Exception('expected %s:%s:%s:%s:%s, but was not present' % (keyspace, column_family, key, column, value) )
Exception: expected Keyspace1:Standard1:key_27:c1:value1, but was not present

----------------------------------------------------------------------
Ran 48 tests in 184.700s

FAILED (errors=1)
{code}",brandon.williams,mdennis,Normal,Resolved,Fixed,02/Apr/10 03:02,16/Apr/19 09:33
Bug,CASSANDRA-945,12460992,org.apache.cassandra.config.CFMetaData defines equals but does not define hashCode,"org.apache.cassandra.config.CFMetaData defines equals but does not define hashCode

On a related note, it should probably be using org.apache.commons.lang.builder.[EqualsBuilder | HashCodeBuider]",mdennis,mdennis,Normal,Resolved,Fixed,02/Apr/10 03:07,16/Apr/19 09:33
Bug,CASSANDRA-948,12461065,Cannot Start Cassandra Under Windows,"I get the following error when I try to launch the RC of Cassandra from the command line:

Starting Cassandra Server
Listening for transport dt_socket at address: 8888
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.CassandraDaemon
        at java.net.URLClassLoader$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
Could not find the main class: org.apache.cassandra.thrift.CassandraDaemon.  Program will exit.",gdusbabek,nberardi,Normal,Resolved,Fixed,02/Apr/10 21:22,16/Apr/19 09:33
Bug,CASSANDRA-952,12461108,DC Quorum broken @ trunk,"Currently DCQuorum is broken in trunk, Suggesting the following fix... 

Write to DC's
1) Move determineBlockFor(int expandedTargets, ConsistencyLevel consistency_level) to AbstractEndpointSnitch
2) Add the same to support DC Quorum in DatacenterShardStategy

Read to DC's
1) find suitable nodes was a list which was returning a list of local DC's earlier but now it is just one node and MD is been sent by other nodes. Need to have an option to even avoid MD from other DC's?",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,04/Apr/10 05:43,16/Apr/19 09:33
Bug,CASSANDRA-963,12461451,loadSchemaFromXml records no migrations,This means that there is nothing to propagate to new nodes when schema is force loaded from xml.,gdusbabek,gdusbabek,Normal,Resolved,Fixed,07/Apr/10 20:01,16/Apr/19 09:33
Bug,CASSANDRA-965,12461539,remove deprecated get_string*_property() thrift methods,"The get_string_property() and get_string_list_property() methods were deprecated in 0.6, they can now be removed in preparation for 0.7.
",urandom,urandom,Low,Resolved,Fixed,08/Apr/10 15:51,16/Apr/19 09:33
Bug,CASSANDRA-966,12461576,StorageService.getPartitioner() and QueryFilter.getColumnComparator() should be statically accessed,,tupshin,tupshin,Low,Resolved,Fixed,08/Apr/10 19:34,16/Apr/19 09:33
Bug,CASSANDRA-969,12461722,Server fails to join cluster if IPv6 only,"When configuring Cassandra for IPv6 connectivity on the server to server side the addition of a second node causes the both servers to loop on ArrayIndexOutOfBoundsExection for 5 minutes

The first server has 

Caused by: java.lang.ArrayIndexOutOfBoundsException: 65536
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:155)

While the second has

Caused by: java.lang.ArrayIndexOutOfBoundsException: 131072
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:155)

the index is double.

These servers work find in a cluster together if they are configured IPv4

server1 in the output is 2607:f3d0:0:2::16
server2 is 2607:f3d0:0:1::f",gdusbabek,cody.lerum,Low,Resolved,Fixed,10/Apr/10 18:16,16/Apr/19 09:33
Bug,CASSANDRA-995,12462287,"restarting node crashes with NPE when, while replaying the commitlog, the cfMetaData is requested","Removing the commitlog directory completely fixes this.   I can reliably reproduce it by 1) starting and configuring a schema with one keyspace, one super CF with LongType supercolumns; 2) inserting data; 3) shutting down and restarting the node.

Here's my schema expressed in cassidy.pl, should be obvious what the parameters are:
./cassidy.pl -server X -port Y -keyspace system 'kdefine test org.apache.cassandra.locator.RackUnawareStrategy 2 org.apache.cassandra.locator.EndPointSnitch'
./cassidy.pl -server X -port Y -keyspace test 'fdefine Status Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'

The problem seems to be related to CASSANDRA-44 as it happens when the CF metadata is requested but I don't know what's causing it.

10/04/16 15:25:11 INFO commitlog.CommitLog: Replaying /home/cassandra/commitlog/CommitLog-1271449410100.log, /home/cassandra/commitlog/CommitLog-1271449378151.log, /home/cassandra/commitlog/CommitLog-1271449415800.log
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.<init>(Table.java:261)
        at org.apache.cassandra.db.Table.open(Table.java:102)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:233)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:104)
        at org.apache.cassandra.thrift.CassandraDaemon.init(CassandraDaemon.java:151)
        ... 5 more
",gdusbabek,tzz,Urgent,Resolved,Fixed,16/Apr/10 20:31,16/Apr/19 09:33
Bug,CASSANDRA-997,12462357,Javadoc for thrift interface is not generated,In both 0.6 and svn trunk no javadoc is generated for thrift-generated classes like org.apache.cassandra.thrift.Cassandra. The problem is that the wrong directory is included in the javadoc ant target (interface/thrift instead of interface/thrift/gen-java). ,hannes@helma.at,hannes@helma.at,Low,Resolved,Fixed,18/Apr/10 15:07,16/Apr/19 09:33
Bug,CASSANDRA-1002,12462441,Fat client example cannot find schema,"Running the client example in contrib shows that it cannot find the schema, possibly caused by CASSANDRA-44.

Throws this error:
Exception in thread ""main"" java.lang.IllegalArgumentException: Unknown ColumnFamily Standard1 in keyspace Keyspace1
	at org.apache.cassandra.config.DatabaseDescriptor.getComparator(DatabaseDescriptor.java:1123)
	at org.apache.cassandra.db.ColumnFamily.getComparatorFor(ColumnFamily.java:437)
	at ClientOnlyExample.testWriting(ClientOnlyExample.java:52)
	at ClientOnlyExample.main(ClientOnlyExample.java:169)
",gdusbabek,johanoskarsson,Normal,Resolved,Fixed,19/Apr/10 14:59,16/Apr/19 09:33
Bug,CASSANDRA-1005,12462487,cassandra-cli doesn't work with system allowed column family names,"Given the following definitions for columns:

<Keyspaces>

<Keyspace Name=""NGram"">

<KeysCachedFraction>0.01</KeysCachedFraction>

<ColumnFamily CompareWith=""UTF8Type"" Name=""1GramR""/>

<ColumnFamily CompareWith=""UTF8Type"" Name=""1GramL""/>

</Keyspaces>

The appropriate keyspaces are created an persisteted on startup. When executing a query or a set operation in the cassandra-cli, you end up with the following error:

******************************************************

cassandra> get NGram.1GramR['hte']

line 1:10 extraneous input '1' expecting Identifier

No such column family: GramR

******************************************************


Following the syntax of the grammer we can see the following:

setStmt
: K_SET columnFamilyExpr '=' value -> ^(NODE_THRIFT_SET columnFamilyExpr value)
;

...

columnFamilyExpr
: table DOT columnFamily '[' rowKey ']'
( '[' a+=columnOrSuperColumn ']'
('[' a+=columnOrSuperColumn ']')?
)?
-> ^(NODE_COLUMN_ACCESS table columnFamily rowKey ($a+)?)
;
...

// syntactic Elements
Identifier
: Letter ( Alnum | '_' )*
;

There is a mismatch on what is appropriate values for this in the system. So either the restriction needs to be lifted in the cli, or the system must have a way of honoring the names.",urandom,kingjamm,Low,Resolved,Fixed,19/Apr/10 20:34,16/Apr/19 09:33
Bug,CASSANDRA-1006,12462502,Exception starting up cluster with ByteOrderedPartitioner without schema set,"Testing out the new ByteOrderedPartitioner, I ran into this exception on the tip:

java.lang.NumberFormatException: For input string: ""To""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at org.apache.cassandra.utils.FBUtilities.hexToBytes(FBUtilities.java:361)
	at org.apache.cassandra.dht.AbstractByteOrderedPartitioner$1.fromString(AbstractByteOrderedPartitioner.java:133)
	at org.apache.cassandra.dht.BootStrapper$BootstrapTokenCallback.response(BootStrapper.java:246)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:36)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR 16:31:21,737 Fatal exception in thread Thread[RESPONSE-STAGE:1,5,main]

It works fine with the RandomPartitioner, however.",stuhood,erickt,Normal,Resolved,Fixed,19/Apr/10 22:35,16/Apr/19 09:33
Bug,CASSANDRA-1008,12462713,Assertion failure loadbalance-ing a ByteOrderedPartitioner cluster,"This seems to be a similar problem to CASSANDRA-1006:

ERROR [GMFD:4] 2010-04-21 15:37:56,942 CassandraDaemon.java (line 77) Fatal exception in thread Thread[GMFD:4,5,main]
java.lang.NumberFormatException: For input string: ""To""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at org.apache.cassandra.utils.FBUtilities.hexToBytes(FBUtilities.java:361)
	at org.apache.cassandra.dht.AbstractByteOrderedPartitioner$1.fromString(AbstractByteOrderedPartitioner.java:133)
	at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:622)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:517)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:705)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:695)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:624)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAckVerbHandler.doVerb(Gossiper.java:966)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
",erickt,erickt,Low,Resolved,Fixed,21/Apr/10 22:42,16/Apr/19 09:33
Bug,CASSANDRA-1009,12462717,Bad EndpointSnitch config in trunk,A few config changes made it into trunk for 990/1000 that shouldn't have.,stuhood,stuhood,Normal,Resolved,Fixed,22/Apr/10 00:55,16/Apr/19 09:33
Bug,CASSANDRA-1011,12462730,Exception auto-bootstrapping two nodes nodes at the same time,"I've got a small cluster of 3 machines, and after starting the first node (which is the seed), I brought up the other two nodes at the same time. This exception then gets raised on the seed node. Looks like the seed node is assigning the same token to the subnodes at the same time:

ERROR 21:46:49,417 Error in ThreadPoolExecutor
java.lang.RuntimeException: Bootstrap Token collision between /10.0.0.2 and /10.0.0.3 (token Token(bytes[4c617374204d6967726174696f6e])
	at org.apache.cassandra.locator.TokenMetadata.addBootstrapToken(TokenMetadata.java:130)
	at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:548)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:511)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:705)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:670)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:624)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1016)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR 21:46:49,418 Fatal exception in thread Thread[GMFD:1,5,main]
java.lang.RuntimeException: Bootstrap Token collision between /10.0.0.2 and /10.0.0.3 (token Token(bytes[4c617374204d6967726174696f6e])
	at org.apache.cassandra.locator.TokenMetadata.addBootstrapToken(TokenMetadata.java:130)
	at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:548)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:511)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:705)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:670)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:624)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1016)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
",gdusbabek,erickt,Low,Resolved,Fixed,22/Apr/10 05:09,16/Apr/19 09:33
Bug,CASSANDRA-1019,12462907,"""java.net.ConnectException: Connection timed out"" in MESSAGE-STREAMING-POOL:1","after doing a nodetool repair on a node in my cluster, i see the following exception on 4 out of the 7 nodes.  replication factor is 3.  no compactions happening.  no client traffic to the cluster.  nodetool streams (on one of the nodes not repaired) shows the following which is not ever increasing:

Mode: Normal
Streaming to: /192.168.132.117
   /data/cassandra-data/data/UdsProfiles/stream/UdsProfiles-43-Data.db 0/523088443
Not receiving any streams.


in addition those same four nodes all show AE-SERVICE-STAGE with pending
work, and been showing this for several hours now. each node in the
cluster has less than 2gb, so it should be finished by now.

here is the exception:

2010-04-23 10:08:43,416 ERROR [MESSAGE-STREAMING-POOL:1]
[DebuggableThreadPoolExecutor.java:101] Error in ThreadPoolExecutor
java.lang.RuntimeException: java.net.ConnectException: Connection timed out
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.ConnectException: Connection timed out
at sun.nio.ch.Net.connect(Native Method)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:507)
at org.apache.cassandra.net.FileStreamTask.runMayThrow(FileStreamTask.java:60)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
... 3 more
2010-04-23 10:08:43,417 ERROR [MESSAGE-STREAMING-POOL:1]
[CassandraDaemon.java:78] Fatal exception in thread
Thread[MESSAGE-STREAMING-POOL:1,5,main]
java.lang.RuntimeException: java.net.ConnectException: Connection timed out
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.ConnectException: Connection timed out
at sun.nio.ch.Net.connect(Native Method)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:507)
at org.apache.cassandra.net.FileStreamTask.runMayThrow(FileStreamTask.java:60)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
... 3 more

",stuhood,btoddb,Normal,Resolved,Fixed,23/Apr/10 21:12,16/Apr/19 09:33
Bug,CASSANDRA-1020,12462918,keys are now byte[] but hashmap cannot have byte[] as keys so they need to be fixed,Thrift client calls use hashmap and needs this fix,vijay2win@yahoo.com,vijay2win@yahoo.com,Urgent,Resolved,Fixed,24/Apr/10 04:54,16/Apr/19 09:33
Bug,CASSANDRA-1022,12463043,separate assignment of current keyspace from login(),"With the completion of CASSANDRA-714, it is now a requirement that login() be called, even when using the AllowAllAuthenticator (effectively disabling auth), since this is how the current/connected keyspace is set. These two disparate functions (assigning keyspace and authentication) should be disentangled.

I propose that the keyspace argument be removed from calls to {{login()}}, and that a new method ({{use_keyspace(string)}}?), be added.
",todd,urandom,Low,Resolved,Fixed,26/Apr/10 18:35,16/Apr/19 09:33
Bug,CASSANDRA-1028,12463120,NPE in AntiEntropyService.getNeighbors,"Sometimes, but not always, I see this during a test run:

    [junit] Testsuite: org.apache.cassandra.service.AntiEntropyServiceTest
    [junit] Tests run: 10, Failures: 0, Errors: 0, Time elapsed: 3.189 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 10:19:09,743 Error in executor futuretask
    [junit] java.util.concurrent.ExecutionException: java.lang.NullPointerException
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit] 	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit] 	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:637)
    [junit] Caused by: java.lang.NullPointerException
    [junit] 	at java.util.AbstractCollection.addAll(AbstractCollection.java:303)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:151)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.rendezvous(AntiEntropyService.java:176)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:86)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:487)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	... 2 more
    [junit] ------------- ---------------- ---------------

Ideally it would be nice if this could cause an actual test failure when it happens.  Not sure how feasible that is.",stuhood,jbellis,Normal,Resolved,Fixed,27/Apr/10 15:25,16/Apr/19 09:33
Bug,CASSANDRA-1030,12463154,fix contrib/word_count build in 0.7,CASSANDRA-44 broke word_count setup (see CASSANDRA-1002) so CASSANDRA-992 and CASSANDRA-1029 can't easily be applied to 0.7 as-is.  This ticket will port those to 0.7 and add schema setup.,jeromatron,jbellis,Normal,Resolved,Fixed,27/Apr/10 21:38,16/Apr/19 09:33
Bug,CASSANDRA-1033,12463264,stress.py broken in trunk,"stress.py is broken on trunk, apparently due to the removal of the keyspace argument to thrift calls.",brandon.williams,brandon.williams,Low,Resolved,Fixed,28/Apr/10 22:05,16/Apr/19 09:33
Bug,CASSANDRA-1034,12463287,Remove assumption that Key to Token is one-to-one,"get_range_slices assumes that Tokens do not collide and converts a KeyRange to an AbstractBounds. For RandomPartitioner, this assumption isn't safe, and would lead to a very weird heisenberg.

Converting AbstractBounds to use a DecoratedKey would solve this, because the byte[] key portion of the DecoratedKey can act as a tiebreaker. Alternatively, we could make DecoratedKey extend Token, and then use DecoratedKeys in places where collisions are unacceptable.",slebresne,stuhood,Low,Resolved,Fixed,29/Apr/10 03:21,16/Apr/19 09:33
Bug,CASSANDRA-1036,12463365,Attempting to mutate a non-existant CF does not propagate an error to the client,"An error gets logged on the server:

ERROR 15:23:21,035 Attempting to mutate non-existant column family Standard1

But nothing is raised on the client side, so it appears the request succeeded.",slebresne,brandon.williams,Low,Resolved,Fixed,29/Apr/10 19:33,16/Apr/19 09:33
Bug,CASSANDRA-1038,12463375,Not all column families are created,"It seems that not all column families will be created via system_add_keyspace in some cases.  To reproduce:

Run stress.py (with CASSANDRA-1033) inserts (I used 1M) against standard columns.  During this run, both Standard1 and Super1 will be created.

Run stress.py again, this time against super columns.  Due to CASSANDRA-1036 no errors will be visible to the client but can be observed in the log.

You can switch the order and stress supers first, in which case Standard1 will not exist.  If you call describe_keyspace on Keyspace1, it will show both CFs even though only one will work.
",gdusbabek,brandon.williams,Normal,Resolved,Fixed,29/Apr/10 21:17,16/Apr/19 09:33
Bug,CASSANDRA-1040,12463432,read failure during flush,"Joost Ouwerkerk writes:
	
On a single-node cassandra cluster with basic config (-Xmx:1G)
loop {
  * insert 5,000 records in a single columnfamily with UUID keys and
random string values (between 1 and 1000 chars) in 5 different columns
spanning two different supercolumns
  * delete all the data by iterating over the rows with
get_range_slices(ONE) and calling remove(QUORUM) on each row id
returned (path containing only columnfamily)
  * count number of non-tombstone rows by iterating over the rows
with get_range_slices(ONE) and testing data.  Break if not zero.
}

while this is running, call ""bin/nodetool -h localhost -p 8081 flush KeySpace"" in the background every minute or so.  When the data hits some critical size, the loop will break.",jbellis,jbellis,Urgent,Resolved,Fixed,30/Apr/10 13:33,16/Apr/19 09:33
Bug,CASSANDRA-1042,12463494,ColumnFamilyRecordReader returns duplicate rows,"There's a bug in ColumnFamilyRecordReader that appears when processing a single split (which happens in most tests that have small number of rows), and potentially in other cases.  When the start and end tokens of the split are equal, duplicate rows can be returned.

Example with 5 rows:
token (start and end) = 53193025635115934196771903670925341736

Tokens returned by first get_range_slices iteration (all 5 rows):
 16955237001963240173058271559858726497
 40670782773005619916245995581909898190
 99079589977253916124855502156832923443
 144992942750327304334463589818972416113
 166860289390734216023086131251507064403

Tokens returned by next iteration (first token is last token from
previous, end token is unchanged)
 16955237001963240173058271559858726497
 40670782773005619916245995581909898190

Tokens returned by final iteration  (first token is last token from
previous, end token is unchanged)
 [] (empty)

In this example, the mapper has processed 7 rows in total, 2 of which
were duplicates.

",jbellis,joosto,Normal,Resolved,Fixed,01/May/10 16:18,16/Apr/19 09:33
Bug,CASSANDRA-1044,12463547,NameSortTest.testNameSort100 fails,"The hudson build is failing due to this test failing: NameSortTest.testNameSort100
http://hudson.zones.apache.org/hudson/job/Cassandra/422/testReport/",gdusbabek,johanoskarsson,Normal,Resolved,Fixed,03/May/10 09:09,16/Apr/19 09:33
Bug,CASSANDRA-1049,12463696,SlicePredicate does not always round-trip correctly,,jbellis,jbellis,Normal,Resolved,Fixed,04/May/10 19:29,16/Apr/19 09:33
Bug,CASSANDRA-1050,12463697,Too many splits for ColumnFamily with only a few rows,"ColumnFamilyInputFormat creates splits for the entire Keyspace.  If one ColumnFamily has 100 Million rows and another has only 100 rows, the number of splits will be the 1,526 (assuming 64k rows per split) for either one, since it is based on the total number of unique keys across the whole keyspace, and not on the number of rows in the ColumnFamily.",johanoskarsson,joosto,Normal,Resolved,Fixed,04/May/10 19:31,16/Apr/19 09:33
Bug,CASSANDRA-1054,12463809,isSuper flag in cfstore is wrongly set in 0.7,"In 0.7, following CASSANDRA-16, the isSuper in ColumnFamilyStore is not set correctly (if I'm correct). 
This is because when the model is applied (AddColumnFamily.applyModels()) the columnFamilyStore
is created before the call to DataDescriptor.setTableDefinition. But the createColumnFamilyStore() 
function retrieve the columnType. This thus always return a null that end up in a ""Super"".equals(null)
that always sets the flag to false.
That being said, the isSuper flag of columnFamilyStore is never used. 
I propose thus to get ride of this flag completely since if needed in the future, the column type can 
always be retrieved from the table and cfname directly (the attached patch do just that).",slebresne,slebresne,Low,Resolved,Fixed,05/May/10 19:22,16/Apr/19 09:33
Bug,CASSANDRA-1056,12463828,size of row in spanned index entries does not include key bytes,"from Anty on the mailing list,

In source code of 0.6.1 ,in SSTableWriter,
private void afterAppend(DecoratedKey decoratedKey, long dataPosition, int dataSize) throws IOException
    {
        String diskKey = partitioner.convertToDiskFormat(decoratedKey);
        bf.add(diskKey);
        lastWrittenKey = decoratedKey;
        long indexPosition = indexFile.getFilePointer();
        indexFile.writeUTF(diskKey);
        indexFile.writeLong(dataPosition);
        if (logger.isTraceEnabled())
            logger.trace(""wrote "" + decoratedKey + "" at "" + dataPosition);
        if (logger.isTraceEnabled())
            logger.trace(""wrote index of "" + decoratedKey + "" at "" + indexPosition);

        indexSummary.maybeAddEntry(decoratedKey, dataPosition, dataSize, indexPosition, indexFile.getFilePointer());
    }
the value of ""dataSize"" is the length of value( column family) ,not including the length of key.

but in  the method  loadIndexFile() of SStableReader
...
    else
                {
                    input.readUTF();
                    nextDataPosition = input.readLong();
                    input.seek(nextIndexPosition);
                }
                indexSummary.maybeAddEntry(decoratedKey, dataPosition, nextDataPosition - dataPosition, indexPosition, nextIndexPosition);
            }
            indexSummary.complete();


the value of nextDataPosition - dataPosition is the length of key and value ,not just the length of value .

",jbellis,jbellis,Low,Resolved,Fixed,05/May/10 22:54,16/Apr/19 09:33
Bug,CASSANDRA-1057,12463842,Login information stored in threads may be reused.,"CassandraServer stores the login information in a ThreadLocal<AccessLevel>.

CassandraDaemon starts the server with 64 threads. When the first 64 clients connect they should get their own thread, but after that threads will be reused.

In a quick test I created a Server with 5 threads, and a ThreadLocal<Integer>, and the value is seen by new clients connecting.

Thrift doesn't destroy the threads when a client disconnects. Maybe an option in Thrift would make more sense to make this method usable.",kenmacd,kenmacd,Low,Resolved,Fixed,06/May/10 02:25,16/Apr/19 09:33
Bug,CASSANDRA-1059,12463896,"Exception when run ""get Keyspace1.Standard1"" command in the CLI","Connected to: ""Test Cluster"" on 10.0.3.44/9160
cassandra> get Keyspace1.Standard1
line 0:-1 mismatched input '<EOF>' expecting '['
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:331)
        at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:74)
        at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:213)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:270)",urandom,jamel.essoussi,Low,Resolved,Fixed,06/May/10 16:26,16/Apr/19 09:33
Bug,CASSANDRA-1062,12463992,update contrib/bmt_example to work post-CASSANDRA-44,the main problem is we need to pass an id to new ColumnFamily(...).  Not sure what the right fix is here.,gdusbabek,jbellis,Low,Resolved,Fixed,07/May/10 14:39,16/Apr/19 09:33
Bug,CASSANDRA-1063,12464004,tombstone-only rows in sstables can be ignored,"ColumnFamilyStoreTest has two tests that pass, that shouldn't.  These are obscuring bugs in tombstone handling.",jbellis,stuhood,Normal,Resolved,Fixed,07/May/10 16:03,16/Apr/19 09:33
Bug,CASSANDRA-1067,12464068,Use batch_mutate in stress.py,"batch_insert was deprecated in trunk, which broke stress.py.",stuhood,stuhood,Normal,Resolved,Fixed,09/May/10 00:21,16/Apr/19 09:33
Bug,CASSANDRA-1076,12464261,StreamingService.StreamDestinations never empties,"The problem is that StreamOutManager.streamManagers never has anything removed from it.  In order for StreamingService.getDestinations() to work properly, we either need to track hosts differently, or remove from StreamOutManager.streamManagers when we are no longer streaming to a node.

I lean towards the former, as any time we call StreamOutManager.get(), we're back in the same boat.",stuhood,gdusbabek,Low,Resolved,Fixed,11/May/10 16:41,12/Aug/20 23:02
Bug,CASSANDRA-1079,12464310,Cache capacity settings done via nodetool get reset on memtable flushes,"In an experiment we set cache capacities via nodetool. The config file had the KeyCache for this CF at 1000000, we set the RowCache to 10000000 via nodetool.

The next time we flushed a memtable for that CF, the cache capacity settings got reverted to what is in the conf file. We repeated the experiment with the same results.",jbellis,kingryan,Normal,Resolved,Fixed,12/May/10 00:18,16/Apr/19 09:33
Bug,CASSANDRA-1080,12464357,NPE when no keyspaces section is found in yaml file,Everything's in the summary,slebresne,slebresne,Low,Resolved,Fixed,12/May/10 15:46,16/Apr/19 09:33
Bug,CASSANDRA-1081,12464359,Thrift sockets leak in 0.6 hadoop interface,"Thrift connections appear not to be closed properly in 0.6 in ColumnFamilyRecordReader, which causes a file descriptor leak on the server and may eventually cause jobs to fail.

This appear to be fixed in 0.7 https://issues.apache.org/jira/browse/CASSANDRA-1017 so it may be worth backporting the patch or add a quick fix to close the Tsockets.",johanoskarsson,riffraff,Low,Resolved,Fixed,12/May/10 15:59,16/Apr/19 09:33
Bug,CASSANDRA-1085,12464395,Minor SliceRange documentation fix,"Minor doc typo, patch attached",yosh,yosh,Low,Resolved,Fixed,12/May/10 21:56,16/Apr/19 09:33
Bug,CASSANDRA-1093,12464558,BinaryMemtable interface silently dropping data.,"I've been attempting to use the Binary Memtable (BMT) interface to load a large number of rows. During my testing, I discovered that on larger loads (~1 million rows), occasionally some of the data never appears in the database. This happens in a non-deterministic manner, as sometimes all the data loads fine, and other times a significant chunk goes missing. No errors are ever logged to indicate a problem. I'm attaching some sample code that approximates my application's usage of Cassandra and explains this bug in more detail.",jbellis,tjungen,Low,Resolved,Fixed,14/May/10 20:13,16/Apr/19 09:33
Bug,CASSANDRA-1094,12464579,Use get_range_slices in stress.py,,stuhood,stuhood,Low,Resolved,Fixed,15/May/10 01:42,16/Apr/19 09:33
Bug,CASSANDRA-1103,12464814,DSS rack-awareness doesn't really work,CASSANDRA-952 fixed most of the DSS issues but the attempted placement of machines on different racks w/in each DC is poor (comparing each node only to the rack of the 1st replica in that DC rather than all).,jbellis,jbellis,Normal,Resolved,Fixed,18/May/10 17:15,16/Apr/19 09:33
Bug,CASSANDRA-1107,12464898,DatacenterShardStrategyTest.testProperties fails,"Stacktrace
junit.framework.AssertionFailedError
	at org.apache.cassandra.locator.DatacenterShardStrategyTest.testProperties(DatacenterShardStrategyTest.java:36)

Standard Error
ERROR 12:39:35,968 Could not find end point information for 127.0.0.1, will use default.

http://hudson.zones.apache.org/hudson/job/Cassandra/440/testReport/junit/org.apache.cassandra.locator/DatacenterShardStrategyTest/testProperties/",jbellis,johanoskarsson,Urgent,Resolved,Fixed,19/May/10 14:58,16/Apr/19 09:33
Bug,CASSANDRA-1109,12464905,Cassandra does not expire data after setting timeToLive argument for each value,"Hello,

I downloaded latest cassandra source code from svn trunk. I wanted to test expire data functionality. Using Thrift API, I set timeToLive parameter for each fieldValue, however cassandra ignored it and did not expire any data.

I debugged cassandra's source code and found a bug in src/java/org/apache/cassandra/db/RowMutation.java.

In RowMutation.addColumnOrSuperColumnToRowMutation() method, QueryPath was not setting timeToLive argument. I updated RowMutation.java locally and tested it and then my data expired after 'n' number of seconds.

I wanted to have this fix in the trunk also.

Index: src/java/org/apache/cassandra/db/RowMutation.java
===================================================================
--- src/java/org/apache/cassandra/db/RowMutation.java   (revision 946222)
+++ src/java/org/apache/cassandra/db/RowMutation.java   (working copy)
@@ -295,12 +295,12 @@
         {
             for (org.apache.cassandra.thrift.Column column : cosc.super_column.columns)
             {
-                rm.add(new QueryPath(cfName, cosc.super_column.name, column.name), column.value, column.timestamp);
+                rm.add(new QueryPath(cfName, cosc.super_column.name, column.name), column.value, column.timestamp, column.ttl);
             }
         }
         else
         {
-            rm.add(new QueryPath(cfName, null, cosc.column.name), cosc.column.value, cosc.column.timestamp);
+            rm.add(new QueryPath(cfName, null, cosc.column.name), cosc.column.value, cosc.column.timestamp, cosc.column.ttl);
         }
     }


Thanks,
Jignesh",jigneshdhruv,jigneshdhruv,Normal,Resolved,Fixed,19/May/10 15:57,16/Apr/19 09:33
Bug,CASSANDRA-1111,12465029,describe_ring() throws on single node clusters and/or probably clusters without a replication factor,"You use Thrift to call describe_ring() on a cluster with only a single node. The Thrift connection is broken, and the system.log shows the exception that has been thrown:

ERROR [pool-1-thread-15] 2010-05-20 13:15:24,753 TThreadPoolServer.java (line 259) Error occurred during processing of message.
java.lang.RuntimeException: No replica strategy configured for L1AbuseReports
        at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:246)
        at org.apache.cassandra.service.StorageService.constructRangeToEndPointMap(StorageService.java:457)
        at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:443)
        at org.apache.cassandra.service.StorageService.getRangeToEndPointMap(StorageService.java:433)
        at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:628)
        at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.process(Cassandra.java:1781)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1125)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:637)",gdusbabek,dccwilliams,Normal,Resolved,Fixed,20/May/10 14:12,16/Apr/19 09:33
Bug,CASSANDRA-1118,12465236,allow overriding existing token owner with a new IP,"We'd like to support replacing one node with another at the same IP (e.g. when the data is on Amazon's EBS and can easily be mounted to a new host), as noted in CASSANDRA-872.  But in practice this is reported to not work w/o a cluster restart (can't find the ML thread now ... ?)",gdusbabek,jbellis,Low,Resolved,Fixed,24/May/10 02:26,16/Apr/19 09:33
Bug,CASSANDRA-1119,12465237,detect incomplete commitlogheader,"Kelvin reported:

I just came across a corrupted CL file.  Here's the stacktrace when starting the server:
Listening for transport dt_socket at address: 8888
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:758)
	at org.apache.cassandra.db.commitlog.CommitLogHeader.readCommitLogHeader(CommitLogHeader.java:145)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:181)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:167)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:95)
	at org.apache.cassandra.thrift.CassandraDaemon.init(CassandraDaemon.java:142)
	... 5 more

He added that the segment is only 6 bytes long, indicating that the header was never completely written.  CLH should catch that EOF and skip the segment when replay is attempted.",mdennis,jbellis,Low,Resolved,Fixed,24/May/10 02:28,16/Apr/19 09:33
Bug,CASSANDRA-1122,12465309,Lack of subcomparator_type will corrupt the keyspace in Thrift system_add_keyspace(),"I had a problem earlier where I create a Keyspace by reading the default yaml config shipped with the above cassandra package and after parsing and creating the keyspace with PHP Thrift system_add_keysapce command, I would get 'TException: Error: Internal error processing describe_keyspace ' when trying to get describe_keyspace. In cassandra-cli I get the same error.

The cassandra log shows a null pointed exception:
ERROR [pool-1-thread-39] 2010-05-24 14:17:35,204 Cassandra.java (line 1943) Internal error processing describe_keyspace
java.lang.NullPointerException
	at org.apache.cassandra.thrift.CassandraServer.describe_keyspace(CassandraServer.java:476)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspace.process(Cassandra.java:1939)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1276)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

I traced down the problem to where I define cassandra_CfDef. When the type is Super and subcomparator_type is not set, Thrift code does not set it to any default value but blank """". The command system_add_keyspace() runs with no problem. Whatever happens in Cassandra afterwards, will create a useless keyspace.

Here is my code snippet to generate the exception:

 <?php
$GLOBALS['THRIFT_ROOT'] = '/usr/share/php/Thrift';
require_once $GLOBALS['THRIFT_ROOT'].'/packages/cassandra/Cassandra.php';
require_once $GLOBALS['THRIFT_ROOT'].'/packages/cassandra/cassandra_types.php';
require_once $GLOBALS['THRIFT_ROOT'].'/transport/TSocket.php';
require_once $GLOBALS['THRIFT_ROOT'].'/protocol/TBinaryProtocol.php';
require_once $GLOBALS['THRIFT_ROOT'].'/transport/TFramedTransport.php';
require_once $GLOBALS['THRIFT_ROOT'].'/transport/TBufferedTransport.php';

try {
  // Make a connection to the Thrift interface to Cassandra
  $socket = new TSocket('127.0.0.1', 9160);
  $transport = new TBufferedTransport($socket, 1024, 1024);
  $protocol = new TBinaryProtocolAccelerated($transport);
  $client = new CassandraClient($protocol);
  $transport->open();

  //Try creating some keyspace/column family defs
  $ks = new cassandra_KsDef();
  $ks->name = 'agoudarzi_Keyspace1';
  $ks->strategy_class = 'org.apache.cassandra.locator.RackUnawareStrategy';
  $ks->replication_factor = '1';
  
  //Now add a column family to it
  $cf = new cassandra_CfDef();
  $cf->name = 'Super3';
  $cf->table = 'agoudarzi_Keyspace1';
  $cf->column_type = 'Super';
  $cf->comparator_type = 'LongType';
  $cf->row_cache_size = '0';
  $cf->key_cache_size = '50';
  $cf->comment = 'A column family with supercolumns, whose column names are Longs (8 bytes)';
  $ks->cf_defs[] = $cf;

  $client->system_add_keyspace($ks);
  
  sleep(2);
  
  //Try to check if keyspace description is good
  $client->set_keyspace('agoudarzi_Keyspace1');
  $rs = $client->describe_keyspace('agoudarzi_Keyspace1');

  $transport->close();

} catch (TException $tx) {
   print 'TException: '.$tx->why. ' Error: '.$tx->getMessage() . ""\n"";
}
?>

I think a default subcomparator should be set by thrift or other defensive method be used to prevent this problem.

Please investigate. 

Thanks.",gdusbabek,arya,Normal,Resolved,Fixed,24/May/10 21:27,16/Apr/19 09:33
Bug,CASSANDRA-1128,12465390,sstable2json spews because it uses DatabaseDescriptor before loadSchemas() is called,sstable2json depends on DatabaseDescriptor for ColumnFamily meta data.  DD requires loadSchemas() is called before the CFMD can be accesed.  nothing in the code path in sstable2json calls loadSchemas().,mdennis,mdennis,Low,Resolved,Fixed,25/May/10 17:14,16/Apr/19 09:33
Bug,CASSANDRA-1129,12465392,"Using KeysCached=""xx%"" results in Key cache capacity: 1","I don't know if this is a general bug or only something related to my instance, but for me (v0.6.1) I've noticed that when defining KeysCached=""50%"" (or KeysCached=""100%"" and I didn't test other values with %) then cfstats reports Key cache capacity: 1

      <ColumnFamily CompareWith=""BytesType"" Name=""KvAds""
        KeysCached=""100%""
        RowsCached=""10000""
        />


                Column Family: KvAds
                SSTable count: 7
                Space used (live): 797535964
                Space used (total): 797535964
                Memtable Columns Count: 42292
                Memtable Data Size: 10514176
                Memtable Switch Count: 24
                Read Count: 2563704
                Read Latency: 4.590 ms.
                Write Count: 1963804
                Write Latency: 0.025 ms.
                Pending Tasks: 0
                Key cache capacity: 1
                Key cache size: 1
                Key cache hit rate: 0.0
                Row cache capacity: 10000
                Row cache size: 10000
                Row cache hit rate: 0.2206178354382234
                Compacted row minimum size: 386
                Compacted row maximum size: 9808
                Compacted row mean size: 616

I'll attach one of the sstable files from this CF",,rantav,Low,Resolved,Fixed,25/May/10 17:23,16/Apr/19 09:33
Bug,CASSANDRA-1130,12465398,Row iteration can stomp start-of-row mark,"Hello,

I am trying to use TTL (timeToLive) feature in SuperColumns.
My usecase is:
- I have a SuperColumn and 3 subcolumns.
- I try to expire data after 60 seconds.

While Cassandra is up and running, I am successfully able to push and read data without any problems. Data compaction and all occurs fine. After inserting say about 100000 records, I stop Cassandra while data is still coming.

On startup Cassandra throws an exception and won't start up. (This happens 1 in every 3 times). Exception varies like:
- EOFException while reading data
- negative value encountered exception
- Heap Space Exception

Cassandra simply won't start up.

Again I get this problem only when I use TTL with SuperColumns. There are no issues with using TTL with regular Columns.

I tried to diagnose the problem and it seems to happen on startup when it sees a Column that is marked Deleted and its trying to read data. Its off by some bytes and hence all these exceptions.

Caused by: java.io.IOException: Corrupt (negative) value length encountered
        at org.apache.cassandra.utils.FBUtilities.readByteArray(FBUtilities.java:317)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:84)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:336)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:285)
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.getNextBlock(SSTableSliceIterator.java:235)
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.pollColumn(SSTableSliceIterator.java:195)
        ... 18 more


Let me know if you need more information.

Thanks,
Jignesh",slebresne,jigneshdhruv,Normal,Resolved,Fixed,25/May/10 18:13,16/Apr/19 09:33
Bug,CASSANDRA-1139,12465613,batch_mutate Deletion with column family type mismatch causes RuntimeException,"When specifying a super column family name inside a Deletion and a standard column family name in the mutations dictionary, we get a RuntimeException in the server and a TimedOutException on the client:

{noformat}
ERROR 14:22:29,757 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.SuperColumn.timestamp(SuperColumn.java:137)
        at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:65)
        at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:29)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:87)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:73)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:337)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:349)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:322)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:275)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:200)
        at org.apache.cassandra.service.StorageProxy$3.runMayThrow(StorageProxy.java:310)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 14:22:29,757 Fatal exception in thread Thread[ROW-MUTATION-STAGE:39,5,main]
{noformat}

{noformat}
Traceback (most recent call last):
  File ""./test.py"", line 15, in <module>
    client.batch_mutate(""Keyspace1"", mutations, ConsistencyLevel.QUORUM)
  File ""cassandra/Cassandra.py"", line 771, in batch_mutate
    self.recv_batch_mutate()
  File ""cassandra/Cassandra.py"", line 798, in recv_batch_mutate
    raise result.te
cassandra.ttypes.TimedOutException: TimedOutException()
{noformat}

To reproduce:

{noformat}
from thrift.transport.TSocket import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocol
from cassandra.Cassandra import Client
from cassandra.ttypes import Deletion, Mutation, ConsistencyLevel

if __name__ == ""__main__"":
    tsocket = TSocket('localhost', 9160)
    tsocket.open()
    tprotocol = TBinaryProtocol(tsocket)
    client = Client(tprotocol)
    deletion = Deletion(1, 'supercolumn', None)
    mutation = Mutation(deletion=deletion)
    mutations = { 'key' : { 'Standard1' : [ mutation ] } }
    client.batch_mutate(""Keyspace1"", mutations, ConsistencyLevel.QUORUM)
{noformat}

",mdennis,tholzer,Low,Resolved,Fixed,28/May/10 02:29,16/Apr/19 09:33
Bug,CASSANDRA-1145,12465813,Reading with CL > ONE returns multiple copies of the same column per key.,"Testing with 0.6-trunk today:

Reading with CL > ONE returns multiple copies of the same column per key consistent with the replicas queried before return. i.e, for RC=3, a QUORUM read yields 2 copies and an ALL read returns 3.
This is with pycassa get_range() which is using get_range_slice()

I see the same behavior with 0.6.1 and 0.6.2 debs

If my experience is not unique, anyone using get_range_slice is now deluged with duplicate data.
",jeromatron,ajslater,Normal,Resolved,Fixed,31/May/10 23:23,16/Apr/19 09:33
Bug,CASSANDRA-1146,12465830,"""Expected both token and generation columns""","From the mailing list:

{code}
ERROR 16:14:35,975 Exception encountered during startup.
java.lang.RuntimeException: Expected both token and generation columns; found ColumnFamily(LocationInfo [Generation:false:4@4,])
    at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:159)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:305)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:99)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:177)
Exception encountered during startup.
{code}

Separately, the same user wrote: ""I added a server to my cluster. It had some junk in the system/LocationInfo files from previous, unsuccessful attempts to add the server to the cluster. (They were unsuccessful because I hadn't opened the port on that computer.)""

Perhaps that is why it was able to create the Generation column but not the Token?
",gdusbabek,jbellis,Normal,Resolved,Fixed,01/Jun/10 06:09,16/Apr/19 09:33
Bug,CASSANDRA-1147,12465832,cassandra cannot bootstrap when using DatacenterShardStrategy,"If C is configured to use DSS, the bootstrap process never completes.
",mdennis,mdennis,Normal,Resolved,Fixed,01/Jun/10 07:03,16/Apr/19 09:33
Bug,CASSANDRA-1148,12465842,Update stress.py to thrift api changes,The stress.py file was not updated to the CASSANDRA-1070 changes,johanoskarsson,johanoskarsson,Normal,Resolved,Fixed,01/Jun/10 09:35,16/Apr/19 09:33
Bug,CASSANDRA-1150,12465974,"pig contrib module not building, other errors","Currently, the pig contrib module fails to build because of dependency issues - it looks like dependencies like hadoop that were at one time in the main cassandra dependency list.

Also, once the dependencies are resolved, there are still errors when running the example pig query in the README.txt in the module.

This ticket would address both of those issues and getting it working both on 0.6.x as well as mainline trunk.",jeromatron,jeromatron,Low,Resolved,Fixed,02/Jun/10 15:53,16/Apr/19 09:33
Bug,CASSANDRA-1152,12466039,Read operation with ConsistencyLevel.ALL throws exception,"Read operations which use thrift.CassandraServer#readColumnFamily should allow consistency_level == ALL.
Current implementation just throws InvalidRequestException when consistency level is ALL.
Same thing applies to avro implementation.",gdusbabek,yukim,Normal,Resolved,Fixed,03/Jun/10 02:18,16/Apr/19 09:33
Bug,CASSANDRA-1160,12466186,race with insufficiently constructed Gossiper,"Gossiper.start needs to be integrated into the constructor.  Currently you can have threads using the gossiper instance before start finishes (or even starts?), resulting in tracebacks like this:

ERROR [GMFD:1] 2010-06-02 10:45:49,878 CassandraDaemon.java (line 78) Fatal exception in thread Thread[GMFD:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.net.Header.<init>(Header.java:56)
	at org.apache.cassandra.net.Header.<init>(Header.java:74)
	at org.apache.cassandra.net.Message.<init>(Message.java:58)
	at org.apache.cassandra.gms.Gossiper.makeGossipDigestAckMessage(Gossiper.java:294)
	at org.apache.cassandra.gms.Gossiper$GossipDigestSynVerbHandler.doVerb(Gossiper.java:935)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
ERROR [GMFD:2] 2010-06-02 10:45:49,880 CassandraDaemon.java (line 78) Fatal exception in thread Thread[GMFD:2,5,main]
java.lang.AssertionError
	at org.apache.cassandra.net.Header.<init>(Header.java:56)
	at org.apache.cassandra.net.Header.<init>(Header.java:74)
	at org.apache.cassandra.net.Message.<init>(Message.java:58)
	at org.apache.cassandra.gms.Gossiper.makeGossipDigestAckMessage(Gossiper.java:294)
	at org.apache.cassandra.gms.Gossiper$GossipDigestSynVerbHandler.doVerb(Gossiper.java:935)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)",mdennis,jbellis,Low,Resolved,Fixed,04/Jun/10 13:59,16/Apr/19 09:33
Bug,CASSANDRA-1168,12466308,system_drop_column_family() and system_rename_column_family() should not take keyspace args,"With the addition of set_keyspace(), things that are scoped by keyspace should no longer take keyspace args.  system_add_column_family() is correct in only taking a cf_def.  system_drop_column_family() and system_rename_column_family() should be changed to match.
",mdennis,benjaminblack,Low,Resolved,Fixed,07/Jun/10 02:25,16/Apr/19 09:33
Bug,CASSANDRA-1169,12466310,AES makes Streaming unhappy,"Streaming service assumes there will only be one stream from S to T at a time for any nodes S and T.  For the original purpose of node movement, this was a reasonable assumption (any node T can only perform one move at a time) but AES throws off streaming tasks much more frequently than that given the right conditions, which will de-sync the fragile file ordering that Streaming assumes (that T knows which files S is going to send, in what order).  Eventually T is expecting file F1 but S sends a smaller file F2, leading to an infinite loop on T while it waits for F1 to finish, and T waits for S to acknowledge F2, which it never will.

For 0.6 maybe the best solution is for AES to manually wait for one of its streaming tasks to finish, before it allows itself to create another.  For 0.7 it would be nice to make Streaming more robust.  The whole 4-stage-ack process seems very fragile, and poking around in parent objects via inetaddress keys makes reasoning about small pieces impossible b/c of encapsulation violations.",gdusbabek,jbellis,Urgent,Resolved,Fixed,07/Jun/10 03:41,16/Apr/19 09:33
Bug,CASSANDRA-1172,12466399,update gc options for debian package,"/etc/default/cassandra needs the new jvm options from cassandra.in.sh

also, it looks like heap size is also set-able in the init.d script, but it was ignored in favor of the values from /etc/default/cassandra, it would be less confusing to leave those out of the init.d script",urandom,jbellis,Low,Resolved,Fixed,07/Jun/10 23:03,16/Apr/19 09:33
Bug,CASSANDRA-1173,12466401,Debian packaging refers to now nonexistent DISCLAIMER.txt,Debian packaging refers to now nonexistent DISCLAIMER.txt. Trivial patch attached.,yosh,yosh,Low,Resolved,Fixed,07/Jun/10 23:08,16/Apr/19 09:33
Bug,CASSANDRA-1174,12466404,"Debian packaging should auto-detect the JVM, not require OpenJDK","The current init.d script for Debian-packaged Cassandra has the OpenJDK's JAVA_HOME hard-coded in, making it impossible to use sun-java6 without modifying the file. Ideally it should use the same sort of auto-detection logic used by other Debian-packaged Java projects to figure out which JVM it should use.

(I have a patch for this that I'll upload shortly.)",codahale,codahale,Low,Resolved,Fixed,07/Jun/10 23:22,16/Apr/19 09:33
Bug,CASSANDRA-1178,12466596,"get_slice calls do not close files when finished resulting in ""too many open files"" exceptions and rendering C unusable","insert ~100K rows.  Read them back in a loop.  Notice ""too many open files"" exceptions in log.  SSTableSliceIterator is never closing the files.

",mdennis,mdennis,Normal,Resolved,Fixed,09/Jun/10 19:47,16/Apr/19 09:33
Bug,CASSANDRA-1179,12466697,split commitlog into header + mutations files,"As mentioned in CASSANDRA-1119, it seems possible that a commitlog header could be corrupted by a power loss during update of the header, post-flush.  We could try to make it more robust (by writing the size of the commitlogheader first, and skipping to the end if we encounter corruption) but it seems to me that the most foolproof method would be to split the log into two files: the header, which we'll overwrite, and the data, which is truly append only.  If If the header is corrupt on reply, we just reply the data from the beginning; the header allows us to avoid replaying data redundantly, but it's strictly an optimization and not required for correctness.",mdennis,jbellis,Normal,Resolved,Fixed,10/Jun/10 23:02,16/Apr/19 09:33
Bug,CASSANDRA-1180,12466698,read_repair_chance is missing from CfDef,CfDef is missing read_repair_chance if that is going to remain a configuration option in 0.7.,gdusbabek,arya,Low,Resolved,Fixed,10/Jun/10 23:02,16/Apr/19 09:33
Bug,CASSANDRA-1188,12466908,multiget_slice calls do not close files resulting in file descriptor leak,"Insert  1000 rows into a super column family. Read them back in a loop using multiget_slice. Note leaked file descriptors with lsof:
lsof -p `ps ax | grep [C]assandraDaemon | awk '{print $1}'` | awk '{print $9}' | sort | uniq -c | sort -n | tail -n 5

Looks like SSTableNamesIterator is never closing the files it creates via the sstable ...?

This is similar to CASSANDRA-1178 except for use of multiget_slice instead of get_slice

",mdennis,wr0ngway,Urgent,Resolved,Fixed,14/Jun/10 15:12,16/Apr/19 09:33
Bug,CASSANDRA-1190,12466935,Remove automatic repair sessions,"Currently both manual and automatic repair sessions use the same timeout value: TREE_STORE_TIMEOUT. This has the very negative effect of setting a maximum time that compaction can take before a manual repair will fail.

For automatic/natural repairs (triggered by two nodes autonomously finishing major compactions around the same time), you want a relatively low TREE_STORE_TIMEOUT value, because trees generated a long time apart will cause a lot of unnecessary repair. The current value is 10 minutes, to optimize for this case.

On the other hand, for manual repairs, TREE_STORE_TIMEOUT needs to be significantly higher. For instance, if a manual repair is triggered for a source node A storing 2 TB of data, and a destination node B with an empty store, then node B needs to wait long enough for node A to finish compacting 2 TB of data, which might take > 12 hours. If a node B times out the local tree before node A sends its tree, then the repair will not occur.",stuhood,stuhood,Normal,Resolved,Fixed,14/Jun/10 21:46,16/Apr/19 09:33
Bug,CASSANDRA-1198,12467113,"In a cluster, get_range_slices() does not return all the keys it should","Row iteration with get_range_slices() does not return all keys. This behaviour only occurs with more than one node and depends on how the nodes are located on the ring.

To reproduce, insert some records into a cluster with more than one node. A subsequent row iteration will return fewer records than were inserted. This has been observed when 1) inserting into a single node, bootstrapping a second node then using get_range_slices() and 2) inserting into a cluster of several nodes then using get_range_slices().

This appears to be similar to https://issues.apache.org/jira/browse/CASSANDRA-781",jbellis,cgist,Normal,Resolved,Fixed,16/Jun/10 18:26,16/Apr/19 09:33
Bug,CASSANDRA-1203,12467270,system_drop_keyspace can cause a node to be unstartable,"calling thriftClient_.system_drop_keyspace(keyspaceName) on a newly created keyspace, then stopping the node renders the node unstartable. Results in the following stacktrace:

10/06/17 14:23:16 ERROR thrift.CassandraDaemon: Fatal exception during initialization
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at java.io.DataInputStream.readUTF(DataInputStream.java:592)
	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
	at org.apache.cassandra.config.KSMetaData.deserialize(KSMetaData.java:92)
	at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:75)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:422)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:103)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:221)

my repro:

start new node with empty data directory
create a new keyspace
drop the keyspace
attempt to restart the node, notice that it fails to start.
",gdusbabek,mbryant,Normal,Resolved,Fixed,17/Jun/10 21:28,16/Apr/19 09:33
Bug,CASSANDRA-1216,12467577,removetoken drops node from ring before re-replicating its data is finished,this means that if something goes wrong during the re-replication (e.g. a source node is restarted) there is (a) no indication that anything has gone wrong and (b) no way to restart the process (other than the Big Hammer of running repair),nickmbailey,jbellis,Normal,Resolved,Fixed,22/Jun/10 14:28,16/Apr/19 09:33
Bug,CASSANDRA-1219,12467621,avoid creating a new byte[] for each mutation replayed,,jbellis,jbellis,Low,Resolved,Fixed,22/Jun/10 21:41,16/Apr/19 09:33
Bug,CASSANDRA-1221,12467688,loadbalance operation never completes on a 3 node cluster,"Arya Goudarzi reports:

Please confirm if this is an issue and should be reported or I am doing something wrong. I could not find anything relevant on JIRA:

Playing with 0.7 nightly (today's build), I setup a 3 node cluster this way:

 - Added one node;
 - Loaded default schema with RF 1 from YAML using JMX;
 - Loaded 2M keys using py_stress;
 - Bootstrapped a second node;
 - Cleaned up the first node;
 - Bootstrapped a third node;
 - Cleaned up the second node;

I got the following ring:

Address       Status     Load          Range                                      Ring
                                      154293670372423273273390365393543806425
10.50.26.132  Up         518.63 MB     69164917636305877859094619660693892452     |<--|
10.50.26.134  Up         234.8 MB      111685517405103688771527967027648896391    |   |
10.50.26.133  Up         235.26 MB     154293670372423273273390365393543806425    |-->|

Now I ran:

nodetool --host 10.50.26.132 loadbalance

It's been going for a while. I checked the streams

nodetool --host 10.50.26.134 streams
Mode: Normal
Not sending any streams.
Streaming from: /10.50.26.132
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-3-Data.db/[(0,22206096), (22206096,27271682)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-4-Data.db/[(0,15180462), (15180462,18656982)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-5-Data.db/[(0,353139829), (353139829,433883659)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-6-Data.db/[(0,366336059), (366336059,450095320)]

nodetool --host 10.50.26.132 streams
Mode: Leaving: streaming data to other nodes
Streaming to: /10.50.26.134
  /var/lib/cassandra/data/Keyspace1/Standard1-d-48-Data.db/[(0,366336059), (366336059,450095320)]
Not receiving any streams.

These have been going for the past 2 hours.

I see in the logs of the node with 134 IP address and I saw this:

INFO [GOSSIP_STAGE:1] 2010-06-22 16:30:54,679 StorageService.java (line 603) Will not change my token ownership to /10.50.26.132

So, to my understanding from wikis loadbalance supposed to decommission and re-bootstrap again by sending its tokens to other nodes and then bootstrap again. It's been stuck in streaming for the past 2 hours and the size of ring has not changed. The log in the first node says it has started streaming for the past hours:

INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,255 StreamOut.java (line 72) Beginning transfer process to /10.50.26.134 for ranges (154293670372423273273390365393543806425,69164917636305877859094619660693892452]
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,255 StreamOut.java (line 82) Flushing memtables for Keyspace1...
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,266 StreamOut.java (line 128) Stream context metadata [/var/lib/cassandra/data/Keyspace1/Standard1-d-48-Data.db/[(0,366336059), (366336059,450095320)]] 1 sstables.
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,267 StreamOut.java (line 135) Sending a stream initiate message to /10.50.26.134 ...
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,267 StreamOut.java (line 140) Waiting for transfer to /10.50.26.134 to complete
 INFO [FLUSH-TIMER] 2010-06-22 17:36:53,370 ColumnFamilyStore.java (line 359) LocationInfo has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1277249454413.log', position=720)
 INFO [FLUSH-TIMER] 2010-06-22 17:36:53,370 ColumnFamilyStore.java (line 622) Enqueuing flush of Memtable(LocationInfo)@1637794189
 INFO [FLUSH-WRITER-POOL:1] 2010-06-22 17:36:53,370 Memtable.java (line 149) Writing Memtable(LocationInfo)@1637794189
 INFO [FLUSH-WRITER-POOL:1] 2010-06-22 17:36:53,528 Memtable.java (line 163) Completed flushing /var/lib/cassandra/data/system/LocationInfo-d-9-Data.db
 INFO [MEMTABLE-POST-FLUSHER:1] 2010-06-22 17:36:53,529 ColumnFamilyStore.java (line 374) Discarding 1000


Nothing more after this line.

Am I doing something wrong?",gdusbabek,gdusbabek,Normal,Resolved,Fixed,23/Jun/10 12:39,16/Apr/19 09:33
Bug,CASSANDRA-1235,12468005,BytesType and batch mutate causes encoded bytes of non-printable characters to be dropped,"When running the two tests, individual column insert works with the values generated.  However, batch insert with the same values causes an encoding failure on the key.  It appears bytes are dropped from the end of the byte array that represents the key value.  See the attached unit test",messi,tnine,Urgent,Resolved,Fixed,28/Jun/10 03:02,16/Apr/19 09:33
Bug,CASSANDRA-1236,12468055,"when I start up the cassandra-cli, a ClassNotFoundException occured:java.lang.ClassNotFoundException: org.apache.cassandra.cli.CliMain","After start up the cassandra server, I went to the bin/ directory and run the cassandra-cli, but there's an Exception throwed out, I have set the CASSANDRA_HOME system variable,  I don't know why
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/cli/CliMain
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.cli.CliMain
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:276)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)",gdusbabek,ialand,Low,Resolved,Fixed,28/Jun/10 13:22,16/Apr/19 09:33
Bug,CASSANDRA-1237,12468077,Store AccessLevels externally to IAuthenticator,"Currently, the concept of authentication (proving the identity of a user) is mixed up with permissions (determining whether a user is able to create/read/write databases). Rather than determining the permissions that a user has, the IAuthenticator should only be capable of authenticating a user, and permissions (specifically, an AccessLevel) should be stored consistently by Cassandra.

EDIT: Adding summary

----

In summary, there appear to be 3 distinct options for how to move forward with authorization. Remember that this ticket is about disconnecting authorization (permissions) from authentication (user/group identification), and its goal is to leave authentication pluggable.

Options:
# Leave authentication and authorization in the same interface. If we choose this option, this ticket is invalid, and CASSANDRA-1271 and CASSANDRA-1320 will add-to/improve IAuthenticator
** Pros:
*** Least change
** Cons:
*** Very little actually implemented by Cassandra: burden is on the backend implementors
*** Each combination of authz and authc backends would require a new implementation (PAM for authc + permissions keyspace for authz, for instance), causing an explosion of implementations
# Separate out a pluggable IAuthority interface to implement authorization
## IAuthenticator interface would be called at login time to determine user/groups membership
## IAuthority would be called at operation time with the user/groups determined earlier, and the required permission for the operation
** Pros:
*** Provides the cleanest separation of concerns
*** Allows plugability for permissions
** Cons:
*** Pluggability of permissions gains limited benefit
*** IAuthority would need to support callbacks for keyspace/cf creation and removal to keep existing keyspaces in sync with their permissions (although technically, option 1 suffers from this as well)
# Separate authorization, but do not make it pluggable
** This option is implemented by the existing patchset by attaching permissions to metadata, but could have an alternative implementation that stores permissions in a permissions keyspace.
** Pros:
*** Cassandra controls the scalability of authorization, and can ensure it does not become a bottleneck
** Cons:
*** Would need to support callbacks for user creation and removal to keep existing users in sync with their permissions",stuhood,stuhood,Normal,Resolved,Fixed,28/Jun/10 18:29,16/Apr/19 09:33
Bug,CASSANDRA-1238,12468180,Cassandra AVRO api is missing system_add_column_family,"The cassandra.avpr does contain the method system_add_keyspace but is missing the system_add_column_family.

Workaround: have to revert to the thrift API to create column families.
",toulmean,next2you,Normal,Resolved,Fixed,29/Jun/10 17:45,16/Apr/19 09:33
Bug,CASSANDRA-1241,12468307,"config file option DiskAccessMode has no-op option ""mmap_index_only""","Per http://wiki.apache.org/cassandra/StorageConfiguration :
""
Access mode. mmapped i/o is substantially faster, but only practical on a 64bit machine (which notably does not include EC2 ""small"" instances) or relatively small datasets. ""auto"", the safe choice, will enable mmapping on a 64bit JVM. Other values are ""mmap"", ""mmap_index_only"" (which may allow you to get part of the benefits of mmap on a 32bit machine by mmapping only index files) and ""standard"". (The buffer size settings that follow only apply to standard, non-mmapped i/o.)
""

The actual code referring to ""mmap_index_only"" is in src/java/org/apache/cassandra/config/DatabaseDescriptor.java :
""
 public static enum DiskAccessMode {
        auto,
        mmap,
        mmap_index_only,
        standard,
    }
...
    private static DiskAccessMode diskAccessMode;
    private static DiskAccessMode indexAccessMode;
...
            if (diskAccessMode == DiskAccessMode.auto)
            {
                diskAccessMode = System.getProperty(""os.arch"").contains(""64"") ? DiskAccessMode.mmap : DiskAccessMode.standard;
                indexAccessMode = diskAccessMode;
                logger.info(""Auto DiskAccessMode determined to be "" + diskAccessMode);
            }
            else if (diskAccessMode == DiskAccessMode.mmap_index_only)
            {
                diskAccessMode = DiskAccessMode.standard;
                indexAccessMode = DiskAccessMode.mmap;
            }
            else
            {
                indexAccessMode = diskAccessMode;
            }
""

As indicated by this snippet, IndexAccessMode is set to ""mmap"" if ""mmap_index_only"" is set in the conf file. However it does not appear that IndexAccessMode or getIndexAccessMode() are used by any other cassandra code.

""
~/repos/cassandra$ grep -ri indexAccessMode .
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:    private static Config.DiskAccessMode indexAccessMode;
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:                indexAccessMode = conf.disk_access_mode;
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:                indexAccessMode = Config.DiskAccessMode.mmap;
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:                indexAccessMode = conf.disk_access_mode;
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:    public static Config.DiskAccessMode getIndexAccessMode()
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:        return indexAccessMode;
""

If I understand the code correctly, this means that setting DiskAccessMode to ""mmap_index_only"" is functionally the same as setting it to ""standard."" As people might be tempted to try/test ""mmap_index_only"" as a mode in-between ""standard"" and ""mmap"" in order to mitigate concerns about https://issues.apache.org/jira/browse/CASSANDRA-1214, it would probably be good to either complete the feature or remove the configuration option. I am willing to submit a patch for the latter and fix the docs if that's the decision.
",jbellis,rcoli,Low,Resolved,Fixed,30/Jun/10 23:35,16/Apr/19 09:33
Bug,CASSANDRA-1242,12468314,"When loading an AbstractType that does not include an instance field, an unhelpful exception is raised making diagnosis difficult","0.7.0 changes the contract for creating AbstractTypes. A custom AbstractType defined against 0.6.x will be incompatible and the error messaging around why the comparator is invalid is obtuse and non-obvious. Specifically, when porting a valid AbstractType from 0.6.x to 0.7 that does not include a public static instance field, the thrift system_add_column_family call will throw an exception whose only message is:

InvalidRequestException(why:instance)

No log messages are generated from the server as to the issue so the root cause is non obvious to developers.

I marked as Major because types defined against 0.6.x did not require an ""instance"" field so at a minimum migration of AbstractTypes to 0.7 should document the change in what is expected of AbstractTypes.

Patch attached for better logging and to create a more helpful exception for better communication to the client as to the issue.
",eonnen,eonnen,Normal,Resolved,Fixed,01/Jul/10 01:21,16/Apr/19 09:33
Bug,CASSANDRA-1246,12468445,Hadoop output SlicePredicate is slow and doesn't work as intended,"The output SlicePredicate is only used to attempt to check that no data exists in the range that we're going to be writing data.  This is 

(a) slow, since it performs get_range_slices across the entire key range, meaning we'll hit every node in the cluster if there is no data (which is supposed to be the normal case)
(b) wrong, since it appears to be intended to use keyList.size to allow data in column X to not interfere with an output to column Y, but that is not how get_range_slices works -- if you have data (or even a tombstone) in any column, you'll get the key back in your result list.  so what you would have to do is scan every key, and check the list of columns returned, which in the case of data actually existing in other columns will be prohibitively slow
",jbellis,jbellis,Normal,Resolved,Fixed,02/Jul/10 15:30,16/Apr/19 09:33
Bug,CASSANDRA-1259,12468770,incorrect package name for property_snitch class files,"The classes in contrib/property_snitch both have ""src.java"" prepended to them (probably a bug introduced by an IDE).

The attached trivial patch fixes this.",urandom,urandom,Low,Resolved,Fixed,07/Jul/10 22:44,16/Apr/19 09:33
Bug,CASSANDRA-1262,12468940,stress.py stdev option should be float not int,"The option to set the standard deviation parameter for the gaussian key generator defaults to 0.1 but has a type of int in the option parser.  As a result, it's impossible to use non-integer standard deviation values when testing.",brandon.williams,oby1,Low,Resolved,Fixed,09/Jul/10 17:00,16/Apr/19 09:33
Bug,CASSANDRA-1270,12469062,Path not found under Windows 7,"I'm not sure that this is bug maybe it is my fault but when I try to run Cassandra using bin\cassandra -f my system returns ""Path not found message"". When i comment ECHO OFF from cassandra.bat I have seen that last line of output contains "".8.jar"";""D:\Cassandra\bin\..\lib\slf4j-log4j12-1.5.8.jar"";""D:\Cassandra\bin\..\build\classes"" ""org.apache.cassandra.thrift.CassandraDaemon""""
D:\Cassandra is my cassandra root directory. Directory ""D:\Casandra\build\classes\org\apache\cassandra\thrift"" contains CassandraDaemon.class, CassandraDaemon$1.class, CassandraDaemon$2.class files.

Apologize for my Vocabulary and Grammar.
",,szogun1987,Low,Resolved,Fixed,12/Jul/10 12:57,16/Apr/19 09:33
Bug,CASSANDRA-1273,12469177,describe_keyspace not returning reconciler info,describe_keyspace should return info on its reconciler.,jeromatron,jeromatron,Low,Resolved,Fixed,13/Jul/10 17:08,16/Apr/19 09:33
Bug,CASSANDRA-1274,12469178,Exception while recovering commitlog when debug logging enabled,"On a cluster with debug logging enabled the commit log fails to recover on start. An UTF8 exception is thrown when trying to toString a column from the system column family LocationInfo. That CF is using UTF8Type but I suspect the column name in this specific case is a byte representation of an ip address, and as such not a valid UTF8 string. That column is most perhaps created in SystemTable line 74.

Full exception stack trace:
ERROR [main] 2010-07-13 11:03:17,050 AbstractCassandraDaemon.java (line 107) Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: invalid UTF8 bytes [10, -48, 40, -124]
        at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:43)
        at org.apache.cassandra.db.Column.getString(Column.java:200)
        at org.apache.cassandra.db.marshal.AbstractType.getColumnsString(AbstractType.java:85)
        at org.apache.cassandra.db.ColumnFamily.toString(ColumnFamily.java:393)
        at org.apache.commons.lang.ObjectUtils.toString(ObjectUtils.java:241)
        at org.apache.commons.lang.StringUtils.join(StringUtils.java:3073)
        at org.apache.commons.lang.StringUtils.join(StringUtils.java:3133)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:250)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:171)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:221)
",mdennis,johanoskarsson,Low,Resolved,Fixed,13/Jul/10 17:10,16/Apr/19 09:33
Bug,CASSANDRA-1279,12469264,heisenbug in RoundRobinSchedulerTest,"Occasionally I see this error in the test suite:

    [junit] Testcase: testScheduling(org.apache.cassandra.scheduler.RoundRobinSchedulerTest):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.cassandra.scheduler.RoundRobinSchedulerTest.testScheduling(RoundRobinSchedulerTest.java:90)
    [junit] 
",rnirmal,jbellis,Normal,Resolved,Fixed,14/Jul/10 15:58,16/Apr/19 09:33
Bug,CASSANDRA-1286,12469470,Nodetool ring crashes when no schema is loaded,"Nodetool ring uses SP.getRangeToEndpointMap(null) that tries to retrieve the first non system keyspace.
Hences it crashes (with a IndexOutOfBoundsException) if no schema is loaded.

Should we return a nice little error message or make it work even no schema is loaded ?",slebresne,slebresne,Low,Resolved,Fixed,16/Jul/10 17:04,16/Apr/19 09:33
Bug,CASSANDRA-1287,12469480,Rename 'table' -> 'keyspace' in public APIs,"thrift.CfDef uses the name 'table' rather than 'keyspace'. We need to make sure that all of our public APIs use consistent naming, despite the fact that our private APIs won't change until 0.7 is branched.",stuhood,stuhood,Normal,Resolved,Fixed,16/Jul/10 19:11,16/Apr/19 09:33
Bug,CASSANDRA-1289,12469488,GossipTimerTask stops running if an Exception occurs,"The GossipTimerTask run() method has a try/catch around its body, but it re-throws all Exceptions as RuntimeExceptions. This causes the GossipTimerTask to no longer run (due to the way the underlying Java Timer implementation works), stopping the periodic gossip status checks.

Combine this problem with a bug like CASSANDRA-757 (not yet fixed in 0.6.x) and you get into a state where the server keeps running, but gossip is no longer occurring, preventing node addition / removal from happening.

I see two potential choices:
1) Log the error but don't re-throw it so that the GossipTimerTask will continue to run on its next interval.
2) Shutdown the server, since continuing to run without gossip subtly breaks other functionality / knowledge of other nodes.",brandon.williams,wadey,Normal,Resolved,Fixed,16/Jul/10 20:29,16/Apr/19 09:33
Bug,CASSANDRA-1290,12469491,Cassandra-cli doesn't use framed transport by default,"Spawned by CASSANDRA-475 .
The cli uses non-framed transport, which causes errors on connection that look like:

cli:{noformat}$ bin/cassandra-cli 
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
[default@unknown] connect 127.0.0.1/9160
Exception retrieving information about the cassandra node, check you have connected to the thrift port.{noformat}

cass:{noformat}ERROR 15:48:12,523 Thrift error occurred during processing of message.
org.apache.thrift.TException: Message length exceeded: 1684370275
	at org.apache.thrift.protocol.TBinaryProtocol.checkReadLength(TBinaryProtocol.java:384)
	at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:350)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:213)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2519)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619){noformat}

Changing it to use framed transport fixes this, and it should be using framed transport by default regardless.",jhermes,jhermes,Low,Resolved,Fixed,16/Jul/10 20:54,16/Apr/19 09:33
Bug,CASSANDRA-1292,12469498,Multiple migrations might run at once,"The service.MigrationManager class manages a MIGRATION_STAGE where nodes should execute db.migration.Migration instances.

The problem is that the node that a client connects to via Thrift or Avro initiates the migration in their client thread (calls migration.apply). Instead, the Thrift and Avro clients should ensure that the migration occurs in MIGRATION_STAGE, and should block until the migration is applied by the stage.",gdusbabek,stuhood,Urgent,Resolved,Fixed,16/Jul/10 21:58,16/Apr/19 09:33
Bug,CASSANDRA-1293,12469514,add locking around row cache accesses,"CASSANDRA-1267 means we need to lock around removeDeleted on the row cache entry and the write path where we merge in new columns (otherwise there can be a race where we incorrectly continue to remove a column, that has been updated by the writer thread to be newly relevant)",jbellis,jbellis,Low,Resolved,Fixed,17/Jul/10 02:16,16/Apr/19 09:33
Bug,CASSANDRA-1294,12469517,MIsc license inclusion booboos with recent index commits,"Noticed some missing license headers, a duped header in one case when going over changes in CASSANDRA-1154",zznate,zznate,Low,Resolved,Fixed,17/Jul/10 04:50,16/Apr/19 09:33
Bug,CASSANDRA-1297,12469585,commitlog recover bug,"class CommitLog.java
when recover log files;
 if one log  file have no dirty , process is break;
{quote}
199  int lowPos = CommitLogHeader.getLowestPosition(clHeader);
 200 if (lowPos == 0)
 201    break;
{quote}

why not continue and read next log file
{quote}
199  int lowPos = CommitLogHeader.getLowestPosition(clHeader);
200 if (lowPos == 0)\{
201   reader.close();
202   continue;
203  \}
{quote}

i am not very sure about that. how can answer?


",jftan,jftan,Urgent,Resolved,Fixed,19/Jul/10 09:02,16/Apr/19 09:33
Bug,CASSANDRA-1298,12469599,avoid replaying fully-flushed commitlog segments,,jbellis,jbellis,Low,Resolved,Fixed,19/Jul/10 12:45,16/Apr/19 09:33
Bug,CASSANDRA-1299,12469630,EOFException in LazilyCompactedRow,"Post CASSANDRA-270, 'ant clean long-test' fails with an EOFException in LazilyCompactedRow.

{code}java.io.IOError: java.io.EOFException
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:103)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:32)
	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:604)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
	at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
	at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:62)
	at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:135)
	at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:107)
	at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:46)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
	at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
	at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:334)
	at org.apache.cassandra.db.LongCompactionSpeedTest.testCompaction(LongCompactionSpeedTest.java:101)
	at org.apache.cassandra.db.LongCompactionSpeedTest.testCompactionWide(LongCompactionSpeedTest.java:49)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:758)
	at org.apache.cassandra.db.TimestampClockSerializer.deserialize(TimestampClock.java:128)
	at org.apache.cassandra.db.TimestampClockSerializer.deserialize(TimestampClock.java:119)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:90)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:31)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:99)
{code}",jbellis,stuhood,Urgent,Resolved,Fixed,19/Jul/10 16:18,16/Apr/19 09:33
Bug,CASSANDRA-1309,12469940,LegacySSTableTest breaks when run from a svn checkout,"Works fine under git where there is no .svn turd.

    [junit] ------------- Standard Error -----------------
    [junit] Failed to read .svn
    [junit] java.io.FileNotFoundException: /Users/jonathan/projects/cassandra/svn-trunk/test/data/legacy-sstables/.svn/Keyspace1/Standard1-.svn-0-Index.db (No such file or directory)
    [junit] 	at java.io.RandomAccessFile.open(Native Method)
    [junit] 	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    [junit] 	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    [junit] 	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    [junit] 	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:256)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:187)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:170)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:150)
    [junit] 	at org.apache.cassandra.io.sstable.LegacySSTableTest.testVersion(LegacySSTableTest.java:102)
    [junit] 	at org.apache.cassandra.io.sstable.LegacySSTableTest.testVersions(LegacySSTableTest.java:95)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
    [junit] 	at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
",stuhood,jbellis,Low,Resolved,Fixed,22/Jul/10 18:47,16/Apr/19 09:33
Bug,CASSANDRA-1310,12469974,Disallow KS definition with RF > # of nodes,"Cassandra 0.7 allows user to create Keyspaces with Replication Factor >  number of endpoints causing in java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1) exception in nodetool and Internal Errors on Thrift making the node useless.

Steps to Reproduce:

From a clean setup of Cassandra:
1. Start a single node out of cluster of 3. This means my configuration has the other two nodes in the seeds list, but have not restarted them yet;
2. Use Thrift API (I am using PHP) and create a Keyspace with replication factor 2;
3. The command executes with no exception or error;
4. Now try writing to it, you will get TException with Internal Error message;
5. Try nodetool ring and you will get Exception:

Exception in thread ""main"" java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1)
	at org.apache.cassandra.locator.RackUnawareStrategy.calculateNaturalEndpoints(RackUnawareStrategy.java:61)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:87)
	at org.apache.cassandra.service.StorageService.constructRangeToEndpointMap(StorageService.java:536)
	at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:522)
	at org.apache.cassandra.service.StorageService.getRangeToEndpointMap(StorageService.java:496)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1449)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1284)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1382)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

Expected:

1. Either step 3 should not let you create the KS with RF 2 and 1 node in ring, or there should be a peaceful way for Cassandra to recover from IllegalStateException and replicate once other nodes become available.",zznate,arya,Normal,Resolved,Fixed,23/Jul/10 01:15,16/Apr/19 09:33
Bug,CASSANDRA-1313,12470053,make cache sizes in CfDef Strings again so we can use %s of rows in the CF as in 0.6,"(note, this was reverted for 0.7 in CASSANDRA-1394)",jhermes,jbellis,Low,Resolved,Fixed,23/Jul/10 22:21,16/Apr/19 09:33
Bug,CASSANDRA-1315,12470089,ColumnFamilyOutputFormat should use client API objects,"ColumnFamilyOutputFormat currently takes IColumns as its input, meaning that users need to understand Cassandra's internals reasonably well in order to use it, and need to hardcode things like the comparator type and clock type into their MapReduce jobs.

Instead, CFOutputFormat should take either Thrift or Avro objects, which are familiar interfaces for users.",stuhood,stuhood,Normal,Resolved,Fixed,24/Jul/10 18:20,16/Apr/19 09:33
Bug,CASSANDRA-1316,12470095,Read repair does not always work correctly,"Read repair does not always work.  At the least, we allow violation of the CL.ALL contract.  To reproduce, create a three node cluster with RF=3, and json2sstable one of the attached json files on each node.  This creates a row whose key is 'test' with 9 columns, but only 3 columns are on each machine.  If you get_count this row in quick succession at CL.ALL, sometimes you will receive a count of 6, sometimes 9.  After the ReadRepairManager has sent the repairs, you will always get 9, which is the desired behavior.

I have another data set obtained in the wild which never fully repairs for some reason, but it's a bit large to attach (600ish columns per machine.)  I'm still trying to figure out why RR isn't working on this set, but I always get different results when reading at any CL including ALL, no matter how long I wait or how many reads I do.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,24/Jul/10 23:50,16/Apr/19 09:33
Bug,CASSANDRA-1317,12470122,StorageProxy ignores snitch when determining whether to do a local read,this primarily affects CASSANDRA-1314 and to a lesser degree CASSANDRA-981,jbellis,jbellis,Low,Resolved,Fixed,25/Jul/10 22:38,16/Apr/19 09:33
Bug,CASSANDRA-1318,12470134,CommitLogTest and RecoveryManager2Test is failing in trunk,"{code}
test:
     [echo] running unit tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/home/mdennis/mdev/trunkclean/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.db.CommitLogTest
    [junit] Tests run: 12, Failures: 1, Errors: 0, Time elapsed: 0.937 sec
    [junit] 
    [junit] Testcase: testCleanup(org.apache.cassandra.db.CommitLogTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.CommitLogTest.testCleanup(CommitLogTest.java:66)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.CommitLogTest FAILED

BUILD FAILED
{code}",mdennis,mdennis,Low,Resolved,Fixed,26/Jul/10 06:00,16/Apr/19 09:33
Bug,CASSANDRA-1323,12470224,digest mismatches are processed serially,for multiget situations this can dramatically increase latency.  need to parallelize these.,jbellis,jbellis,Low,Resolved,Fixed,27/Jul/10 02:32,16/Apr/19 09:33
Bug,CASSANDRA-1326,12470290,contrib/pig's cassandra.yaml is out of date,It just needs updating.,jeromatron,jeromatron,Normal,Resolved,Fixed,27/Jul/10 19:39,16/Apr/19 09:33
Bug,CASSANDRA-1330,12470364,AssertionError: discard at CommitLogContext(file=...) is not after last flush at  ...,"Looks related to CASSANDRA-936?

ERROR [MEMTABLE-POST-FLUSHER:1] 2010-07-28 11:39:36,909 CassandraDaemon.java (line 83) Uncaught exception in thread Thread[MEMTABLE-POST-FLUSHER:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:373)
        at org.apache.cassandra.db.ColumnFamilyStore$1.runMayThrow(ColumnFamilyStore.java:371)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:365)
        ... 8 more
Caused by: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:394)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:70)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:359)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
",jbellis,vilda,Normal,Resolved,Fixed,28/Jul/10 17:26,16/Apr/19 09:33
Bug,CASSANDRA-1332,12470387,Scan results out of order,"After inserting 10 keys ('0', '1', ... '9') and running scan() with start_key='' and count=7, scan() returns the keys  ['7', '3', '6', '5', '0', '8', '2']. When I scan() again with start_key='2' and count=7, I get the keys  ['2', '1', '9', '4', '7']. Notice that key ""7"" appears in both result sets, and the relative order of keys ""7"" and ""2"" is inconsistent between the two scan results. 

I see the problem when running on a 4-node cluster. When I run on a 1-node cluster, the problem does not occur. So the attached system test always passes, since system tests use a 1-node cluster, so the test doesn't actually demonstrate the problem.

A standalone Python program that reproduces the problem is at: http://pastebin.com/FwitG4wf",jbellis,drevell,Normal,Resolved,Fixed,28/Jul/10 22:43,16/Apr/19 09:33
Bug,CASSANDRA-1333,12470395,cassandra-cli cannot connect ,"I cannot connect to any of my nodes using Cassandra-CLI. I think this has happened about 2 weeks ago:

[agoudarzi@cas-test1 bin]$ cassandra-cli --host 10.50.26.132 --port 9160 --debug
Exception retrieving information about the cassandra node, check you have connected to the thrift port.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:369)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:295)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:202)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_describe_cluster_name(Cassandra.java:1117)
	at org.apache.cassandra.thrift.Cassandra$Client.describe_cluster_name(Cassandra.java:1103)
	at org.apache.cassandra.cli.CliMain.connect(CliMain.java:164)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:255)
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
[default@unknown] exit                     

However using Thrift PHP Client I have no problem connecting and executing describe_cluster_name().

I have configured Cassandra RPC port and IP as follows:

# The address to bind the Thrift RPC service to
rpc_address: 10.50.26.132
# port for Thrift to listen on
rpc_port: 9160

Steps to Reproduce:
1. Start from a clean setup;
2. Run py_stress to insert some keys and create the default keyspace;
3. Try connecting using cassandra-cli like command above. You'll get the Exception.
",jbellis,arya,Normal,Resolved,Fixed,29/Jul/10 00:06,16/Apr/19 09:33
Bug,CASSANDRA-1334,12470463,cfstats shows deleted cfs,"cfstats shows deleted CFs inside the keyspace after the CF was deleted from the keyspace using thrift service call system_drop_column_family

Steps to Reproduce:
1. Setup a 3 node cluster with clean slate from trunc;
2. create a keyspace with rf=2 and a standard cf using service call system_add_keyspace
3. create another cf with system_add_column_family
4. batch_mutate some rows into the new column family you created in step 3
5. call describe_keyspace to get a list of cfs inside your KS
6. iterate through the result and call system_drop_column_family for each
7. look at cfstats result. it is still showing the very first cf we create in step 2 in the list",gdusbabek,arya,Normal,Resolved,Fixed,29/Jul/10 19:00,16/Apr/19 09:33
Bug,CASSANDRA-1340,12470478,making cassandra-cli friendlier to scripts,"It is currently possible (and useful) to execute bulk commands with cassandra-cli using shell redirection. However, there is  no mechanism for handling errors, and the same output seen in an interactive session is echoed to the terminal.

The patch that follows accepts a new argument, (--batch), which:
* disables initialization of the history file
* suppresses output (stdout only)
* exits on error with status 2 for invalid syntax, 4 for invalid requests, and 8 for everything else.",urandom,urandom,Low,Resolved,Fixed,29/Jul/10 22:43,16/Apr/19 09:33
Bug,CASSANDRA-1343,12470543,gossip throws IllegalStateException,"when starting a second node, gossip throws IllegalStateException when KS with RF>1 defined on an existing 1-node cluster.

we should be able to define keyspaces on a cluster of any size.",gdusbabek,gdusbabek,Low,Resolved,Fixed,30/Jul/10 19:34,16/Apr/19 09:33
Bug,CASSANDRA-1348,12470702,Failed to delete commitlog when restarting service,"When restarting any Cassandra node we've got exception:

ERROR 09:42:27,869 Exception encountered during startup.
java.io.IOException: Failed to delete C:\cassandra\data\commitlog\CommitLog-1280817512228.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:45)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:177)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
	at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:57)
	at org.apache.cassandra.contrib.windows.service.WindowsService.start(Unknown Source)
	at org.apache.cassandra.contrib.windows.service.WindowsService.main(Unknown Source)
 INFO 09:42:27,869 Exception encountered during startup.
 INFO 09:42:27,869 Cassandra Service Finished: Tue Aug 03 09:42:27 EEST 2010

Aftrer exception was thrown and Cassandra didn't started, within commitlog directory there's only one file: CommitLog-1280817512228.log
and no CommitLog-1280817512228.log.header. Looks like CommitLog-1280817512228.log was a new file, not the last one after stopping service.

After few restarts .log file is deleted and Cassandra is working fine until next restart.",jbellis,vjevdokimov,Low,Resolved,Fixed,03/Aug/10 08:42,16/Apr/19 09:33
Bug,CASSANDRA-1349,12470719,"DynamicEndpointSnitch should implement AbstractRackAwareSnitch, not AbstractEndpointSnitch",Everything's in the summary I believe,slebresne,slebresne,Low,Resolved,Fixed,03/Aug/10 12:32,16/Apr/19 09:33
Bug,CASSANDRA-1350,12470730,DynamicEndpointSnitch is defeated by the caching done in Strategy,"can we move the caching into AbstractEndpointSnitch instead?

also: AES.register appears to never be called.",gdusbabek,jbellis,Normal,Resolved,Fixed,03/Aug/10 14:56,16/Apr/19 09:33
Bug,CASSANDRA-1351,12470732,Avro Schema Swap,"Due to misreading Avro's docs, I swapped the Schema parameters to Avro's schema resolver.

The schema resolver allows for backwards compatibility by accepting a writer's (ser.) and reader's (deser.) schema, and resolving them to determine which fields to add or ignore. The reader's schema was not being set correctly, which was breaking backwards compatibility (although not the disk format).",stuhood,stuhood,Urgent,Resolved,Fixed,03/Aug/10 15:58,16/Apr/19 09:33
Bug,CASSANDRA-1352,12470747,Update py_stress to reflect udpated KsDef params,"The following occurs while running stress.py. 

Traceback (most recent call last):
  File ""stress.py"", line 387, in <module>
    make_keyspaces()
  File ""stress.py"", line 160, in make_keyspaces
    client.system_add_keyspace(keyspace)
  File ""/Users/nirmal.ranganathan/Documents/workspace/cassandra.git/interface/thrift/gen-py/cassandra/Cassandra.py"", line 1288, in system_add_keyspace
    self.send_system_add_keyspace(ks_def)
  File ""/Users/nirmal.ranganathan/Documents/workspace/cassandra.git/interface/thrift/gen-py/cassandra/Cassandra.py"", line 1295, in send_system_add_keyspace
    args.write(self._oprot)
  File ""/Users/nirmal.ranganathan/Documents/workspace/cassandra.git/interface/thrift/gen-py/cassandra/Cassandra.py"", line 5745, in write
    oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
SystemError: Objects/dictobject.c:1562: bad argument to internal function

KsDef added an extra parameter and needs to be updated here.",rnirmal,rnirmal,Low,Resolved,Fixed,03/Aug/10 18:11,16/Apr/19 09:33
Bug,CASSANDRA-1353,12470756,commit log recovery is broken when the CL contains mutations to CFs that have been dropped,"This was working, so the fix should include a RecoveryManager test that checks for regressions.",gdusbabek,gdusbabek,Normal,Resolved,Fixed,03/Aug/10 19:51,16/Apr/19 09:33
Bug,CASSANDRA-1354,12470764,Static CFMetaData objects are using the wrong constructor.,"This means they are getting assigned ids > 1000.  Static CFMs should be using the private constructor that uses a specific cfid that is <1000

I'm pretty sure they just need to be getting a readRepairChance=0.",gdusbabek,gdusbabek,Normal,Resolved,Fixed,03/Aug/10 21:29,16/Apr/19 09:33
Bug,CASSANDRA-1356,12470810,"""ant"" always compiles at least 36 files, even if no changes were made",,messi,jbellis,Normal,Resolved,Fixed,04/Aug/10 15:38,16/Apr/19 09:33
Bug,CASSANDRA-1357,12470811,get rid of internode directory,code should generate to src/gen-java.  the genavro file should be renamed and located in interface.,gdusbabek,gdusbabek,Normal,Resolved,Fixed,04/Aug/10 15:54,16/Apr/19 09:33
Bug,CASSANDRA-1360,12470834,"nosetests are busted because ""Not enough live nodes to support this keyspace""","the keyspaces created by nosetests should all have RF=1.

I'm pretty sure some of the keyspaces can be removed altogether.",gdusbabek,gdusbabek,Normal,Resolved,Fixed,04/Aug/10 20:25,16/Apr/19 09:33
Bug,CASSANDRA-1362,12470899,AsynchResult does not respect TimeUnit in get(),"When waiting for a blocking get in AsynchResult, the parameter {{TimeUnit tu}} is ignored. The passed parameter {{long timeout}} is assumed to be milliseconds. Attached you will find my quick fix to convert {{timeout}} to milliseconds with respect to {{tu}}.",hemartin,hemartin,Low,Resolved,Fixed,05/Aug/10 12:51,16/Apr/19 09:33
Bug,CASSANDRA-1373,12471158,OrderPreservingPartitioner with type validated indexed columns causes ClassCastException,"If OrderPreservingPartitioner is used and you have an indexed column with a type validator, using batch_mutate to insert column values (like pycassa does) on the same key and indexed column causes a ClassCastException to be thrown the *second* time you execute it.  That is, the first batch_mutate succeeds, but the following ones fail.  CollatedOrderPreservingPartitioner seems to avoid this problem.  Also, it appears that the row key is being compared to the column value at some point using the validator's Comparator class (such as LongType) which is where the actual exception is thrown.

Stack trace below:
{noformat}
java.lang.RuntimeException: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:27)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:82)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:37)
	at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:878)
	at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:127)
	at org.apache.cassandra.db.Memtable.put(Memtable.java:119)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:508)
	at org.apache.cassandra.db.Table.applyCF(Table.java:452)
	at org.apache.cassandra.db.Table.apply(Table.java:409)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
	at org.apache.cassandra.service.StorageProxy$2.runMayThrow(StorageProxy.java:276)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}",jbellis,thobbs,Normal,Resolved,Fixed,10/Aug/10 01:28,16/Apr/19 09:33
Bug,CASSANDRA-1377,12471303,NPE aborts streaming operations for keyspaces with hyphens ('-') in their names,"When streaming starts for operations such as repair or bootstrap, it will fail due to an NPE if they rows are in a keyspace that has a hyphen in its name.  One workaround for this issue would be to not use keyspace names containing hyphens.  It would be even nicer if streaming worked for keyspace names with hyphens, since keyspaces named like that seem to be fine in all other ways.

To reproduce:
 1. With a multi-node ring, load up a keyspace with a hyphen in its name
 2. Add some data to that keyspace
 3. nodetool repair

Expected results:
Repair operations complete normally

Actual results:
Repair operations don't complete normally.  The stacktrace below is correlated with the repair request.  

 INFO [AE-SERVICE-STAGE:1] 2010-06-30 14:11:29,744 AntiEntropyService.java (line 619) Performing streaming repair of 1 ranges to /10.255.0.20 for (my-keyspace,AColumnFamily)
ERROR [MESSAGE-DESERIALIZER-POOL:1] 2010-06-30 14:11:30,034 DebuggableThreadPoolExecutor.java (line 101) Error in ThreadPoolExecutor
java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInitiateVerbHandler.getNewNames(StreamInitiateVerbHandler.java:154)
        at org.apache.cassandra.streaming.StreamInitiateVerbHandler.doVerb(StreamInitiateVerbHandler.java:76)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",gdusbabek,bhoyt,Normal,Resolved,Fixed,11/Aug/10 14:54,16/Apr/19 09:33
Bug,CASSANDRA-1378,12471309,add then drop Keyspace without putting anything in it causes exception,"The following from python causes an exception on apache-cassandra-2010-08-10_13-08-19-bin.tar.gz and a bunch of earlier builds in the 0.7 line:
        socket = TSocket.TSocket(host, 9160)
        transport = TTransport.TFramedTransport(socket)
        protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
        client = Cassandra.Client(protocol)
        transport.open()
        try:
            client.describe_keyspace(dbName)
        except NotFoundException, e:
            keyspaceDef = KsDef(name=dbName,
 
strategy_class='org.apache.cassandra.locator.RackUnawareStrategy',
                                replication_factor=replicationFactor,
                                cf_defs=[])
            client.set_keyspace('system')
            client.system_add_keyspace(keyspaceDef)

        try:
            client.describe_keyspace(dbName)
            client.set_keyspace('system')
            client.system_drop_keyspace(dbName)
        except NotFoundException, e:
            pass

The system_drop_keyspace throws:
InvalidRequestException(why='java.util.concurrent.ExecutionException:
java.lang.NullPointerException')

If I put a system_add_column_family in the middle it doesn't crash.
I think this broke sometime after apache-cassandra-2010-07-06_13-27-21",gdusbabek,jjordan,Low,Resolved,Fixed,11/Aug/10 15:33,16/Apr/19 09:33
Bug,CASSANDRA-1382,12471371,Race condition leads to FileNotFoundException on startup,"On startup LocationInfo file is deleted then attempted to be read from.

Steps to reproduce: Kill then quickly restart

Switching to ParallelGC to avoid CMS/CompressedOops incompatibility
INFO 17:05:08,680 DiskAccessMode isstandard, indexAccessMode is mmap
 INFO 17:05:08,786 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,797 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,807 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,833 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,834 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,839 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,862 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,864 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,876 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,885 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,892 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,893 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,897 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,901 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,906 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,909 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,918 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,922 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,928 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:08,933 Deleted /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db
 INFO 17:05:08,936 Deleted /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db
 INFO 17:05:08,936 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,937 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,947 Deleted /var/lib/cassandra/data/system/LocationInfo-e-14-Data.db
 INFO 17:05:08,947 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,948 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-15-<>
ERROR 17:05:08,948 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-15-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,950 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,951 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,970 Deleted /var/lib/cassandra/data/system/LocationInfo-e-13-Data.db
 INFO 17:05:08,971 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-14-<>
ERROR 17:05:08,971 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-14-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-14-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,972 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,973 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-15-<>
ERROR 17:05:08,973 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-15-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,974 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,974 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,996 Loading schema version acc5646a-a59d-11df-83fb-e700f669bcfc
 WARN 17:05:09,158 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO 17:05:09,164 Replaying /var/lib/cassandra/commitlog/CommitLog-1281571453475.log, /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:09,172 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281571453475.log
 INFO 17:05:09,172 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:09,173 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1281571508928.log', position=592)
 INFO 17:05:09,183 Enqueuing flush of Memtable-LocationInfo@137493297(17 bytes, 1 operations)
 INFO 17:05:09,183 Writing Memtable-LocationInfo@137493297(17 bytes, 1 operations)
 INFO 17:05:09,184 switching in a fresh Memtable for Statistics at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1281571508928.log', position=592)
 INFO 17:05:09,184 Enqueuing flush of Memtable-Statistics@86823325(0 bytes, 0 operations)
 INFO 17:05:09,265 Completed flushing /var/lib/cassandra/data/system/LocationInfo-e-18-Data.db
 INFO 17:05:09,273 Writing Memtable-Statistics@86823325(0 bytes, 0 operations)
 INFO 17:05:09,352 Completed flushing /var/lib/cassandra/data/system/Statistics-e-1-Data.db
 INFO 17:05:09,353 Recovery complete ",gdusbabek,rzotter,Low,Resolved,Fixed,12/Aug/10 04:08,16/Apr/19 09:33
Bug,CASSANDRA-1384,12471466,ERROR [MIGRATION-STAGE:1] Previous Version Mistmatch,"I fired up a 3 node cluster. I created few keyspaces using API and inserted to them with no problem. Now I tried to add more CFs to one of those existing Keyspaces in a loop. I got the following exception:

ERROR [MIGRATION-STAGE:1] 2010-08-12 14:46:40,493 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[MIGRATION-STAGE:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: Previous version mismatch. cannot apply.
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: Previous version mismatch. cannot apply.
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	... 2 more
Caused by: org.apache.cassandra.config.ConfigurationException: Previous version mismatch. cannot apply.
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:101)
	at org.apache.cassandra.db.DefinitionsUpdateResponseVerbHandler$1.runMayThrow(DefinitionsUpdateResponseVerbHandler.java:70)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 6 more

The above exception is logged in the log of node in which I send the request to and not other seeds. It is noteworthy that my schem_agreement is stuck in a disagreed state:

Array
(
    [1775e847-a658-11df-960f-7d867dfef3ae] => Array
        (
            [0] => 10.50.26.134
        )

    [163d874a-a65b-11df-aef0-d73a63bafff3] => Array
        (
            [0] => 10.50.26.133
        )

    [14869031-a658-11df-8553-930ba61048ac] => Array
        (
            [0] => 10.50.26.132
        )

)

And this does not change. Affect is that some keyspaces would not respond to reads any more giving Internal Error:

ERROR [pool-1-thread-26] 2010-08-12 14:50:57,034 Cassandra.java (line 2988) Internal error processing batch_mutate
java.lang.AssertionError
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:91)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1289)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1277)
	at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:193)
	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:474)
	at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:438)
	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:2980)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2499)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

In my CF creation, I block for CF creation of the same name and not different names. 

Please advice.
",gdusbabek,arya,Normal,Resolved,Fixed,12/Aug/10 22:04,16/Apr/19 09:33
Bug,CASSANDRA-1385,12471485,nodetool cfstats does not update after adding new cfs through API,Start a 3 node cluster. Add a new Keyspace with API. Then add more CFs to that Keyspace. ndoetool cfstats will only show you the CF which was originally part of KsDef creation and not the CfDefs that were added later.,gdusbabek,arya,Low,Resolved,Fixed,13/Aug/10 02:41,16/Apr/19 09:33
Bug,CASSANDRA-1394,12471650,Blank listen_address/rpc_address no longer binds based on hostname,"With the switch to yamlbeans, if I make listen_address or rpc_address blank, they bind to localhost.  Instead, they should bind to whatever hostname and /etc/hosts point at.  This makes it much easier to use a unified config across a cluster.  Also, yamlbeans doesn't respect the 'null' keyword, which should be the same as a blank value, but is instead treated as a literal string.  This makes programmatic generation of the config file difficult.",,brandon.williams,Normal,Resolved,Fixed,15/Aug/10 20:11,16/Apr/19 09:33
Bug,CASSANDRA-1399,12471759,CLI addColumnFamily - setting read_repair_chance modifies the keys_cache_size instead,"case KEY_CACHE_SIZE:
                cfDef.setKey_cache_size(Double.parseDouble(mValue));
                break;

case READ_REPAIR_CHANCE:
                cfDef.setKey_cache_size(Double.parseDouble(CliUtils.unescapeSQLString(mValue)));
                break;

Also it would be good to add gc_grace_seconds for this operation.",rnirmal,rnirmal,Low,Resolved,Fixed,16/Aug/10 22:43,16/Apr/19 09:33
Bug,CASSANDRA-1402,12471824,indexed writes fail with exception,"Indexed writes fail with an ArrayIndexOutOfBoundsException depending on the length of the key:


{noformat}
java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: -95
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -95
	at org.apache.cassandra.db.Table.apply(Table.java:379)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
	at org.apache.cassandra.service.StorageProxy$1.runMayThrow(StorageProxy.java:194)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}

The patch that follows updates ColumnFamilyStoreTest to demonstrate.",jbellis,urandom,Normal,Resolved,Fixed,17/Aug/10 17:16,16/Apr/19 09:33
Bug,CASSANDRA-1403,12471843,CFMetaData id gets out of sync,"stand up two nodes.
load a KS + cf on A.
add another CF on A.
Let the cluster quiesce.
add a CF on B.

You get the out of sync error.  I'm pretty sure this is because AddColumnFamily doesn't CFM.fixMaxId() like AddKeyspace does.",gdusbabek,gdusbabek,Normal,Resolved,Fixed,17/Aug/10 20:32,16/Apr/19 09:33
Bug,CASSANDRA-1406,12471923,Dropping column families doesn't clean up secondary indexes,,jbellis,gdusbabek,Normal,Resolved,Fixed,18/Aug/10 16:58,16/Apr/19 09:33
Bug,CASSANDRA-1407,12471941,debian init script could be more consistent w/ bin/cassandra,"With the introduction of conf/cassandra-env.sh it should be possible to eliminate the separately maintained /etc/default/cassandra.  The init script should also use a JAVA_HOME derived from the java binary in PATH (if it exists), both because this is how things work in Debian (_the_ Java is the one chosen by alternatives), and because this is how bin/cassandra works as well.

Patches to follow.",urandom,urandom,Normal,Resolved,Fixed,18/Aug/10 20:17,16/Apr/19 09:33
Bug,CASSANDRA-1408,12471949,nodetool drain attempts to delete a deleted file,"Running `nodetool drain` presented me with a pretty stack-trace.
The drain itself finished successfully and nothing showed up in the system.log.

{noformat}
$ bin/nodetool -h 127.0.0.1 -p 8080 drain
Exception in thread ""main"" java.lang.AssertionError: attempted to delete non-existing file CommitLog-1282166457787.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:40)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:178)
	at org.apache.cassandra.service.StorageService.drain(StorageService.java:1653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
{noformat}",brandon.williams,jhermes,Low,Resolved,Fixed,18/Aug/10 21:49,16/Apr/19 09:33
Bug,CASSANDRA-1411,12472038,FBUtilities.hexToBytes() doesn't accommodate odd-length strings.,"This is a problem when a user specifies ByteOrderedPartitioner with an odd-length initial token (like ""0"").",gdusbabek,gdusbabek,Low,Resolved,Fixed,19/Aug/10 20:49,16/Apr/19 09:33
Bug,CASSANDRA-1413,12472062,EstimatedHistogram.max is buggy,"EH.max returns the largest bucket floor, which will will be LESS than the largest value added to the histogram, which is not the usual behavior expected of a method called max.",jbellis,jbellis,Low,Resolved,Fixed,20/Aug/10 06:56,16/Apr/19 09:33
Bug,CASSANDRA-1416,12472125,SStableSliceIterator leaks FDs,,jbellis,jbellis,Normal,Resolved,Fixed,20/Aug/10 20:33,16/Apr/19 09:33
Bug,CASSANDRA-1422,12472286,multiget_count() should not take a keyspace arg,"With the addition of set_keyspace(), things that are scoped by keyspace should no longer take keyspace args.",jhermes,cgist,Low,Resolved,Fixed,23/Aug/10 19:09,16/Apr/19 09:33
Bug,CASSANDRA-1428,12472428,Rejecting keyspace creation when RF > N is incorrect,"The behavior introduced in this patch http://www.mail-archive.com/commits@cassandra.apache.org/msg05913.html is incorrect.

Disallowing keyspace creation when RF > N is semantically incorrect and makes both scaling a cluster up and down more difficult than it should be.  This is compounded by the current lack of any API methods to change the replication factor.  Most dynamo style systems allow RF to be set > N for smaller clusters.  The cluster will behave as if RF = N until enough nodes are added such that RF < N.",jbellis,moonpolysoft,Low,Resolved,Fixed,24/Aug/10 23:29,16/Apr/19 09:33
Bug,CASSANDRA-1429,12472473,The dynamic snitch can't be used with network topology strategies,also ported to 0.6 and committed there,jbellis,slebresne,Normal,Resolved,Fixed,25/Aug/10 13:27,16/Apr/19 09:33
Bug,CASSANDRA-1430,12472490,SSTable statistics causing intermittent CL test failures in trunk.,"    [junit] Testcase: testCleanup(org.apache.cassandra.db.CommitLogTest):	FAILED
    [junit] 2 != 1
    [junit] junit.framework.AssertionFailedError: 2 != 1
    [junit] 	at org.apache.cassandra.db.CommitLogTest.testCleanup(CommitLogTest.java:69)
    [junit] 
    [junit] 

I see this 1-2 times a day.",brandon.williams,gdusbabek,Normal,Resolved,Fixed,25/Aug/10 15:40,16/Apr/19 09:33
Bug,CASSANDRA-1432,12472526,java.util.NoSuchElementException when returning a node to the cluster,"I'm running the v0.7-beta1 in a 4 nodes cluster and just doing some simple testing. One of the nodes had been down (machine off, unclean shutdown) for an hour or so not sure how many writes were going on, when I bought it back up this message appears in the other 3 nodes...


INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 584) Node /192.168.34.27 has restarted, now UP again
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,200 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.27
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,201 StorageService.java (line 636) Node /192.168.34.27 state jump to normal
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,201 StorageService.java (line 643) Will not change my token ownership to /192.168.34.27
ERROR [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,640 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[HINTED-HANDOFF-POOL:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.NoSuchElementException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.util.NoSuchElementException
        at orgapache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.util.NoSuchElementException
        at java.util.concurrent.ConcurrentSkipListMap.lastKey(ConcurrentSkipListMap.java:1981)
        at java.util.concurrent.ConcurrentSkipListMap$KeySet.last(ConcurrentSkipListMap.java:2331)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:121)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:218)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:78)
        at org.apache.cassandra.db.HintedHandOffManager$1.runMayThrow(HintedHandOffManager.java:296) not sure how many writes were going on
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more

On the machine that was off (34.27) there are no errors in the logs, and here are the entries for around the same time...

 INFO [main] 2010-08-25 19:29:50,679 CommitLog.java (line 340) Recovery complete
 INFO [main] 2010-08-25 19:29:50,769 CommitLog.java (line 180) Log replay complete
 INFO [main] 2010-08-25 19:29:50,797 StorageService.java (line 342) Cassandra version: 0.7.0-beta1-SNAPSHOT
 INFO [main] 2010-08-25 19:29:50,797 StorageService.java (line 343) Thrift API version: 10.0.0
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 240) Saved Token found: 85070591730234615865843651857942052864
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 257) Saved ClusterName found: FOO
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 272) Saved partitioner not found. Using org.apache.cassandra.dht.RandomPartitioner
 INFO [main] 2010-08-25 19:29:50,814 ColumnFamilyStore.java (line 422) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/local1/junkbox/cassandra/commitlog/CommitLog-12827213897
70.log', position=41336)
 INFO [main] 2010-08-25 19:29:50,814 ColumnFamilyStore.java (line 706) Enqueuing flush of Memtable-LocationInfo@916236367(95 bytes, 2 operations)
 INFO [FLUSH-WRITER-POOL:1] 2010-08-25 19:29:50,815 Memtable.java (line 150) Writing Memtable-LocationInfo@916236367(95 bytes, 2 operations)
 INFO [FLUSH-WRITER-POOL:1] 2010-08-25 19:29:50,873 Memtable.java (line 157) Completed flushing /local1/junkbox/cassandra/data/system/LocationInfo-e-6-Data.db
 INFO [main] 2010-08-25 19:29:50,917 StorageService.java (line 374) Starting up server gossip
 INFO [main] 2010-08-25 19:29:51,093 ColumnFamilyStore.java (line 1239) Loaded 0 rows into the Super2 cache
 INFO [main] 2010-08-25 19:29:51,170 CassandraDaemon.java (line 153) Binding thrift service to /0.0.0.0:9160
 INFO [main] 2010-08-25 19:29:51,174 CassandraDaemon.java (line 167) Using TFramedTransport with a max frame size of 15728640 bytes.
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,198 Gossiper.java (line 578) Node /192.168.34.28 is now part of the cluster
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 578) Node /192.168.34.29 is now part of the cluster
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 578) Node /192.168.34.26 is now part of the cluster
 INFO [main] 2010-08-25 19:29:51,204 CassandraDaemon.java (line 208) Listening for thrift clients...
 INFO [main] 2010-08-25 19:29:51,210 Mx4jTool.java (line 73) Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,417 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.28
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,417 Gossiper.java (line 570) InetAddress /192.168.34.28 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,418 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.28
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,855 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.29
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,855 Gossiper.java (line 570) InetAddress /192.168.34.29 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,860 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.29
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:52,930 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.26
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:52,930 Gossiper.java (line 570) InetAddress /192.168.34.26 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:52,930 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.26

I ran a repair on all the nodes and this was all that they each logged 
 INFO [manual-repair-fe7c5abb-bb0a-4415-aa75-0d72ba4e7f1b] 2010-08-25 19:49:24,194 AntiEntropyService.java (line 803) Waiting for repair requests to: []

The cluster seemed OK and kept on working.",jbellis,amorton,Low,Resolved,Fixed,25/Aug/10 21:40,16/Apr/19 09:33
Bug,CASSANDRA-1433,12472528,nodetool cfstats broken on TRUNK,"""nodetool -h localhost cfstats"" doesn't print anything.  Other commands work fine.
",jbellis,jjordan,Low,Resolved,Fixed,25/Aug/10 21:58,16/Apr/19 09:33
Bug,CASSANDRA-1434,12472542,ColumnFamilyOutputFormat performs blocking writes for large batches,"By default, ColumnFamilyOutputFormat batches {{mapreduce.output.columnfamilyoutputformat.batch.threshold}} or {{Long.MAX_VALUE}} mutations, and then performs a blocking write.",jbellis,stuhood,Normal,Resolved,Fixed,25/Aug/10 23:20,16/Apr/19 09:33
Bug,CASSANDRA-1435,12472560,CommitLogHeader raises an AssertionError during  startup,"On a cluster that was pretty sick due to CASSANDRA-1416 and CASSANDRA-1432 I got the error below when starting up a node. The node failed to start.

After retrying the node started. 
ERROR [main] 2010-08-26 14:59:22,315 AbstractCassandraDaemon.java (line 107) Exception encountered during startup.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:549)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:339)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:174)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:545)
        ... 5 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:408)
        at org.apache.cassandra.db.ColumnFamilyStore$2.runMayThrow(ColumnFamilyStore.java:445)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:400)
        ... 8 more
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.commitlog.CommitLogHeader$CommitLogHeaderSerializer.serialize(CommitLogHeader.java:157)
        at org.apache.cassandra.db.commitlog.CommitLogHeader.writeCommitLogHeader(CommitLogHeader.java:124)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.writeHeader(CommitLogSegment.java:70)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:450)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:75)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:394)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
",jbellis,amorton,Normal,Resolved,Fixed,26/Aug/10 06:52,16/Apr/19 09:33
Bug,CASSANDRA-1439,12472648,OPP makes HH unhappy,"as reported multiple times on the mailing list and IRC:

ERROR [HINTED-HANDOFF-POOL:1] 2010-08-26 15:58:20,310 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[HINTED-HANDOFF-POOL:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:169)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:41)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:199)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:78)
        at org.apache.cassandra.db.HintedHandOffManager$1.runMayThrow(HintedHandOffManager.java:296)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:483)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:165)
        ... 11 more",brandon.williams,jbellis,Normal,Resolved,Fixed,26/Aug/10 21:47,16/Apr/19 09:33
Bug,CASSANDRA-1442,12472820,Get Range Slices is broken,"HI,
We just recently tried to use 0.6.4 and 0.6.5 in our production environment and
had some serious problem.
The getRangeSlices functionality is broken.
We have a cluster of 5 machines.
We use getRangeSlices to iterate over all of the keys in a cf (2062 keys total).
We are using OrderPreservingPartitioner.
We use getRangeSlices with KeyRange using keys (not tokens).
If we set the requestBlockCount (aka: KeyRange.setCount()) to a number
greater than 2062 we get all keys in one shot (all is good).
If we try to fetch the keys in smaller blocks (requestBlockCount=100)
we get BAD RESULTS.
We get only 800 unique keys back.
We start with (startKey="""" and endKey="""") then, after each iteration, we use the lastKey to set the startKey for the next page.
Except on first page, we always skip the first item of the page (knowing that it is a repeat, the last one, of the prior page).
To get the lastKey we tried two strategies: [1] set the lastKey to the last item in the page, and [2] use String.compareTo to get the largest ley. Neither strategy worked.
Our keys are strings (obviously the only option in 0.6) that represent numbers.
Some Sample keys are: (in correct lexi order)
-1
11113
11457
6831
7035
8060
8839
------
This code (without any changes) was working correctly under 0.6.3 (we
got same response from getRangeSlices if using requestBlockCounts of
10,000 or 100).
We tried it under 0.6.4 and 0.6.5 and it stopped working.
We reverted back to 0.6.3 and (again, without changing the code) it
started working again.
------
I tried inserting all the keys into a test cluster of one (1 machine) and it worked fine.
So this must be related to how the page is build in a cluster of more than 1 nodes.
We have a cluster of 5 nodes with replication factor of 3.",stuhood,molezam,Urgent,Resolved,Fixed,29/Aug/10 20:18,16/Apr/19 09:33
Bug,CASSANDRA-1446,12472890,cassandra-cli still relies on cassandra.in.sh instead of cassandra-env.sh,"When we switched to cassandra-env.sh, we neglected to change the cli as well.  This leads to people unable to launch to the client due to heap size, and not having any idea how to change the heap for the cli itself.",urandom,brandon.williams,Low,Resolved,Fixed,30/Aug/10 22:45,16/Apr/19 09:33
Bug,CASSANDRA-1447,12472895,SimpleAuthenticator MD5 support,"...is broken, or not working as expected. Needs a look before 0.7.0.",rnirmal,stuhood,Low,Resolved,Fixed,31/Aug/10 01:06,16/Apr/19 09:33
Bug,CASSANDRA-1450,12472952,"Memtable flush causes bad ""reversed"" get_slice","If columns are inserted into a row before and after a memtable flush, a get_slice() after the flush with reversed=True will return incorrect results.  See attached patch to reproduce.",jbellis,thobbs,Normal,Resolved,Fixed,31/Aug/10 17:14,16/Apr/19 09:33
Bug,CASSANDRA-1454,12473158,avronateSubcolumns was assuming an avro array.,,gdusbabek,gdusbabek,Low,Resolved,Fixed,02/Sep/10 16:39,16/Apr/19 09:33
Bug,CASSANDRA-1455,12473166,Hadoop streaming -> Cassandra ColumnFamilyOutputFormat not respecting partitioner,"The Hadoop streaming shim (hadoop streaming client => avro => ColumnFamilyOutputFormat => cassandra ring) is only connecting to one or a couple clients on the ring.  With 24 hadoop clients launched,  ` sudo netstat -antp | grep 9160 | wc -l ` gave 24 on one machine, and only 1-3 on any other node.

I'll attach the script and runner I used.",stuhood,mrflip,Urgent,Resolved,Fixed,02/Sep/10 17:42,16/Apr/19 09:33
Bug,CASSANDRA-1456,12473167,avro get_range_slices should default to CL.ONE,,jeromatron,jeromatron,Normal,Resolved,Fixed,02/Sep/10 17:46,16/Apr/19 09:33
Bug,CASSANDRA-1458,12473181,SSTable cleanup killed by IllegalStateException,"Compacted SSTables were not being deleted even after a forced GC. The following stack traces were observed:

ERROR [SSTABLE-CLEANUP-TIMER] 2010-09-01 15:54:07,254 CassandraDaemon.java (line 85) Uncaught exception in thread Thread[SSTABLE-CLEANUP-TIMER,5,main]
java.lang.IllegalStateException: Task already scheduled or cancelled
        at java.util.Timer.sched(Timer.java:380)
        at java.util.Timer.schedule(Timer.java:192)
        at org.apache.cassandra.io.sstable.SSTableDeletingReference$CleanupTask.run(SSTableDeletingReference.java:86)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)

ERROR [SSTABLE-DELETER] 2010-09-01 16:20:22,587 CassandraDaemon.java (line 85) Uncaught exception in thread Thread[SSTABLE-DELETER,5,main]
java.lang.IllegalStateException: Timer already cancelled.
        at java.util.Timer.sched(Timer.java:376)
        at java.util.Timer.schedule(Timer.java:192)
        at org.apache.cassandra.io.sstable.SSTableDeletingReference.cleanup(SSTableDeletingReference.java:70)
        at org.apache.cassandra.io.sstable.SSTableReader$1$1.run(SSTableReader.java:85)
        at java.lang.Thread.run(Thread.java:636)

If the SSTableDeletingReference$CleanupTask cannot delete a file, it reschedules itself for later. TimerTasks (which CleanupTask subclasses) are intended to be scheduled only once and will cause an IllegalStateException in the timer when it tries to schedule itself again. The exception causes timer to effectively cancel itself and the next attempt to schedule a task will cause an IllegalStateException in the SSTABLE-DELETER.

It appears this could be fixed by scheduling a new CleanupTask instead of the same one that failed (SSTableDeletingReference.java:86).",jbellis,cgist,Low,Resolved,Fixed,02/Sep/10 19:51,16/Apr/19 09:33
Bug,CASSANDRA-1463,12473282,"Failed bootstrap can cause NPE in batch_mutate on every node, taking down the entire cluster","In adding a node to the cluster, the bootstrap failed (still investigating the cause). An hour later, the entire cluster failed, preventing any writes from being accepted. This exception started being printed to the logs:

{quote}
 INFO [Timer-0] 2010-09-03 12:23:33,282 Gossiper.java (line 402) FatClient /10.251.243.191 has been silent for 3600000ms, removing from gossip
ERROR [Timer-0] 2010-09-03 12:23:33,318 Gossiper.java (line 99) Gossip error
java.util.ConcurrentModificationException
        at java.util.Hashtable$Enumerator.next(Hashtable.java:1048)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:383)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:93)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)
ERROR [pool-1-thread-69153] 2010-09-03 12:23:33,857 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
        at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:135)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:85)
        at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:204)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:415)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:1651)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR [pool-1-thread-69154] 2010-09-03 12:23:33,869 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
        at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:135)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:85)
        at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:204)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:415)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:1651)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{quote}

After a large number of iterations of that (at least thousands), the printed exception was shortened (this shortening is what made me mistakenly file #1462) to

{quote}
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,857 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,883 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,894 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68970] 2010-09-03 12:39:22,985 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68970] 2010-09-03 12:39:23,084 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
{quote}

Rolling a restart over the cluster fixed it, but every node had to be restarted before it started accepting writes again.",jbellis,ketralnis,Normal,Resolved,Fixed,03/Sep/10 20:50,16/Apr/19 09:33
Bug,CASSANDRA-1467,12473291,"replication factor exceeds number of endpoints, when attempting to join a new node (but cluster has enough running nodes to fulfill RF)","What happens here the following:
* given a healthy running cluster of 2 nodes (it used to be 3 but I manually killed one down)
* with a Keyspace having a ReplicationFactor of 2
* (this cluster is operational, loaded with data and working well)

 as soon as I want to bring up a new 3rd node:
* the node is detected by the current cluster
* but as soon as it tries to initiate bootstrap sequence it dies with:  replication factor (2) exceeds number of endpoints (1)

I believe this is similar to #1343, but not quite the same, since the keyspace and everything is already created, and I'm not attempting any modification. This is purely bringing a new node up.

One extra tidbit of information, in case it's important...maybe it is not: 
I have only set 1 seed node configured (this is a test setup). And when the new node starts coming up (before the crash) its nodetool ring only reports the seed one.

From the new node coming up (before it crashes):
root@node1-3:~# nodetool -h localhost -p 8080 ring
Address         Status State   Load            Token                                       
10.250.106.111  Up     Normal  116.3 GB        0          < = this the only configured seed node

From one of the other running nodes (this is the real status of the cluster):
root@node1-2:~# nodetool -h localhost -p 8080 ring
Address         Status State   Load            Token                                       
                                       113416112894748789872342756657008344878    
10.250.106.111  Up     Normal  116.3 GB        0                                           
10.215.195.81   Up     Normal  116.31 GB       56713727820156410577229101238628035242      
10.246.65.221   Up     Joining 7.63 KB         113416112894748789872342756657008344878     


Here's the full boot trace of the node is trying to join:
Java HotSpot(TM) Client VM warning: Can't detect initial thread stack location - find_vma failed
Create RMI registry on port 8081
Get the platform's MBean server
Initialize the environment map
Create an RMI connector server
Start the RMI connector server on port 8081
 INFO 22:32:50,448 Loading settings from /etc/cassandra/cassandra.yaml
DEBUG 22:32:50,537 Syncing log with a period of 10000
 INFO 22:32:50,538 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
DEBUG 22:32:50,548 setting auto_bootstrap to true
DEBUG 22:32:50,702 Starting CFS Statistics
DEBUG 22:32:50,710 key cache capacity for Statistics is 1
DEBUG 22:32:50,711 Starting CFS Schema
DEBUG 22:32:50,712 key cache capacity for Schema is 1
DEBUG 22:32:50,712 Starting CFS Migrations
DEBUG 22:32:50,713 key cache capacity for Migrations is 1
DEBUG 22:32:50,713 Starting CFS LocationInfo
DEBUG 22:32:50,713 key cache capacity for LocationInfo is 1
DEBUG 22:32:50,714 Starting CFS HintsColumnFamily
DEBUG 22:32:50,714 key cache capacity for HintsColumnFamily is 1
 INFO 22:32:50,736 Couldn't detect any schema definitions in local storage.
 INFO 22:32:50,737 Found table data in data directories. Consider using JMX to call org.apache.cassandra.service.StorageService.loadSchemaFromYaml().
DEBUG 22:32:50,738 opening keyspace system
 INFO 22:32:50,757 Cassandra version: 
 INFO 22:32:50,757 Thrift API version: 10.0.0
 INFO 22:32:50,758 Saved Token not found. Using 113416112894748789872342756657008344878
 INFO 22:32:50,758 Saved ClusterName not found. Using SOMETHING 
 INFO 22:32:50,763 Creating new commitlog segment /mnt/cassandra/commitlog/CommitLog-1283553170763.log
DEBUG 22:32:50,767 Estimating compactions for LocationInfo
DEBUG 22:32:50,768 Estimating compactions for HintsColumnFamily
DEBUG 22:32:50,768 Estimating compactions for Migrations
DEBUG 22:32:50,768 Estimating compactions for Schema
DEBUG 22:32:50,768 Estimating compactions for Statistics
DEBUG 22:32:50,769 Checking to see if compaction of LocationInfo would be useful
DEBUG 22:32:50,769 Checking to see if compaction of HintsColumnFamily would be useful
DEBUG 22:32:50,769 Checking to see if compaction of Migrations would be useful
DEBUG 22:32:50,769 Checking to see if compaction of Schema would be useful
DEBUG 22:32:50,769 Checking to see if compaction of Statistics would be useful
 INFO 22:32:50,779 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=276)
 INFO 22:32:50,782 Enqueuing flush of Memtable-LocationInfo@18721294(192 bytes, 4 operations)
 INFO 22:32:50,783 Writing Memtable-LocationInfo@18721294(192 bytes, 4 operations)
 INFO 22:32:50,897 Completed flushing /mnt/ebs/data/system/LocationInfo-e-1-Data.db
DEBUG 22:32:50,898 Checking to see if compaction of LocationInfo would be useful
DEBUG 22:32:50,898 Discarding 0
DEBUG 22:32:50,899 discard completed log segments for CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=276), column family 0.
DEBUG 22:32:50,899 Marking replay position 276 on commit log CommitLogSegment(/mnt/cassandra/commitlog/CommitLog-1283553170763.log)
 INFO 22:32:50,908 Starting up server gossip
 INFO 22:32:50,928 Joining: getting load information
 INFO 22:32:50,928 Sleeping 90000 ms to wait for load information...
DEBUG 22:32:50,940 attempting to connect to node1-1.domain.com/10.250.106.111
DEBUG 22:32:51,044 attempting to connect to node1-1.domain.com/10.250.106.111
DEBUG 22:32:51,045 attempting to connect to /10.215.195.81
 INFO 22:32:51,051 Node /10.215.195.81 is now part of the cluster
DEBUG 22:32:51,051 Resetting pool for /10.215.195.81
DEBUG 22:32:51,052 Token 113416112894748789872342756657008344877 removed manually (endpoint was unknown)
 INFO 22:32:51,052 Node /10.250.106.111 is now part of the cluster
DEBUG 22:32:51,053 Resetting pool for /10.250.106.111
DEBUG 22:32:51,053 Node /10.250.106.111 state normal, token 0
DEBUG 22:32:51,053 clearing cached endpoints
DEBUG 22:32:51,171 Applying AddKeyspace from /10.250.106.111
DEBUG 22:32:51,188 Applying migration 77e97d6c-b625-11df-8596-318df8b646e8
 INFO 22:32:51,189 switching in a fresh Memtable for Migrations at CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417)
 INFO 22:32:51,189 Enqueuing flush of Memtable-Migrations@18093512(4938 bytes, 1 operations)
 INFO 22:32:51,189 Writing Memtable-Migrations@18093512(4938 bytes, 1 operations)
 INFO 22:32:51,194 switching in a fresh Memtable for Schema at CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417)
 INFO 22:32:51,194 Enqueuing flush of Memtable-Schema@27402470(1784 bytes, 3 operations)
 INFO 22:32:51,274 Completed flushing /mnt/ebs/data/system/Migrations-e-1-Data.db
DEBUG 22:32:51,274 Checking to see if compaction of Migrations would be useful
DEBUG 22:32:51,275 Discarding 2
DEBUG 22:32:51,275 discard completed log segments for CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417), column family 2
.
DEBUG 22:32:51,275 Marking replay position 7417 on commit log CommitLogSegment(/mnt/cassandra/commitlog/CommitLog-1283553170763.log)
 INFO 22:32:51,275 Writing Memtable-Schema@27402470(1784 bytes, 3 operations)
 INFO 22:32:51,388 Completed flushing /mnt/ebs/data/system/Schema-e-1-Data.db
DEBUG 22:32:51,389 Checking to see if compaction of Schema would be useful
DEBUG 22:32:51,389 Discarding 3
DEBUG 22:32:51,389 discard completed log segments for CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417), column family 3
.
DEBUG 22:32:51,389 Marking replay position 7417 on commit log CommitLogSegment(/mnt/cassandra/commitlog/CommitLog-1283553170763.log)
DEBUG 22:32:51,406 Starting CFS MyColumnFamily
DEBUG 22:32:51,406 key cache capacity for MyColumnFamily is 200000
 INFO 22:32:51,407 Creating new commitlog segment /mnt/cassandra/commitlog/CommitLog-1283553171407.log
DEBUG 22:32:51,604 attempting to connect to node1-1.domain.com/10.250.106.111
 INFO 22:32:51,622 InetAddress /10.215.195.81 is now UP
 INFO 22:32:51,623 InetAddress /10.250.106.111 is now UP
 INFO 22:32:51,623 Started hinted handoff for endpoint /10.215.195.81
 INFO 22:32:51,631 Finished hinted handoff of 0 rows to endpoint /10.215.195.81
 INFO 22:32:51,632 Started hinted handoff for endpoint /10.250.106.111
 INFO 22:32:51,632 Finished hinted handoff of 0 rows to endpoint /10.250.106.111
DEBUG 22:32:51,918 GC for ParNew: 14 ms, 20561696 reclaimed leaving 149257720 used; max is 902627328
DEBUG 22:32:51,919 GC for ConcurrentMarkSweep: 73 ms, 4178480 reclaimed leaving 8520856 used; max is 902627328
DEBUG 22:32:52,924 Disseminating load info ...
DEBUG 22:32:53,929 attempting to connect to /10.215.195.81
DEBUG 22:32:54,925 GC for ConcurrentMarkSweep: 73 ms, 212034048 reclaimed leaving 15214608 used; max is 902627328
DEBUG 22:33:52,931 Disseminating load info ...
DEBUG 22:34:20,929 ... got load info
 INFO 22:34:20,929 Joining: getting bootstrap token
DEBUG 22:34:20,931 token manually specified as 113416112894748789872342756657008344878
 INFO 22:34:20,932 Joining: sleeping 30000 ms for pending range setup
 INFO 22:34:50,935 Bootstrapping
DEBUG 22:34:50,935 Beginning bootstrap process
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1)
        at org.apache.cassandra.locator.RackUnawareStrategy.calculateNaturalEndpoints(RackUnawareStrategy.java:57)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getRangeAddresses(AbstractReplicationStrategy.java:195)
        at org.apache.cassandra.dht.BootStrapper.getRangesWithSources(BootStrapper.java:155)
        at org.apache.cassandra.dht.BootStrapper.startBootstrap(BootStrapper.java:73)
        at org.apache.cassandra.service.StorageService.startBootstrap(StorageService.java:467)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:408)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:134)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:57)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3
",gdusbabek,blanquer,Normal,Resolved,Fixed,03/Sep/10 23:24,16/Apr/19 09:33
Bug,CASSANDRA-1477,12473494,drop/recreate column family race condition,"using 0.7 latest from trunk as of few minutes ago.  1 client, 1 node

i have the scenario where i want to drop a column family and recreate it 
- unit testing for instance, is a good reason you may want to do this 
(always start fresh).

the problem i observe is that if i do the following:

1 - drop the column family
2 - recreate it
3 - read data from a key that existed before dropping, but doesn't exist now

if those steps happen fast enough, i will get the old row - definitely 
no good.

if they happen slow enough, get_slice throws:

""org.apache.thrift.TApplicationException: Internal error processing 
get_slice""

.. and on the server i see:

2010-09-07 13:53:48,086 ERROR 
[org.apache.cassandra.thrift.Cassandra$Processor] (pool-1-thread-4:) - 
Internal error processing get_slice
java.lang.RuntimeException: java.util.concurrent.ExecutionException: 
java.io.IOError: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at 
org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:275)
     at 
org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:218)
     at 
org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:114)
     at 
org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:220)
     at 
org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:299)
     at 
org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:260)
     at 
org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:2795)
     at 
org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2651)
     at 
org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
     at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
     at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
     at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: 
java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
     at 
org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:271)
     ... 11 more
Caused by: java.io.IOError: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at 
org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:68)
     at 
org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:509)
     at 
org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:49)
     at 
org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:65)
     at 
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:76)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:961)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:856)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:826)
     at org.apache.cassandra.db.Table.getRow(Table.java:321)
     at 
org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
     at 
org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:737)
     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
     ... 3 more
Caused by: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at java.io.RandomAccessFile.open(Native Method)
     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
     at 
org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
     at 
org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:62)
     ... 15 more

",gdusbabek,btoddb,Urgent,Resolved,Fixed,07/Sep/10 21:21,16/Apr/19 09:33
Bug,CASSANDRA-1480,12473524,CFMetaData.convertToThrift makes subcomparator_type empty string instead of null,"As a result of CASSANDRA-891 adding a CFMetaData.convertToThrift method, the values such as subcomparator_type are defaulted to empty string instead of null.  That makes it so, for example, in ColumnFamilyRecordReader, in its RowIterator, the check for only null is insufficient.  It also needs to check for a blank value.

After a discussion about it in IRC, Jonathan said it was probably easier to just change the creation to give a null value instead of empty string.",jhermes,jeromatron,Normal,Resolved,Fixed,08/Sep/10 03:27,16/Apr/19 09:33
Bug,CASSANDRA-1484,12473633,API Version Mismatch,"Updated to Cassandra Trunk and Thrift Trunk. API versions mismatch when describe_version() is called. 

AssertionError: Thrift API version mismatch. (Client: 14, Server: 13)

This causes some API clients that do version validation like pycassa to fail. 
",jbellis,arya,Low,Resolved,Fixed,08/Sep/10 23:02,16/Apr/19 09:33
Bug,CASSANDRA-1487,12473700,Bug in calculating QUORUM,"Hello,

It seems that there is a bug in calculating QUORUM in src/java/org/apache/cassandra/service/QuorumResponseHandler.java

Currently the QUORUM formula in place will return correct QUORUM if replication factor <= 3. However if you have a Replication Factor > 3, it will return incorrect result.

-----------------------
--- src/java/org/apache/cassandra/service/QuorumResponseHandler.java    (revision 995482)
+++ src/java/org/apache/cassandra/service/QuorumResponseHandler.java    (working copy)
@@ -109,7 +109,7 @@
             case ANY:
                 return 1;
             case QUORUM:
-                return (DatabaseDescriptor.getQuorum(table)/ 2) + 1;
+                return DatabaseDescriptor.getQuorum(table);
             case ALL:
                 return DatabaseDescriptor.getReplicationFactor(table);
             default:
-------------------
In QuorumResponseHandler:determineBlockFor()
DatabaseDescriptor.getQuorum(table) is already returning a quorum value which is further divided by 2 and a one is added.

So say if your RF=6, it is suppose to check 4 replicas, (6/2)+1=4 but it ends up checking only 3 replicas as DatabaseDescriptor.getQuorum returns 4, so determineBlockFor will return (4/2)+1=3.

Let me know if you have any questions.

Jignesh",jigneshdhruv,jigneshdhruv,Normal,Resolved,Fixed,09/Sep/10 16:34,16/Apr/19 09:33
Bug,CASSANDRA-1492,12473815,"Upgrade from 0.6.3 to 0.6.5, exception when replay commitlog","When start cassandra 0.6.5 with commitlog of 0.6.3, got exception when replay the commitlog. 

ERROR 15:14:16,174 Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: invalid UTF8 bytes [-84, 16, 10, -105]
	at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:43)
	at org.apache.cassandra.db.Column.getString(Column.java:215)
	at org.apache.cassandra.db.marshal.AbstractType.getColumnsString(AbstractType.java:85)
	at org.apache.cassandra.db.ColumnFamily.toString(ColumnFamily.java:334)
	at org.apache.commons.lang.ObjectUtils.toString(ObjectUtils.java:241)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3073)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3133)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:241)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:173)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:114)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:214)
",jbellis,apache.zli,Normal,Resolved,Fixed,10/Sep/10 15:28,16/Apr/19 09:33
Bug,CASSANDRA-1493,12473820,AssertionError in MessagingService.receive for READ_REPAIR verb,"Read repair messages are causing an assertion error in MessagingService. Looks like the enum introduced in CASSANDRA-1465 is missing a verb?

Added two lines of debug output, so lines are a bit off:
DEBUG [pool-1-thread-1] 2010-09-10 15:39:23,555 MessagingService.java (line 373) Verb: READ_REPAIR
DEBUG [pool-1-thread-1] 2010-09-10 15:39:23,555 MessagingService.java (line 374) MessageType: null
ERROR [pool-1-thread-1] 2010-09-10 15:39:23,555 Cassandra.java (line 1744) Internal error processing get
java.lang.AssertionError
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:376)
        at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:285)
        at org.apache.cassandra.service.ReadResponseResolver.maybeScheduleRepairs(ReadResponseResolver.java:163)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:116)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:43)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:89)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:430)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:266)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:113)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:317)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:1734)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1634)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",johanoskarsson,johanoskarsson,Normal,Resolved,Fixed,10/Sep/10 16:11,16/Apr/19 09:33
Bug,CASSANDRA-1507,12474211,make adaptive heap size calculation portable,"The adaptive heap size calculation is dependent on the 'free' tool which is typically available on Linux machines (GNU toolset) but not others.

I'm attaching a patch which makes FreeBSD specifically supported as well as falling back to static 1024m default if the operating system is unrecognized.
",scode,scode,Low,Resolved,Fixed,15/Sep/10 20:27,16/Apr/19 09:33
Bug,CASSANDRA-1508,12474213,only attempt to set size on Linux (for portability),"-Xss128k causes the JVM to refuse to start or crash on 64 bit FreeBSD 8 (this goes for two wildly differing openjdk 1.6:es and for the current openjdk7 branch). Attaching patch to only pass -Xss on Linux.

The motivation here is that out-of-the-box behavior is important for first-comers, and for people in production on a non-Linux platform where -Xss128k would work are presumably committed enough that they can tweak this themselves.
",scode,scode,Low,Resolved,Fixed,15/Sep/10 20:53,16/Apr/19 09:33
Bug,CASSANDRA-1509,12474242,CassandraServiceDataCleaner doesn't remove subdirectories properly,"CassandraServiceDataCleaner.cleanDir assumes all files in the directory are normal files, not directories.  Suggested fix is to change FileUtils.delete(dirFile.listFiles()) to FileUtils.deleteRecursive(f) to remove recursively which will delete all data files.
",btoddb,btoddb,Low,Resolved,Fixed,16/Sep/10 03:05,16/Apr/19 09:33
Bug,CASSANDRA-1512,12474406,cassandra will replay the last mutation in a commitlog when it shouldn't,,jbellis,jbellis,Low,Resolved,Fixed,17/Sep/10 17:15,16/Apr/19 09:33
Bug,CASSANDRA-1513,12474434,BufferUnderflowExceptions,"Seeing a number of these in my log when running a trunk build from 9/11/2010
No idea how to duplicate it, hopefully you can make sense of it from the stack trace

ERROR [MUTATION_STAGE:19] 2010-09-14 02:24:50,704 DebuggableThreadPoolExecutor.
java (line 102) Error in ThreadPoolExecutorjava.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:145)
        at java.nio.ByteBuffer.get(ByteBuffer.java:692)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

",,wr0ngway,Normal,Resolved,Fixed,17/Sep/10 19:48,16/Apr/19 09:33
Bug,CASSANDRA-1520,12474520,stress.py's multiget option sends increasingly inefficient queries as more test data is inserted,"MultiGetter's key list sizes should be broken up better for more efficient queries. Setting an initial value that breaks up the key list into N sub lists (where N is the number of threads) yielded more efficient queries. (The choice of thread count here was a stop-gap for demonstration purposes. End result should probably be chunk-size config option with a sane default).

Pre patch:
---
python stress.py -o multiget -t 25 -n 250000 -c 5
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
6,0,6000,8.6109764576,10
10,0,4000,18.6666852832,20
17,0,7000,27.4705835751,30
23,0,6000,36.6091703971,41
25,0,2000,41.8415510654,42

Post patch:
---
python mstress.py -o multiget -t 25 -n 250000 -c 5
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
172,17,6880,1.44215503127,10
314,14,5680,1.8667214538,20
466,15,6080,1.69888155084,31
624,15,6320,1.55442555947,41
625,0,40,0.0914790630341,41",brandon.williams,zznate,Low,Resolved,Fixed,19/Sep/10 05:58,16/Apr/19 09:33
Bug,CASSANDRA-1522,12474614,Methods removed from FileUtils break CassandraServiceDataCleaner in contrib/javautils,CassandraServiceDataCleaner relied on methods in FileUtils that were removed in revision: 998464,zznate,zznate,Normal,Resolved,Fixed,20/Sep/10 17:14,16/Apr/19 09:33
Bug,CASSANDRA-1527,12474746,ensure compaction thresholds are sane,"make sure min <= max and neither is negative.

also make sure that min=max=0 works (this is ""no compaction"")",jhermes,jbellis,Low,Resolved,Fixed,21/Sep/10 19:45,16/Apr/19 09:33
Bug,CASSANDRA-1528,12474752,Cassandra holds a socket in CLOSE_WAIT on the storage port,"To repro: telnet to 7000, disconnect.  You have a socket in CLOSE_WAIT that will stay there until the server is restarted.",brandon.williams,brandon.williams,Low,Resolved,Fixed,21/Sep/10 20:18,16/Apr/19 09:33
Bug,CASSANDRA-1529,12474767,word count contrib module broken as a result of removing Clock interface,Just needs a quick patch.,jeromatron,jeromatron,Low,Resolved,Fixed,21/Sep/10 22:50,16/Apr/19 09:33
Bug,CASSANDRA-1534,12474870,errors reading while bootstrapping,"I loaded a 4 node cluster with 1M rows from stress.py, decommissioned a node, and then began bootstrapping it while performing constant reads against the others with stress.py.  After sleeping for 90s, the bootstrapping node started throwing many errors like this:

ERROR 16:51:48,667 Fatal exception in thread Thread[READ_STAGE:1270,5,main]
java.lang.RuntimeException: Cannot service reads while bootstrapping!
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:67)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

And I began receiving timeout errors with stress.py.",jbellis,brandon.williams,Normal,Resolved,Fixed,22/Sep/10 20:56,16/Apr/19 09:33
Bug,CASSANDRA-1535,12474874,Repair doesn't do anything when a CF isn't specified,"Invoking repair on a node does not work.  With RF > 1 all I get is:

INFO 15:04:59,987 Waiting for repair requests to: []

And nothing happens.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,22/Sep/10 21:44,16/Apr/19 09:33
Bug,CASSANDRA-1536,12474897,Detect use of secondary indexes with TTL'd columns,"We don't currently make any provisions for using TTL'd columns with secondary indexes. A reasonable temporary solution would be to fail inserts of TTL'd entries for indexed columns.

A longer term solution might be to record/detect the presence of expiring columns for an expression, and use that information to require post filtering in CFS.scan.",stuhood,stuhood,Normal,Resolved,Fixed,23/Sep/10 02:55,16/Apr/19 09:33
Bug,CASSANDRA-1538,12474921,0.7 on Windows,"bat file needs to be told to look for log4j-server.properties

anything else?",vloncar,jbellis,Low,Resolved,Fixed,23/Sep/10 11:03,16/Apr/19 09:33
Bug,CASSANDRA-1539,12474925,ExpiringColumn wrongly inherits Column.getMarkedForDeleteAt,"An ExpiringColumn could be 'markedAsDeleted', but Column.getMarkedForDeleteAt() always throw an exception.",slebresne,slebresne,Urgent,Resolved,Fixed,23/Sep/10 12:25,16/Apr/19 09:33
Bug,CASSANDRA-1540,12474972,cfstats is broken,"There appears to be a problem reading the cache stats:

bin/nodetool -h cassandra-6 cfstats
Keyspace: org.apache.cassandra.db.Table@620a3d3b
        Read Count: 9
        Read Latency: 2.563222222222222 ms.
        Write Count: 11
        Write Latency: 0.1572727272727273 ms.
        Pending Tasks: 0
                Column Family: LocationInfo
                SSTable count: 1
                Space used (live): 4886
                Space used (total): 4886
                Memtable Columns Count: 6
                Memtable Data Size: 179
                Memtable Switch Count: 1
                Read Count: 4
                Read Latency: 5.369 ms.
                Write Count: 8
                Write Latency: 0.210 ms.
                Pending Tasks: 0
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
        at $Proxy5.getCapacity(Unknown Source)
        at org.apache.cassandra.tools.NodeCmd.printColumnFamilyStats(NodeCmd.java:326)
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:439)
Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.db:type=Caches,keyspace=org.apache.cassandra.db.Table@620a3d3b,cache=LocationInfoKeyCache
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1118)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:679)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:672)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1284)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1382)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:619)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:273)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:251)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:160)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:885)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:280)
        ... 3 more
",brandon.williams,brandon.williams,Normal,Resolved,Fixed,23/Sep/10 19:36,16/Apr/19 09:33
Bug,CASSANDRA-1541,12474997,KEYS indexes recreated after first restart,"I made sure I waited at least {{commitlog_sync_period_in_ms}}, but the {{SystemTable.isIndexBuilt}} flag doesn't appear to be sticking after a call to system_add_column_family (via CASSANDRA-1531). During the first restart after creation, the index is recreated, logging: ""Creating index"" and ""Index _ complete"" again. The second restart works as it should.",jbellis,stuhood,Normal,Resolved,Fixed,24/Sep/10 02:09,16/Apr/19 09:33
Bug,CASSANDRA-1542,12475000,Data directories not being properly scrubbed,"AbstractCassandraDaemon is trying to scrub data directories once the server has already been initialized (CASSANDRA-1477, r997490).

To reproduce, delete a single component of an SSTable and restart.",gdusbabek,stuhood,Low,Resolved,Fixed,24/Sep/10 02:57,16/Apr/19 09:33
Bug,CASSANDRA-1544,12475051,Compacted SSTables not properly removed,It looks like SSTableDeletingReference isn't doing its job... SSTable.conditionalDelete is never being called for sstables that have been marked compacted.,stuhood,stuhood,Normal,Resolved,Fixed,24/Sep/10 15:58,16/Apr/19 09:33
Bug,CASSANDRA-1545,12475072,NullPointerException on startup after upgrade,"Running a cluster on trunk of 0.7.0beta-2 and updated to tip of trunk. On startup of node got the following NullPointerException. Was using r997774 and switched to r1000247

ERROR [main] 2010-09-22 12:30:14,110 AbstractCassandraDaemon.java (line 216) Exception encountered during startup.
java.lang.NullPointerException
    at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:373)
    at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:118)
    at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:106)
    at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:441)
    at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:109)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:54)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:199)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:133)",jhermes,bterm,Urgent,Resolved,Fixed,24/Sep/10 19:33,16/Apr/19 09:33
Bug,CASSANDRA-1547,12475245,lost+found directories cause problems for cassandra,ext3/4 make lost+found directories at the root of the file system.  if you then point C* at the root of the FS (e.g. you have a mount point of /cassandra_data and/or /cassandra_commitlog) C* thinks lost+found is a keyspace and spews.,mdennis,mdennis,Low,Resolved,Fixed,27/Sep/10 21:40,16/Apr/19 09:33
Bug,CASSANDRA-1548,12475250,"renaming a keyspace, then trying to use original name again makes errorations","My test case does the following:

* Create a keyspace with at least one CF in it.
* Rename that keyspace
* Create a new keyspace with the same original name, containing a CF with the same name as earlier.

The second keyspace creation receives an error (although the keyspace does get created):

{{javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=keyspacename,columnfamily=cfname}}

After that point, trying to do almost anything with the new keyspace will generate the same error- even trying to drop it. This persists until cassandra itself is restarted.

One supposes that some JMX thing is lacking reregistration upon keyspace rename.",gdusbabek,thepaul,Low,Resolved,Fixed,27/Sep/10 22:41,16/Apr/19 09:33
Bug,CASSANDRA-1549,12475251,cassandra-cli craps its pants and dies when 'gc_grace_seconds' is used in cf creation,"{noformat}
trunk$ bin/cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
[default@unknown] use Keyspace1
Authenticated to keyspace: Keyspace1
[default@Keyspace1] create column family cfname with gc_grace_seconds=86400
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:817)
        at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:105)
        at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:230)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:302)
{noformat}

it's just a missing ""break;"" statement in CliClient.java.",thepaul,thepaul,Low,Resolved,Fixed,27/Sep/10 22:52,16/Apr/19 09:33
Bug,CASSANDRA-1550,12475318,enable/disable HH via JMX,,jbellis,jbellis,Low,Resolved,Fixed,28/Sep/10 17:20,16/Apr/19 09:33
Bug,CASSANDRA-1554,12475364,extend authorization to column families,"Authorization is now based on a hierarchy of resources, but the hierarchy only extends as far as keyspaces.  At the very least, it should be possible to implement an authority that can distinguish between the creation, modification and deletion of column families, and reading and writing the data contained in them.",urandom,urandom,Normal,Resolved,Fixed,28/Sep/10 23:46,16/Apr/19 09:33
Bug,CASSANDRA-1556,12475384,InvalidRequestException(why='') returned from system_add_keyspace when strategy_class not found,"In thrift/CassandraServer system_add_keyspace() the strategy_class string from the KsDef is used to load a class. The ClassNotFoundError is then caught and used to build an InvalidRequestException. If the strategy_class is missing or empty, the error returned to the client is 

(python)
InvalidRequestException: InvalidRequestException(why='')

or 

InvalidRequestException: InvalidRequestException(why='foo')",amorton,amorton,Low,Resolved,Fixed,29/Sep/10 07:50,16/Apr/19 09:33
Bug,CASSANDRA-1557,12475385,ColumnFamilyStore masking IOException from FileUtils as IOError,"The code in ColumnFamilyStore.snapshot() line 1368 is catching an IOException from the call to FileUtils.createHardLink() and wrapping it in an IOError. However the code in TruncateVerbHandler:56 is looking for the IOException. This can result  in the client not getting a response to a truncate() API call. 

When running on a machine with very low memory I attempted to truncate a CF with few rows, the following error occurred in the logs.

ERROR [MUTATION_STAGE:25] 2010-09-29 16:44:39,341 AbstractCassandraDaemon.java (line 88) Fatal exception in thread Thread[MUTATION_STAGE:25,5,main]
java.io.IOError: java.io.IOException: Cannot run program ""ln"": java.io.IOException: error=12, Cannot allocate memory
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1368)
        at org.apache.cassandra.db.ColumnFamilyStore.truncate(ColumnFamilyStore.java:1511)
        at org.apache.cassandra.db.Table.truncate(Table.java:633)
        at org.apache.cassandra.db.TruncateVerbHandler.doVerb(TruncateVerbHandler.java:54)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at javautil.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Cannot run program ""ln"": java.io.IOException: error=12, Cannot allocate memory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:460)
        at org.apache.cassandra.io.util.FileUtils.createHardLinkWithExec(FileUtils.java:263)
        at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:229)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1360)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=12, Cannot allocate memory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:453)
        ... 10 more

On the client I got this:

  File ""/tech/home//git_home/trojan/trojan/cassandra/Cassandrapy"", line 846, in truncate
    self.recv_truncate()
  File ""/tech/home//git_home/trojan/trojan/cassandra/Cassandra.py"", line 857, in recv_truncate
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""/tech/home//git_home/trojan/trojan/thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
<snip>
    chunk = self.read(sz-have)
  File ""/tech/home//git_home/trojan/trojan/thrift/transport/TSocket.py"", line 92, in read
    buff = self.handle.recv(sz)
timeout: timed out",jbellis,amorton,Low,Resolved,Fixed,29/Sep/10 08:02,16/Apr/19 09:33
Bug,CASSANDRA-1560,12475456,describe_keyspace() should include strategy_options,describe_keyspace() does not include the strategy_options in the returned KsDef,jhermes,thobbs,Low,Resolved,Fixed,29/Sep/10 23:19,16/Apr/19 09:33
Bug,CASSANDRA-1561,12475517,Disallow bootstrapping to a token that is already owned by a live node,,jbellis,jbellis,Low,Resolved,Fixed,30/Sep/10 16:54,16/Apr/19 09:33
Bug,CASSANDRA-1564,12475568,incomplete sstables for System keyspace are not scrubbed before opening it,(From CASSANDRA-1477),jbellis,jbellis,Normal,Resolved,Fixed,01/Oct/10 03:18,16/Apr/19 09:33
Bug,CASSANDRA-1568,12475718,nodeprobe help message is missing option to compact specific keyspace,"{noformat}
-                ""%nAvailable commands: ring, info, version, cleanup, compact, cfstats, snapshot [snapshotname], clearsnapshot, "" +
-                ""tpstats, flush, drain, repair, decommission, move, loadbalance, removetoken [status|force]|[token], "" +
+                ""%nAvailable commands: ring, info, version, cleanup, compact [keyspacename], cfstats, snapshot [snapshotname], "" +
+                ""clearsnapshot, tpstats, flush, drain, repair, decommission, move, loadbalance, removetoken [status|force]|[token], "" +
{noformat}",appodictic,appodictic,Low,Resolved,Fixed,03/Oct/10 18:17,16/Apr/19 09:33
Bug,CASSANDRA-1571,12475744,Secondary Indexes aren't updated when removing whole row,"When I remove a whole row in a CF
del SomeColumnFamily['row']

SI is not updated and get_indexed_slices still returns the deleted row.",jbellis,xodut,Normal,Resolved,Fixed,04/Oct/10 09:17,16/Apr/19 09:33
Bug,CASSANDRA-1573,12475792,StreamOut fails to start an empty stream,StreamOut only starts a stream if there are actually files to transfer. This means callbacks will never get called for streams that don't actually have anything to transfer.,jbellis,nickmbailey,Urgent,Resolved,Fixed,04/Oct/10 18:53,16/Apr/19 09:33
Bug,CASSANDRA-1574,12475799,Bootstrapping is broken,Bootstrap doesn't block for streaming requests which means nodetool move isn't blocking.  More importantly bootstrap fails to call finishBootstrapping if no stream requests are ever made. This means its impossible to perform moves if you have no keyspaces.,nickmbailey,nickmbailey,Urgent,Resolved,Fixed,04/Oct/10 20:06,16/Apr/19 09:33
Bug,CASSANDRA-1575,12475803,suggest avoiding broken openjdk6 on Debian as build-dep,"I ran into this myself and then today someone was reporting having the same problem on IRC; there is a packaging bug in openjdk6 in lenny:

   http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=501487

The effect is that when ant tries to download files over SSL, it fails complaining about:

   ""java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty""

It turns out this works fine with the Sun JVM. I'm attaching a patch which makes Cassandra build on both lenny and squeeze; however, I am not sure whether other platforms may be negatively affected. The patch just requires an openjdk sufficiently new that the lenny openjdk won't quality. If there are other platforms where we do want an older openjdk, this patch might break that.

In addition, I removed the ""java6-sdk"" as a sufficient dependency because that resolved to openjdk-6-jdk on lenny.

I think it's a good idea to consider changing this just to decrease the initial threshold of adoption for those trying to build from source.

So: This does fix the build issue on lenny, and doesn't seem to break squeeze, but I cannot promise anything about e.g. ubuntu.

For the record, I'm also attaching a small self-contained test case which, when run, tries to download one of the offending pom files. It can be used to easily test weather the SSL download with work with a particular JVM.",urandom,scode,Low,Resolved,Fixed,04/Oct/10 20:28,16/Apr/19 09:33
Bug,CASSANDRA-1590,12476720,Pig loadfunc fails with java.io.FileNotFoundException: ...:.../job.jar!/storage-conf.xml,"Trying to run the example job from contrib/pig (after fixing it to start at all in the first place; details later) results in this:


2010-10-06 15:43:32,117 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - (Name: Store(hdfs://localhost/tmp/temp-1257182404/tmp1075428643:org.apache.pig.builtin.BinStorage) - 1-60 Operator Key: 1-60)
2010-10-06 15:43:32,164 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer - Choosing to move algebraic foreach to combiner
2010-10-06 15:43:32,224 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 3
2010-10-06 15:43:32,224 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 3
2010-10-06 15:43:32,302 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2010-10-06 15:43:40,356 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
2010-10-06 15:43:40,450 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.
2010-10-06 15:43:40,457 [Thread-12] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
2010-10-06 15:43:40,950 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2010-10-06 15:43:41,038 [Thread-12] INFO  org.apache.cassandra.config.DatabaseDescriptor - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
2010-10-06 15:43:41,211 [Thread-12] WARN  org.apache.cassandra.config.DatabaseDescriptor - KeysCachedFraction is deprecated: use KeysCached instead.
2010-10-06 15:43:41,232 [Thread-12] WARN  org.apache.cassandra.config.DatabaseDescriptor - KeysCachedFraction is deprecated: use KeysCached instead.
2010-10-06 15:43:42,305 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201010061447_0008
2010-10-06 15:43:42,305 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://localhost:50030/jobdetails.jsp?jobid=job_201010061447_0008
2010-10-06 15:44:15,025 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 33% complete
2010-10-06 15:44:17,037 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2010-10-06 15:44:17,037 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map reduce job(s) failed!
2010-10-06 15:44:17,067 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!
2010-10-06 15:44:17,199 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2997: Unable to recreate exception from backed error: Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
Details at logfile: /home/tv/casspig/cassandra/contrib/pig/pig_1286405010154.log

Contents of that pig_*.log:


Backend error message
---------------------
Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:542)
	at org.apache.cassandra.hadoop.ConfigHelper.getThriftPort(ConfigHelper.java:188)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.<init>(ColumnFamilyRecordReader.java:118)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.<init>(ColumnFamilyRecordReader.java:104)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:93)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.initialize(PigRecordReader.java:133)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
Caused by: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:106)
	at java.io.FileInputStream.<init>(FileInputStream.java:66)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:70)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:161)
	at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:653)
	at com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:186)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:772)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119)
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:235)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:284)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:208)
	at org.apache.cassandra.utils.XMLUtils.<init>(XMLUtils.java:43)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:167)
	... 9 more

Pig Stack Trace
---------------
ERROR 2997: Unable to recreate exception from backed error: Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias topnames
	at org.apache.pig.PigServer.openIterator(PigServer.java:607)
	at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:544)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:162)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:138)
	at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:75)
	at org.apache.pig.Main.main(Main.java:380)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2997: Unable to recreate exception from backed error: Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getErrorMessages(Launcher.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getStats(Launcher.java:175)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:270)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:308)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1007)
	at org.apache.pig.PigServer.store(PigServer.java:697)
	at org.apache.pig.PigServer.openIterator(PigServer.java:590)
	... 6 more
================================================================================


I'm attaching a tarball with everything needed to reproduce this, see the run script there.",,tv42,Normal,Resolved,Fixed,06/Oct/10 23:05,16/Apr/19 09:33
Bug,CASSANDRA-1591,12476722,"get_range_slices always returns super columns that's been removed/restored, regardless of count value in slicerange","I'm seeing cases where the count in slicerange predicate is not respected. This is only happening for super columns. I'm running Cassandra 0.6.4 in a single node.

Steps to reproduce, using the Keyspace1.Super1 CF:
* insert three super columns, bar1 bar 2, and bar3, under the same key
* delete bar1
* insert bar1 again
* run a get_range_slices on Super1, with start=bar1, finish=bar3, and count=1
* I expected only bar1 to be returned, but both both bar1 and bar2 are returned. bar3 isn't, though. so count is somewhat respected.

perl code to reproduce is attached
when I tried the same test on a standard CF it worked. only super CF seem to have this problem.",jbellis,hujn,Low,Resolved,Fixed,07/Oct/10 00:06,16/Apr/19 09:33
Bug,CASSANDRA-1604,12477040,Database Descriptor has log message that mashes words,"-                logger.info(""DiskAccessMode is"" + conf.disk_access_mode + "", indexAccessMode is "" + indexAccessMode );
+                logger.info(""DiskAccessMode is "" + conf.disk_access_mode + "", indexAccessMode is "" + indexAccessMode );",appodictic,appodictic,Low,Resolved,Fixed,11/Oct/10 16:26,16/Apr/19 09:33
Bug,CASSANDRA-1605,12477046,RemoveToken waits for dead nodes,RemoveToken will wait for replication confirmation from nodes that are down.   It should only wait for live nodes.,nickmbailey,nickmbailey,Normal,Resolved,Fixed,11/Oct/10 17:59,16/Apr/19 09:33
Bug,CASSANDRA-1606,12477075,Thrift CfDef incomplete; missing row/key_save_period_in_seconds,"Missed from CASSANDRA-1417.
",jhermes,jhermes,Normal,Resolved,Fixed,11/Oct/10 21:38,16/Apr/19 09:33
Bug,CASSANDRA-1609,12477154,Cluster restart re-adds removed tokens,"After a cluster restart one of our nodes began reporting tokens that had been removed a good while ago (week or more) in it's nodetool ring output.  This probably has something to do with our change to persist the ring in CASSANDRA-1518 and removetoken changes in CASSANDRA-1216. The node didn't actually gossip the removed tokens so they showed up in TMD but not gossip.

Additionally all nodes began reporting a node that had been removed maybe an hour ago.  ",jbellis,nickmbailey,Normal,Resolved,Fixed,12/Oct/10 16:49,16/Apr/19 09:33
Bug,CASSANDRA-1612,12477205,cli support for strategy_options,"[default@unknown] create keyspace MyCollections with placement_strategy='org.apache.cassandra.locator.NetworkTopologyStrategy' and strategy_options=[{DC1:2, DC2:2}] and replication_factor=4
No enum const class org.apache.cassandra.cli.CliClient$AddKeyspaceArgument.STRATEGY_OPTIONS
",xedin,jbellis,Normal,Resolved,Fixed,13/Oct/10 03:43,16/Apr/19 09:33
Bug,CASSANDRA-1613,12477206,index created from cli does not show up in keyspace metadata,"create column family Category with comparator=UTF8Type and column_metadata=[{column_name:level, validation_class:IntegerType, index_type:0, index_name:CategoryLevelIdx}]

succeeds, but after this
'show keyspaces' does not reveal the presence of the index. Only the column family is shown.",xedin,jbellis,Normal,Resolved,Fixed,13/Oct/10 03:44,16/Apr/19 09:33
Bug,CASSANDRA-1617,12477257,BufferUnderflowException occurs in RowMutationVerbHandler,"There might be a bug in hinted handoff?

I have a cluster of 8, replication factor of 3, doing reads/writes with QUORUM.
I have a single thread doing reads/writes of about 2kb across all nodes, running about 200hps.
When I shut down one node, within a few seconds I start seeing some very big recent write latencies, 4-5 seconds.
I looked at the system.log on the node with the adjacent token to the node that I shut down, and see a bad looking BufferUnderflowException:

INFO [WRITE-kv2-app02.dev.real.com/172.27.109.32] 2010-10-12 12:13:36,712 
OutboundTcpConnection.java (line 115) error writing to 
kv2-app02.dev.real.com/172.27.109.32
 INFO [WRITE-kv2-app02.dev.real.com/172.27.109.32] 2010-10-12 12:13:50,336 
OutboundTcpConnection.java (line 115) error writing to 
kv2-app02.dev.real.com/172.27.109.32
 INFO [Timer-0] 2010-10-12 12:14:22,792 Gossiper.java (line 196) InetAddress 
/172.27.109.32 is now dead.
ERROR [MUTATION_STAGE:1315] 2010-10-12 12:14:24,917 
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:1315] 2010-10-12 12:14:24,918 
AbstractCassandraDaemon.java (line 88) Fatal exception in thread 
Thread[MUTATION_STAGE:1315,5,main]
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:1605] 2010-10-12 12:14:28,919 
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
....
....

I restarted the previously stopped node, and the system recovers, but with a 
few more underlflow exceptions:

 INFO [GOSSIP_STAGE:1] 2010-10-12 12:15:44,537 Gossiper.java (line 594) Node 
/172.27.109.32 has restarted, now UP again
 INFO [HINTED-HANDOFF-POOL:1] 2010-10-12 12:15:44,537 HintedHandOffManager.java 
(line 196) Started hinted handoff for endpoint /172.27.109.32
 INFO [GOSSIP_STAGE:1] 2010-10-12 12:15:44,537 StorageService.java (line 643) 
Node /172.27.109.32 state jump to normal
 INFO [HINTED-HANDOFF-POOL:1] 2010-10-12 12:15:44,538 HintedHandOffManager.java 
(line 252) Finished hinted handoff of 0 rows to endpoint /172.27.109.32
 INFO [GOSSIP_STAGE:1] 2010-10-12 12:15:44,538 StorageService.java (line 650) 
Will not change my token ownership to /172.27.109.32
ERROR [MUTATION_STAGE:1635] 2010-10-12 12:15:45,083 
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",brandon.williams,moores,Normal,Resolved,Fixed,13/Oct/10 17:01,16/Apr/19 09:33
Bug,CASSANDRA-1620,12477420,statistics not created after streaming data,"after loadbalance operation, cfstats NPEs:

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamilyStore.getMinRowSize(ColumnFamilyStore.java:332)
        at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",jbellis,jbellis,Normal,Resolved,Fixed,15/Oct/10 04:25,16/Apr/19 09:33
Bug,CASSANDRA-1623,12477501,BUG: secondaryIndexes AND multiple index expressions can cause timesouts,"1. Given this Column Family definition

    Column Family Name: Requests
      Column Family Type: Standard
      Column Sorted By: org.apache.cassandra.db.marshal.UTF8Type
      Column Metadata:
        Column Name: requested
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Type: KEYS
        Column Name: requestor
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Type: KEYS

If I have an entry that has the following column/value pairs:

""request-uuid1"" : [  { ""requested"",""person-uuid1"" }, { ""requestor"",""person-uuid2""}, { ""is_confirmed"",""true"" } ]

If I do an index lookup (pseudo coded) :

get_index_slices( Connection,
                                 ColumnParent.column_family=""Requests"",
                                 [ { ""requested"",""eq"", ""person-uuid1"" }, { ""is_confirmed"",""eq"", ""false"" } ],      % Index Expressions
                                 """",100,   % StartKey, KeyCount
                                 """","""",false,100   % StartCol, EndCol, Reversed, ColCount )

for ""requested"" = ""person-uuid1"" and ""is_confirmed"" = false 

then I get the following entries in my log and the request times out along with all other requests on all clients.

DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,878 CassandraServer.java (line 531) scan
DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,897 StorageProxy.java (line 563) restricted single token match for query [0,0]
DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,897 StorageProxy.java (line 649) scan ranges are [0,0]
DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,925 StorageProxy.java (line 669) reading org.apache.cassandra.db.IndexScanCommand@42a6eb from 52@localhost/127.0.0.1
DEBUG [ReadStage:2] 2010-10-15 19:00:27,931 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,933 SliceQueryFilter.java (line 121) collecting 0 of 2147483647: is_confirmed:false:4@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,934 SliceQueryFilter.java (line 121) collecting 1 of 2147483647: request_type:false:6@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,935 SliceQueryFilter.java (line 121) collecting 2 of 2147483647: requested:false:58@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,935 SliceQueryFilter.java (line 121) collecting 3 of 2147483647: requested_network:false:57@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,936 SliceQueryFilter.java (line 121) collecting 4 of 2147483647: requestor:false:58@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,937 SliceQueryFilter.java (line 121) collecting 5 of 2147483647: requestor_network:false:57@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,942 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,943 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,945 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,946 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
 this last line repeats forever until I stop the server.

If instead I do the lookup where both terms match or just the last term matches then nothing goes wrong, I get a valid (empty or otherwise) result set.

It only seems to happen if the 2nd expression does not match.

I am using the very latest code from trunk.


Jason
                                       ",jbellis,jasontanner,Normal,Resolved,Fixed,15/Oct/10 19:59,16/Apr/19 09:33
Bug,CASSANDRA-1628,12477654,RedHat init script status is a no-op,"The current bare-bones init script is a little too bare-bones. The ""status"" argument make a successful no-op, which can be pretty misleading.",addumb,addumb,Low,Resolved,Fixed,18/Oct/10 17:37,16/Apr/19 09:33
Bug,CASSANDRA-1630,12477742,system_rename_* methods need to be removed until we can solve compaction and flush races.,,gdusbabek,gdusbabek,Normal,Resolved,Fixed,19/Oct/10 13:36,16/Apr/19 09:33
Bug,CASSANDRA-1631,12477746,dropping column families and keyspaces races with compaction and flushing,,gdusbabek,gdusbabek,Normal,Resolved,Fixed,19/Oct/10 14:10,16/Apr/19 09:33
Bug,CASSANDRA-1635,12477874,cli not picking up type annotation on set,"{code}
[default@Keyspace1] create column family Users
create column family Users
737c7a71-dc56-11df-8240-e700f669bcfc
[default@Keyspace1] set Users[jsmith][first] = 'John'
set Users[jsmith][first] = 'John'
Value inserted.
[default@Keyspace1] set Users[jsmith][last] = 'Smith'
set Users[jsmith][last] = 'Smith'
Value inserted.
[default@Keyspace1] set Users[jsmith][age] = long(42)
set Users[jsmith][age] = long(42)
Value inserted.
[default@Keyspace1] get Users[jsmith]
get Users[jsmith]
=> (column=6c617374, value=Smith, timestamp=1287584999695000)
=> (column=6669727374, value=John, timestamp=1287584990126000)
=> (column=616765, value=^@^@^@^@^@^@^@*, timestamp=1287585014593000)
Returned 3 results.
{code}",xedin,jbellis,Normal,Resolved,Fixed,20/Oct/10 14:40,16/Apr/19 09:33
Bug,CASSANDRA-1638,12477915,initMetadata does not load partitioner from disk,"The function initMetadata() in org/apache/cassandra/db/SystemTable.java does not add the PARTITIONER constant to columns. So when the NamesQueryFilter runs, it will never pull the value from disk. This makes this portion of the code always null:

        if (partitionerColumn == null)
        {
            Column c = new Column(PARTITIONER, partitioner.getBytes(""UTF-8""), TimestampClock.ZERO);
            cf.addColumn(c);
            logger.info(""Saved partitioner not found. Using "" + partitioner);
        }",lenn0x,lenn0x,Low,Resolved,Fixed,20/Oct/10 20:23,16/Apr/19 09:33
Bug,CASSANDRA-1639,12477942,NPE in StorageService when cluster is first being created,"Saw this exception on the 0.7.0-beta2 version of cassandra right after bringing up a cluster and trying to get the number of live nodes.

java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.stringify(StorageService.java:1151)
        at org.apache.cassandra.service.StorageService.getLiveNodes(StorageService.java:1138)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)


I fixed this by adding some null checks to the stringify methods 

    private Set<String> stringify(Collection<InetAddress> endpoints)
    {
        Set<String> stringEndpoints = new HashSet<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

    private List<String> stringify(List<InetAddress> endpoints)
    {
        List<String> stringEndpoints = new ArrayList<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

After adding those checks, then I got more reasonable/realistic errors from a different part of the code since the service wasn't up yet as the cluster was still initializing:

Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.service:type=StorageService
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:662)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:255)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:233)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:142)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:878)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:263)
",,davew,Low,Resolved,Fixed,21/Oct/10 00:40,16/Apr/19 09:33
Bug,CASSANDRA-1641,12477951,auto-guessed memtable sizes are too high,"I've seen two cases now of the memtable sizes being too large, causing OOMing.  Too-small memtables hurt performance, but too-large hurts worse when you start GC storming.",jbellis,jbellis,Normal,Resolved,Fixed,21/Oct/10 05:22,16/Apr/19 09:33
Bug,CASSANDRA-1642,12477992,"JOIN verb was removed, breaking serialization by ordinal.",,gdusbabek,gdusbabek,Low,Resolved,Fixed,21/Oct/10 14:09,16/Apr/19 09:33
Bug,CASSANDRA-1643,12478014,Endpoint cache for a token should be part of AbstractReplicationStrategy and not Snitch,"There is a single DynamicEndpointSnitch object for all ReplicationStrategy objects. This DynamicEndpointSnitch object contains a single IEndpointSnitch subsnitch object. This subsnitch object contains the Endpoint cache for a token. Thus there is a single endpoint cache for all ReplicationStrategy objects. This implies that replica nodes for a Token as returned by the Cache would be same irrespective of the ReplicationStrategy object. This is a bug, the Endpoint cache should be a part of ""AbstractReplicationStrategy"" object rather than the IEndpointSnitch object.
",jbellis,khichrishi,Normal,Resolved,Fixed,21/Oct/10 18:31,16/Apr/19 09:33
Bug,CASSANDRA-1644,12478025,Commitlog.recover can delete the commitlog segment instance() just created,,jbellis,jbellis,Urgent,Resolved,Fixed,21/Oct/10 19:51,16/Apr/19 09:33
Bug,CASSANDRA-1645,12478033,Bootstrapping new node causes RowMutationVerbHandler Couldn't find cfId,"Existing 0.7.0-beta2 cluster adding 1 new node with no data data on it. Enable bootstrapping and start new node and received stream of Couldn't find cfId, added keyspaces via cli and errors stopped. Node did not bootstrap.


ERROR [MUTATION_STAGE:6] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1011
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:113)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:365)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:375)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:21] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1011
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:113)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:365)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:375)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:31] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1016
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:113)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:365)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:375)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:4] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation",gdusbabek,bterm,Normal,Resolved,Fixed,21/Oct/10 21:26,16/Apr/19 09:33
Bug,CASSANDRA-1646,12478042,Using QUORUM and replication factor of 1 now causes a timeout exception,See the attached path to the python thrift tests.  On the source from 2010-10-14 20:00 UTC this passed.  From the latest HEAD as of today this fails. ,jbellis,tnine,Urgent,Resolved,Fixed,22/Oct/10 02:56,16/Apr/19 09:33
Bug,CASSANDRA-1648,12478071,CliTest crashing intermittently,"About 50% of the time I get

{code}
$ ant test -Dtest.name=CliTest
...
    [junit] Test org.apache.cassandra.cli.CliTest FAILED (crashed)
{code}",xedin,jbellis,Low,Resolved,Fixed,22/Oct/10 14:37,16/Apr/19 09:33
Bug,CASSANDRA-1649,12478089,Queries on system keyspace over thrift api all fail,"as far as I can tell, any calls to get, get_slice, get_range_slices, get_count, etc on any ColumnFamily in the ""system"" keyspace results in an error like the following:

{noformat}
ERROR 16:29:41,278 Internal error processing get
java.lang.AssertionError: No replica strategy configured for system
        at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:315)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1459)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1447)
        at org.apache.cassandra.service.StorageService.findSuitableEndpoint(StorageService.java:1493)
        at org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:245)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:224)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:131) 
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:324)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2655)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

Might be only when this is done over the thrift api.  I don't even know how to use the avro api or query in any other way.  But at least this sort of thing used to work around a week ago.",jbellis,thepaul,Low,Resolved,Fixed,22/Oct/10 16:37,16/Apr/19 09:33
Bug,CASSANDRA-1650,12478109,removetoken is broken,"When running removetoken on a dead node, it hangs forever.  The debug log shows:

DEBUG 19:45:53,794 Node /10.179.111.137 ranges [(115049868157599339472315320703867977321,62676456546693435176060154681903071729]]
DEBUG 19:45:53,795 Range (115049868157599339472315320703867977321,62676456546693435176060154681903071729] will be responsibility of cassandra-1/10.179.65.102
DEBUG 19:45:53,798 Pending ranges:
cassandra-1/10.179.65.102:(115049868157599339472315320703867977321,62676456546693435176060154681903071729]

DEBUG 19:45:53,798 Node /10.179.111.137 ranges [(115049868157599339472315320703867977321,62676456546693435176060154681903071729]]
DEBUG 19:45:53,799 Range (115049868157599339472315320703867977321,62676456546693435176060154681903071729] will be responsibility of cassandra-1/10.179.65.102
",nickmbailey,brandon.williams,Normal,Resolved,Fixed,22/Oct/10 19:44,16/Apr/19 09:33
Bug,CASSANDRA-1655,12478153,Config converter fails,"Trying to run the config converter for an 0.6.6 -> 0.7.0-rc1 upgrade failed with the following exception:
{code:java}
stuhood@stu-laptop:~/src/cassandra/apache-cassandra-0.7.0-rc1$ bin/config-converter storage-conf.xml cassandra.yaml
WARN : Thrift uses framed Transport by default in 0.7! Setting TFramedTransportSize to 0MB (disabled).
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:63)
	at org.apache.cassandra.config.Converter.readTablesFromXml(Converter.java:77)
	at org.apache.cassandra.config.Converter.loadPreviousConfig(Converter.java:308)
	at org.apache.cassandra.config.Converter.main(Converter.java:359)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:389)
	... 4 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.config.KSMetaData.<init>(KSMetaData.java:50)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:349)
	... 4 more
Exception in thread ""PERIODIC-COMMIT-LOG-SYNCER"" java.lang.NoClassDefFoundError: Could not initialize class org.apache.cassandra.config.DatabaseDescriptor
	at org.apache.cassandra.db.commitlog.CommitLog$2.run(CommitLog.java:136)
	at java.lang.Thread.run(Thread.java:619)
{code}",stuhood,stuhood,Urgent,Resolved,Fixed,23/Oct/10 16:27,16/Apr/19 09:33
Bug,CASSANDRA-1656,12478159,Hinted handoff does not replay data,,jbellis,jbellis,Normal,Resolved,Fixed,23/Oct/10 18:21,16/Apr/19 09:33
Bug,CASSANDRA-1659,12478166,Config converter should set framed transport by default,"Our built in clients require the framed transport, so it makes sense to enable it by default in the config converter.",stuhood,stuhood,Normal,Resolved,Fixed,23/Oct/10 23:12,16/Apr/19 09:33
Bug,CASSANDRA-1661,12478239,Use of ByteBuffer limit() must account for arrayOffset(),"There are a few places in the code where it loops across a byte buffers backing array wrong:


        for (int i=bytes.position()+bytes.arrayOffset(); i<bytes.limit(); i++)

This is incorrect as the limit() does not account for arrayOffset()

              for (int i=bytes.position()+bytes.arrayOffset(); i<bytes.limit()+bytes.arrayOffset(); i++)

is the correct code.

There is also a few places where the unit tests would fail if we used non wrapped byte arrays.
",tjake,tjake,Normal,Resolved,Fixed,25/Oct/10 15:06,16/Apr/19 09:33
Bug,CASSANDRA-1662,12478253,RPM spec file should create saved_caches directory,"After installing the 0.6.6 RPM from rpm.riptano.com, the directories /var/lib/cassandra/data and /var/lib/cassandra/commitlog exist, but /var/lib/cassandra saved_caches does not exist. Cassandra fails to startup with ""java.io.IOException: unable to mkdirs /var/lib/cassandra/saved_caches"".

After manually creating /var/lib/cassandra/saved_caches, Cassandra can start.",drevell,drevell,Low,Resolved,Fixed,25/Oct/10 18:07,16/Apr/19 09:33
Bug,CASSANDRA-1665,12478392,JMX threads leak in NodeProbe,There is a JMX threads leak in NodeProbe.  It creates and uses a JMXConnector but never calls its close() method.  I am working on a patch which add a close() method to NodeProbe  that calls JMXConnector.close().,billa,billa,Low,Resolved,Fixed,26/Oct/10 20:32,16/Apr/19 09:33
Bug,CASSANDRA-1666,12478413,logging error on C* startup: Nodes /10.194.241.188 and /10.194.241.188 have the same token 85070591730234615865843651857942052863,"When restarting my cluster, I noticed that a

{code}Nodes /10.194.241.188 and /10.194.241.188 have the same token 85070591730234615865843651857942052863{code} error message.  Notice the IPs are the same.  The cluster came up fine and nodetool ring shows all nodes up so it's likely just a logging error.
",jbellis,mdennis,Low,Resolved,Fixed,27/Oct/10 01:44,16/Apr/19 09:33
Bug,CASSANDRA-1670,12478510,cannot move a node,"two node cluster (node0, node1).  node0 is listed as the only seed on both nodes.  Listen addresses explicitly set to an IP on both nodes. No initial token, no autobootstrap (but see below).  Bring up the ring.  Everything is fine on both nodes.

decom node1.  verify decom completed correctly by reading the logs on both nodes.  rm all data/logs on node1.  bring node1 up again.

One of two things happen:

* node0 thinks it is in a ring by itself, node1 thinks both nodes are in the ring.
* both node0 and node1 think they are in rings by themselves

If you restart node0 after decom, it appears to work normally.

Similar issues seem to present if you kill node1 (either when autobootstrapping before it completes or after it is in the ring) and removetoken.

",gdusbabek,mdennis,Normal,Resolved,Fixed,27/Oct/10 22:28,16/Apr/19 09:33
Bug,CASSANDRA-1672,12478557,Hinted HandOff are not delivered correctly following change to ByteBuffer,"In trunk (rc1-snapshot), I get a Fatal Exception during hints delivery, the exception being: 
java.lang.RuntimeException: Corrupted hint name java.nio.HeapByteBuffer[pos=0 lim=22 cap=22]

This is due to a misuse of the 3rd parameter of ArrayUtils.lastIndexOf.",slebresne,slebresne,Normal,Resolved,Fixed,28/Oct/10 09:20,16/Apr/19 09:33
Bug,CASSANDRA-1674,12478581,Repair using abnormally large amounts of disk space,"I'm watching a repair on a 7 node cluster.  Repair was sent to one node; the node had 18G of data.  No other node has more than 28G.  The node where the repair initiated is now up to 261G with 53/60 AES tasks outstanding.

I have seen repair take more space than expected on 0.6 but nothing this extreme.

Other nodes in the cluster are occasionally logging
WARN [ScheduledTasks:1] 2010-10-28 08:31:14,305 MessagingService.java (line 515) Dropped 7 messages in the last 1000ms

The cluster is quiesced except for the repair.  Not sure if the dropped messages are contributing the the disk space (b/c of retries?).",stuhood,jbellis,Normal,Resolved,Fixed,28/Oct/10 14:38,16/Apr/19 09:33
Bug,CASSANDRA-1676,12478591,Avoid dropping messages off the client request path,,jbellis,jbellis,Normal,Resolved,Fixed,28/Oct/10 15:36,16/Apr/19 09:33
Bug,CASSANDRA-1678,12478609,describe_schema_versions does not list downed hosts,"According to the description unreachable hosts should be listed. It does not seem like they are.
{noformat}
 map<string, list<string>> describe_schema_versions()

  [java] key:c3f38ebc-e1c5-11df-95a0-e700f669bcfc
     [java] 	127.0.0.2
     [java] 	127.0.0.3
     [java] 	127.0.0.4
     [java] 	127.0.0.1

Address         Status State   Load            Token                                       
                                       105444142448428656124184491892431731479    
127.0.0.3       Up     Normal  56.53 KB        43021486531749787992103274496183765897      
127.0.0.1       Up     Normal  56.24 KB        49910048177093876350019363877113991186      
127.0.0.5       Down   Normal  52.49 KB        64377498999076014343862177049497951437      
127.0.0.2       Up     Normal  65.27 KB        84713069031498515281943177906254878023      
127.0.0.4       Up     Normal  55.95 KB        105444142448428656124184491892431731479
{noformat}

The code looks like this:
{noformat}
 Cassandra.Client client = fcw.getClient();
    Map<String,List<String>> sv =client.describe_schema_versions();
    for (Map.Entry<String,List<String>> mapEntry: sv.entrySet()){
      System.out.println(""key:""+mapEntry.getKey());
      for (String listForKey : mapEntry.getValue()){
        System.out.println(""\t""+listForKey);
      }
    }
{noformat}",gdusbabek,appodictic,Low,Resolved,Fixed,28/Oct/10 18:15,16/Apr/19 09:33
Bug,CASSANDRA-1679,12478619,ByteBuffer bug in ExpiringColumn.updateDigest() ,"The MessageDigest calls in ExpringColumn change the position of the bytebuffer.

",tjake,tjake,Normal,Resolved,Fixed,28/Oct/10 20:40,16/Apr/19 09:33
Bug,CASSANDRA-1681,12478635,IntegerType.toString() handles ByteBuffer incorrectly,IntegerType.getString doesn't correctly extract the byte array from the ByteBuffer passed to it. This causes incorrect results in cassandra-cli as described in [CASSANDRA-1680|https://issues.apache.org/jira/browse/CASSANDRA-1680].,jancona,jancona,Normal,Resolved,Fixed,29/Oct/10 01:41,16/Apr/19 09:33
Bug,CASSANDRA-1686,12478703,o.a.c.dht.AbstractBounds missing serialVersionUID,"o.a.c.dht.AbstractBounds does not have a  serialVersionUID set, and as a result, tools that make use of getRangeToEndpointMap() on the StorageService mbean must be from the exact same build or they fail with an java.io.InvalidClassException.  This is very inconvenient.",urandom,urandom,Low,Resolved,Fixed,29/Oct/10 22:00,16/Apr/19 09:33
Bug,CASSANDRA-1691,12478827,Repair blocking forever when RF=1,"Tested on single and multi-node, RF=1.  Repair runs but never reports that it's complete, hence the blocking forever.",stuhood,mbulman,Normal,Resolved,Fixed,01/Nov/10 19:15,16/Apr/19 09:33
Bug,CASSANDRA-1692,12478832,gen-thrift-py can fail but claims success,"{code}
Buildfile: /home/mdennis/mdev/cassandra-0.7.0-beta2/build.xml

gen-thrift-py:
     [echo] Generating Thrift Python code from /home/mdennis/mdev/cassandra-0.7.0-beta2/interface/cassandra.thrift ....
     [exec] 
     [exec] [FAILURE:/home/mdennis/mdev/cassandra-0.7.0-beta2/interface/cassandra.thrift:374] error: identifier ONE is unqualified!
     [exec] Result: 1

BUILD SUCCESSFUL
Total time: 1 second
{code}

""BUILD SUCCESSFUL"" is not the phrase I would use to describe the outcome of the command in this case :P",mdennis,mdennis,Low,Resolved,Fixed,01/Nov/10 19:53,16/Apr/19 09:33
Bug,CASSANDRA-1694,12478855,fix jna errno reporting,,jbellis,jbellis,Low,Resolved,Fixed,02/Nov/10 05:20,16/Apr/19 09:33
Bug,CASSANDRA-1697,12478906,Startup can fail if DNS lookup fails for seed node,"This might fall into one of those WONT FIX scenarios, since not many things are going to work well with flaky DNS. This has only happened to me once, but I someone might be interested in the stack trace. In this case cdbsd01.hadoop.pvt is one of my seed nodes.

{noformat}
 INFO [main] 2010-11-02 11:52:00,141 CLibrary.java (line 43) JNA not found. Native methods will be disabled.
 INFO [main] 2010-11-02 11:52:00,421 DatabaseDescriptor.java (line 246) DiskAccessMode ismmap, indexAccessMode is mmap
ERROR [main] 2010-11-02 11:52:25,591 CassandraDaemon.java (line 232) Exception encountered during startup.
java.lang.ExceptionInInitializerError
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:72)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:214)
Caused by: java.lang.RuntimeException: java.net.UnknownHostException: cdbsd01.hadoop.pvt
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:551)
	... 2 more
Caused by: java.net.UnknownHostException: cdbsd01.hadoop.pvt
	at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:850)
	at java.net.InetAddress.getAddressFromNameService(InetAddress.java:1201)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1154)
	at java.net.InetAddress.getAllByName(InetAddress.java:1084)
	at java.net.InetAddress.getAllByName(InetAddress.java:1020)
	at java.net.InetAddress.getByName(InetAddress.java:970)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:540)
	... 2 more
{noformat}
",jbellis,appodictic,Low,Resolved,Fixed,02/Nov/10 16:21,16/Apr/19 09:33
Bug,CASSANDRA-1700,12478945,mapreduce support is broken,Running from a vanilla download of beta3 src.  Tried the word count example and it's broken.  Attaching the stack trace.,stuhood,jeromatron,Normal,Resolved,Fixed,03/Nov/10 01:16,16/Apr/19 09:33
Bug,CASSANDRA-1701,12478959,cassandra-cli doesn't handle non-string column names well,"cassandra-cli has several bugs when using column names that aren't strings. Attached is a patch that updates CliTest to show the problems and fixes CliClient by properly converting non-string column and sub-column values passed to the GET, SET and COUNT commands.",jancona,jancona,Normal,Resolved,Fixed,03/Nov/10 05:21,16/Apr/19 09:33
Bug,CASSANDRA-1712,12479178,Creating a SuperColumnFamily other than BytesType results in incorrect comparator types ,"CF 1
    ColumnFamily: CFCli (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
      Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType

was created with cli using 

create column family CFCli with column_type= 'Super' and comparator= 'LongType' and subcomparator='UTF8Type'

 CF 2
 ColumnFamily: CFYaml (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
      Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType

was created with yaml using 

  column_families:
        - name: CFYaml
          column_type: Super
          compare_with: LongType
          compare_subcolumns_with: UTF8Type

In both cases Subcolumn comparator was defined as UTF8Type but CF was created with subcomparatortype of LongType

",jbellis,ceocoder,Low,Resolved,Fixed,05/Nov/10 08:00,16/Apr/19 09:33
Bug,CASSANDRA-1713,12479195,Windows batch files use incorrect paths,"Windows .bat files (with the exception of cassandra.bat) use %CD% to set CASSANDRA_HOME, and since that is incorrect, they fail to start with ClassNotFoundException.",vloncar,vloncar,Low,Resolved,Fixed,05/Nov/10 12:42,16/Apr/19 09:33
Bug,CASSANDRA-1715,12479261,More schema migration race conditions,"Related to CASSANDRA-1631.

This is still a bug with schema updates to an existing CF, since reloadCf is doing a unload/init cycle. So flushing + compaction is an issue there as well. Here is a stacktrace from during an index creation where it stubbed its toe on an incomplete sstable from an in-progress compaction (path names anonymized):
{code}
INFO [CompactionExecutor:1] 2010-11-02 16:31:00,553 CompactionManager.java (line 224) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-6-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-7-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-8-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-9-Data.db')]
...
ERROR [MigrationStage:1] 2010-11-02 16:31:10,939 ColumnFamilyStore.java (line 244) Corrupt sstable Standard1-tmp-e-10-<>=[Data.db, Index.db]; skipped
java.io.EOFException
        at org.apache.cassandra.utils.FBUtilities.skipShortByteArray(FBUtilities.java:308)
        at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:231)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:286)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:202)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:443)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:431)
        at org.apache.cassandra.db.Table.initCf(Table.java:335)
        at org.apache.cassandra.db.Table.reloadCf(Table.java:343)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.applyModels(UpdateColumnFamily.java:89)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:158)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:672)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
...
 INFO [CompactionExecutor:1] 2010-11-02 16:31:31,970 CompactionManager.java (line 303) Compacted to Standard1-tmp-e-10-Data.db.  213,657,983 to 213,657,983 (~100% of original) bytes for 626,563 keys.  Time: 31,416ms.
{code}

There is also a race between schema modification and streaming.",gdusbabek,jbellis,Urgent,Resolved,Fixed,05/Nov/10 21:12,16/Apr/19 09:33
Bug,CASSANDRA-1718,12479400,cassandra should chdir / when daemonizing,"Common practice when daemonizing is to cd / to avoid pinning a filesystem.  For example, if the oper happens to start Cassandra (by itself, or with a manual jsvc invocation, or with the initscript) in /mnt/usb-storage, and there is something mounted there, then the oper will not be able to unmount the usb device that was mounted at that location, since the cassandra process has it open as its cwd.

evidence that this isn't being done already:

{noformat}
~% sudo lsof -p 9775 | awk '$4==""cwd""'
jsvc    9775 cassandra  cwd    DIR                8,1     4096 147675 /home/paul/packages/cassandra/trunk
{noformat}

(That instance was invoked using the Debian initscript.)

Obviously chdir(""/"") isn't necessary when not daemonizing, although it shouldn't hurt either.

If there are concerns about Cassandra having an ongoing ability to open filenames relative to its original working directory, then it should be sufficient just to do a ""cd /"" in the initscript before starting Cassandra.  That case, at least, is particularly important.",urandom,thepaul,Low,Resolved,Fixed,08/Nov/10 19:40,16/Apr/19 09:33
Bug,CASSANDRA-1720,12479458,DecoratedKey equals() only tests Token,"I'm working on a new Partitioner for Lucandra that lets many keys share the same token.

When I use this partitioner SliceQueryFilter class returns all rows that match key with the same Token.  This isn't correct in my mind. 
Tokens should only be used to route a Key in the ring.  DecoratedKey equals() hashCode() and compare() should consider Token *and* Key

Thoughts?",tjake,tjake,Low,Resolved,Fixed,09/Nov/10 04:23,16/Apr/19 09:33
Bug,CASSANDRA-1722,12479555,Unbounded key range only ever scans first node in ring,"{code:Java}
        List<AbstractBounds> ranges = getRestrictedRanges(new Bounds(leftToken, p.getMinimumToken()));
{code}

when called with empty start key this means we have a Bounds(minToken, minToken), which hits the getRR special case

{code:Java}
        // special case for bounds containing exactly 1 token
        if (queryRange instanceof Bounds && queryRange.left.equals(queryRange.right))
        {
            if (logger.isDebugEnabled())
                logger.debug(""restricted single token match for query "" + queryRange);
            return Collections.singletonList(queryRange);
        }
{code}

Looks like this broke as a side effect of CASSANDRA-1442.  Prior to that a bounds from [T, minToken] was considered ""up to infinity"" by getRR so would span multiple nodes.",stuhood,jbellis,Normal,Resolved,Fixed,10/Nov/10 03:20,16/Apr/19 09:33
Bug,CASSANDRA-1725,12479620,word_count/pig loadfunc don't match the ColumnFamilyInputFormat ByteBuffer signature,"In recent commits, ColumnFamilyInputFormat's signature has changed to use ByteBuffers.  This signature needs to match the word_count example and the pig load func.  There are a few options:
1) The ColumnFamilyInputFormat signature itself didn't need to change - it could translate from byte[] to ByteBuffer internally.
2) Change the word_count and pig load func to use ByteBuffer
3) Just use the AvroColumnFamilyInputFormat.

I think option 1 would be best for now since that's less of a change.",tjake,jeromatron,Normal,Resolved,Fixed,10/Nov/10 18:32,16/Apr/19 09:33
Bug,CASSANDRA-1727,12479656,Read repair IndexOutOfBoundsException,,jbellis,jbellis,Normal,Resolved,Fixed,10/Nov/10 23:22,16/Apr/19 09:33
Bug,CASSANDRA-1729,12479731,Fix misuse of DataOutputBuffer.getData in AntiEntropyService,"As reported by Schubert Zhang, AntiEntropyService is ignoring the length of the input buffer.
{code:java}
byte[] rowhash = FBUtilities.hash(""SHA-256"", row.key.key.getBytes(), row.buffer.getData());
{code}

While this isn't affecting our accuracy, it would break validation if we started reusing buffers in CompactedRow. This issue has already been fixed in 0.7.",stuhood,stuhood,Low,Resolved,Fixed,11/Nov/10 17:29,16/Apr/19 09:33
Bug,CASSANDRA-1730,12479741,Fat clients are never removed,"After a failed bootstrap, these lines repeat infinitely:

 INFO [Timer-0] 2010-11-11 01:58:32,708 Gossiper.java (line 406) FatClient /10.104.73.164 has been silent for 3600000ms, removing from gossip
 INFO [GMFD:1] 2010-11-11 01:59:03,685 Gossiper.java (line 591) Node /10.104.73.164 is now part of the cluster

Changing the IP on the node but using the same token causes a conflict, requiring either a full cluster restart or changing the token.  This is especially easy to run into in practice in a virtual environment such as ec2.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,11/Nov/10 19:17,16/Apr/19 09:33
Bug,CASSANDRA-1731,12479746,"cli ""list"" gives unhelpful error when not authenticated","{code}
[default@unknown] list Userline
Using default limit of 100
null
{code}
",xedin,jbellis,Low,Resolved,Fixed,11/Nov/10 20:32,16/Apr/19 09:33
Bug,CASSANDRA-1732,12479762,nodetool move throws Assertion Error,"Started from a clean slate 3 node cluster. I first started 1 node and bootstrapped the second and third node into the cluster. I created some Keyspaces and inserted some test data, I ended up with this ring:

[agoudarzi@cas-test1 ~]$ nodetool --host localhost ring
Address         Status State   Load            Token                                       
                                       142685436305748685139980028665762955655    
10.50.26.133    Up     Normal  160.51 KB       57614844575514069274136376807820902791      
10.50.26.134    Up     Normal  160.51 KB       100150140440631377207058202736791929223     
10.50.26.132    Up     Normal  165.48 KB       142685436305748685139980028665762955655     

Now I wanted to test manual moving nodes to balanced tokens:
stage1:agoudarzi:~:$ python test.py 3
56713727820156410577229101238628035242
113427455640312821154458202477256070484
170141183460469231731687303715884105727

So I did nodetool move on 10.50.26.132:
[agoudarzi@cas-test1 ~]$ nodetool --host localhost move 56713727820156410577229101238628035242

All went fine. 
[agoudarzi@cas-test1 ~]$ nodetool --host localhost ring
Address         Status State   Load            Token                                       
                                       100150140440631377207058202736791929223    
10.50.26.132    Up     Normal  603.03 KB       56713727820156410577229101238628035242      
10.50.26.133    Up     Normal  15.18 MB        57614844575514069274136376807820902791      
10.50.26.134    Up     Normal  15.19 MB        100150140440631377207058202736791929223     

Now I wanted to move the second node 10.50.26.133:

[agoudarzi@cas-test2 ~]$ nodetool --host localhost move 113427455640312821154458202477256070484
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.service.StorageService.getLocalToken(StorageService.java:1128)
	at org.apache.cassandra.service.StorageService.startLeaving(StorageService.java:1527)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:1666)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:1641)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1449)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1284)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1382)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

 I am attacking the logs for my 3 nodes. For your reference I refer to these IPs as these nodes:

node1: 10.50.26.132
node2: 10.50.26.133
node3: 10.50.26.134

I have seen similar exception being thrown in CASSANDRA-1670.

Please investigate.

-Arya
",tjake,arya,Normal,Resolved,Fixed,12/Nov/10 00:34,16/Apr/19 09:33
Bug,CASSANDRA-1733,12479768,get_range_slices doesn't return key from start_key in KeyRange any more,"The following is a test case which used to work for me before, but after upgrading to trunk it fails:

-Insert 50 or so keys to StandardByUUID1 CF in Keyspace1
-Try using get_range_slices with a KeyRange that has one of those keys you inserted earlier as its start_key with a count of 20

You will get 20 rows, but you won't get the row with specified start_key

Expected Result: you should get 20 rows back with the specified start_key as one of the rows

I am suing Random Partitioner with RF2 and CL1. I understand that Random Partitioner will not give you keys in order, but the behavior before at least returned a random subset including the row with specified start_key.

Please investigate.

-Arya

",tjake,arya,Normal,Resolved,Fixed,12/Nov/10 02:51,16/Apr/19 09:33
Bug,CASSANDRA-1734,12479769,New subcolumn resurrect deleted subcolumns,"The followings are the conversation I had with jbellis on IRC:
have a question on deletion of super column....deletion of SuperColumn works fine....but when I add any new sub-column, the 'old' sub-columns reappear..how can I tell they are tombstoned
the 'old' sub-columns still have all the data in place, including timestamp
and the timestamp is older than markDeletaAt of the super column
<jbellis> appoji: right.  that is expected.  the subcolumns won't be tombstoned, the supercolumn tombstone should supress them.
<jbellis> shouldn't*
<jbellis> appoji: but writing a new subcolumn should resurrect the others.  can you submit a test case?

I am able to reproduce it with cli as followings:

1.  create column family UserGroup with column_type = 'Super' and gc_grace=5 and comparator = 'AsciiType' and subcomparator = 'BytesType'
2. set UserGroup ['100'] ['memberList']['paul']=''
3. del UserGroup ['100'] ['memberList']
4. wait for 5 seconds and ""list UserGroup"" does not return any data, which is correct
5. set UserGroup ['100'] ['memberList']['andrew']=''
6. now ""list UserGroup"" returns 2 sub-columns ('paul' and 'andrew')

here is server log for step 6:

DEBUG 22:40:52,378 range_slice
DEBUG 22:40:52,379 RangeSliceCommand{keyspace='Appoji', column_family='UserGroup
1', super_column=null, predicate=SlicePredicate(slice_range:SliceRange(start:80
01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 28
 0C 00 01 0B 00 03 00 00 00 0A 55 73 65 72 47 72 6F 75 70 31 00 0C 00 02 0C 00 0
2 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 6
5 5F 73 6C 69 63 65 73 00 00 00 28 0C 00 01 0B 00 03 00 00 00 0A 55 73 65 72 47
72 6F 75 70 31 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, r
eversed:false, count:100)), range=[0,0], max_keys=100}
DEBUG 22:40:52,380 restricted single token match for query [0,0]
DEBUG 22:40:52,381 local range slice
DEBUG 22:40:52,382 collecting 0 of 100: SuperColumn(memberList -delete at 128953
3231194000- [616e64726577:false:0@1289533250404000,7061756c:false:0@128953321401
5000,])
DEBUG 22:40:52,383 scanned DecoratedKey(9839004666223652184852086760848439587, 3
13030)

",jbellis,wenjun@appoji.com,Low,Resolved,Fixed,12/Nov/10 03:43,16/Apr/19 09:33
Bug,CASSANDRA-1736,12479813,ConcurrentModificationException when updating column family metadata,"From cli
> update column family Tweet with column_metadata=[{column_name:state, validation_class:UTF8Type}]
> set Tweet [x][state] = TX
> get Tweet where state = TX
No index columns present
> update column family Tweet with column_metadata=[{column_name:state, index_type:0, validation_class:UTF8Type}]
null
> list Tweet
java.net.SocketException: Broken pipe


ERROR [MigrationStage:1] 2010-11-12 09:12:28,618 AbstractCassandraDaemon.java (line 90) Fatal exception in thread Thread[Migra$
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$KeyIterator.next(HashMap.java:828)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1495)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.beforeApplyModels(UpdateColumnFamily.java:76)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:109)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:672)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [pool-1-thread-5] 2010-11-12 09:12:28,636 CustomTThreadPoolServer.java (line 175) Thrift error occurred during processin$
org.apache.thrift.protocol.TProtocolException: Required field 'why' was not present! Struct: InvalidRequestException(why:null)
        at org.apache.cassandra.thrift.InvalidRequestException.validate(InvalidRequestException.java:340)
        at org.apache.cassandra.thrift.InvalidRequestException.write(InvalidRequestException.java:309)
        at org.apache.cassandra.thrift.Cassandra$system_update_column_family_result.write(Cassandra.java:26764)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:3605)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",jbellis,bterm,Normal,Resolved,Fixed,12/Nov/10 17:31,16/Apr/19 09:33
Bug,CASSANDRA-1739,12479839,bootstrapping nodes do not reject range slice requests,ReadVerbHandler rejects the request if the node is bootstrapping. I think RangeSliceVerbHandler should probably do the same.,tilgovi,tilgovi,Normal,Resolved,Fixed,12/Nov/10 23:00,16/Apr/19 09:33
Bug,CASSANDRA-1741,12479850,clear endpoint cache after updating keyspace metadata,"If the replication factor or strategy (or options) change, we need to clear the cache so it can be repopulated with the new options. ",jbellis,jbellis,Low,Resolved,Fixed,13/Nov/10 04:51,16/Apr/19 09:33
Bug,CASSANDRA-1745,12479922,index scan should treat not-present columns as not matching index expressions,"As reported on the mailing list,

{code}
I created a column family and added index on column A,B,C. 

Now I insert three rows. 

row1 : A=123, B=456, C=789 
row2 : A=123, C=789 
row3 : A=123, B=789, C=789 

Now if I perform an indexed query for A=123 and B=456, both row1 and row2 are returned. 
{code}",jbellis,jbellis,Normal,Resolved,Fixed,14/Nov/10 18:49,16/Apr/19 09:33
Bug,CASSANDRA-1747,12479981,truncate is not secondary index-aware,we need to drop the index data files as well as the base CF ones on truncate.,jbellis,jbellis,Normal,Resolved,Fixed,15/Nov/10 17:44,16/Apr/19 09:33
Bug,CASSANDRA-1748,12479991,Flush before repair,"We don't currently flush before beginning a validation compaction, meaning that depending on the state of the memtables, we might end up with content on disk that is as different as a single memtable can make it (potentially, very different).",thobbs,stuhood,Normal,Resolved,Fixed,15/Nov/10 20:04,16/Apr/19 09:33
Bug,CASSANDRA-1749,12479993,Streaming should hold a reference to the source SSTR to prevent GC races,"An SSTable waiting to be streamed will be GC'd and deleted from disk if there are no references being held to its SSTableReader. While streaming an SSTable, we should hold an SSTR reference.",stuhood,stuhood,Urgent,Resolved,Fixed,15/Nov/10 20:20,16/Apr/19 09:33
Bug,CASSANDRA-1752,12480189,repair leaving FDs unclosed,"""We noticed that after a `nodetool repair` was ran, several of our nodes reported high disk usage; -- even one node hit 100% disk usage. After a restart of that node, disk usage drop instantly by 80 gigabytes -- well that was confusing, but we quickly formed the theory that Cassandra must of been holding open references to deleted file descriptors.

""Later, i found this node as an example, it is using about 8-10 gigabytes more than it should be -- 118 gigabytes reported by df, yet du reports only 106 gigabytes in the cassandra directory (nothing else on the mahcine). As you can see from the lsof listing, it is holding open FDs to files that no longer exist on the filesystem, and there are no open streams or as far as I can tell other reasons for the deleted sstable to be open.

""This seems to be related to running a repair, as we haven't seen it in any other situations before.""

A quick check of FileStreamTask shows that the obvious base is covered:
{code}
        finally
        {
            try
            {
                raf.close();
            }
            catch (IOException e)
            {
                throw new AssertionError(e);
            }
        }
{code}

So it seems that either the transfer loop is never finishing to get to that finally block (in which case why isn't it showing up in outbound streams?) or something else is the problem.",thobbs,jbellis,Normal,Resolved,Fixed,17/Nov/10 09:15,16/Apr/19 09:33
Bug,CASSANDRA-1753,12480245,SSTableImport adds columns marked for delete incorrectly in methods addToStandardCF & addToSuperCF,"The logic for adding column families in the methods addToStandardCF & addToSuperCF appears to be backwards

            if (col.isDeleted) {
                cfamily.addColumn(path, hexToBytes(col.value), new TimestampClock(col.timestamp));
            } else {
                cfamily.addTombstone(path, hexToBytes(col.value), new TimestampClock(col.timestamp));
            }

",bryantower,pushpinder.heer,Normal,Resolved,Fixed,17/Nov/10 19:50,16/Apr/19 09:33
Bug,CASSANDRA-1754,12480308,SSTable Import/Export doesn't support ExpiringColumns,,slebresne,slebresne,Low,Resolved,Fixed,18/Nov/10 08:34,16/Apr/19 09:33
Bug,CASSANDRA-1755,12480376,handle replica unavailability in index scan,StorageProxy.scan should return UnavailableException to clients under this condition,tjake,jbellis,Low,Resolved,Fixed,18/Nov/10 21:46,16/Apr/19 09:33
Bug,CASSANDRA-1756,12480378,DatabaseDescriptor static initialization circular reference when initialized through call to StorageService.instance.initClient ,"In trunk, attempting to invoke StorageService.instance.initClient results in an NPE due to static definition field ordering in StorageService and a circular reference from DatabaseDescriptor back into an uninitialized field (scheduledTasks). Changing the ordering of the static fields such that scheduledTasks is defined before the static partitioner fixes the issue.

I've also marked the scheduledTasks executor as final as it doesn't seem to make sense changing it.

All tests pass with this change locally.

I suspect this hasn't surfaced in tests as calling initServer first in the same JVM will allow later calls to initClient to see the correctly defined scheduledTasks fields.

I'm following the recommended way to do this from ClientOnlyExample, if this isn't the right way to initialize things let me know.
",gdusbabek,eonnen,Low,Resolved,Fixed,18/Nov/10 21:58,16/Apr/19 09:33
Bug,CASSANDRA-1759,12480444,Column MetaData: Index_name should not be allowed if index_type is not set.,"Giving an indexName starts the automatic index creation process.
If indexType is not also set, then that process barfs.
If a name is present, a type must be also (but the reverse is not necessarily true).",jhermes,jhermes,Low,Resolved,Fixed,19/Nov/10 20:24,16/Apr/19 09:33
Bug,CASSANDRA-1760,12480451,JNA Native check can throw a NoSuchMethodError,"Looks like older versions of JNA have a different Native.register() method

From IRC:
hi, i'm having trouble starting cassandra up...the error is very bizarre: java.lang.NoSuchMethodError: com.sun.jna.Native.register(Ljava/lang/String;)V",tjake,tjake,Low,Resolved,Fixed,19/Nov/10 21:33,16/Apr/19 09:33
Bug,CASSANDRA-1761,12480462,Indexes: Auto-generating the CFname may collide with user-generated names,"{noformat}column_families:
  - name: CF
    comparator: BytesType
    column_metadata: 
      - name: foo
        index_name: 626172
        index_type: KEYS
      - name: bar
        index_type: KEYS{noformat}

Auto-generated versus user-supplied names collide in the YAML above. The code:
{code}cfname = parentCf + ""."" + (info.getIndexName() == null ? FBUtilities.bytesToHex(info.name) : info.getIndexName()){code}

From the first ColumnDefinition, we create cfname = ""CF.626172"" (from the fail clause of the ternany, user-supplied name)
From the second ColumnDefinition, we create cfname = ""CF.626172"" (from the pass clause of the ternary, we generate the name)

They're in hex form. This is possible, but fairly unlikely that someone will do this.",jhermes,jhermes,Low,Resolved,Fixed,19/Nov/10 22:51,16/Apr/19 09:33
Bug,CASSANDRA-1762,12480464,UpdateKeyspace does not modify strategy or strategy_options (until restart),,jbellis,jbellis,Normal,Resolved,Fixed,19/Nov/10 22:54,16/Apr/19 09:33
Bug,CASSANDRA-1764,12480549,NPE on system_update_cf when adding an index to a column without existing metadata,"When trying to create a secondary index using system_update_column_family(), if you try to add an index on a column that does not already have an existing entry in the CfDef's column_metadata, a NullPointerException is thrown.

Looks like the logic in o.a.c.config.CFMetaData.apply() is faulty.  Specifically, creating a toUpdate Set (similar to the toAdd and toDelete) sets and using that for the loop ~ line 663 would fix this.",gdusbabek,thobbs,Normal,Resolved,Fixed,22/Nov/10 01:34,16/Apr/19 09:33
Bug,CASSANDRA-1766,12480607,Streaming never makes progress,"I have a client that can never complete a bootstrap.  AC finishes, streaming begins.  Stream initiate completes, and the sources wait on the transfer to finish, but progress is never made on any stream.  Nodetool reports streaming is happening, the socket is held open, but nothing happens.",jbellis,brandon.williams,Normal,Resolved,Fixed,22/Nov/10 20:25,16/Apr/19 09:33
Bug,CASSANDRA-1768,12480629,UnsupportedOperationException in system_update_column_family,"During testing I hit this section of code:

CFMetaData.java:662
{code}
 // remove the ones leaving.
       for (ByteBuffer indexName : toRemove)
           column_metadata.remove(indexName);
{code}

but column_metadata is defined as:

{code}
       this.column_metadata = Collections.unmodifiableMap(column_metadata);
{code}

So remove() will throw an exception.

{code}
java.lang.UnsupportedOperationException
        at java.util.Collections$UnmodifiableMap.remove(Collections.java:1288)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:662)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.<init>(UpdateColumnFamily.java:56)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:863)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:3592)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}

This was introduced by CASSANDRA-1715",gdusbabek,tjake,Normal,Resolved,Fixed,23/Nov/10 02:14,16/Apr/19 09:33
Bug,CASSANDRA-1770,12480726,cassandra-cli help output should mention the need for semicolons,"When running cassandra-cli for the first time after upgrading beta3 -> rc1, I thought it was broken, because any command I typed (""?"", ""quit"", ""exit"", ""use system"", etc) just printed some empty space and sat there until I killed it. I didn't know it was a continuation prompt, or that semicolons were needed.

The startup banner message currently says:

{noformat}
Connected to: ""Test Cluster"" on localhost/9160
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
{noformat}

I believe that the example commands should have semicolons after them, to let the user know about the change.

Also, a ""?"" by itself should probably not require a semicolon.",xedin,thepaul,Low,Resolved,Fixed,23/Nov/10 20:15,16/Apr/19 09:33
Bug,CASSANDRA-1771,12480756,Script in debian/cassandra.in.sh is significantly out of date,Looks like the correct version is in TRUNK given the 0.7 environment property changes.,urandom,zznate,Low,Resolved,Fixed,24/Nov/10 00:06,16/Apr/19 09:33
Bug,CASSANDRA-1772,12480761,"debian initscript sometimes mistakenly thinks it failed, gives extraneous output","On my test systems, which are all relatively slow VMs, the Cassandra debian initscript usually thinks it fails to start, even though the startup was successful.  It appears that jsvc forks the daemon process and exits, and the initscript check for the running Cassandra service occurs before the new daemon is able to initialize itself and create its pidfile.

On top of that, most invocations end up spitting out a small amount of garbage from /bin/ps, in addition to the typical ""Stopping Cassandra: cassandra."" log messages one sees if verbose=yes in /etc/default/rcS.  This is not very flattering.

Finally, the initscript should provide the ""status"" command to meet current LSB spec. The functionality is mostly complete already anyway, and it can be quite useful.",thepaul,thepaul,Low,Resolved,Fixed,24/Nov/10 01:15,16/Apr/19 09:33
Bug,CASSANDRA-1774,12480793,ColumnFamilyOutputFormat only writes one column (per key),"From mailing list http://thread.gmane.org/gmane.comp.db.cassandra.user/10385

ColumnFamilyOutputFormat will only write out one column
per key.

Alex Burkoff also reported this nearly two months ago, but nobody ever
replied...
 http://article.gmane.org/gmane.comp.db.cassandra.user/9325

has anyone any ideas? 
should it be possible to write multiple columns out?

This is very easy to reproduce. Use the contrib/wordcount example, with
OUTPUT_REDUCER=cassandra and in WordCount.java add at line 132

>              results.add(getMutation(key, sum));
> +            results.add(getMutation(new Text(""doubled""), sum*2));

Only the last mutation for any key seems to be written.",mck,mck,Normal,Resolved,Fixed,24/Nov/10 12:17,16/Apr/19 09:33
Bug,CASSANDRA-1777,12480888,The describe_host API method is misleading in that it returns the interface associated with gossip traffic,"If the hardware is configured to use separate interfaces for thrift and gossip, the gossip interface will be returned, given the results come out of the ReplicationStrategy eventually.

I understand the approach, but given this is on the API, it effective worthless in situations of host auto discovery via describe_ring from a client. I actually see this as the primary use case of this method - why else would I care about the gossip iface from the client perspective? It's current form should be relegated to JMX only. 

At the same time, we should add port information as well. 

describe_splits probably has similar issues.

I see the potential cart-before-horse issues here and that this will probably be non-trivial to fix, but I think ""give me a set of all the hosts to which I can talk"" is pretty important from a client perspective.",brandon.williams,zznate,Normal,Resolved,Fixed,25/Nov/10 05:06,16/Apr/19 09:33
Bug,CASSANDRA-1781,12480961,Internal error processing get_range_slices,"Runnig mapreduce task on two or more Cassandra nodes gives following error:

DEBUG 16:51:48,653 range_slice
DEBUG 16:51:48,653 RangeSliceCommand{keyspace='TEST', column_family='Url', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=57 lim=67 cap=177]]), range=(162950022446285318630909295651345252065,9481098247439719900692337295923514899], max_keys=4096}
DEBUG 16:51:48,653 restricted ranges for query (162950022446285318630909295651345252065,9481098247439719900692337295923514899] are [(162950022446285318630909295651345252065,9481098247439719900692337295923514899]]
DEBUG 16:51:48,653 local range slice
ERROR 16:51:48,653 Internal error processing get_range_slices
java.lang.AssertionError: (162950022446285318630909295651345252065,9481098247439719900692337295923514899]
at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1264)
at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:429)
at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:514)
at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.process(Cassandra.java:2868)
at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
DEBUG 16:51:48,838 logged out: #<User allow_all groups=[]>

You can reproduce this by just running contrib/word_count example. Mapreduce last worked with Cassandra 0.7-beta2. Important is to run more than one node.",stuhood,patrik.modesto,Normal,Resolved,Fixed,26/Nov/10 09:51,16/Apr/19 09:33
Bug,CASSANDRA-1782,12481001,Example session in README is missing semicolons,The example session in the README is missing the semicolons at the end of each command.,justinazoff,justinazoff,Low,Resolved,Fixed,26/Nov/10 23:01,16/Apr/19 09:33
Bug,CASSANDRA-1783,12491450,spurious failures of o.a.c.db.NameSortTest:testNameSort100,"{noformat}
    [junit] Cobertura: Loaded information on 961 classes.
    [junit] Cobertura: Saved information on 961 classes.
    [junit] Testsuite: org.apache.cassandra.db.NameSortTest
    [junit] Testsuite: org.apache.cassandra.db.NameSortTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.NameSortTest:testNameSort100:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit]
{noformat}

See also: https://hudson.apache.org/hudson/job/Cassandra-0.7/33/console",jbellis,urandom,Low,Resolved,Fixed,27/Nov/10 16:09,16/Apr/19 09:33
Bug,CASSANDRA-1785,12491506,"[patch] use long math, if long values are expected.","code does math assuming the result is a long, but uses integer math. Might as well use long math to avoid possible truncation.",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,28/Nov/10 20:34,16/Apr/19 09:33
Bug,CASSANDRA-1786,12491508,[patch] use cross platform new lines in printf calls,"code uses printf(""\n"") for new lines, should use ""%n"" instead.",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,28/Nov/10 20:51,16/Apr/19 09:33
Bug,CASSANDRA-1790,12491618,Booting fails on Windows 7/Windows 2003 because of a file rename failure,"Cassandra 0.7.0 rc will not boot on Windows 7 and Windows 2003 because of a file rename failure. The logging:

{noformat}
 INFO [FlushWriter:1] 2010-11-25 17:12:43,796 Memtable.java (line 155) Writing Memtable-LocationInfo@691789110(435 bytes, 8 operations)
ERROR [FlushWriter:1] 2010-11-25 17:12:43,993 AbstractCassandraDaemon.java (line 90) Fatal exception in thread Thread[FlushWriter:1,5,main]
java.io.IOError: java.io.IOException: rename failed of d:\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:214)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:184)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:167)
	at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:161)
	at org.apache.cassandra.db.Memtable.access$000(Memtable.java:49)
	at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:174)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: rename failed of d:\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.utils.FBUtilities.renameWithConfirm(FBUtilities.java:359)
	at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:210)
	... 12 more
 INFO [main] 2010-11-29 09:16:36,219 AbstractCassandraDaemon.java (line 73) Heap size: 1067253760/1067253760
{noformat}
",jbellis,rockxwre,Urgent,Resolved,Fixed,30/Nov/10 08:16,16/Apr/19 09:33
Bug,CASSANDRA-1801,12491821,bytebuffers of column data written locally prevent GC of original thrift mutation byte[],It appears C* isn't releasing buffers that were used during the writes of batch mutates.  See attached screenshot of heap dump.,tjake,mdennis,Normal,Resolved,Fixed,01/Dec/10 22:03,16/Apr/19 09:33
Bug,CASSANDRA-1804,12491833,Quorum calculation code needs to use polymorphic Strategy method for determining replica counts,"Keyspace.replication_factor is only valid for SimpleStrategy and ONTS, we shouldn't use it directly anywhere.

Related: CASSANDRA-1263 would remove the temptation to do the Wrong Thing.",jbellis,jbellis,Normal,Resolved,Fixed,01/Dec/10 23:01,16/Apr/19 09:33
Bug,CASSANDRA-1809,12491943,cli executeGetWithConditions is not case-insensitive for CF names,"{code}
        String columnFamily = statement.getChild(0).getText();
{code}

is not being normalized for case...

I tried
{code}
-        String columnFamily = statement.getChild(0).getText();
+        String columnFamily = CliCompiler.getColumnFamily(statement.getChild(0), keyspacesMap.get(keySpace).cf_defs);
{code}

but that broke it, all gets returned null (missing exception message?)",xedin,jbellis,Low,Resolved,Fixed,02/Dec/10 23:57,16/Apr/19 09:33
Bug,CASSANDRA-1810,12491944,cli support index_type enum names in column_metadata,currently you have to write index_type: 0 when index_type: KEYS would be better,xedin,jbellis,Low,Resolved,Fixed,02/Dec/10 23:58,16/Apr/19 09:33
Bug,CASSANDRA-1813,12491990,return invalidrequest when client attempts to create secondary index on supercolumns,,jbellis,jbellis,Low,Resolved,Fixed,03/Dec/10 14:52,16/Apr/19 09:33
Bug,CASSANDRA-1816,12492012,cassandra deb should depend on adduser,"The cassandra debian package uses the addgroup and adduser commands in its postinst script, which are found in the 'adduser' package, but the cassandra debian package does not depend on adduser.   When a user does not already have adduser installed, this will lead to an error like:

{noformat}
Setting up cassandra (0.7.0~rc1) ...
/var/lib/dpkg/info/cassandra.postinst: 50: addgroup: not found
dpkg: error processing cassandra (--configure):
 subprocess installed post-installation script returned error exit status 127
{noformat}

Yes, this won't happen much, because systems without adduser installed are rare.  But adduser is not ""Essential: yes"", so it will happen sometimes in bare-bones VMs or development environments.",thepaul,thepaul,Low,Resolved,Fixed,03/Dec/10 18:04,16/Apr/19 09:33
Bug,CASSANDRA-1824,12492268,Schema only fully propagates from seeds,"If you have nodes X, Y, and Z, and Y already has some schema, but X and Z do not, and X is the seed node for the cluster, X will pick up the schema from Y, but it will never propagate to Z.  If X has the schema, it will propagate to both Y and Z.",gdusbabek,brandon.williams,Normal,Resolved,Fixed,06/Dec/10 18:10,16/Apr/19 09:33
Bug,CASSANDRA-1826,12492539,system_create_cf() makes a SuperCF when column_type is Standard and subcolumn_comparator_type is present,"If you create a CF with system_create_column_family() and the CfDef has column_type = 'Standard' and subcomparator_type is present, it creates a SuperCF.  I would expect an InvalidRequestException, instead.",,thobbs,Normal,Resolved,Fixed,06/Dec/10 20:39,16/Apr/19 09:33
Bug,CASSANDRA-1829,12492542,Nodetool move is broken,The code from finishBootstrapping that finishes a move was removed. This means a move will leave a node stuck in a bootstrapping state forever.,nickmbailey,nickmbailey,Urgent,Resolved,Fixed,06/Dec/10 21:49,16/Apr/19 09:33
Bug,CASSANDRA-1830,12492543,ReadResponseResolver might miss an inconsistency,"Rather than comparing the digests of all the digest requests to one another, the last one seen ""wins"" and is compared to the digest of each version seen from a data request.",tilgovi,jbellis,Low,Resolved,Fixed,06/Dec/10 21:52,16/Apr/19 09:33
Bug,CASSANDRA-1832,12492636,mx4j does not load,"Adding mx4j-tools.jar (latest) to the library causes following exception:

{code}
 WARN 20:22:25,123 Could not start register mbean in JMX
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.cassandra.utils.Mx4jTool.maybeLoad(Mx4jTool.java:67)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:169)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
Caused by: java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:365)
	at java.net.ServerSocket.bind(ServerSocket.java:319)
	at java.net.ServerSocket.<init>(ServerSocket.java:185)
	at mx4j.tools.adaptor.PlainAdaptorServerSocketFactory.createServerSocket(PlainAdaptorServerSocketFactory.java:24)
	at mx4j.tools.adaptor.http.HttpAdaptor.createServerSocket(HttpAdaptor.java:672)
	at mx4j.tools.adaptor.http.HttpAdaptor.start(HttpAdaptor.java:478)
	... 9 more
{code}",rantav,rstml,Low,Resolved,Fixed,07/Dec/10 16:28,16/Apr/19 09:33
Bug,CASSANDRA-1833,12492646,"clustertool get_endpoints needs key as String, not ByteBuffer",java RMI does not serialize ByteBuffer; revert clustertool to use String for key,kelvin,kelvin,Low,Resolved,Fixed,07/Dec/10 18:12,16/Apr/19 09:33
Bug,CASSANDRA-1834,12492681,"hudson test failure: ""Forked Java VM exited abnormally.""","https://hudson.apache.org/hudson/view/A-F/view/Cassandra/job/Cassandra-0.7/56/

{noformat}
    [junit] Testsuite: org.apache.cassandra.service.EmbeddedCassandraServiceTest
    [junit] Testsuite: org.apache.cassandra.service.EmbeddedCassandraServiceTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.service.EmbeddedCassandraServiceTest:BeforeFirstTest:	Caused an ERROR
    [junit] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
    [junit] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.service.EmbeddedCassandraServiceTest FAILED (crashed)
{noformat}",jbellis,urandom,Normal,Resolved,Fixed,07/Dec/10 23:04,16/Apr/19 09:33
Bug,CASSANDRA-1835,12492695,"""null"" error creating CF from cli","This fails with only ""null"" as the failure message:

{code}
create column family test1 with column_type = 'Super' and comparator = 'LongType' and column_metadata=[{column_name:a,validation_class:LongType}];
{code}",xedin,jbellis,Low,Resolved,Fixed,08/Dec/10 01:37,16/Apr/19 09:33
Bug,CASSANDRA-1837,12492744,Deleted columns are resurrected after a flush,"Easily reproduced with the cli:

{noformat}
[default@unknown] create keyspace testks;
2785d67c-02df-11e0-ac09-e700f669bcfc
[default@unknown] use testks;
Authenticated to keyspace: testks
[default@testks] create column family testcf;
2fbad20d-02df-11e0-ac09-e700f669bcfc
[default@testks] set testcf['test']['foo'] = 'foo';
Value inserted.
[default@testks] set testcf['test']['bar'] = 'bar';
Value inserted.
[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test
=> (column=626172, value=626172, timestamp=1291821869120000)
=> (column=666f6f, value=666f6f, timestamp=1291821857320000)

1 Row Returned.
[default@testks] del testcf['test'];
row removed.
[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test

1 Row Returned.
{noformat}

Now flush testks and look again:

{noformat}

[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test
=> (column=626172, value=626172, timestamp=1291821869120000)
=> (column=666f6f, value=666f6f, timestamp=1291821857320000)

1 Row Returned.
{noformat}",gdusbabek,brandon.williams,Urgent,Resolved,Fixed,08/Dec/10 15:25,16/Apr/19 09:33
Bug,CASSANDRA-1841,12492953,cassandra-cli formatted help width,"Most of cassandra-cli's help output justifies to 81 chars, just enough to cause line wrap on most default sized terminals.  It would improve appearance here if one character of the separating white space was removed so that it justified at 80 chars.",urandom,urandom,Low,Resolved,Fixed,10/Dec/10 16:04,16/Apr/19 09:33
Bug,CASSANDRA-1842,12492961,ColumnFamilyOutputFormat only writes the first column,"In CASSANDRA-1774 we fixed a problem where only the last column was being written.  However, it appears that we only write the first one now.  This is easy to observe in the word count example:

{noformat}
RowKey: text2
=> (column=word1, value=1, timestamp=1291666461685000)
{noformat}

is what the output for text2 looks like, but there should be another column, word2.  If the word count is run without CFOF it works correctly.",jeromatron,brandon.williams,Normal,Resolved,Fixed,10/Dec/10 17:24,16/Apr/19 09:33
Bug,CASSANDRA-1843,12492991,Indexes: CF MBeans for automatic indexes are never unregistered when they are deleted.,"Add, delete, and add the same index and you should get a stacktrace to this effect:
{noformat}
java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=IndexColumnFamilies,keyspace=Keyspace1,columnfamily=Standard1.616765
  at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:259)
  at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:447)
  at org.apache.cassandra.db.ColumnFamilyStore.addIndex(ColumnFamilyStore.java:304)
  at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:193)
  at org.apache.cassandra.db.migration.UpdateColumnFamily.applyModels(UpdateColumnFamily.java:80)
  at org.apache.cassandra.db.migration.Migration.apply(Migration.java:171)
  at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:663)
  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
  at java.util.concurrent.FutureTask.run(FutureTask.java:138)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:662)
Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=IndexColumnFamilies,keyspace=Keyspace1,columnfamily=Standard1.616765
  at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:453)
  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1484)
  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:963)
  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)
  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
  at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
  at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:255)
  ... 11 more{noformat}
CFS.reload() manages index deletion, but never unregisters the MBeans it creates during initialization.",jhermes,jhermes,Low,Resolved,Fixed,10/Dec/10 23:33,16/Apr/19 09:33
Bug,CASSANDRA-1847,12493038,ByteBufferUtil.clone shouldn't mutate the passed bytebuffer,"Currently ByteBufferUtil.clone uses .mark and .reset on the passed ByteBuffer.

This is fine when using thrift because no two ByteBuffer are being accessed at the same time, but this could be thread-unsafe if the same BB was passed concurrently.

Solandra is having problems with this (Solandra shares JVM with Cassandra).",tjake,tjake,Low,Resolved,Fixed,13/Dec/10 01:57,16/Apr/19 09:33
Bug,CASSANDRA-1850,12493071,When the ANTLR code generation fails the build continues and ignores the failure,"When trying to tweak the build, I broke the ANTLR code generation tasks, but as I was not doing a full clean build the generated code was still present, so I missed the fact that the code generation was broken. It would be nice if the antlr tasks had failonerror=""true""",stephenc,stephenc,Low,Resolved,Fixed,13/Dec/10 13:49,16/Apr/19 09:33
Bug,CASSANDRA-1852,12493087,When restarting Cassandra I get this error: java.io.IOError: java.io.IOException: Failed to delete  ..data\system\LocationInfo-e-1-Data.db,"java.io.IOError: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:483)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:102)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at com.remainsoftware.gravity.cassandra.internal.GCassandra$1.run(GCassandra.java:21)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:479)
	... 7 more
ERROR 16:54:52,984 Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:483)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:102)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at com.remainsoftware.gravity.cassandra.internal.GCassandra$1.run(GCassandra.java:21)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:479)
	... 7 more",,aadel,Normal,Resolved,Fixed,13/Dec/10 16:06,16/Apr/19 09:33
Bug,CASSANDRA-1853,12493092,"changing row cache save interval is reflected in 'describe keyspace' on node it was submitted to, but not nodes it was propagated to","This is minor unless it indicates a bigger issue. On our test cluster (running cassandra 0.7 branch from today) we noticed that on submission of a new row cache save period, such as:

   update column family KeyValue with row_cache_save_period=3600;

The change would be reflected in describe_keyspace() (describe keyspace ... in cassandra-cli) on the node to which the schema migration was submitted, but not on other nodes in the cluster.

This in spite of the schema migration having propagated, judging by Schema['Last Migration'] being identical on all nodes. It is not n and of itself is not a big problem, but it does give the impression that the migrations have trouble propagating throughout the cluster even though they do.

(I had a quick (only) look in the code paths of migration application and did not find any obvious special casing of the node that happens to be local. Filing bug instead, hoping someone knows off hand what the reason is.)


",gdusbabek,scode,Low,Resolved,Fixed,13/Dec/10 17:19,16/Apr/19 09:33
Bug,CASSANDRA-1862,12493250,StorageProxy throws an InvalidRequestException in readProtocol during bootstrap,"Though the error message provides details, IRE is supposed to signify poorly formed API requests. In the context of a client request, an UnavailableException is more appropriate. This would allow the client to take action like removing the node from its host list. ",zznate,zznate,Low,Resolved,Fixed,14/Dec/10 22:40,16/Apr/19 09:33
Bug,CASSANDRA-1866,12493352,Internal error from malformed remove with supercolumns,"From the ML: 

""I just call ""remove"" method where ColumnPath structure has ""column_family"" and ""column"" members set (""super_column"" not set).""
{code}
ERROR 17:57:46,924 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:318)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:298)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:82)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:68)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:344)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:355)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:269)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:194)
        at org.apache.cassandra.service.StorageProxy$1.runMayThrow(StorageProxy.java:197)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 17:57:46,928 Fatal exception in thread Thread[MUTATION_STAGE:2,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:318)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:298)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:82)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:68)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:344)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:355)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:269)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:194)
        at org.apache.cassandra.service.StorageProxy$1.runMayThrow(StorageProxy.java:197)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{code}

Fix: add system test to catch this & raise invalidrequestexception from ThriftValidation",thobbs,jbellis,Low,Resolved,Fixed,15/Dec/10 17:56,16/Apr/19 09:33
Bug,CASSANDRA-1873,12493477,Read Repair behavior thwarts DynamicEndpointSnitch at CL.ONE,"When doing a CL.ONE read, the coordinator node selects the data node from the list of replicas via snitch sortByProximity.  The data node (_not_ the coordinator) then sends digest requests to the remaining replicas, and compares their answers to its own (in ConsistencyChecker).

This means that, in a multi-datacenter situation, for any given range R with replicas X in dc1 and Y in dc2, the only node with latency information for Y will be X.  Since DES falls back to subsnitch (static) order when latency information is missing for any replica it is asked to sort, DES will be unable to direct requests to Y no matter how overwhelmed X becomes.

To fix this, we should move the digest-checking code into the coordinator node (probably starting with the 0.7 ConsistencyChecker, which represents a cleanup of the 0.6 one).",jbellis,jbellis,Normal,Resolved,Fixed,17/Dec/10 00:23,16/Apr/19 09:33
Bug,CASSANDRA-1875,12493522,backgrounding cli makes it crash,"{code}
$ bin/cassandra-cli 
Welcome to cassandra CLI.

Type 'help;' or '?' for help. Type 'quit;' or 'exit;' to quit.
[default@unknown] 
[1]+  Stopped                 bin/cassandra-cli
$ fg
bin/cassandra-cli
Exception in thread ""main"" java.io.IOException: Interrupted system call
	at java.io.FileInputStream.read(Native Method)
	at jline.Terminal.readCharacter(Terminal.java:99)
	at jline.UnixTerminal.readVirtualKey(UnixTerminal.java:128)
	at jline.ConsoleReader.readVirtualKey(ConsoleReader.java:1453)
	at jline.ConsoleReader.readBinding(ConsoleReader.java:654)
	at jline.ConsoleReader.readLine(ConsoleReader.java:494)
	at jline.ConsoleReader.readLine(ConsoleReader.java:448)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:328)
{code}
",xedin,jbellis,Low,Resolved,Fixed,17/Dec/10 15:32,16/Apr/19 09:33
Bug,CASSANDRA-1883,12493608,NPE in get_slice quorum read,"Getting this NPE as of the 2010-12-17 0.7 trunk.  Some data may be corrupt somewhere on a node.  It could be a null key somewhere.

ERROR [pool-1-thread-28] 2010-12-18 12:53:20,411 Cassandra.java (line 2707) Internal error processing get_slice
java.lang.NullPointerException
        at org.apache.cassandra.service.DigestMismatchException.<init>(DigestMismatchException.java:30)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:92)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:43)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:91)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:362)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:229)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:225)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:301)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:263)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:2699)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)",jbellis,kmueller,Low,Resolved,Fixed,18/Dec/10 21:16,16/Apr/19 09:33
Bug,CASSANDRA-1892,12493807,heisenbug in SSTableExportTest,"{code}
    [junit] java.lang.IndexOutOfBoundsException: Index: 4, Size: 4
    [junit] 	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
    [junit] 	at java.util.ArrayList.get(ArrayList.java:322)
    [junit] 	at org.apache.cassandra.tools.SSTableExportTest.testExportSimpleCf(SSTableExportTest.java:130)
{code}",tjake,jbellis,Low,Resolved,Fixed,21/Dec/10 21:21,16/Apr/19 09:33
Bug,CASSANDRA-1893,12493818,Timed out reads not counted in metrics,Reads that experience timeouts (or other exceptions) are not tracked in StorageProxy latencies.,stuhood,stuhood,Low,Resolved,Fixed,21/Dec/10 23:07,16/Apr/19 09:33
Bug,CASSANDRA-1895,12493949,Loadbalance during gossip issues leaves cluster in bad state,Running loadbalance against a node in a 4 node cluster leaves gossip in a wonky state.,brandon.williams,stuhood,Low,Resolved,Fixed,23/Dec/10 07:31,16/Apr/19 09:33
Bug,CASSANDRA-1899,12494000,Failed to get columns from super column in cassandra-cli (0.7-rc2),"I'm using cassandra 0.7.0-rc2. 

When I tried to get column contents in a super column of Super CF like below;
{code}
get myCF['key']['scName'];
{code}

the client reply: supercolumn parameter is not optional for super CF user
",xedin,xedin,Low,Resolved,Fixed,23/Dec/10 18:48,16/Apr/19 09:33
Bug,CASSANDRA-1901,12494015,getRestrictedRanges bug where node owns minimum token,"From the ML, there are two RF=1 nodes, 0 for the local node (17.224.36.17) and 85070591730234615865843651857942052864 for the remote node (17.224.109.80).  Debug log shows

{code}
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,958 CassandraServer.java (line 479) range_slice
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,958 StorageProxy.java (line 412) RangeSliceCommand{keyspace='Harvest', column_family='TestCentroids', super_column=null, predicate=SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 0C 0C 00 01 0B 00 03 00 00 00 0D 54 65 73 74 43 65 6E 74 72 6F 69 64 73 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 0C 0C 00 01 0B 00 03 00 00 00 0D 54 65 73 74 43 65 6E 74 72 6F 69 64 73 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1)), range=[0,0], max_keys=11}
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,958 StorageProxy.java (line 597) restricted ranges for query [0,0] are [[0,0]]
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,959 StorageProxy.java (line 423) === endpoint: belize1.apple.com/17.224.36.17 for range.right 0
{code}

Thus, node 85070591730234615865843651857942052864 is left out.",stuhood,jbellis,Normal,Resolved,Fixed,23/Dec/10 23:44,16/Apr/19 09:33
Bug,CASSANDRA-1903,12494050,NullPointerException from o.a.c.db.ReplicateOnWriteTask,"I'm seeing a whole lot of these when writing to a node.
 
{noformat}
java.lang.NullPointerException
	at org.apache.cassandra.db.ReplicateOnWriteTask.run(ReplicateOnWriteTask.java:97)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{noformat}

I don't think it will be difficult to reproduce, but the script I'm using is attached.

I bisected the source tree and and http://svn.apache.org/viewvc?view=rev&revision=1052356 seems to the culprit (a merge from 0.7).  Maybe CASSANDRA-1530?",kelvin,urandom,Normal,Resolved,Fixed,25/Dec/10 00:27,16/Apr/19 09:33
Bug,CASSANDRA-1904,12494118,Crash during startup: SSTable doesn't handle corrupt (empty) tmp files,"Applies to 0.7rc3 as well, but not yet selectable in Jira.

cassandra stumbles upons empty Data files and crashes during startup rather than ignoring these files:

java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:233)
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:284)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:200)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:225)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.addIndex(ColumnFamilyStore.java:305)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:246)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:436)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
Exception encountered during startup.
java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:233)
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:284)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:200)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:225)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.addIndex(ColumnFamilyStore.java:305)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:246)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:436)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)

The empty Data/Index tmp files were in my case created and left over when I attempted to create a secondary index at runtime which crashed the JVM due to OOM.

SSTable handles IOExceptions so it should be an easy fix: in SSTable.estimateRowsFromIndex() just check for ifile.length() ==ifile.getFilePointer()==keys==0 and throw an IOException.",gdusbabek,tcn,Normal,Resolved,Fixed,27/Dec/10 11:01,16/Apr/19 09:33
Bug,CASSANDRA-1905,12494126,count timeouts towards dynamicsnitch latencies,receiveTiming is only called by ResponseVerbHandler; we need to add timing information for timed-out requests as well.,jbellis,jbellis,Normal,Resolved,Fixed,27/Dec/10 15:54,16/Apr/19 09:33
Bug,CASSANDRA-1907,12494138,AbstractCassandraDaemon blows up when log4j config is specified using a physical file.,,gdusbabek,gdusbabek,Low,Resolved,Fixed,27/Dec/10 20:56,16/Apr/19 09:33
Bug,CASSANDRA-1910,12494185,validation of time uuid is incorrect,"It appears _TimeUUIDType_ (as of 12/9) is checking the wrong bits when validating a time UUID as version 1.

Per the comment and rfc4122, ""_version is bits 4-7 of byte 6_"", however validate() is actually checking the least significant bits:

> _if ((slice.get() & 0x0f) != 1)_

Sample java/hector code:

{code}
// displays ""version 1"" but validation fails
java.util.UUID uuid1 = java.util.UUID.fromString(""00000000-0000-1000-0000-000000000000"");
System.out.println(uuid1 + "" "" + uuid1.version());
TimeUUIDType.instance.validate(UUIDSerializer.get().toByteBuffer(uuid1));

// displays ""version 2"" but validation succeeds
java.util.UUID uuid2 = java.util.UUID.fromString(""00000000-0000-2100-0000-000000000000"");
System.out.println(uuid2 + "" "" + uuid2.version());
TimeUUIDType.instance.validate(UUIDSerializer.get().toByteBuffer(uuid2));
{code}

The issue can be seen with any UUID where the timestamp doesn't start with 1:

b54adc00-67f9-10d9-9669-0800200c9a66, (timestamp year 1776) version 1 fails
b54adc00-67f9-12d9-9669-0800200c9a66, (timestamp year 2233) version 1 fails
",gdusbabek,dave111,Low,Resolved,Fixed,28/Dec/10 18:18,16/Apr/19 09:33
Bug,CASSANDRA-1915,12494256,stress.java doesn't actually read its data,"stress.java doesn't actually read back the keys it inserts, but also reports no errors.  This is evident on larger (1M) runs where the read request rate is equal to what the bloom filter can do.  Stress.py also cannot find the rows that stress.java inserts.",xedin,brandon.williams,Normal,Resolved,Fixed,29/Dec/10 16:58,16/Apr/19 09:33
Bug,CASSANDRA-1916,12494258,Cleanup needs to remove secondary index entries,,jbellis,jbellis,Normal,Resolved,Fixed,29/Dec/10 17:39,16/Apr/19 09:33
Bug,CASSANDRA-1917,12494259,Insert performance regression,"We caused a performance regression in CASSANDRA-1780, costing us about 33% on inserts.",jbellis,brandon.williams,Urgent,Resolved,Fixed,29/Dec/10 17:52,16/Apr/19 09:33
Bug,CASSANDRA-1922,12494314,exceptions after cleanup,"It looks like CASSANDRA-1916 may have introduced a regression.  After running a cleanup, I get the following exception when trying to read:

{noformat}

ERROR 17:25:23,574 Fatal exception in thread Thread[ReadStage:99,5,main]
java.lang.AssertionError: skipping negative bytes is illegal: -1393754107
        at org.apache.cassandra.io.util.MappedFileDataInput.skipBytes(MappedFileDataInput.java:96)
        at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:50)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:56)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:91)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:67)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:68)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1215)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1107)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1077)
        at org.apache.cassandra.db.Table.getRow(Table.java:384)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:68)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",jbellis,brandon.williams,Normal,Resolved,Fixed,30/Dec/10 17:29,16/Apr/19 09:33
Bug,CASSANDRA-1924,12494336,Broken keyspace strategy_option with zero replicas,"When a keyspace is defined that has strategy options specifying zero replicas should be place in a datacenter (e.g. DCX:0), an assert is violated for any insert and LOCAL_QUORUM reads fail.  I'm not sure if the issue is that there are no nodes in DCX or that I'm saying DCX shouldn't get any replicas, or a combination of the two.

The broken keyspace:

create keyspace KeyspaceDC1 with
  replication_factor = 1 and
  placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and
  strategy_options = [{DC1:1, DC2:0}];

The fixed keyspace:

create keyspace KeyspaceDC1 with
  replication_factor = 1 and
  placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and
  strategy_options = [{DC1:1}];


To reproduce:

* Install the 0.7rc3 rpm on a single node in ""DC1"".
* In cassandra.yaml set initial_token = 1 and specify PropertyFileSnitch.
* cassandra-topology.properties:

10.5.64.26=DC1:R1
default=DC2:R1

* Schema loaded via cassandra-cli:

create keyspace KeyspaceDC1 with
  replication_factor = 1 and
  placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and
  strategy_options = [{DC1:1, DC2:0}];

use KeyspaceDC1;

create column family TestCF with
  column_type = 'Standard' and
  comparator = 'BytesType' and
  keys_cached = 200000 and
  rows_cached = 2000 and
  gc_grace = 0 and
  read_repair_chance = 0.0;

* In cassandra-cli execute the following:

[default@unknown] use KeyspaceDC1;
Authenticated to keyspace: KeyspaceDC1
[default@KeyspaceDC1] set TestCF['some key']['some col'] = 'some value';
Internal error processing insert

* If you have asserts enabled, check system.log where you should find the assertion error: 

DEBUG [pool-1-thread-3] 2010-12-29 12:10:38,897 CassandraServer.java (line 362) insert
ERROR [pool-1-thread-3] 2010-12-29 12:10:38,906 Cassandra.java (line 2960) Internal error processing insert
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.firstTokenIndex(TokenMetadata.java:392) 
        at org.apache.cassandra.locator.TokenMetadata.ringIterator(TokenMetadata.java:417)
        at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:95)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:99)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1411)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1394)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:109)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:442)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:379)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:2952)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
        at java.lang.Thread.run(Thread.java:619)

* If you don't have asserts enabled, you should find that no errors are logged but LOCAL_QUORUM reads cause TimedOutExceptions on the client.
",slebresne,thorcarpenter,Low,Resolved,Fixed,30/Dec/10 22:59,16/Apr/19 09:33
Bug,CAMEL-2325,12487738,@Produce - Causes BeanInvocation to be kept as Body which causes problem with subsequent bean invocations,"The route
{code}
    <camelContext id=""camel"" trace=""true"" xmlns=""http://camel.apache.org/schema/spring"">
        <route>
            <from uri=""direct:start""/>
            <to uri=""log:foo""/>
            <split>
                <method bean=""cool"" method=""split""/>
                <transform><simple>Hello ${body}</simple></transform>
                <to uri=""mock:split""/>
            </split>
        </route>
    </camelContext>
{code}

Is caused problem when a @Produce bean is used to send a message to the route.
2010-01-01 18:07:43,720 [main           ] ERROR DefaultErrorHandler            - Failed delivery for exchangeId: b7098680-4a16-4289-bdf4-3197b0b37aea. Exhausted after delivery attempt: 1 caught: org.apache.camel.language.bean.RuntimeBeanExpressionException: Failed to invoke method: split on cool due to: org.apache.camel.RuntimeCamelException: java.lang.IllegalArgumentException: object is not an instance of declaring class
",davsclaus,davsclaus,Major,Closed,Fixed,01/Jan/10 17:08,03/Nov/10 05:13
Bug,CAMEL-2327,12487740,The NP check of the remote service is wrong in CamelServiceExporter ,"We should check the export service instead of the camelContext.
--- components/camel-spring/src/main/java/org/apache/camel/spring/remoting/CamelServiceExporter.jav(revision 895109)
+++ components/camel-spring/src/main/java/org/apache/camel/spring/remoting/CamelServiceExporter.jav(working copy)
@@ -83,7 +83,7 @@
         }
 
         Endpoint endpoint = CamelContextHelper.getMandatoryEndpoint(camelContext, uri);
-        notNull(camelContext, ""service"");
+        notNull(getService(), ""service"");
         Object proxy = getProxyForService();
 
         consumer = endpoint.createConsumer(new BeanProcessor(proxy, camelContext));
",njiang,njiang,Minor,Closed,Fixed,02/Jan/10 04:59,03/Nov/10 05:13
Bug,CAMEL-2336,12487748,Spring DSL - Some route scoped concerns such as onException etc. can vanish in some edge cases,"The JAXB creates the Spring DSL model in a bit wacky way so we need to work on it a bit before Camel can create the runtime routes.

In some cases some of the cross functions such as onCompletion, onException could potentially vanish. 
The problem is really that we should have divided the route into a upper / lower section where upper is the cross functions and the lower the actual route.",davsclaus,davsclaus,Critical,Closed,Fixed,05/Jan/10 16:07,03/Nov/10 05:13
Bug,CAMEL-2341,12487752,camel-web - Using Sun marshal resolver not possible on non SUN JDK platforms,"For example on RH 5.2 64bit JDK 1.6
{code}
Compilation failure 
/x1/TeamCity/buildAgent/work/1ad9cee4e4314563/camel-fuse-trunk/components/camel-web/src/main/java/org/apache/camel/web/util/JAXBMarshallerResolver.java:[25,34] package com.sun.xml.bind.marshaller does not exist 
/x1/TeamCity/buildAgent/work/1ad9cee4e4314563/camel-fuse-trunk/components/camel-web/src/main/java/org/apache/camel/web/util/JAXBMarshallerResolver.java:[42,12] cannot find symbol 
symbol : class NamespacePrefixMapper 
location: class org.apache.camel.web.util.JAXBMarshallerResolver 
/x1/TeamCity/buildAgent/work/1ad9cee4e4314563/camel-fuse-trunk/components/camel-web/src/main/java/org/apache/camel/web/util/JAXBMarshallerResolver.java:[42,56] cannot find symbol 
symbol : class NamespacePrefixMapper 
location: class org.apache.camel.web.util.JAXBMarshallerResolver 
{code}

I assume AIX, HP-UX and others do have this issue as well.

James maybe we need to revert that change you made?",davsclaus,davsclaus,Major,Closed,Fixed,07/Jan/10 08:35,03/Nov/10 05:13
Bug,CAMEL-2345,12487755,One should not have to explicitly provide the service/port for cxf is there's only one,"This is due to the following code in {{Client createClient()}} in {{CxfEnpoint}}

{code}
        } else {
            ObjectHelper.notNull(portName, ""Please provide endpoint/port name"");
            ObjectHelper.notNull(serviceName, ""Please provide service name"");
            ClientFactoryBean factoryBean = createClientFactoryBean();
{code}

When there's only one service/port, we should just use it.",njiang,hadrian,Major,Closed,Fixed,08/Jan/10 15:23,03/Nov/10 05:13
Bug,CAMEL-2353,12487760,Simple Language - Parsing complex date patterns fails if they contain additional colons,"See nabble
http://old.nabble.com/Camel-%3A-Date-Pattern-ts27131137.html",davsclaus,davsclaus,Minor,Closed,Fixed,13/Jan/10 05:38,03/Nov/10 05:13
Bug,CAMEL-2354,12487762,Examples running with ANT need commons-managment on the classpath,"See nabble
http://old.nabble.com/pojo-example-fails-to-build-with-ant-ts27137019.html",davsclaus,davsclaus,Minor,Closed,Fixed,13/Jan/10 05:53,03/Nov/10 05:13
Bug,CAMEL-2360,12487225,recipientList retryUntil not working,"summary:
the bean gets intialized but it looks like the method retryUntil is never called, could it be an error of implementation? am I doing something wrong?

using following route:
{code}
from(""jms-test:queue:queue.delivery.notification.test"") 
.process(processor) 
.onException(Exception.class).retryUntil(bean(""myRetryBean"")).end() 
.recipientList(header(""recipientListHeader"").tokenize("","")) 
.parallelProcessing().executorService(customThreadPoolExecutor) 
.aggregationStrategy(new RecipientAggregationStrategy()) 
.to(""direct:chunk.completed""); 
{code}

bean is registered in such way: 
{code}
JndiRegistry jndi = new JndiRegistry(new JndiContext()); 
jndi.bind(""myRetryBean"", new RetryBean()); 
{code}

bean class is: 
{code}
public class RetryBean { 

        private int _invoked; 
        private Logger _logger; 
        
    public RetryBean() { 
    this._logger = Logger.getLogger(RetryBean.class); 
    this._invoked = 0; 
    _logger.debug(""BEAN INITIALIZED "" + _invoked); 
    } 
        
    // using bean binding we can bind the information from the exchange to the types we have in our method signature 
    public boolean retryUntil(@Header(Exchange.REDELIVERY_COUNTER) Integer counter, @Body String body, @ExchangeException Exception causedBy) { 
        // NOTE: counter is the redelivery attempt, will start from 1 
    _invoked++; 
    
    
    _logger.debug(""invoked"" + _invoked); 
    _logger.debug(""counter"" + counter); 
    _logger.debug(""result"" + (counter < 2)); 
    
        // we can of course do what ever we want to determine the result but this is a unit test so we end after 3 attempts 
        return counter < 7; 
    } 
{code}
",davsclaus,crive,Major,Closed,Fixed,14/Jan/10 10:14,24/Apr/11 10:01
Bug,CAMEL-2362,12487777,FTP assumes that directory name do not have dot ,"
See http://old.nabble.com/FTP-assume-directory-do-not-have-dot-td27153817.html#a27160086
(COpy paste)
>
> I have the following error:
>
> Caused by: java.lang.IllegalArgumentException: Only directory is supported.
> Endpoint must be configured with a valid directory: ftp.test
>        at
> org.apache.camel.component.file.remote.RemoteFileEndpoint.createConsumer(RemoteFileEndpoint.java:68)
>        at
> org.apache.camel.component.file.remote.RemoteFileEndpoint.createConsumer(RemoteFileEndpoint.java:31)
>        at
> org.apache.camel.impl.EventDrivenConsumerRoute.addServices(EventDrivenConsumerRoute.java:60)
>        at
> org.apache.camel.impl.DefaultRoute.onStartingServices(DefaultRoute.java:83)
>        at org.apache.camel.impl.RouteService.doStart(RouteService.java:123)
>        at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:55)
>        at
> org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:906)
>        ... 1 more
>
> By looking at the source code I saw the following :
>        // we assume its a file if the name has a dot in it (eg foo.txt)
>        if (configuration.getDirectory().contains(""."")) {
>            throw new IllegalArgumentException(""Only directory is supported.
> Endpoint must be configured with a valid directory: ""
>                    + configuration.getDirectory());
>        }
>
> I remove that code and my code is working!! :-)
>
> I think the above code should not assume that.....
> What do you think ?
>
... [show rest of quote]

Its really a bad habit having a dot in a directory name.
On windows will that not often be considered a file?.

And since its a remote path we cannot on startup determine if its a
path or file.

The check was made in the transition from 1.x to 2.x where in 1.x you
could define a starting file or directory.
In 2.x you must specify a directory only.

We could relax the check but then people may still enter a filename
and think Camel will pickup that file.

[SIMON]
Windows directory can contains a dot... a good example is in your eclipse installation... example:
Directory name=org.eclipse.emf.teneo.jpox_1.0.1.v200902271808

Also I think that check is useless because you can still enter something without a dot and it is in fact a file... so the endpoint should verify that after the connection.

Simon 

Yeah good call

Do you mind creating a ticket in JIRA about this?
http://issues.apache.org/activemq/browse/CAMEL",davsclaus,smcduff@hotmail.com,Major,Closed,Fixed,14/Jan/10 11:54,03/Nov/10 05:13
Bug,CAMEL-2363,12487776,Remove sample caches from Camel Cache component,"Camel Cache has sample caches created by the component via its ehCache settings.

These should not be created in order to avoid unnecessary taking up of ports by the sample caches.",akarpe,akarpe,Major,Closed,Fixed,14/Jan/10 18:42,03/Nov/10 05:13
Bug,CAMEL-2376,12487788,org.apache.camel.issues.RetryRouteScopedUntilRecipientListParallelIssueTest failing on build on trunk,"Unit test failing on mvn install on latest head from trunk.

mvn -e output
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.camel.issues.RetryRouteScopedUntilRecipientListParallelIssueTest
Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 11.866 sec <<< FAILURE!

Results :

Failed tests: 
  testRetryUntilRecipientListOkAndFail(org.apache.camel.issues.RetryRouteScopedUntilRecipientListParallelIssueTest)

Tests run: 7, Failures: 1, Errors: 0, Skipped: 0

[INFO] ------------------------------------------------------------------------
[ERROR] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] There are test failures.

Please refer to /data/workspace/camel/camel-core/target/surefire-reports for the individual test results.
[INFO] ------------------------------------------------------------------------
[INFO] Trace
org.apache.maven.BuildFailureException: There are test failures.

Please refer to /data/workspace/camel/camel-core/target/surefire-reports for the individual test results.
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:715)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalWithLifecycle(DefaultLifecycleExecutor.java:556)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:535)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:387)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:348)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:180)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:328)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:138)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:362)
	at org.apache.maven.cli.compat.CompatibleMain.main(CompatibleMain.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)
	at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)
	at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)
	at org.codehaus.classworlds.Launcher.main(Launcher.java:375)
Caused by: org.apache.maven.plugin.MojoFailureException: There are test failures.

Please refer to /data/workspace/camel/camel-core/target/surefire-reports for the individual test results.
	at org.apache.maven.plugin.surefire.SurefirePlugin.execute(SurefirePlugin.java:575)
	at org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginManager.java:490)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:694)
	... 17 more


Surefire output
-------------------------------
-------------------------------------------------------------------------------
Test set: org.apache.camel.issues.RetryRouteScopedUntilRecipientListParallelIssueTest
-------------------------------------------------------------------------------
Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 10.217 sec <<< FAILURE!
testRetryUntilRecipientListOkAndFail(org.apache.camel.issues.RetryRouteScopedUntilRecipientListParallelIssueTest)  Time elapsed: 3.729 sec  <<< FAILURE!
junit.framework.AssertionFailedError: expected:<3> but was:<0>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:280)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at junit.framework.Assert.assertEquals(Assert.java:198)
	at junit.framework.Assert.assertEquals(Assert.java:204)
	at org.apache.camel.issues.RetryRouteScopedUntilRecipientListParallelIssueTest.testRetryUntilRecipientListOkAndFail(RetryRouteScopedUntilRecipientListParallelIssueTest.java:38)


",davsclaus,chrislovecnm,Major,Closed,Fixed,18/Jan/10 18:07,03/Nov/10 05:13
Bug,CAMEL-2377,12487787,org.apache.camel.impl.DefaultProducerTemplateAsyncTest failing on build on trunk,"Unit test failing on mvn install on latest head from trunk.

mvn -e output
------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.camel.impl.DefaultProducerTemplateAsyncTest
Tests run: 22, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 17.012 sec <<< FAILURE!

Results :

Failed tests: 
  testAsyncCallbackExchangeInOut(org.apache.camel.impl.DefaultProducerTemplateAsyncTest)
  testAsyncCallbackBodyInOut(org.apache.camel.impl.DefaultProducerTemplateAsyncTest)
  testAsyncCallbackBodyInOutGetResult(org.apache.camel.impl.DefaultProducerTemplateAsyncTest)

Tests run: 22, Failures: 3, Errors: 0, Skipped: 0

[INFO] ------------------------------------------------------------------------
[ERROR] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] There are test failures.

Please refer to /data/workspace/camel/camel-core/target/surefire-reports for the individual test results.
[INFO] ------------------------------------------------------------------------
[INFO] Trace
org.apache.maven.BuildFailureException: There are test failures.

Please refer to /data/workspace/camel/camel-core/target/surefire-reports for the individual test results.
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:715)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalWithLifecycle(DefaultLifecycleExecutor.java:556)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:535)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:387)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:348)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:180)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:328)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:138)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:362)
	at org.apache.maven.cli.compat.CompatibleMain.main(CompatibleMain.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)
	at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)
	at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)
	at org.codehaus.classworlds.Launcher.main(Launcher.java:375)
Caused by: org.apache.maven.plugin.MojoFailureException: There are test failures.

Please refer to /data/workspace/camel/camel-core/target/surefire-reports for the individual test results.
	at org.apache.maven.plugin.surefire.SurefirePlugin.execute(SurefirePlugin.java:575)
	at org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginManager.java:490)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:694)
	... 17 more


Surefire Output

-------------------------------------------------------------------------------
Test set: org.apache.camel.impl.DefaultProducerTemplateAsyncTest
-------------------------------------------------------------------------------
Tests run: 22, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 17.013 sec <<< FAILURE!
testAsyncCallbackExchangeInOut(org.apache.camel.impl.DefaultProducerTemplateAsyncTest)  Time elapsed: 0.431 sec  <<< FAILURE!
junit.framework.ComparisonFailure: expected:<[AB]C> but was:<[BA]C>
	at junit.framework.Assert.assertEquals(Assert.java:81)
	at junit.framework.Assert.assertEquals(Assert.java:87)
	at org.apache.camel.impl.DefaultProducerTemplateAsyncTest.testAsyncCallbackExchangeInOut(DefaultProducerTemplateAsyncTest.java:357)

testAsyncCallbackBodyInOut(org.apache.camel.impl.DefaultProducerTemplateAsyncTest)  Time elapsed: 0.206 sec  <<< FAILURE!
junit.framework.ComparisonFailure: expected:<[AB]C> but was:<[BA]C>
	at junit.framework.Assert.assertEquals(Assert.java:81)
	at junit.framework.Assert.assertEquals(Assert.java:87)
	at org.apache.camel.impl.DefaultProducerTemplateAsyncTest.testAsyncCallbackBodyInOut(DefaultProducerTemplateAsyncTest.java:451)

testAsyncCallbackBodyInOutGetResult(org.apache.camel.impl.DefaultProducerTemplateAsyncTest)  Time elapsed: 0.176 sec  <<< FAILURE!
junit.framework.ComparisonFailure: expected:<[AB]C> but was:<[BA]C>
	at junit.framework.Assert.assertEquals(Assert.java:81)
	at junit.framework.Assert.assertEquals(Assert.java:87)
	at org.apache.camel.impl.DefaultProducerTemplateAsyncTest.testAsyncCallbackBodyInOutGetResult(DefaultProducerTemplateAsyncTest.java:492)

",davsclaus,chrislovecnm,Minor,Closed,Fixed,18/Jan/10 18:19,03/Nov/10 05:13
Bug,CAMEL-2378,12487786,Maven build not installing camel-spring-2.2-SNAPSHOT-tests,"I am running mvn install -Dmaven.test.skip=true and I am getting the following error

Missing:
----------
1) org.apache.camel:camel-spring:test-jar:tests:2.2-SNAPSHOT

The pom for camel-spring has a plug-in section for creating the test jar, but it is not getting installed.

I am getting the following output running mvn -e

Interesting part:
 [INFO] Skipping packaging of the test-jar


Full output
--------------------------------
+ Error stacktraces are turned on.
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Building Camel :: Spring
[INFO]    task-segment: [install]
[INFO] ------------------------------------------------------------------------
Downloading: http://repo.open.iona.com/maven2/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:pom:2.1.5-dev' in repository open.iona.m2 (http://repo.open.iona.com/maven2)
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:pom:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
Downloading: http://download.java.net/maven/1/com.sun.xml.bind/poms/jaxb-impl-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:pom:2.1.5-dev' in repository java.net (http://download.java.net/maven/1)
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:pom:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
Downloading: http://repo.open.iona.com/maven2/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:pom:2.1.5-dev' in repository open.iona.m2 (http://repo.open.iona.com/maven2)
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:pom:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
Downloading: http://download.java.net/maven/1/com.sun.xml.bind/poms/jaxb-xjc-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:pom:2.1.5-dev' in repository java.net (http://download.java.net/maven/1)
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:pom:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
[INFO] [antrun:run {execution: create-prop}]
[INFO] Executing tasks
     [echo] Maven version: 2.2-SNAPSHOT
     [echo] OSGi version: 2.2.0.SNAPSHOT
[INFO] Executed tasks
[INFO] [properties:read-project-properties {execution: default}]
[INFO] [antrun:run {execution: process-sources}]
[INFO] Executing tasks
     [echo] Copying to code together for the XSD generation
     [echo] Generating XSD schema
[schemagen] Generating schema from 142 source files
     [copy] Copying 1 file to /data/workspace/camel/components/camel-spring/target/schema
[INFO] Executed tasks
[INFO] Setting property: classpath.resource.loader.class => 'org.codehaus.plexus.velocity.ContextClassLoaderResourceLoader'.
[INFO] Setting property: velocimacro.messages.on => 'false'.
[INFO] Setting property: resource.loader => 'classpath'.
[INFO] Setting property: resource.manager.logwhenfound => 'false'.
[INFO] [remote-resources:process {execution: default}]
[INFO] [resources:resources {execution: default-resources}]
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 10 resources
[INFO] Copying 3 resources
[INFO] [compiler:compile {execution: default-compile}]
[INFO] Nothing to compile - all classes are up to date
[INFO] [resources:testResources {execution: default-testResources}]
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 290 resources
[INFO] Copying 3 resources
[INFO] [compiler:testCompile {execution: default-testCompile}]
[INFO] Not compiling test sources
[INFO] [surefire:test {execution: default-test}]
[INFO] Tests are skipped.
[INFO] [bundle:bundle {execution: default-bundle}]
[INFO] [antrun:run {execution: package}]
[INFO] Executing tasks
     [echo] Deleting unwanted resources from the test-jar
   [delete] Deleting: /data/workspace/camel/components/camel-spring/target/test-classes/log4j.properties
[INFO] Executed tasks
[INFO] [jar:test-jar {execution: default}]
[INFO] Skipping packaging of the test-jar
[INFO] [build-helper:attach-artifact {execution: attach-artifacts}]
[INFO] [ianal:verify-legal-files {execution: default}]
[INFO] Checking legal files in: camel-spring-2.2-SNAPSHOT.jar
[INFO] [install:install {execution: default-install}]
[INFO] Installing /data/workspace/camel/components/camel-spring/target/camel-spring-2.2-SNAPSHOT.jar to /home/clove/.m2/repository/org/apache/camel/camel-spring/2.2-SNAPSHOT/camel-spring-2.2-SNAPSHOT.jar
[INFO] Installing /data/workspace/camel/components/camel-spring/target/schema/camel-spring.xsd to /home/clove/.m2/repository/org/apache/camel/camel-spring/2.2-SNAPSHOT/camel-spring-2.2-SNAPSHOT.xsd
[INFO] [bundle:install {execution: default-install}]
[INFO] Parsing file:/home/clove/.m2/repository/repository.xml
[INFO] Installing org/apache/camel/camel-spring/2.2-SNAPSHOT/camel-spring-2.2-SNAPSHOT.jar
[INFO] Writing OBR metadata
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESSFUL
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 57 seconds
[INFO] Finished at: Mon Jan 18 11:57:54 MST 2010
[INFO] Final Memory: 57M/186M
[INFO] ------------------------------------------------------------------------

",janstey,chrislovecnm,Major,Closed,Fixed,18/Jan/10 18:59,03/Nov/10 05:13
Bug,CAMEL-2385,12487797,BindingOperationInfos that are stored in exchange are inconsistent between CxfProducer and CxfConsumer,"CxfProducer could store a unwrapped version of the BindingOperationInfo in the exchange which is inconsistent with CxfConsumer and it puts burden on the application to revert back to the wrapped version.  CxfProducer should store the original version before calling.

{code}
                boi = boi.getUnwrappedOperation();
{code}
",wtam,wtam,Major,Closed,Fixed,20/Jan/10 20:49,03/Nov/10 05:13
Bug,CAMEL-2387,12487800,GenericFileConverter should honor charset from Exchange,"GenericFileConverter should pass in Exchange as parameter to converter so it can leverage any CHARSET set on the Exchange such as from a {{.convertBodyTo(String.class, ""UTF-8"");}}",davsclaus,davsclaus,Minor,Closed,Fixed,21/Jan/10 11:04,03/Nov/10 05:13
Bug,CAMEL-2394,12487773,Renamer failing to rename 'From' File when using multiple endpoints via Multicast,"[Original Issue at Nabble|http://old.nabble.com/Renamer-failing-to-rename-%27From%27-File-when-using-multiple-endpoints-via-Multicast-td27272407s22882.html]


I have a route that looks for a file in a directory => unmarshals the file using <CSV> => transforms the output from the unmarshal (which is List<List<String>>) to a List of data objects => passes the List of data objects to a processor which does the intelligent mapping of the input data to the output which is a List of Objects => passes the List of Objects to a <multicast> pipeline who's endpoints traverse the List of Objects and extract the data, that they are interested in, formatting the output in to a List of Map objects so that the <CSV> marshaller can marshal to a named file. 

This is the route: 

{code}
        <route id=""iq""> 
            <from uri=""file:/data/iq/inbound/""/> 
            <unmarshal> 
                <csv id=""pipeCsvDataFormat""/> 
            </unmarshal> 
            <bean ref=""iqTransform"" method=""doTransform""/> 
            <to uri=""bean:iqProcessor?method=process""/> 
            <multicast parallelProcessing=""true""> 
                <pipeline> 
                    <to uri=""bean:formatOutput?method=formatHeader""/> 
                    <marshal> 
                        <csv /> 
                    </marshal> 
                    <to uri=""file:/data/iq/outbound/?fileName=Txn_Header.txt""/> 
                </pipeline> 
                <pipeline> 
                    <to uri=""bean:formatOutput?method=formatLineHeader""/> 
                    <marshal> 
                        <csv /> 
                    </marshal> 
                    <to uri=""file:/data/iq/outbound/?fileName=Line_Header.txt""/> 
                </pipeline> 
            </multicast> 
        </route> 

{code}

As expected, I get a correctly formatted file named Txn_Header.txt and a file named Line_Header.txt in the /outbound directory. So the routing seems to have worked fine. 

The problem is that the original file does not get renamed in to the .camel subdirectory and so the file gets processed again and again. 

If I remove the second endpoint in the multicast so that only the Txn_Header.txt file is produced then the original file does get renamed and the route ends successfully. ",davsclaus,andy bourke,Major,Closed,Fixed,22/Jan/10 15:26,03/Nov/10 05:13
Bug,CAMEL-2395,12487770,file component - Using fileName option should better detect constant vs dynamic name,"eg using fileName=header.txt makes Camel think its header with the name txt you want to use.

",davsclaus,davsclaus,Major,Closed,Fixed,24/Jan/10 13:38,03/Nov/10 05:13
Bug,CAMEL-2397,12487772,camel-cxf producer should copy the inMessage headers to the outMessage,"If we try to chain some different endponit together in the camel route, we need to make sure the in message header be copied to the out message. ",njiang,njiang,Major,Closed,Fixed,25/Jan/10 02:33,03/Nov/10 05:13
Bug,CAMEL-2410,12487814,Cannot build http://camel.apache.org/spring-example.html,"Problem with the checksum on the JARS in the repo? [ I can turn that off, but wanted to let you know ]

""C:\Program Files\Java\jdk1.6.0_14\bin\java"" -Xmx1024m -Dclassworlds.conf=C:\apache-maven-2.2.1\bin\m2.conf -Dmaven.home=C:\apache-maven-2.2.1 -Didea.launcher.port=7536 ""-Didea.launcher.bin.path=C:\Program Files\JetBrains\IntelliJ IDEA 9.0.1\bin"" -Dfile.encoding=windows-1252 -classpath ""C:\apache-maven-2.2.1\boot\classworlds-1.1.jar;C:\Program Files\JetBrains\IntelliJ IDEA 9.0.1\lib\idea_rt.jar"" com.intellij.rt.execution.application.AppMain org.codehaus.classworlds.Launcher --no-plugin-registry --fail-fast --no-plugin-updates --strict-checksums --update-snapshots -f C:\apache-camel-2.1.0\examples\camel-example-spring\pom.xml compile
+ Enabling strict checksum verification on all artifact downloads.
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Building Camel :: Example :: Spring
[INFO]    task-segment: [compile]
[INFO] ------------------------------------------------------------------------
Downloading: http://onejar-maven-plugin.googlecode.com/svn/mavenrepo/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:pom:2.1.5-dev' in repository onejar-maven-plugin.googlecode.com (http://onejar-maven-plugin.googlecode.com/svn/mavenrepo)
Downloading: http://people.apache.org/repo/m2-ibiblio-rsync-repository/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:pom:2.1.5-dev' in repository apache.m2.repo (http://people.apache.org/repo/m2-ibiblio-rsync-repository)
Downloading: http://repo.open.iona.com/maven2/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:pom:2.1.5-dev' in repository open.iona.m2 (http://repo.open.iona.com/maven2)
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:pom:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
Downloading: http://people.apache.org/repo/m2-ibiblio-rsync-repository/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:pom:2.1.5-dev' in repository apache.m2.repo (http://people.apache.org/repo/m2-ibiblio-rsync-repository)
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:pom:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
Downloading: http://onejar-maven-plugin.googlecode.com/svn/mavenrepo/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:pom:2.1.5-dev' in repository onejar-maven-plugin.googlecode.com (http://onejar-maven-plugin.googlecode.com/svn/mavenrepo)
Downloading: http://people.apache.org/repo/m2-ibiblio-rsync-repository/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:pom:2.1.5-dev' in repository apache.m2.repo (http://people.apache.org/repo/m2-ibiblio-rsync-repository)
Downloading: http://repo.open.iona.com/maven2/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:pom:2.1.5-dev' in repository open.iona.m2 (http://repo.open.iona.com/maven2)
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:pom:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
Downloading: http://people.apache.org/repo/m2-ibiblio-rsync-repository/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:pom:2.1.5-dev' in repository apache.m2.repo (http://people.apache.org/repo/m2-ibiblio-rsync-repository)
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.pom
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:pom:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
Downloading: http://onejar-maven-plugin.googlecode.com/svn/mavenrepo/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.jar
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:jar:2.1.5-dev' in repository onejar-maven-plugin.googlecode.com (http://onejar-maven-plugin.googlecode.com/svn/mavenrepo)
Downloading: http://people.apache.org/repo/m2-ibiblio-rsync-repository/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.jar
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:jar:2.1.5-dev' in repository apache.m2.repo (http://people.apache.org/repo/m2-ibiblio-rsync-repository)
Downloading: http://repo.open.iona.com/maven2/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.jar
4/818K
...
818/818K
818K downloaded  (jaxb-impl-2.1.5-dev.jar)
[WARNING] Unable to get resource 'com.sun.xml.bind:jaxb-impl:jar:2.1.5-dev' from repository open.iona.m2 (http://repo.open.iona.com/maven2): Error retrieving checksum file for com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.jar
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.jar
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:jar:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
Downloading: http://people.apache.org/repo/m2-ibiblio-rsync-repository/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.jar
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:jar:2.1.5-dev' in repository apache.m2.repo (http://people.apache.org/repo/m2-ibiblio-rsync-repository)
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.1.5-dev/jaxb-impl-2.1.5-dev.jar
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-impl:jar:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
Downloading: http://onejar-maven-plugin.googlecode.com/svn/mavenrepo/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.jar
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:jar:2.1.5-dev' in repository onejar-maven-plugin.googlecode.com (http://onejar-maven-plugin.googlecode.com/svn/mavenrepo)
Downloading: http://people.apache.org/repo/m2-ibiblio-rsync-repository/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.jar
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:jar:2.1.5-dev' in repository apache.m2.repo (http://people.apache.org/repo/m2-ibiblio-rsync-repository)
Downloading: http://repo.open.iona.com/maven2/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.jar
4/2984K
...
2984/2984K
2984K downloaded  (jaxb-xjc-2.1.5-dev.jar)
[WARNING] Unable to get resource 'com.sun.xml.bind:jaxb-xjc:jar:2.1.5-dev' from repository open.iona.m2 (http://repo.open.iona.com/maven2): Error retrieving checksum file for com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.jar
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.jar
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:jar:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
Downloading: http://people.apache.org/repo/m2-ibiblio-rsync-repository/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.jar
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:jar:2.1.5-dev' in repository apache.m2.repo (http://people.apache.org/repo/m2-ibiblio-rsync-repository)
Downloading: http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-xjc/2.1.5-dev/jaxb-xjc-2.1.5-dev.jar
[INFO] Unable to find resource 'com.sun.xml.bind:jaxb-xjc:jar:2.1.5-dev' in repository central (http://repo1.maven.org/maven2)
[INFO] ------------------------------------------------------------------------
[ERROR] BUILD ERROR
[INFO] ------------------------------------------------------------------------
[INFO] Failed to resolve artifact.

Missing:
----------
1) com.sun.xml.bind:jaxb-impl:jar:2.1.5-dev

  Try downloading the file manually from the project website.

  Then, install it using the command: 
      mvn install:install-file -DgroupId=com.sun.xml.bind -DartifactId=jaxb-impl -Dversion=2.1.5-dev -Dpackaging=jar -Dfile=/path/to/file

  Alternatively, if you host your own repository you can deploy the file there: 
      mvn deploy:deploy-file -DgroupId=com.sun.xml.bind -DartifactId=jaxb-impl -Dversion=2.1.5-dev -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]

  Path to dependency: 
  	1) org.apache.maven.plugins:maven-antrun-plugin:maven-plugin:1.2
  	2) com.sun.xml.bind:jaxb-impl:jar:2.1.5-dev

2) com.sun.xml.bind:jaxb-xjc:jar:2.1.5-dev

  Try downloading the file manually from the project website.

  Then, install it using the command: 
      mvn install:install-file -DgroupId=com.sun.xml.bind -DartifactId=jaxb-xjc -Dversion=2.1.5-dev -Dpackaging=jar -Dfile=/path/to/file

  Alternatively, if you host your own repository you can deploy the file there: 
      mvn deploy:deploy-file -DgroupId=com.sun.xml.bind -DartifactId=jaxb-xjc -Dversion=2.1.5-dev -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]

  Path to dependency: 
  	1) org.apache.maven.plugins:maven-antrun-plugin:maven-plugin:1.2
  	2) com.sun.xml.bind:jaxb-xjc:jar:2.1.5-dev

----------
2 required artifacts are missing.

for artifact: 
  org.apache.maven.plugins:maven-antrun-plugin:maven-plugin:1.2

from the specified remote repositories:
  apache.m2.repo (http://people.apache.org/repo/m2-ibiblio-rsync-repository),
  apache.snapshots (http://repository.apache.org/snapshots),
  central (http://repo1.maven.org/maven2),
  onejar-maven-plugin.googlecode.com (http://onejar-maven-plugin.googlecode.com/svn/mavenrepo),
  open.iona.m2 (http://repo.open.iona.com/maven2)



[INFO] ------------------------------------------------------------------------
[INFO] For more information, run Maven with the -e switch
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 19 seconds
[INFO] Finished at: Wed Jan 27 11:49:28 CET 2010
[INFO] Final Memory: 14M/26M
[INFO] ------------------------------------------------------------------------

Process finished with exit code 1
",njiang,mondraymond,Major,Closed,Fixed,27/Jan/10 10:58,03/Nov/10 05:13
Bug,CAMEL-2420,12487822,"In camel-cache CacheConfiguration ""diskExpiryThreadIntervalSeconds"" not set correctly","See CacheConfiguration.java

{code:title=CacheConfiguration.java|borderStyle=solid}

 if (cacheSettings.containsKey(""diskExpiryThreadIntervalSeconds"")) {
	setTimeToLiveSeconds(Long.valueOf((String) cacheSettings.get(""diskExpiryThreadIntervalSeconds"")).longValue());
} 

{code} ",davsclaus,skydjol,Major,Closed,Fixed,28/Jan/10 16:42,03/Nov/10 05:13
Bug,CAMEL-2425,12487828,Test failed of CxfMtomRouterPayloadModeTest,"After upgraded the CXF version to 2.2.6, we got a random test error of CxfMtomRouterPayloadModeTest,testInvokingServiceFromCXFClient() on TeamCity
Here is the stack trace:
{code}
javax.xml.ws.soap.SOAPFaultException: Unmarshalling Error: null
at org.apache.cxf.jaxws.JaxWsClientProxy.invoke(JaxWsClientProxy.java:146)
at $Proxy72.detail(Unknown Source)
at org.apache.camel.component.cxf.mtom.CxfMtomRouterPayloadModeTest.testInvokingServiceFromCXFClient(CxfMtomRouterPayloadModeTest.java:89)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.springframework.test.context.junit4.SpringTestMethod.invoke(SpringTestMethod.java:160)
at org.springframework.test.context.junit4.SpringMethodRoadie.runTestMethod(SpringMethodRoadie.java:233)
at org.springframework.test.context.junit4.SpringMethodRoadie$RunBeforesThenTestThenAfters.run(SpringMethodRoadie.java:333)
at org.springframework.test.context.junit4.SpringMethodRoadie.runWithRepetitions(SpringMethodRoadie.java:217)
at org.springframework.test.context.junit4.SpringMethodRoadie.runTest(SpringMethodRoadie.java:197)
at org.springframework.test.context.junit4.SpringMethodRoadie.run(SpringMethodRoadie.java:143)
at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.invokeTestMethod(SpringJUnit4ClassRunner.java:160)
at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:97)
at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:62)
at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:140)
at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:165)
at org.apache.maven.surefire.Surefire.run(Surefire.java:107)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:289)
at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1005)
Caused by: javax.xml.bind.UnmarshalException
- with linked exception:
[javax.xml.bind.UnmarshalException
- with linked exception:
[java.lang.NullPointerException]]
at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallerImpl.handleStreamException(UnmarshallerImpl.java:425)
at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallerImpl.unmarshal0(UnmarshallerImpl.java:362)
at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallerImpl.unmarshal(UnmarshallerImpl.java:339)
at org.apache.cxf.jaxb.JAXBEncoderDecoder.unmarshall(JAXBEncoderDecoder.java:749)
at org.apache.cxf.jaxb.JAXBEncoderDecoder.unmarshall(JAXBEncoderDecoder.java:623)
at org.apache.cxf.jaxb.io.DataReaderImpl.read(DataReaderImpl.java:128)
at org.apache.cxf.interceptor.DocLiteralInInterceptor.handleMessage(DocLiteralInInterceptor.java:106)
at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:239)
at org.apache.cxf.endpoint.ClientImpl.onMessage(ClientImpl.java:671)
at org.apache.cxf.transport.http.HTTPConduit$WrappedOutputStream.handleResponseInternal(HTTPConduit.java:2205)
at org.apache.cxf.transport.http.HTTPConduit$WrappedOutputStream.handleResponse(HTTPConduit.java:2084)
at org.apache.cxf.transport.http.HTTPConduit$WrappedOutputStream.close(HTTPConduit.java:1982)
at org.apache.cxf.transport.AbstractConduit.close(AbstractConduit.java:66)
at org.apache.cxf.transport.http.HTTPConduit.close(HTTPConduit.java:637)
at org.apache.cxf.interceptor.MessageSenderInterceptor$MessageSenderEndingInterceptor.handleMessage(MessageSenderInterceptor.java:62)
at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:239)
at org.apache.cxf.endpoint.ClientImpl.invoke(ClientImpl.java:483)
at org.apache.cxf.endpoint.ClientImpl.invoke(ClientImpl.java:309)
at org.apache.cxf.endpoint.ClientImpl.invoke(ClientImpl.java:261)
at org.apache.cxf.frontend.ClientProxy.invokeSync(ClientProxy.java:73)
at org.apache.cxf.jaxws.JaxWsClientProxy.invoke(JaxWsClientProxy.java:124)
... 29 more
Caused by: javax.xml.bind.UnmarshalException
- with linked exception:
[java.lang.NullPointerException]
at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.handleEvent(UnmarshallingContext.java:642)
at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.handleError(UnmarshallingContext.java:671)
at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.handleError(UnmarshallingContext.java:667)
at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.handleParseConversionException(Loader.java:267)
at com.sun.xml.bind.v2.runtime.unmarshaller.LeafPropertyLoader.text(LeafPropertyLoader.java:65)
at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.text(UnmarshallingContext.java:494)
at com.sun.xml.bind.v2.runtime.unmarshaller.MTOMDecorator.startElement(MTOMDecorator.java:103)
at com.sun.xml.bind.v2.runtime.unmarshaller.StAXStreamConnector.handleStartElement(StAXStreamConnector.java:242)
at com.sun.xml.bind.v2.runtime.unmarshaller.StAXStreamConnector.bridge(StAXStreamConnector.java:176)
at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallerImpl.unmarshal0(UnmarshallerImpl.java:360)
... 48 more
Caused by: javax.xml.bind.UnmarshalException
- with linked exception:
[java.lang.NullPointerException]
... 58 more
Caused by: java.lang.NullPointerException
at org.apache.cxf.attachment.LazyDataSource.getInputStream(LazyDataSource.java:67)
at com.sun.xml.bind.v2.runtime.unmarshaller.Base64Data.get(Base64Data.java:188)
at com.sun.xml.bind.v2.runtime.unmarshaller.Base64Data.getExact(Base64Data.java:154)
at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl.decodeBase64(RuntimeBuiltinLeafInfoImpl.java:827)
at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl.access$100(RuntimeBuiltinLeafInfoImpl.java:111)
at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl$15.parse(RuntimeBuiltinLeafInfoImpl.java:652)
at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl$15.parse(RuntimeBuiltinLeafInfoImpl.java:655)
at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor$CompositeTransducedAccessorImpl.parse(TransducedAccessor.java:241)
at com.sun.xml.bind.v2.runtime.unmarshaller.LeafPropertyLoader.text(LeafPropertyLoader.java:61)
... 53 more 
{code}",wtam,njiang,Major,Closed,Fixed,29/Jan/10 02:39,03/Nov/10 05:13
Bug,CAMEL-2426,12487554,"CXF Header ""ResponseContext"" cannot be filtered by HeaderFilterStrategy",,wtam,wtam,Major,Closed,Fixed,29/Jan/10 02:41,03/Nov/10 05:13
Bug,CAMEL-2428,12487826,"In camel-cache ""diskStorePath"" property not efficient","If you want to change path to persisent cache with ""diskStorePath"", it has no effect  because cacheManager erase this value.

In ""addCacheNoCheck"" method, ""setDiskStorePath"" call with ""diskStorePath"" CacheManager property value.

{code:title=CacheManager.java|borderStyle=solid}

    public void addCache(Ehcache cache) throws IllegalStateException,
            ObjectExistsException, CacheException {
        checkStatus();
        if (cache == null) {
            return;
        }
        addCacheNoCheck(cache);
    }

    private void addCacheNoCheck(Ehcache cache) throws IllegalStateException,
            ObjectExistsException, CacheException {
        if (ehcaches.get(cache.getName()) != null) {
            throw new ObjectExistsException(""Cache "" + cache.getName() + "" already exists"");
        }
        cache.setCacheManager(this);
        cache.setDiskStorePath(diskStorePath);
        cache.initialise();
        try {
            cache.bootstrap();
        } catch (CacheException e) {
            LOG.log(Level.WARNING, ""Cache "" + cache.getName() + ""requested bootstrap but a CacheException occured. "" + e.getMessage(), e);
        }
        ehcaches.put(cache.getName(), cache);
        if (cache instanceof Cache) {
            caches.put(cache.getName(), cache);
        }

        //Don't notify initial config. The init method of each listener should take care of this.
        if (status.equals(Status.STATUS_ALIVE)) {
            cacheManagerEventListenerRegistry.notifyCacheAdded(cache.getName());
        }
    }
{code} 

",davsclaus,skydjol,Minor,Closed,Fixed,29/Jan/10 11:39,03/Nov/10 05:13
Bug,CAMEL-2430,12487829,camel-example-reportincident throwing exception,"When I execute mvn jetty:run the following exception is thrown:

{code}WARNING: Could not find endpoint/port for {http://reportincident.example.camel.apache.org}ReportIncidentEndpointPort in wsdl. Using {http://reportincident.example.camel.apache.org}ReportIncidentService.
2010-01-31 15:07:35,342 [main           ] ERROR ContextLoader                  - Context initialization failed
org.apache.camel.RuntimeCamelException: org.apache.cxf.service.factory.ServiceConstructionException
        at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1055)
        at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:103)
        at org.apache.camel.spring.CamelContextFactoryBean.onApplicationEvent(CamelContextFactoryBean.java:593)
        at org.springframework.context.event.SimpleApplicationEventMulticaster$1.run(SimpleApplicationEventMulticaster.java:78)

2010-01-31 15:07:35.347::WARN:  Failed startup of context org.mortbay.jetty.plugin.Jetty6PluginWebAppContext@5952e9a8{/camel-example-reportincident,/data/workspace/examples_orig/camel-example-reportincident/src/main/webapp}
org.apache.camel.RuntimeCamelException: org.apache.cxf.service.factory.ServiceConstructionException
        at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1055)
        at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:103)
        at org.apache.camel.spring.CamelContextFactoryBean.onApplicationEvent(CamelContextFactoryBean.java:593

Caused by: java.net.MalformedURLException: no protocol: /incident
        at java.net.URL.<init>(URL.java:567)
        at java.net.URL.<init>(URL.java:464)
        at java.net.URL.<init>(URL.java:413)
        at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.<init>(JettyHTTPDestination.java:96)
        at org.apache.cxf.transport.http_jetty.JettyHTTPTransportFactory.createDestination(JettyHTTPTransportFactory.java:116)
        at org.apache.cxf.transport.http_jetty.JettyHTTPTransportFactory.getDestination(JettyHTTPTransportFactory.java:103)
        at org.apache.cxf.endpoint.ServerImpl.initDestination(ServerImpl.java:90)
        at org.apache.cxf.endpoint.ServerImpl.<init>(ServerImpl.java:69)
        at org.apache.cxf.frontend.ServerFactoryBean.create(ServerFactoryBean.java:106){code}

I have edited the exception to make it shorter.",njiang,chrislovecnm,Major,Closed,Fixed,31/Jan/10 22:11,03/Nov/10 05:13
Bug,CAMEL-2436,12487774,BeanProcessor - Should differentiate between void and method returning null values,"When invoking a bean method it should better detect whether the bean is a *void* or has a return type.
And if it returns {{null}} it should be regarded as a valid response and set as body.

See nabble
http://old.nabble.com/filtering-messages-ts27403559.html",davsclaus,davsclaus,Major,Closed,Fixed,02/Feb/10 09:14,26/Aug/11 06:41
Bug,CAMEL-2439,12487831,File consumer - Consuming from absolute paths can cause issue on Windows when moving file when done,"When using a file consumer with an absolute path, eg {{from(""file:/data"")}} then when the process is done and it wants to move the file to {{.camel}} sub dir of {{/data}} it may not do that correctly on Windows.

The issue is the logic in Camel is based upon that a path starting with \ is considered absolute, as it is on Unix OS and other platforms. Where as on Windows {{java.io.File}} returns {{false}} for such files. The other return {{true}}. So we need to cater for this and make Windows return {{true}} as well so the logic is consistent across OS.",davsclaus,davsclaus,Major,Closed,Fixed,02/Feb/10 16:59,03/Nov/10 05:13
Bug,CAMEL-2442,12487827,Explicit version ranges on javax.* imports in camel-cxf bundle manifest cause a problem when running on equinox via pax-runner,"There's a mismatch between the explicit version constraints on the javax.activation, javax.annotation, javax.jws, javax.xml.ws etc. packages in the camel-cxf manifest and the actual version assigned to these packages by Equinox when pulled in via the system packages from the Java6 rt.jar.

This causes bundle resolution failures such as the following:

{code}
org.osgi.framework.BundleException: The bundle could not be resolved. Reason: Package uses conflict: Import-Package: org.apache.camel; version=""2.2.something""
{code}

Normally this issue could be suppressed by configuring the Equinox system packages to exclude the problematic javax.* packages. However, we don't have enough control over this in pax-runner 1.3 (which unilaterally adds these packages in the generated config.ini).

The solution is to loosen the version constraints on these imports in the camel-cxf bundle manifest.",njiang,eglynn,Major,Closed,Fixed,03/Feb/10 10:50,03/Nov/10 05:13
Bug,CAMEL-2444,12487835,"Log Component documentation for ""showAll"" option is misleading","Log Component documentation for ""showAll"" option is misleading or the option does not work as expected.

""showAll 	 false 	 Quick option for turning all options on.""

There appear to be several options that are not turned as expected such as ""multiline"" and ""showStackTrace"", etc..  

",davsclaus,mbmather,Trivial,Closed,Fixed,03/Feb/10 20:27,03/Nov/10 05:13
Bug,CAMEL-2445,12487837,BatchProcesser.processExchange needs to catch Throwable,"If an aggregator sends an exchange to a processor that throws an Error this causes the thread started by BatchProcessor to exit, exchanges will then accumulate in the aggregator until an OutOfMemoryError occurs.

This patch sorts that out and adds a unit test, however there's another problem that I'm still looking into, namely that BatchProcessor just uses an instance of LoggingErrorHandler as it's exception handler, so any exceptions/errors caught by this processor will just get logged and not follow the normal Camel error handling.",slewis,slewis,Major,Closed,Fixed,03/Feb/10 21:57,03/Nov/10 05:13
Bug,CAMEL-2447,12487836,cxfbean should  propagate CONTENT_TYPE for other camel component to use,"There is no ""content-type"" header return from the below route
{code}
 <route>

      <from uri=""jetty:http://localhost:9006/employeesBase?matchOnUriPrefix=true""/>

      <to uri=""cxfbean:EmployeesPOJO""/>

  </route>
{code}",njiang,njiang,Major,Closed,Fixed,04/Feb/10 02:45,03/Nov/10 05:13
Bug,CAMEL-2450,12487842,camel-spring-osgi module should include the META-INF/services/... file from camel-spring,"The shade plugin doesn't include the files in META-INF/services from camel-spring bundle, we should fix it.",njiang,njiang,Major,Closed,Fixed,04/Feb/10 12:42,03/Nov/10 05:13
Bug,CAMEL-2451,12487176,HL7MLLPDecoder fails if message length is exactly 1022,When an HL7 message length is exactly 1022 then the two end control charcters are not read from the same packet/buffer. This causes HL7MLLPDecoder.scan() method to fail.,mrt1nz,mrt1nz,Major,Closed,Fixed,05/Feb/10 08:56,03/Nov/10 05:13
Bug,CAMEL-2452,12487843,"HttpHeaderFilterStrategy dosn't filters out 'Cache-Control', 'Connection', 'Pragma', 'Trailer', 'Transfer-Encoding', 'Upgrade', 'Via' and 'Warning' in method applyFilterToCamelHeaders",HttpHeaderFilterStrategy uses the HTTP headers with upper case characters on the beginning (e. g. 'Transfer-Encoding' instead of 'transfer-encoding').,muellerc,muellerc,Major,Closed,Fixed,06/Feb/10 13:56,03/Nov/10 22:04
Bug,CAMEL-2456,12487228,"WARNING log of JmsTemporaryTopicEndpoint , JmsTemporaryQueueEndpoint don't have the ManagedResource annotation","Here is the mail thread which discusses about this issue.
http://old.nabble.com/Attemp-to-send-message-to-activemq-temporary-queue-using-producerTemplate%3A-InvalidMetadataException-tp27520096p27520096.html",njiang,njiang,Minor,Closed,Fixed,10/Feb/10 02:13,24/Apr/11 10:01
Bug,CAMEL-2458,12486474,HttpHeaderFilterStrategy should filter the HTTP 'Date' header when applyFilterToCamelHeaders is called,See [rfc2616|http://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.5].,muellerc,muellerc,Minor,Closed,Fixed,10/Feb/10 17:39,24/Apr/11 10:01
Bug,CAMEL-2464,12487303,camel-xmpp not sends message if camel-application deployed to Felix Karaf,"Through camel-xmpp my message sended succesfully in standalone camel application, but message not sends if application as a bundle deployed to Felix Karaf.

My route:
from(""timer://foo?fixedRate=true&period=10000&delay=10000"").setBody(constant(""Hello, i am message"")). 
  to(""xmpp://"" + XMPP_FROM_NAME + ""/masny@websphere"" + ""?password="" + XMPP_FROM_PASS); 

""camel:run"" command runs this route and succesfully sends message. 
But if i create a bundle and deploy it to Felix Karaf, my route not sends message, in DEBUG log i see :

DEBUG | xtenderThread-42 | DefaultCamelContext              | e.camel.impl.DefaultCamelContext  994 | ... Routes started 
 INFO  | xtenderThread-42 | DefaultCamelContext              | e.camel.impl.DefaultCamelContext  997 | Apache Camel 2.1.0 (CamelContext:camelContext) started 
 INFO  | xtenderThread-42 | OsgiBundleXmlApplicationContext  | ractOsgiBundleApplicationContext  327 | Publishing application context as OSGi service with properties {org.springframework.context.service.name=try-xmpp, Bundle-SymbolicName=try-xmpp, Bundle-Version=0.1.0} 
 DEBUG | xtenderThread-42 | OsgiBundleXmlApplicationContext  | ractOsgiBundleApplicationContext  341 | Publishing service under classes {org.springframework.osgi.context.DelegatedExecutionOsgiBundleApplicationContext, org.springframework.osgi.context.ConfigurableOsgiBundleApplicationContext, org.springframework.context.ConfigurableApplicationContext, org.springframework.context.ApplicationContext, org.springframework.context.Lifecycle, org.springframework.beans.factory.ListableBeanFactory, org.springframework.beans.factory.HierarchicalBeanFactory, org.springframework.context.MessageSource, org.springframework.context.ApplicationEventPublisher, org.springframework.core.io.support.ResourcePatternResolver, org.springframework.beans.factory.BeanFactory, org.springframework.core.io.ResourceLoader, org.springframework.beans.factory.DisposableBean} 
 DEBUG | xtenderThread-42 | try-xmpp                         | ?                                   ? | ServiceEvent REGISTERED 
 DEBUG | xtenderThread-42 | BlueprintListener                | actory$SpringApplicationListener  140 | Spring app state changed to Started for bundle 93 
 DEBUG | xtenderThread-42 | BlueprintListener                | actory$SpringApplicationListener  140 | Spring app state changed to Started for bundle 93 
 INFO  | xtenderThread-42 | ContextLoaderListener            | BundleApplicationContextListener   45 | Application context successfully refreshed (OsgiBundleXmlApplicationContext(bundle=try-xmpp, config=osgibundle:/META-INF/spring/*.xml)) 
 INFO  | foo              | Tracer                           | rg.apache.camel.processor.Logger   88 | 8f812d3e-15cc-4ca3-b6c0-4077beef75bc >>> (route30) from(timer://foo?delay=10000&fixedRate=true&period=10000) --> setBody[Hello, i am message] <<< Pattern:InOnly, Headers:{firedTime=Thu Feb 11 12:18:13 EET 2010}, BodyType:null, Body:null 
 INFO  | foo              | Tracer                           | rg.apache.camel.processor.Logger   88 | 8f812d3e-15cc-4ca3-b6c0-4077beef75bc >>> (route30) setBody[Hello, i am message] --> xmpp://wsbise@websphere/masny@websphere?password=123456789 <<< Pattern:InOnly, Headers:{firedTime=Thu Feb 11 12:18:13 EET 2010}, BodyType:String, Body:Hello, i am message 
 DEBUG | foo              | XmppPrivateChatProducer          | ent.xmpp.XmppPrivateChatProducer   46 | Creating XmppPrivateChatProducer to participant masny@websphere 
 DEBUG | foo              | CachedIntrospectionResults       | beans.CachedIntrospectionResults  151 | Not strongly caching class [org.apache.camel.management.mbean.ManagedProducer] because it is not cache-safe 
 DEBUG | foo              | DefaultManagementAgent           | anagement.DefaultManagementAgent  304 | Registered MBean with objectname: org.apache.camel:context=masny/camelContext,type=producers,name=XmppPrivateChatProducer(0x11c0dc6) 
 DEBUG | foo              | XmppPrivateChatProducer          | pache.camel.impl.DefaultProducer   66 | Starting producer: Producer[xmpp://wsbise@websphere/masny@websphere?password=123456789] 
 DEBUG | foo              | ProducerCache                    | .apache.camel.impl.ProducerCache  211 | Adding to producer cache with key: Endpoint[xmpp://wsbise@websphere/masny@websphere?password=123456789] for producer: Producer[xmpp://wsbise@websphere/masny@websphere?password=123456789] 
 DEBUG | foo              | DefaultErrorHandler              | rg.apache.camel.processor.Logger  197 | Failed delivery for exchangeId: 8f812d3e-15cc-4ca3-b6c0-4077beef75bc. On delivery attempt: 0 caught: java.lang.NullPointerException 
 DEBUG | foo              | DefaultErrorHandler              | processor.RedeliveryErrorHandler  411 | This exchange is not handled so its marked as failed: Exchange[Message: Hello, i am message] 
 DEBUG | foo              | Pipeline                         | .apache.camel.processor.Pipeline   99 | Message exchange has failed so breaking out of pipeline: Exchange[Message: Hello, i am message] Exception: java.lang.NullPointerException 
 ERROR | foo              | TimerConsumer                    | rg.apache.camel.processor.Logger  248 | 
java.lang.NullPointerException 
        at org.jivesoftware.smackx.muc.MultiUserChat$1.connectionCreated(MultiUserChat.java:84) 
        at org.jivesoftware.smack.XMPPConnection.initConnection(XMPPConnection.java:887) 
        at org.jivesoftware.smack.XMPPConnection.connectUsingConfiguration(XMPPConnection.java:834) 
        at org.jivesoftware.smack.XMPPConnection.connect(XMPPConnection.java:1276) 
        at org.apache.camel.component.xmpp.XmppEndpoint.createConnection(XmppEndpoint.java:140) 
        at org.apache.camel.component.xmpp.XmppPrivateChatProducer.process(XmppPrivateChatProducer.java:52) 
        at org.apache.camel.processor.SendProcessor$1.doInProducer(SendProcessor.java:97) 
        at org.apache.camel.processor.SendProcessor$1.doInProducer(SendProcessor.java:95) 
        at org.apache.camel.impl.ProducerCache.doInProducer(ProducerCache.java:146) 
        at org.apache.camel.processor.SendProcessor.doProcess(SendProcessor.java:94) 
        at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:82) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67) 
        at org.apache.camel.processor.DelegateProcessor.processNext(DelegateProcessor.java:53) 
        at org.apache.camel.processor.DelegateProcessor.proceed(DelegateProcessor.java:82) 
        at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:162) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67) 
        at org.apache.camel.processor.RedeliveryErrorHandler.processExchange(RedeliveryErrorHandler.java:223) 
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:153) 
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:91) 
        at org.apache.camel.processor.DefaultErrorHandler.process(DefaultErrorHandler.java:49) 
        at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:206) 
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:74) 
        at org.apache.camel.processor.UnitOfWorkProcessor.processNext(UnitOfWorkProcessor.java:54) 
        at org.apache.camel.processor.DelegateProcessor.process(DelegateProcessor.java:48) 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67) 
        at org.apache.camel.component.timer.TimerConsumer.sendTimerExchange(TimerConsumer.java:103) 
        at org.apache.camel.component.timer.TimerConsumer$1.run(TimerConsumer.java:50) 
        at java.util.TimerThread.mainLoop(Timer.java:512) 
        at java.util.TimerThread.run(Timer.java:462) 

On Felix installed all requirement bundles (camel-osgi, camel-xmpp, etc).

Full project attached in topic
http://old.nabble.com/camel-xmpp-not-sends-message-in-Felix-Karaf-td27544316.html",slewis,pilgr,Major,Closed,Fixed,11/Feb/10 13:37,24/Apr/11 09:57
Bug,CAMEL-2475,12487238,HTTP (Stream) to File only writes part of InputStream to File,"The follwoing route only writes part of the http input stream to the file output (always
in chunks of 1,2,4 or 8KB).:

               <route id=""RestToTempDir"">
                       <from uri=""jetty:http://0.0.0.0:8162/muyrl"" />
                       <setHeader headerName=""CamelFileExchangeFile"">
                               <header>message.id</header>
                       </setHeader>
                       <to
                               uri=""file://c:/temp/?fileName=${date:now:yyyyMMdd}/asm-${id}.xml"" />
                       <setBody>
                               <header>CamelFileExchangeFile</header>
                       </setBody>
               </route>

There are workarounds, but I would change streams to always read until EOF (if possible)

Possible workaround 1:
<convertBodyTo type=""java.lang.String""/> to force it to convert the stream to string before writing.

Possible workaround 2 is to write to a queue first

               <route id=""RestToTempDir"">
                       <from uri=""jetty:http://0.0.0.0:8162/myurl"" />
                       <inOnly uri=""activemq:topic=TempTopic"" />
                       <setBody>
                               <header>message.id</header>
                       </setBody>
                       <setHeader headerName=""CamelFileExchangeFile"">
                               <header>message.id</header>
                       </setHeader>
               </route>
               <route id=""TempAssemblyToTempArchiveDirectory"">
                       <from uri=""activemq:topic:TempTopic"" />
                       <setHeader headerName=""CamelFileExchangeFile"">
                               <header>message.id</header>
                       </setHeader>
                       <to
                               uri=""file://c:/xmlarchive_noxslt/?fileName=${date:now:yyyyMMdd}/asm-${id}.xml""/>
               </route>",njiang,toelen,Major,Closed,Fixed,16/Feb/10 21:44,24/Apr/11 10:01
Bug,CAMEL-2476,12487014,Camel Velocity: change the case of all fields properties Header,"Using the velocity component doesn't respect the case matching of all fields of the header.

For example: the exchange of properties CorrelationID change the case matching, after using velocity component, it puts the property in lowercase correlationid.",muellerc,abdellatif,Minor,Closed,Fixed,17/Feb/10 09:01,24/Apr/11 10:00
Bug,CAMEL-2477,12486978,Camel Freemarker: change the case of all fields properties Header,"Using the Freemarker component doesn't respect the case matching of all fields of the header.

For example: the exchange of properties CorrelationID change the case matching, after using Freemarker component, it puts the property in lowercase correlationid.",muellerc,abdellatif,Minor,Closed,Fixed,17/Feb/10 09:04,24/Apr/11 10:01
Bug,CAMEL-2478,12487021,Camel Stringtemplate : lose all fields properties,the component Camel-stringtemplate loses all fields properties after the generation of Result.,muellerc,abdellatif,Minor,Closed,Fixed,17/Feb/10 09:09,24/Apr/11 10:01
Bug,CAMEL-2484,12486727,camel-mina - Using close session could potentially cause memory to be not released,"See nabble
http://old.nabble.com/SocketSessionImpl-in-Mina-component-retained-in-memory-indefinitely-ts27624487.html",slewis,davsclaus,Major,Closed,Fixed,18/Feb/10 09:21,24/Apr/11 10:01
Bug,CAMEL-2486,12487085,DefaultCamelContext throws NPE in start() for route with no destination,"Not easy to track down when you have a lot of routes.  This might be new with the introduction of StartupRouteHolder (I think that was introduced in 2.1, right?)

{code}
public class TriggerNPETest extends CamelTestSupport {
	@Override
	protected RouteBuilder createRouteBuilder() throws Exception {
        return new RouteBuilder() {
            public void configure() {
            	// This will generate an NPE on DefaultCamelContext.start() (line 973)
            	// If this is an invalid route a proper message should be given far before 
            	// this and/or NPE should be tested on line 973
            	from(""direct:psd.nowhere"");
            }
        };
	}
	@Test
	public void testme() throws Exception {
		Thread.sleep(10000);
	}
}
{code}

java.lang.NullPointerException
	at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:973)
	at org.apache.camel.test.junit4.CamelTestSupport.startCamelContext(CamelTestSupport.java:145)
	at org.apache.camel.test.junit4.CamelTestSupport.setUp(CamelTestSupport.java:98)

{code}
            // now start the inputs for all the route services as we have prepared Camel
            // yeah open the floods so messages can start flow into Camel
            for (Map.Entry<Integer, StartupRouteHolder> entry : inputs.entrySet()) {
                Integer order = entry.getKey();
                Route route = entry.getValue().getRoute();  //  <<<-------  Right here.
{code}",davsclaus,mbmather,Minor,Closed,Fixed,18/Feb/10 20:37,24/Apr/11 10:01
Bug,CAMEL-2491,12487217,Cannot deploy camel-core in OSGI-runtime using Spring 3.0,"It is not possible to start a bundle with a camelContext in dm-server 2.0 while using camel-core version 2.2.0, because camel-core requires Spring packages of version below 3.0. This seems like an issue to me, since the release notes of version 2.2.0 suggest that it works with version 3.0 of Spring. 
 
Environment: Spring dm-server 2.0 (comes pre-packaged with Spring 3.0.0.RELEASE)
Steps to reproduce the problem:
- start dm-server
- install bundle camel-core-2.2.0.jar
- install a bundle containing any kind of camelContext
Error: 
Caused by: com.springsource.kernel.osgi.framework.ExtendedClassNotFoundException: org.springframework.jmx.export.metadata.JmxAttributeSource in KernelBundleClassLoader: [bundle=org.apache.camel.camel-core_2.2.0]
(full details of stack trace have been attached)

Cause: 
The version range for the JMX spring packages in the MANIFEST.MF:
org.springframework.jmx.export.annotation;resolution:=optional;version=""[2.5,3)""
,org.springframework.jmx.export.assembler;resolution:=optional;version=""[2.5,3)""
,org.springframework.jmx.export.metadata;resolution:=optional;version=""[2.5,3)"" 

As a workaround I disabled the jmxAgent in my camel-context and changed the 
version range to [2.5,4) in the MANIFEST.MF file in my camel-core-2.2.0.jar.
I redeployed the camel-core bundle. After that my bundle started without any problems. 

Proposed solution:
Change the version range for Spring packages in camel-core from [2.5,3) to [2.5,4)",njiang,michiel.eggermont@gmail.com,Major,Closed,Fixed,22/Feb/10 10:48,24/Apr/11 10:01
Bug,CAMEL-2495,12487219,Application SOAP fault support in camel-cxf PAYLOAD mode,Camel-cxf component does not seem to support application SOAP fault in PAYLOAD mode.,wtam,wtam,Major,Closed,Fixed,23/Feb/10 18:54,24/Apr/11 10:01
Bug,CAMEL-2496,12486755,StreamCache/Splitter race condition,"Attached patch introduces a test that shows the following symptom:

org.apache.camel.RuntimeCamelException: java.io.FileNotFoundException: /var/folders/vC/vCA487MkHEeSaIu9LmlkrU+++TI/-Tmp-/camel-tmp-627613/cos3755307044377901165.tmp (No such file or directory)
org.apache.camel.RuntimeCamelException: java.io.FileNotFoundException: /var/folders/vC/vCA487MkHEeSaIu9LmlkrU+++TI/-Tmp-/camel-tmp-627613/cos3755307044377901165.tmp (No such file or directory)
	at org.apache.camel.converter.stream.FileInputStreamCache.reset(FileInputStreamCache.java:52)
	at org.apache.camel.converter.stream.StreamCacheConverter$StreamSourceCache.reset(StreamCacheConverter.java:126)
	at org.apache.camel.util.MessageHelper.resetStreamCache(MessageHelper.java:105)
	at org.apache.camel.builder.xml.XPathBuilder.getDocument(XPathBuilder.java:548)
	at org.apache.camel.builder.xml.XPathBuilder.evaluateAs(XPathBuilder.java:428)
	at org.apache.camel.builder.xml.XPathBuilder.evaluate(XPathBuilder.java:118)
	at org.apache.camel.processor.Splitter.createProcessorExchangePairs(Splitter.java:72)
	at org.apache.camel.processor.MulticastProcessor.process(MulticastProcessor.java:155)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:80)
	at org.apache.camel.processor.DeadLetterChannel.process(DeadLetterChannel.java:189)
	at org.apache.camel.processor.DeadLetterChannel.process(DeadLetterChannel.java:133)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)
	at org.apache.camel.processor.interceptor.StreamCachingInterceptor.proceed(StreamCachingInterceptor.java:87)
	at org.apache.camel.processor.interceptor.StreamCachingInterceptor.process(StreamCachingInterceptor.java:82)
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:52)
	at org.apache.camel.component.seda.SedaConsumer.run(SedaConsumer.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)

I believe it must be a race condition when several threads are passing through the splitter, in the above case I'm using a seda queue with 5 threads.  Am logging this now as I'm not sure what time I'll have to investigate further this week.",njiang,slewis,Major,Closed,Fixed,24/Feb/10 15:38,24/Apr/11 10:00
Bug,CAMEL-2498,12487082,Inconsitent definition of resequencer DSL,One of the {{ProcessorDefinition}} methods is still has the old {{resequencer}} name from Camel 1.x. Should be renamed to {{resequence}}.,mrt1nz,mrt1nz,Minor,Closed,Fixed,25/Feb/10 05:48,24/Apr/11 10:00
Bug,CAMEL-2509,12486416,[jboss-camel]  Cannot find class 'WEB-INF/lib/camel-core-2.2.0.jar/org/apache/camel/component/bean/ParameterMappingStrategy.class' in any classloaders,"Using jboss-extra class JBossPackageScanClassResolver, i could not get the converter loaded.

I was using camel 2.2.0 and JBoss 5.1

The error was :
Cannot find class 'WEB-INF/lib/camel-core-2.2.0.jar/org/apache/camel/component/bean/ParameterMappingStrategy.class' in any classloaders: [BaseClassLoader@107e733{vfszip:/D:/JBoss/jboss-5.1.0.GA/server/testWS/deploy/router-component-02-0.0.2-SNAPSHOT.war/}, org.jboss.web.tomcat.service.WebCtxLoader$ENCLoader@1ea6b44]

Looking at the JBossPackageScanClassResolver source class, i changed :

public void visit(VirtualFile file) {

  if (file.getName().endsWith("".class"")) {
    String fqn = file.getPathName();
    addIfMatching(filter, fqn.substring(""/"") + 1), classes);
  }
}
with :

public void visit(VirtualFile file) {

  if (file.getName().endsWith("".class"")) {
    String fqn = file.getPathName();
    addIfMatching(filter, fqn.substring(fqn.indexOf(""jar/"") + 4), classes);
  }
}

as the fqn was /WEB-INF/lib/camel-core-2.2.0.jar/org/apache/camel/component/bean/ParameterMappingStrategy.class instead of /org/apache/camel/component/bean/ParameterMappingStrategy.class
It's probably different from different version of JBoss ?
Hope this help sombody...",davsclaus,gribo,Major,Closed,Fixed,01/Mar/10 12:35,24/Apr/11 10:01
Bug,CAMEL-2510,12484711,Mixing jetty/http in a route screws up the URI used by HttpClient,"Below test shows the Http producer can't build up right HttpRequest URI as a bridgeEndpoint.

{code}
   public class JettyHttpTest extends CamelTestSupport {

    private String targetProducerUri = ""http://localhost:8542/someservice?bridgeEndpoint=true&throwExceptionOnFailure=false"";
    private String targetConsumerUri = ""jetty:http://localhost:8542/someservice?matchOnUriPrefix=true"";
    private String sourceUri = ""jetty:http://localhost:6323/myservice?matchOnUriPrefix=true"";
    private String sourceProducerUri = ""http://localhost:6323/myservice"";

    @Test
    public void testGetRootPath() throws Exception {
        MockEndpoint mock = getMockEndpoint(""mock:result"");
        mock.expectedBodiesReceived(""Hi! /someservice"");

        template.sendBody(""direct:root"", """");

        assertMockEndpointsSatisfied();
    }
    
    @Test
    public void testGetWithRelativePath() throws Exception {
        MockEndpoint mock = getMockEndpoint(""mock:result"");
        mock.expectedBodiesReceived(""Hi! /someservice/relative"");
        
        template.sendBody(""direct:relative"", """");
        assertMockEndpointsSatisfied();
        
    }

    @Override
    protected RouteBuilder createRouteBuilder() throws Exception {
        return new RouteBuilder() {
            @Override
            public void configure() throws Exception {

                from(targetConsumerUri)
                    .process(new Processor() {
                        public void process(Exchange exchange) throws Exception {
                            String path = exchange.getIn().getHeader(Exchange.HTTP_PATH, String.class);
                            exchange.getOut().setBody(""Hi! "" + path);
                        }   
                    });

                from(sourceUri)
                    .to(targetProducerUri);

                from(""direct:root"")
                    .to(sourceProducerUri)
                    .to(""mock:result"");
                
                from(""direct:relative"")
                    .to(sourceProducerUri + ""/relative"")
                    .to(""mock:result"");

            }
        };
    }
}
{code}",njiang,njiang,Major,Closed,Fixed,01/Mar/10 13:45,24/Apr/11 10:00
Bug,CAMEL-2512,12484995,Camel-soap jar does not contain the generated classes,"Seems I forgot to add the generated soap package to the exported packages. So these classes are not packed into the jar.
",njiang,cschneider,Major,Closed,Fixed,01/Mar/10 23:03,18/Mar/10 19:12
Bug,CAMEL-2514,12486446,karaf features.xml  should use servicemix commons-pool bundle ,"as commons-pool/commons-pool/1.5.4 osgi manifest header isn't correct, which shouldn't import org.apache.commons.pool package.
SMX4-491[1] create servicemix wrapper bundle to fix this issue, so the karaf features.xml should use servicemix wrapper bundle
[1]https://issues.apache.org/activemq/browse/SMX4-491",njiang,ffang,Major,Closed,Fixed,02/Mar/10 07:37,17/Jan/11 16:50
Bug,CAMEL-2528,12484736,should use servicemix jaxws 2.1 spec bundle but not the geronimo one,"otherwise will see the exception like 
javax.xml.ws.spi.FactoryFinder$ConfigurationError: Provider org.apache.axis2.jaxws.spi.Provider not found
at javax.xml.ws.spi.FactoryFinder$2.run(FactoryFinder.java:135)
at javax.xml.ws.spi.FactoryFinder.doPrivileged(FactoryFinder.java:264)
at javax.xml.ws.spi.FactoryFinder.newInstance(FactoryFinder.java:122)
at javax.xml.ws.spi.FactoryFinder.access$200(FactoryFinder.java:42)
at javax.xml.ws.spi.FactoryFinder$3.run(FactoryFinder.java:255)
at javax.xml.ws.spi.FactoryFinder.doPrivileged(FactoryFinder.java:264)
at javax.xml.ws.spi.FactoryFinder.find(FactoryFinder.java:165)
at javax.xml.ws.spi.Provider.provider(Provider.java:39)
at javax.xml.ws.Service.<init>(Service.java:36)",njiang,ffang,Major,Closed,Fixed,08/Mar/10 03:36,24/Apr/11 10:01
Bug,CAMEL-2529,12487201,ConsumerTemplate don't support the jms uri with selector,"This unit test can shows the issue that selector option don't work for ConsumerTemplate
{code}
@Test
    public void testConsumerTemplate() throws Exception {
        template.sendBodyAndHeader(""activemq:queue:consumer"", ""Message1"", ""SIZE_NUMBER"", 1505);
        template.sendBodyAndHeader(""activemq:queue:consumer"", ""Message3"", ""SIZE_NUMBER"", 1300);
        template.sendBodyAndHeader(""activemq:queue:consumer"", ""Message2"", ""SIZE_NUMBER"", 1600);

        // process every exchange which is ready. If no exchange is left break
        // the loop
        while (true) {
            Exchange ex = consumer.receiveNoWait(""activemq:queue:consumer?selector=SIZE_NUMBER<1500"");
            if (ex != null) {
                Message message = ex.getIn();
                int size = message.getHeader(""SIZE_NUMBER"", int.class);
                assertTrue(""The message header SIZE_NUMBER should be less than 1500"", size < 1500);
                assertEquals(""The message body is wrong"", ""Message3"", message.getBody());
            } else {
                break;
            }
        }

    }

{code}

And here is [mail thread|http://old.nabble.com/activemq-component-and-selector-ts27813752.html] which discusses about it.",njiang,njiang,Major,Closed,Fixed,08/Mar/10 10:18,24/Apr/11 10:00
Bug,CAMEL-2540,12486602,async route not working with Mina,"here is my route:

		from(""mina:tcp://localhost:6202?textline=true&sync=true"").process(new Processor() {
		    public void process(Exchange exchange) throws Exception {
		        String body = exchange.getIn().getBody(String.class);
				Thread.sleep(5000);
		        exchange.getOut().setBody(""Bye 1"" + body);
		    }
		});
		
		from(""file:///test/test/response"")
		.convertBodyTo(String.class)//.threads(1)
		.toAsync(""mina:tcp://localhost:6202?sync=true&textline=true"",10)
		.to(""log:+++ reply++++""); 

here is the output, you can see all reply logs have a same message body and async process is releasing all threads at the first response, this is probably Mina component bug in the context of aync route.


2010-03-11 10:21:16,405 [Camel thread 11: FileComponent] FileConsumer                   DEBUG Total 3 files to consume
2010-03-11 10:21:16,405 [Camel thread 11: FileComponent] FileConsumer                   DEBUG About to process file: GenericFile[C:\test\test\response\Copy (2) of New Text Document (3).txt] using exchange: Exchange[GenericFileMessage with file: GenericFile[C:\test\test\response\Copy (2) of New Text Document (3).txt]]
2010-03-11 10:21:16,452 [Camel thread 11: FileComponent] DefaultManagementAgent         DEBUG Registered MBean with objectname: org.apache.camel:context=tamlft0spj1/cameltest,type=producers,name=MinaProducer(0x2e6c66)
2010-03-11 10:21:16,452 [Camel thread 11: FileComponent] MinaProducer                   DEBUG Starting producer: Producer[mina://tcp://localhost:6202?sync=true&textline=true]
2010-03-11 10:21:16,452 [Camel thread 11: FileComponent] ProducerCache                  DEBUG Adding to producer service pool with key: Endpoint[mina://tcp://localhost:6202?sync=true&textline=true] for producer: Producer[mina://tcp://localhost:6202?sync=true&textline=true]
2010-03-11 10:21:16,452 [2: SendAsyncProcessor-Producer] MinaProducer                   DEBUG Creating connector to address: localhost/127.0.0.1:6202 using connector: org.apache.mina.transport.socket.nio.SocketConnector@16cefa8 timeout: 30000 millis.
2010-03-11 10:21:16,452 [Camel thread 11: FileComponent] FileConsumer                   DEBUG About to process file: GenericFile[C:\test\test\response\Copy of New Text Document (3).txt] using exchange: Exchange[GenericFileMessage with file: GenericFile[C:\test\test\response\Copy of New Text Document (3).txt]]
2010-03-11 10:21:16,467 [3: SendAsyncProcessor-Producer] MinaProducer                   DEBUG Creating connector to address: localhost/127.0.0.1:6202 using connector: org.apache.mina.transport.socket.nio.SocketConnector@16cefa8 timeout: 30000 millis.
2010-03-11 10:21:16,467 [Camel thread 11: FileComponent] FileConsumer                   DEBUG About to process file: GenericFile[C:\test\test\response\New Text Document (3).txt] using exchange: Exchange[GenericFileMessage with file: GenericFile[C:\test\test\response\New Text Document (3).txt]]
2010-03-11 10:21:16,467 [4: SendAsyncProcessor-Producer] MinaProducer                   DEBUG Creating connector to address: localhost/127.0.0.1:6202 using connector: org.apache.mina.transport.socket.nio.SocketConnector@16cefa8 timeout: 30000 millis.
2010-03-11 10:21:16,499 [SocketConnectorIoProcessor-0.1] ExecutorFilter                 DEBUG Launching thread for localhost/127.0.0.1:6202
2010-03-11 10:21:16,499 [ SocketAcceptorIoProcessor-0.0] ExecutorFilter                 DEBUG Launching thread for /127.0.0.1:3362
2010-03-11 10:21:16,499 [ SocketAcceptorIoProcessor-0.1] ExecutorFilter                 DEBUG Launching thread for /127.0.0.1:3363
2010-03-11 10:21:16,499 [SocketConnectorIoProcessor-0.0] ExecutorFilter                 DEBUG Launching thread for localhost/127.0.0.1:6202
2010-03-11 10:21:16,499 [ SocketAcceptorIoProcessor-0.2] ExecutorFilter                 DEBUG Launching thread for /127.0.0.1:3374
2010-03-11 10:21:16,499 [SocketConnectorIoProcessor-0.2] ExecutorFilter                 DEBUG Launching thread for localhost/127.0.0.1:6202
2010-03-11 10:21:16,514 [amel thread 22: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for /127.0.0.1:3363
2010-03-11 10:21:16,514 [amel thread 24: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for /127.0.0.1:3362
2010-03-11 10:21:16,514 [amel thread 23: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for localhost/127.0.0.1:6202
2010-03-11 10:21:16,514 [2: SendAsyncProcessor-Producer] MinaProducer                   DEBUG Writing body : hello2
2010-03-11 10:21:16,514 [3: SendAsyncProcessor-Producer] MinaProducer                   DEBUG Writing body : hello3
2010-03-11 10:21:16,514 [amel thread 26: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for /127.0.0.1:3374
2010-03-11 10:21:16,514 [amel thread 25: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for localhost/127.0.0.1:6202
2010-03-11 10:21:16,514 [amel thread 27: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for localhost/127.0.0.1:6202
2010-03-11 10:21:16,514 [4: SendAsyncProcessor-Producer] MinaProducer                   DEBUG Writing body : hello1
2010-03-11 10:21:16,514 [SocketConnectorIoProcessor-0.2] ExecutorFilter                 DEBUG Launching thread for localhost/127.0.0.1:6202
2010-03-11 10:21:16,514 [3: SendAsyncProcessor-Producer] MinaProducer                   DEBUG Waiting for response
2010-03-11 10:21:16,530 [2: SendAsyncProcessor-Producer] MinaProducer                   DEBUG Waiting for response
2010-03-11 10:21:16,530 [4: SendAsyncProcessor-Producer] MinaProducer                   DEBUG Waiting for response
2010-03-11 10:21:16,530 [ SocketAcceptorIoProcessor-0.2] ExecutorFilter                 DEBUG Launching thread for /127.0.0.1:3374
2010-03-11 10:21:16,530 [amel thread 27: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for localhost/127.0.0.1:6202
2010-03-11 10:21:16,530 [amel thread 26: MinaThreadPool] MinaConsumer                   DEBUG Received body: hello2
2010-03-11 10:21:21,530 [amel thread 26: MinaThreadPool] MinaConsumer                   DEBUG Writing body: Bye 1hello2
2010-03-11 10:21:21,530 [SocketConnectorIoProcessor-0.2] ExecutorFilter                 DEBUG Launching thread for localhost/127.0.0.1:6202
2010-03-11 10:21:21,530 [amel thread 27: MinaThreadPool] MinaProducer                   DEBUG Message received: Bye 1hello2
2010-03-11 10:21:21,530 [amel thread 27: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for localhost/127.0.0.1:6202
2010-03-11 10:21:21,530 [amel thread 26: MinaThreadPool] MinaConsumer                   DEBUG Received body: hello3
2010-03-11 10:21:21,530 [:6202?sync=true&textline=true]] SendProcessor                  DEBUG Async reply received now routing the Exchange: Exchange[Message: Bye 1hello2]
2010-03-11 10:21:21,530 [:6202?sync=true&textline=true]] SendProcessor                  DEBUG Async reply received now routing the Exchange: Exchange[Message: Bye 1hello2]
2010-03-11 10:21:21,530 [:6202?sync=true&textline=true]] SendProcessor                  DEBUG Async reply received now routing the Exchange: Exchange[Message: Bye 1hello2]
2010-03-11 10:21:21,546 [:6202?sync=true&textline=true]] DefaultManagementAgent         DEBUG Registered MBean with objectname: org.apache.camel:context=tamlft0spj1/cameltest,type=producers,name=Producer(0x16321e6)
2010-03-11 10:21:21,546 [:6202?sync=true&textline=true]] ProcessorEndpoint$1            DEBUG Starting producer: Producer[log://+++ reply++++]
2010-03-11 10:21:21,546 [:6202?sync=true&textline=true]] ProducerCache                  DEBUG Adding to producer cache with key: Endpoint[log://+++ reply++++] for producer: Producer[log://+++ reply++++]
2010-03-11 10:21:21,546 [:6202?sync=true&textline=true]] +++ reply++++                  INFO  Exchange[BodyType:String, Body:Bye 1hello2]
2010-03-11 10:21:21,546 [:6202?sync=true&textline=true]] GenericFileOnCompletion        DEBUG Done processing file: GenericFile[C:\test\test\response\Copy (2) of New Text Document (3).txt] using exchange: Exchange[Message: Bye 1hello2]
2010-03-11 10:21:21,546 [:6202?sync=true&textline=true]] +++ reply++++                  INFO  Exchange[BodyType:String, Body:Bye 1hello2]
2010-03-11 10:21:21,546 [:6202?sync=true&textline=true]] GenericFileOnCompletion        DEBUG Done processing file: GenericFile[C:\test\test\response\Copy of New Text Document (3).txt] using exchange: Exchange[Message: Bye 1hello2]
2010-03-11 10:21:21,546 [:6202?sync=true&textline=true]] FileUtil                       DEBUG Tried 1 to delete file: C:\test\test\response\Copy (2) of New Text Document (3).txt.camelLock with result: true
2010-03-11 10:21:21,561 [:6202?sync=true&textline=true]] +++ reply++++                  INFO  Exchange[BodyType:String, Body:Bye 1hello2]
2010-03-11 10:21:21,561 [:6202?sync=true&textline=true]] GenericFileOnCompletion        DEBUG Done processing file: GenericFile[C:\test\test\response\New Text Document (3).txt] using exchange: Exchange[Message: Bye 1hello2]
2010-03-11 10:21:21,561 [:6202?sync=true&textline=true]] FileUtil                       DEBUG Tried 1 to delete file: C:\test\test\response\Copy of New Text Document (3).txt.camelLock with result: true
2010-03-11 10:21:21,561 [:6202?sync=true&textline=true]] nericFileRenameProcessStrategy DEBUG Renaming file: GenericFile[C:\test\test\response\Copy (2) of New Text Document (3).txt] to: GenericFile[C:\test\test\response\.camel\Copy (2) of New Text Document (3).txt]
2010-03-11 10:21:21,561 [:6202?sync=true&textline=true]] FileUtil                       DEBUG Tried 1 to delete file: C:\test\test\response\New Text Document (3).txt.camelLock with result: true
2010-03-11 10:21:21,561 [:6202?sync=true&textline=true]] nericFileRenameProcessStrategy DEBUG Renaming file: GenericFile[C:\test\test\response\Copy of New Text Document (3).txt] to: GenericFile[C:\test\test\response\.camel\Copy of New Text Document (3).txt]
2010-03-11 10:21:21,561 [:6202?sync=true&textline=true]] nericFileRenameProcessStrategy DEBUG Renaming file: GenericFile[C:\test\test\response\New Text Document (3).txt] to: GenericFile[C:\test\test\response\.camel\New Text Document (3).txt]
2010-03-11 10:21:21,561 [:6202?sync=true&textline=true]] FileUtil                       DEBUG Tried 1 to rename file: C:\test\test\response\Copy (2) of New Text Document (3).txt to: C:\test\test\response\.camel\Copy (2) of New Text Document (3).txt with result: true
2010-03-11 10:21:21,561 [:6202?sync=true&textline=true]] FileUtil                       DEBUG Tried 1 to rename file: C:\test\test\response\Copy of New Text Document (3).txt to: C:\test\test\response\.camel\Copy of New Text Document (3).txt with result: true
2010-03-11 10:21:21,577 [:6202?sync=true&textline=true]] FileUtil                       DEBUG Tried 1 to rename file: C:\test\test\response\New Text Document (3).txt to: C:\test\test\response\.camel\New Text Document (3).txt with result: true
2010-03-11 10:21:26,530 [amel thread 26: MinaThreadPool] MinaConsumer                   DEBUG Writing body: Bye 1hello3
2010-03-11 10:21:26,530 [amel thread 26: MinaThreadPool] MinaConsumer                   DEBUG Received body: hello1
2010-03-11 10:21:26,530 [SocketConnectorIoProcessor-0.2] ExecutorFilter                 DEBUG Launching thread for localhost/127.0.0.1:6202
2010-03-11 10:21:26,530 [amel thread 27: MinaThreadPool] MinaProducer                   DEBUG Message received: Bye 1hello3
2010-03-11 10:21:26,530 [amel thread 27: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for localhost/127.0.0.1:6202
2010-03-11 10:21:31,531 [amel thread 26: MinaThreadPool] MinaConsumer                   DEBUG Writing body: Bye 1hello1
2010-03-11 10:21:31,531 [SocketConnectorIoProcessor-0.2] ExecutorFilter                 DEBUG Launching thread for localhost/127.0.0.1:6202
2010-03-11 10:21:31,531 [amel thread 26: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for /127.0.0.1:3374
2010-03-11 10:21:31,531 [amel thread 27: MinaThreadPool] MinaProducer                   DEBUG Message received: Bye 1hello1
2010-03-11 10:21:31,531 [amel thread 27: MinaThreadPool] ExecutorFilter                 DEBUG Exiting since queue is empty for localhost/127.0.0.1:6202
",davsclaus,skusma@arccorp.com,Major,Closed,Fixed,11/Mar/10 15:33,24/Apr/11 10:01
Bug,CAMEL-2544,12486478,Application SOAP 12 fault support in camel-cxf PAYLOAD mode,This issue is related to https://issues.apache.org/activemq/browse/CAMEL-2495.  We need to make sure SOAP 1.2 Fault works.,wtam,wtam,Major,Closed,Fixed,12/Mar/10 19:22,24/Apr/11 10:00
Bug,CAMEL-2545,12486721,Camel-Mail: Alternative body part does not handle charset,Mail component does not set charset for alternative body part. See - http://old.nabble.com/Camel-Mail:-Alternative-part-does-not-handle-charset--td27882178.html for more information.,njiang,tide08,Major,Closed,Fixed,12/Mar/10 22:51,24/Apr/11 10:01
Bug,CAMEL-2551,12487124,File component does not correctly handle PipedInputStream in message body.,"Streams that do not have their contents length at immediate disposal, like PipedInputStream, are not processed correctly by the file component.

\\

{code}
    private void writeFileByStream(InputStream in, File target) throws IOException {
        FileChannel out = null;
        try {
            out = prepareOutputFileChannel(target, out);

            if (LOG.isTraceEnabled()) {
                LOG.trace(""Using InputStream to transfer from: "" + in + "" to: "" + out);
            }
            int size = endpoint.getBufferSize();
            byte[] buffer = new byte[size];
            ByteBuffer byteBuffer = ByteBuffer.wrap(buffer);
            while (true) {
                int count = in.read(buffer);
                if (count <= 0) {
                    break;
                } else if (count < size) {
                    byteBuffer = ByteBuffer.wrap(buffer, 0, count);
                    out.write(byteBuffer);
                    break;
                } else {
                    out.write(byteBuffer);
                    byteBuffer.clear();
                }
            }
        } finally {
            ObjectHelper.close(in, target.getName(), LOG);
            ObjectHelper.close(out, target.getName(), LOG);
        }
    }

{code}

The code 

{code}
                } else if (count < size) {
                    byteBuffer = ByteBuffer.wrap(buffer, 0, count);
                    out.write(byteBuffer);
                    break;
                } else {
{code}

does not take into account that bytes read can be less than the size of the buffer passed into the InputStream.read method and stream can still have more content. The only indication that EOF was reached is -1 returned from the read method according to Java API.",njiang,anydoby,Major,Closed,Fixed,15/Mar/10 11:17,24/Apr/11 10:01
Bug,CAMEL-2554,12487069,IndexOutOfBoundsException occur in UnsafeUriCharactersEncoder.class,Here is the [mail thread|http://old.nabble.com/IndexOutOfBoundsException-occur-in-UnsafeUriCharactersEncoder.class-tp27919179p27919179.html] for the detail discussion.,njiang,njiang,Major,Closed,Fixed,16/Mar/10 15:58,17/Jan/11 16:50
Bug,CAMEL-2559,12487072,"HttpComponent only checks the registry for an HttpClientConfigurer, setHttpClientConfigurer no longer works.","Basically you can't currently do this:

{code}
            public void configure() {

                ProxyHttpClientConfigurer configurer = new ProxyHttpClientConfigurer(
                        ""proxyhost"",
                        80,
                        ""user"",
                        ""password"",
                        null,
                        null);

                getContext().getComponent(""http"", HttpComponent.class).setHttpClientConfigurer(configurer);

                from(""direct:start"")
                    .to(""http://www.google.com/search"");
            }
{code}

HttpComponent only looks in the registry for an HttpClientConfigurer.",slewis,slewis,Major,Closed,Fixed,18/Mar/10 15:04,24/Apr/11 10:01
Bug,CAMEL-2566,12487068,camel-http component should set Transfer-Encoding as chunked header for response message when checkChunked is true,"so that the client side which send request will know this is a chunked message.
It's important when the response http headers is more than 4096 and we want to use chunked response message
",davsclaus,ffang,Major,Closed,Fixed,22/Mar/10 08:58,24/Apr/11 10:00
Bug,CAMEL-2575,12486943,CXFRS Routing in 2.2.0 does not behave like in 2.0.0,"CXFRS Routing in Camel 2.0.0 works  well with these endPoints:

<cxf:rsServer id=""restRouter"" address=""/restRouter/""	
      serviceClass=""com.project.service.impl.ServiceManagerImpl""   />
      
     <cxf:rsClient id=""restEndpoint"" address=""http://localhost:8080/services/rest""
      serviceClass=""com.project.service.impl.ServiceManagerImpl"" />

In Camel 2.2.0, Routing fails with error causing the CXF Client to invoke a 404 not found Rest Service which is 'http://localhost:8080/services/rest/restRouter/Path'.

The address of cxf:rsClient is being appended by the cxf:rsServer's address",njiang,jejmaster,Major,Closed,Fixed,26/Mar/10 03:00,24/Apr/11 10:01
Bug,CAMEL-2577,12487252,doTry .. doCatch should disabled nested child error handlers,"Follow route use a default-error-handler for the route, and also defined  a
custom exception handle logic using try-catch-pattern.

But if the ""myProcessRaiseError"" raised exception the catch-clause cannot be
invoked; Why? Is it camel's bug?

{code:xml}

<route>
     <from uri=""ws:... ""/>
        <bean ...>
       <doTry>
        <filter>
               <simple>${body} == 'firstChoice'</simple>
                     <bean ref=""myProcess"" />
                     <bean ref=""myProcessRaiseError""/>
             </filter>
               <doCatch>
                       ..some error handler...
               </doCatch>
       </doTry>
    <process ...>
</route>
{code}",davsclaus,davsclaus,Minor,Closed,Fixed,26/Mar/10 07:10,24/Apr/11 10:01
Bug,CAMEL-2601,12486987,Stream documentation contains invalid sample,"On the following page: http://camel.apache.org/stream.html

In last example relating to monitoring an open file (like tail) the line is incorrect...
from(""stream:file?fileName=/server/logs/server.log?scanStream=true&scanStreamDelay=1000"").to(""bean:logService?method=parseLogLine"");

should be
from(""stream:file?fileName=/server/logs/server.log&scanStream=true&scanStreamDelay=1000"").to(""bean:logService?method=parseLogLine"");

There is a question mark after the filename field whereas this should actually be an ampersand. Unfortunately I don't have edit rights on the page to update it myself.",njiang,grahamrb,Minor,Closed,Fixed,30/Mar/10 04:58,24/Apr/11 10:01
Bug,CAMEL-2605,12487286,Asynchronous processing in DLC endpoint breaks message handling,"When using an asynchronous processor to handle the message in a deadletterchannel, the async message handling is broken (not all AsyncCallbacks get invoked correctly), causing the exchange never to be terminated correctly - e.g. the call to sendBody will never return.
",gertvanthienen,gertvanthienen,Major,Closed,Fixed,31/Mar/10 12:59,17/Jan/11 16:50
Bug,CAMEL-2614,12487332,Camel-cxf can't handle multi part message in PAYLOAD mode,"If I have a multi part (bare) message defined in WSDL (such as follow), camel-cxf component cannot parse the incoming message in PAYLOAD mode.

{code}
	<wsdl:message name=""MultiPartStringIntRequest"">
		<wsdl:part name=""StringDefaultInput"" element=""MultiPartStringInt:StringDefaultInputElem"" />
		<wsdl:part name=""IntParamIn"" element=""MultiPartStringInt:IntParamInElem"" />
	</wsdl:message>
{code}

I get a stack trace similar to this.

Mar 12, 2010 7:15:48 PM org.apache.camel.component.cxf.interceptors.AbstractMessageInInterceptor createDOMMessage
INFO: AbstractMessageInInterceptor Converting Stax Stream to DOM
Mar 12, 2010 7:15:48 PM org.apache.camel.component.cxf.interceptors.AbstractMessageInInterceptor handleMessage
INFO: AbstractRoutingMessageInInterceptor Infer BindingOperationInfo.
Mar 12, 2010 7:15:48 PM org.apache.cxf.phase.PhaseInterceptorChain doIntercept
WARNING: Interceptor has thrown exception, unwinding now
org.apache.cxf.interceptor.Fault: Could not read request. Operation {urn:MultiPart/resources/wsdl/MultiPartStringInt/MultiPartStringInt/types}StringDefaultInputElem is unknown.
                at org.apache.camel.component.cxf.interceptors.AbstractMessageInInterceptor.handleMessage(AbstractMessageInInterceptor.java:103)
                at org.apache.camel.component.cxf.interceptors.DOMInInterceptor.handleMessage(DOMInInterceptor.java:43)
                at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:236)
                at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:109)
                at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.serviceRequest(JettyHTTPDestination.java:312)
                at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.doService(JettyHTTPDestination.java:276)
                at org.apache.cxf.transport.http_jetty.JettyHTTPHandler.handle(JettyHTTPHandler.java:70)
                at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
                at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
                at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
                at org.mortbay.jetty.Server.handle(Server.java:326)
                at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:536)
                at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:930)
                at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:747)
                at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)
                at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:405)
                at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
                at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",wtam,wtam,Major,Closed,Fixed,05/Apr/10 21:16,24/Apr/11 10:01
Bug,CAMEL-2615,12487182,Karaf features.xml camel-juel can't work well within ServiceMix ,"As servicemix install the Pax-web jsp support, which exports the javax.el package with the version 2.1.0, and camel-juel imports the javax.el with version [1.0,2), so the camel-juel feature can't be installed in ServiceMix 4.2.0.
Because javax.el is introduced with the jsp 2.1, so we need to update the camel-juel jaxax.el imports version at the same time.
",njiang,njiang,Major,Closed,Fixed,06/Apr/10 04:00,24/Apr/11 10:01
Bug,CAMEL-2621,12487050,File consumer - Polling from network share on Windows may regard files as not a file,"{code}
file.isFile()
{code}

May return {{false}} on Windows if consuming from a network share etc. So we should just regard anything that is *not* a directory as a file.",davsclaus,davsclaus,Major,Closed,Fixed,07/Apr/10 08:45,24/Apr/11 10:01
Bug,CAMEL-2622,12487130,Invocation of hasNext() on org.apache.camel.util.ObjectHelper.createIterator(...).new Iterator<Node>() {...} return different results,"{code}
                    public boolean hasNext() {
                        // empty string should not be regarded as having next
                        return ++idx == 0 && ObjectHelper.isNotEmpty(s);
                    }

                    public String next() {
                        return s;
                    }
{code}

This is incorrect. Should be:

{code}
                    public boolean hasNext() {
                        // empty string should not be regarded as having next
                        return idx+1 == 0 && ObjectHelper.isNotEmpty(s);
                    }

                    public String next() {
						idx++;
                        return s;
                    }
{code}
",davsclaus,anydoby,Minor,Closed,Fixed,07/Apr/10 09:45,24/Apr/11 10:01
Bug,CAMEL-2629,12487347,Aggregator - Completion by batch consumer should trigger completion for the batch group if multiple correlation keys was used,"Yeah its really an odd use case if you use multiple correlation keys for the same batch consumer, but its still possible to do.",davsclaus,davsclaus,Minor,Closed,Fixed,11/Apr/10 07:09,24/Apr/11 10:00
Bug,CAMEL-2633,12487350,camel-http - Endpoint options should not change the http component configured options,"If a http endpoint is using {{httpBindingRef}} option to use a special binding, then it would change the binding on the http component, which means than any new http endpoints created thereafter will use what binding that aforementioned endpoint was using.",davsclaus,davsclaus,Major,Closed,Fixed,12/Apr/10 07:11,24/Apr/11 10:00
Bug,CAMEL-2635,12487393,Combining .delay() and .backOffMultiplier() in an errorhandler gives error that is ,"
When doing the following in a route:

	.onException(HaltOperationsError.class)
	
			.maximumRedeliveries(10)
			.backOffMultiplier(10) // 1 s base	
			
			.handled(true)
			.delay(10)			
			.log(""Halting operations for some time"")
			//.process(new RouteStoppingProccesor(""PostProcessorRoute""))
			.end()

I get the following stacktrace:
ERROR ContextLoader                  - Context initialization failed
org.apache.camel.RuntimeCamelException: org.apache.camel.FailedToStartRouteException: java.util.NoSuchElementException
	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1075)
	at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:103)
	at org.apache.camel.spring.CamelContextFactoryBean.onApplicationEvent(CamelContextFactoryBean.java:652)
	at org.springframework.context.event.SimpleApplicationEventMulticaster$1.run(SimpleApplicationEventMulticaster.java:78)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:76)
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:274)
	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:736)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:383)
	at org.springframework.web.context.ContextLoader.createWebApplicationContext(ContextLoader.java:255)
	at org.springframework.web.context.ContextLoader.initWebApplicationContext(ContextLoader.java:199)
	at org.springframework.web.context.ContextLoaderListener.contextInitialized(ContextLoaderListener.java:45)
	at org.mortbay.jetty.handler.ContextHandler.startContext(ContextHandler.java:548)
	at org.mortbay.jetty.servlet.Context.startContext(Context.java:136)
	at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1250)
	at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:517)
	at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:467)
	at org.mortbay.jetty.plugin.Jetty6PluginWebAppContext.doStart(Jetty6PluginWebAppContext.java:115)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)
	at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)
	at org.mortbay.jetty.Server.doStart(Server.java:224)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.plugin.Jetty6PluginServer.start(Jetty6PluginServer.java:132)
	at org.mortbay.jetty.plugin.AbstractJettyMojo.startJetty(AbstractJettyMojo.java:441)
	at org.mortbay.jetty.plugin.AbstractJettyMojo.execute(AbstractJettyMojo.java:383)
	at org.mortbay.jetty.plugin.AbstractJettyRunMojo.execute(AbstractJettyRunMojo.java:210)
	at org.mortbay.jetty.plugin.Jetty6RunMojo.execute(Jetty6RunMojo.java:184)
	at org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginManager.java:490)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:694)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeStandaloneGoal(DefaultLifecycleExecutor.java:569)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:539)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:387)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:348)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:180)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:328)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:138)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:362)
	at org.apache.maven.cli.compat.CompatibleMain.main(CompatibleMain.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)
	at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)
	at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)
	at org.codehaus.classworlds.Launcher.main(Launcher.java:375)
Caused by: org.apache.camel.FailedToStartRouteException: java.util.NoSuchElementException
	at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:1044)
	at org.apache.camel.spring.SpringCamelContext.maybeStart(SpringCamelContext.java:203)
	at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:101)
	... 49 more
Caused by: java.util.NoSuchElementException
	at java.util.AbstractList$Itr.next(AbstractList.java:350)
	at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:1028)
	... 51 more


I think this is a minor error, but it would be nice to know if something could be done with it so that the errormessage is easier to understand.",davsclaus,tarjei@scanmine.com,Minor,Closed,Fixed,12/Apr/10 09:11,24/Apr/11 10:00
Bug,CAMEL-2636,12487289,IOException: Bad file descriptor and FileNotFoundException,"When I try to stream BINARY (pdf) file using camel-http I get the java.io.IOException: Bad file descriptor
The pdf isn't recieved succesfully by reciever (0kb)

This seems to be caused by a bug in java (on linux systems), closing inputstream twice causes problems. It seemed to me this is exactly what is happening, see also link:
http://256.com/gray/docs/misc/java_bad_file_descriptor_close_bug.shtml

I fixed this by (checking out apache camel-core and camel-http 2.2.0):
In FileInputStreamCache.java:
In method close() wrapped getInputStream().close() in if:
if (stream != null && stream instanceof FileInputStream && ((FileInputStream) stream).getChannel().isOpen()) {
getInputStream().close() ;
}

In method reset() also:
if (stream != null && stream instanceof FileInputStream && ((FileInputStream) stream).getChannel().isOpen()) {
getInputStream().close() ;
}


Second I needed to fix a filenotfoundexception, the tempfile created by camel was deleted to early.
I changed CachedOutputStream.java
- Reimplemented constructor:
public CachedOutputStream(Exchange exchange) {
        String hold = exchange.getContext().getProperties().get(THRESHOLD);
        String dir = exchange.getContext().getProperties().get(TEMP_DIR);
        if (hold != null) {
            this.threshold = exchange.getContext().getTypeConverter().convertTo(Long.class, hold);
        }
        if (dir != null) {
            this.outputDir = exchange.getContext().getTypeConverter().convertTo(File.class, dir);
        }

        // add on completion so we can cleanup after the exchange is done such
        // as deleting temporary files
        exchange.addOnCompletion(new SynchronizationAdapter() {
            @Override
            public void onDone(Exchange exchange) {
                try {
                    // close the stream and FileInputStreamCache
                    // close();
                    // for (FileInputStreamCache cache : fileInputStreamCaches)
                    // {
                    // cache.close();
                    // }
                    // cleanup temporary file
                    if (tempFile != null) {
                        System.err.println(""####################################################"");
                        System.err.println(""DISABLED tempFile.delete:89"");
                        System.err.println(""####################################################"");
                        // boolean deleted = tempFile.delete();
                        // if (!deleted) {
                        // LOG.warn(""Cannot delete temporary cache file: "" +
                        // tempFile);
                        // } else if (LOG.isTraceEnabled()) {
                        // LOG.trace(""Deleted temporary cache file: "" +
                        // tempFile);
                        // }
                        tempFile = null;
                    }
                } catch (Exception e) {
                    LOG.warn(""Error deleting temporary cache file: "" + tempFile, e);
                }
            }

            @Override
            public String toString() {
                return ""OnCompletion[CachedOutputStream]"";
            }
        });
    }

Reimplemented close():
public void close() throws IOException {
        System.err.println(""####################################################"");
        System.err.println(""outputStream.close:119 -> delete tempFile"");
        System.err.println(""####################################################"");
        new Exception().printStackTrace();
        currentStream.close();
        boolean deleted = tempFile.delete();
        if (!deleted) {
            LOG.warn(""Cannot delete temporary cache file: "" + tempFile);
        } else if (LOG.isTraceEnabled()) {
            LOG.trace(""Deleted temporary cache file: "" + tempFile);
        }
    }
",njiang,rdomingo,Major,Closed,Fixed,12/Apr/10 15:17,24/Apr/11 10:01
Bug,CAMEL-2637,12486167,ErrorHandler - errorHandlerRef should use mandatory lookup to fail if id is not found in Registry,To fail if end user have mistyped an id,davsclaus,davsclaus,Major,Closed,Fixed,12/Apr/10 17:25,24/Apr/11 10:01
Bug,CAMEL-2638,12487386,Restlet component is URL encoding the POST message body.  It should encode it based on content-type request header.,"I attempted to POST a JSON document to couchdb via restlet.  The post fails with and ""Invalid JSON format"" error from couchdb.  This is because the POST data was being URL encoded which substitutes %XX sequences for all the curly braces.  I believe the encoding should be done based on the content-type header, or possibly not at all for POST requests.  ",wtam,richbolen,Major,Closed,Fixed,12/Apr/10 19:26,24/Apr/11 10:01
Bug,CAMEL-2639,12487348,Some examples does not run with ANT,"See nabble
http://old.nabble.com/camel-example-etl-and-ant-won%27t-run-ts28213644.html",davsclaus,davsclaus,Minor,Closed,Fixed,13/Apr/10 06:21,24/Apr/11 10:01
Bug,CAMEL-2640,12487373,file component - Fix recursive and noop not picking up files with similar name in sibling folders,"See nabble
http://old.nabble.com/File-consumer-with-noop%3Dtrue-recursive%3Dtrue-ts28229501.html",davsclaus,davsclaus,Minor,Closed,Fixed,13/Apr/10 15:15,24/Apr/11 10:01
Bug,CAMEL-2642,12486758,Inconsistency between IntrospectionSupport.getProperties() and IntrospectionSupport.getProperty(),"IntrospectionSupport.getProperties() and IntrospectionSupport.getProperty() work in an inconsistency way:

{code}
ExampleBean bean = new ExampleBean();
Date date = new Date(0);
bean.setDate(date);

assertSame(date, IntrospectionSupport.getProperty(bean, ""date"")); // succeed

Map<String, Object> map = new HashMap<String, Object>();
IntrospectionSupport.getProperties(bean, map, null);
assertSame(date, map.get(""date"")); // fails
{code}",davsclaus,muellerc,Minor,Closed,Fixed,14/Apr/10 09:26,24/Apr/11 10:01
Bug,CAMEL-2649,12487405,FactoryBeans for ProducerTemplate and ConsumerTemplate should be singleton to avoid,"You want the {{ProducerTemplate}} with the assigned id to be a singleton scoped instance, so its shared.

Currently it creates a new instance which it should not.",davsclaus,davsclaus,Major,Closed,Fixed,15/Apr/10 10:58,24/Apr/11 10:00
Bug,CAMEL-2653,12487950,Route with only an endpoint defined causes app failure with only a vague stack trace,"I recently upgraded an application from Camel 2.0 to Camel 2.2 and was greeted with this stack trace:

{code}
org.apache.camel.RuntimeCamelException: org.apache.camel.FailedToStartRouteException: java.util.NoSuchElementException
	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1055)
	at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:103)
	at org.apache.camel.spring.CamelContextFactoryBean.onApplicationEvent(CamelContextFactoryBean.java:593)
	at org.springframework.context.event.SimpleApplicationEventMulticaster$1.run(SimpleApplicationEventMulticaster.java:78)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:76)
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:274)
	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:736)
	at org.springframework.osgi.context.support.AbstractOsgiBundleApplicationContext.finishRefresh(AbstractOsgiBundleApplicationContext.java:235)
	at org.springframework.osgi.context.support.AbstractDelegatedExecutionApplicationContext$4.run(AbstractDelegatedExecutionApplicationContext.java:358)
	at org.springframework.osgi.util.internal.PrivilegedUtils.executeWithCustomTCCL(PrivilegedUtils.java:85)
	at org.springframework.osgi.context.support.AbstractDelegatedExecutionApplicationContext.completeRefresh(AbstractDelegatedExecutionApplicationContext.java:320)
	at org.springframework.osgi.extender.internal.dependencies.startup.DependencyWaiterApplicationContextExecutor$CompleteRefreshTask.run(DependencyWaiterApplicationContextExecutor.java:136)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.apache.camel.FailedToStartRouteException: java.util.NoSuchElementException
	at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:939)
	at org.apache.camel.spring.SpringCamelContext.maybeStart(SpringCamelContext.java:197)
	at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:101)
	... 12 more
Caused by: java.util.NoSuchElementException
	at java.util.AbstractList$Itr.next(AbstractList.java:350)
	at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:923)
	... 14 more
{code}

I attached a debugger and walked through DefaultCamelContext to locate the (admittedly useless) route that caused the problem, commented it out, and was on my way. However, it seems like this could be trapped and reported better to the user and save the next poor guy some time. 

I also noticed that there's no useful TRACE or DEBUG output in here that would have allowed anyone to easily debug it.

{code:title=Example}
<route>
    <from uri=""activemq:queue:somequeue"" />
</route>
{code}

I believe this issue is related to CAMEL-2635.",davsclaus,cott@internetstaff.com,Major,Closed,Fixed,16/Apr/10 16:44,24/Apr/11 10:00
Bug,CAMEL-2662,12487357,ftp - exception thrown from begin should remove the file from in progress so the file can be polled on subsequent calls,"The SFTP component can throw an exception in its begin logic, which causes Camel to not remove the file from its internal in progress cache.

See nabble
http://old.nabble.com/SFTP-rename-problems-ts28254146.html",davsclaus,davsclaus,Major,Closed,Fixed,21/Apr/10 05:11,24/Apr/11 10:01
Bug,CAMEL-2663,12487328,DefaultPackageScanClassResolver can't read entries in an eclipse rcp app because it can't handle bundleresource - protocol,"scenario:

I have an eclipse rcp based app and use the apache camel libs within my plugin, i.e. the libs are one classpath of the plugin. I use a route using camel-mail to process some mailqueues.

CamelContext context = new DefaultCamelContext();
context.addRoutes(new RouteBuilder() {
    public void configure()	{
        from(""imaps://mailserver?username=user&password=password&consumer.delay=5000&delete=false&unseen=true"")
            .to(""log: new mail"");
   }});
   context.start();

When I start my app all seems to be ok. But in my log I get a lot of error messages regarding the loading of converters. If I ask my context to find a converter f.i. to converty to byte[] form inputstream (as I did see in another mail example reagding attachments) I get null because the context can't find one.

I debugged  deeper into the camel code and it seems that the DefaultPackageScanClassResolver can't load from urls starting with bundleresource like ""bundleresource://109.fwk32380043:4/org/apache/camel/component/file/""

using camel-osgi didn't worked because all camel libs are loaded as inner libs of my single plugin and not as plugins. (loading as plugins didn't worked but thats another problem/bug)

 
LOG Messages:

2010-04-21 11:15:34,829 DEBUG org.apache.camel.impl.converter.DefaultTypeConverter loadTypeConverters - Loading type converters ...
2010-04-21 11:15:34,829 DEBUG org.apache.camel.impl.DefaultPackageScanClassResolver findAnnotated - Searching for annotations of org.apache.camel.Converter in packages: [org.apache.camel.component.file, org.apache.camel.component.bean, org.apache.camel.converter, org.apache.camel.component.mail, org.apache.camel.component.spring.integration.converter]
2010-04-21 11:15:34,829 DEBUG org.apache.camel.impl.DefaultPackageScanClassResolver getClassLoaders - The thread context class loader: org.eclipse.core.runtime.internal.adaptor.ContextFinder@1a76eff  is used to load the class
2010-04-21 11:15:34,829 DEBUG org.apache.camel.impl.DefaultPackageScanClassResolver find - Searching for: annotated with @Converter in package: org/apache/camel/component/file using classloader: org.eclipse.core.runtime.internal.adaptor.ContextFinder
2010-04-21 11:15:39,954 DEBUG org.apache.camel.impl.DefaultPackageScanClassResolver getResources - Getting resource URL for package: org/apache/camel/component/file with classloader: org.eclipse.core.runtime.internal.adaptor.ContextFinder@1a76eff
2010-04-21 11:16:44,970 DEBUG org.apache.camel.impl.DefaultPackageScanClassResolver find - URL from classloader: bundleresource://109.fwk32380043:4/org/apache/camel/component/file/
2010-04-21 11:17:01,642 DEBUG org.apache.camel.impl.DefaultPackageScanClassResolver find - Decoded urlPath: /org/apache/camel/component/file/ with protocol: bundleresource
2010-04-21 11:23:59,814 DEBUG org.apache.camel.impl.DefaultPackageScanClassResolver find - isLocalFileSystem: false
2010-04-21 11:24:00,470 DEBUG org.apache.camel.impl.DefaultPackageScanClassResolver find - Scanning for classes in [/org/apache/camel/component/file/] matching criteria: annotated with @Converter
2010-04-21 11:24:16,079 DEBUG org.apache.camel.impl.DefaultPackageScanClassResolver find - Loading from jar using http/https: /org/apache/camel/component/file/
2010-04-21 11:24:26,626 DEBUG org.apache.camel.impl.DefaultPackageScanClassResolver find - Cannot read entries in url: bundleresource://109.fwk32380043:4/org/apache/camel/component/file/
java.net.MalformedURLException: no protocol: /org/apache/camel/component/file/
	at java.net.URL.<init>(URL.java:567)
	at java.net.URL.<init>(URL.java:464)
	at java.net.URL.<init>(URL.java:413)
	at org.apache.camel.impl.DefaultPackageScanClassResolver.find(DefaultPackageScanClassResolver.java:264)
	at org.apache.camel.impl.DefaultPackageScanClassResolver.find(DefaultPackageScanClassResolver.java:180)
	at org.apache.camel.impl.DefaultPackageScanClassResolver.findAnnotated(DefaultPackageScanClassResolver.java:100)
	at org.apache.camel.impl.converter.AnnotationTypeConverterLoader.load(AnnotationTypeConverterLoader.java:63)
	at org.apache.camel.impl.converter.DefaultTypeConverter.loadTypeConverters(DefaultTypeConverter.java:361)
	at org.apache.camel.impl.converter.DefaultTypeConverter.doStart(DefaultTypeConverter.java:384)
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:53)
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:49)
	at org.apache.camel.impl.DefaultCamelContext.startServices(DefaultCamelContext.java:1174)
	at org.apache.camel.impl.DefaultCamelContext.addService(DefaultCamelContext.java:644)
	at org.apache.camel.impl.DefaultCamelContext.getTypeConverter(DefaultCamelContext.java:691)
	at org.apache.camel.util.CamelContextHelper.convertTo(CamelContextHelper.java:68)
",njiang,cdittberner,Major,Closed,Fixed,21/Apr/10 10:21,24/Apr/11 10:01
Bug,CAMEL-2665,12487311,"Policy should handle lifecycle of wrapped processor, otherwise it will not be automatic started on startup","If you use policy to wrap a Processor, then the wrapped Processor is not automatic started/stopped as lifecycle cannot see this wrapped processor.

And to prevent end users from remembering to do this themselves we should let a WrapProcessor handle this for us.",davsclaus,davsclaus,Minor,Closed,Fixed,22/Apr/10 06:50,24/Apr/11 10:00
Bug,CAMEL-2675,12487371,camel-example-guicy-jms GuiceTest failed,"Here is the stack trace 
{code}
[//target/routeOutput?noop=true] GenericFileOnCompletion        ERROR Caused by: [org.apache.camel.CamelExecutionException - Exception occurred during execution on the exchange: Exchange[GenericFileMessage with file: GenericFile[message2.xml]]]
org.apache.camel.CamelExecutionException: Exception occurred during execution on the exchange: Exchange[GenericFileMessage with file: GenericFile[message2.xml]]
	at org.apache.camel.util.ObjectHelper.wrapCamelExecutionException(ObjectHelper.java:1107)
	at org.apache.camel.builder.ExpressionBuilder$25.evaluate(ExpressionBuilder.java:625)
	at org.apache.camel.impl.ExpressionAdapter.evaluate(ExpressionAdapter.java:36)
	at org.apache.camel.component.bean.MethodInfo$2.evaluate(MethodInfo.java:264)
	at org.apache.camel.component.bean.MethodInfo.createMethodInvocation(MethodInfo.java:131)
	at org.apache.camel.component.bean.BeanInfo.createInvocation(BeanInfo.java:167)
	at org.apache.camel.component.bean.BeanProcessor.process(BeanProcessor.java:127)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67)
	at org.apache.camel.processor.DelegateProcessor.processNext(DelegateProcessor.java:53)
	at org.apache.camel.processor.DelegateProcessor.proceed(DelegateProcessor.java:82)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:97)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67)
	at org.apache.camel.processor.RedeliveryErrorHandler.processExchange(RedeliveryErrorHandler.java:177)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:143)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:88)
	at org.apache.camel.processor.DefaultErrorHandler.process(DefaultErrorHandler.java:49)
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:228)
	at org.apache.camel.processor.UnitOfWorkProcessor.processNext(UnitOfWorkProcessor.java:70)
	at org.apache.camel.processor.DelegateProcessor.process(DelegateProcessor.java:48)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67)
	at org.apache.camel.component.file.GenericFileConsumer.processExchange(GenericFileConsumer.java:269)
	at org.apache.camel.component.file.GenericFileConsumer.processBatch(GenericFileConsumer.java:137)
	at org.apache.camel.component.file.GenericFileConsumer.poll(GenericFileConsumer.java:103)
	at org.apache.camel.impl.ScheduledPollConsumer.run(ScheduledPollConsumer.java:98)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:417)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:280)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:135)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:65)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:146)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:170)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:651)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:676)
	at java.lang.Thread.run(Thread.java:613)
Caused by: org.apache.camel.InvalidPayloadException: No body available of type: java.lang.String but has value: GenericFile[message2.xml] of type: org.apache.camel.component.file.GenericFile on: GenericFileMessage with file: GenericFile[message2.xml]. Caused by: No type converter available to convert from type: org.apache.camel.component.file.GenericFile to the required type: java.lang.String with value GenericFile[message2.xml]. Exchange[GenericFileMessage with file: GenericFile[message2.xml]]. Caused by: [org.apache.camel.NoTypeConversionAvailableException - No type converter available to convert from type: org.apache.camel.component.file.GenericFile to the required type: java.lang.String with value GenericFile[message2.xml]]
	at org.apache.camel.impl.MessageSupport.getMandatoryBody(MessageSupport.java:103)
	at org.apache.camel.builder.ExpressionBuilder$25.evaluate(ExpressionBuilder.java:623)
	... 31 more
Caused by: org.apache.camel.NoTypeConversionAvailableException: No type converter available to convert from type: org.apache.camel.component.file.GenericFile to the required type: java.lang.String with value GenericFile[message2.xml]
	at org.apache.camel.impl.converter.DefaultTypeConverter.mandatoryConvertTo(DefaultTypeConverter.java:124)
	at org.apache.camel.impl.MessageSupport.getMandatoryBody(MessageSupport.java:101)
	... 32 more
{code}",njiang,njiang,Major,Closed,Fixed,26/Apr/10 09:34,24/Apr/11 10:01
Bug,CAMEL-2678,12487360,Removing entries from FileIdempotentRepository don't get persisted,"Removing an entry (file name) programatically from a file idempotent repository has only effect on the cache and does not get persisted on the file repository. 
There are some situations in which one may need to rerun a file through a workflow and it makes neccessary to remove the file from the idempotent repository. A specific problem arises when upon a server restart, the file is not picked up by the workflow because the remove was not persisted/spooled on the disk.",davsclaus,fdehghani,Minor,Closed,Fixed,27/Apr/10 09:36,24/Apr/11 10:01
Bug,CAMEL-2687,12487331,exec component fails after receiving empty output,"I have a simple route here:

{quote}
public void configure() {
    from(""timer://foo?period=5000"")
    .to(""C:/apps/putty/PLINK.EXE?args=cadams@labmachine  -i C:/apps/putty/priv/chuck.ppk \""ls /tmp/foo\"""")
    .to(""log:experiments.cameltest"");
}
{quote}

This just runs a dummy command on a remote machine every five seconds.  Normally it has no problems as long as /tmp/foo has contents.  For example, I create 'bar' and 'baz' files, and the log reflects this:

{quote}
[                          main] DefaultCamelContext            INFO  Apache Camel 2.3-SNAPSHOT (CamelContext: camelContext) started in 734 millis
[                           foo] ExecProducer                   INFO  Executing ExecCommand [args=[cadams@labmachine, -i, C:/apps/putty/priv/chuck.ppk, ls /tmp/foo], executable=C:/apps/putty/PLINK.EXE, timeout=9223372036854775807, outFile=null, workingDir=null]
[                           foo] ExecProducer                   INFO  The command ExecCommand [args=[cadams@labmachine, -i, C:/apps/putty/priv/chuck.ppk, ls /tmp/foo], executable=C:/apps/putty/PLINK.EXE, timeout=9223372036854775807, outFile=null, workingDir=null] had exit value 0
[                           foo] cameltest                      INFO  Exchange[ExchangePattern:InOnly, BodyType:org.apache.camel.component.exec.ExecResult, Body:bar
baz
blah
]
[                           foo] ExecProducer                   INFO  Executing ExecCommand [args=[cadams@spamlab-bizintel-corpus01.eng.symantec.com, -i, C:/apps/putty/priv/chuck.ppk, ls /tmp/foo], executable=C:/apps/putty/PLINK.EXE, timeout=9223372036854775807, outFile=null, workingDir=null]
[                           foo] ExecProducer                   INFO  The command ExecCommand [args=[cadams@spamlab-bizintel-corpus01.eng.symantec.com, -i, C:/apps/putty/priv/chuck.ppk, ls /tmp/foo], executable=C:/apps/putty/PLINK.EXE, timeout=9223372036854775807, outFile=null, workingDir=null] had exit value 0
[                           foo] cameltest                      INFO  Exchange[ExchangePattern:InOnly, BodyType:org.apache.camel.component.exec.ExecResult, Body:bar
baz
blah
]
{quote}



And so on....  But if I delete both files, this is what happens:


{quote}
[                           foo] ExecProducer                   INFO  Executing ExecCommand [args=[cadams@labmachine, -i, C:/apps/putty/priv/chuck.ppk, ls /tmp/foo], executable=C:/apps/putty/PLINK.EXE, timeout=9223372036854775807, outFile=null, workingDir=null]
[                           foo] ExecProducer                   INFO  The command ExecCommand [args=[cadams@labmachine, -i, C:/apps/putty/priv/chuck.ppk, ls /tmp/foo], executable=C:/apps/putty/PLINK.EXE, timeout=9223372036854775807, outFile=null, workingDir=null] had exit value 0
[                           foo] ExecResultConverter            WARN  Received null stdout of the ExecResult for conversion!
[                           foo] DefaultTypeConverter           WARN  Overriding type converter from: StaticMethodTypeConverter: public static java.lang.String org.apache.camel.component.exec.ExecResultConverter.convertToString(org.apache.camel.component.exec.ExecResult,org.apache.camel.Exchange) throws java.io.FileNotFoundException to: org.apache.camel.impl.converter.ToStringTypeConverter@1e3d24a
[                           foo] cameltest                      INFO  Exchange[ExchangePattern:InOnly, BodyType:org.apache.camel.component.exec.ExecResult, Body:org.apache.camel.component.exec.ExecResult@d767dc]
{quote}


And from that point on, even if I add files to the directory again, I get nothing back, just the following log section over and over.  It seems that the behavior of suddenly switching the converter to ToStringTypeConverter had the effect of suppressing all output for good.  

{quote}
[                           foo] ExecProducer                   INFO  Executing ExecCommand [args=[cadams@labmachine, -i, C:/apps/putty/priv/chuck.ppk, ls /tmp/foo], executable=C:/apps/putty/PLINK.EXE, timeout=9223372036854775807, outFile=null, workingDir=null]
[                           foo] ExecProducer                   INFO  The command ExecCommand [args=[cadams@labmachine, -i, C:/apps/putty/priv/chuck.ppk, ls /tmp/foo], executable=C:/apps/putty/PLINK.EXE, timeout=9223372036854775807, outFile=null, workingDir=null] had exit value 0
[                           foo] cameltest                      INFO  Exchange[ExchangePattern:InOnly, BodyType:org.apache.camel.component.exec.ExecResult, Body:org.apache.camel.component.exec.ExecResult@3aacb4]
{quote}

This behavior is certainly unexpected and undesireable.  I've been working around this by making my remote script always produce some dummy output when they otherwise have nothing to return, but any workaround I could use on the Camel side pending a more permanent fix would be very much appreciated.
",davsclaus,chuck,Major,Closed,Fixed,30/Apr/10 15:54,24/Apr/11 10:00
Bug,CAMEL-2692,12487318,Multithreading bug: getBody sporadically returns null,"Note that the only workaround for this bug is to remove the parallelProcessing() call in the builder.

I have a simple route that processes a file by splitting on a tag and processing the DOM of each split message. The problem is that getBody is randomly returning null but ONLY when using the parallelProcessing feature of split. For some runs of the same XML file, the error does not occur at all (the file is about 2MB of data), others it will happen once or twice. I am currently using the latest 2.3-SNAPSHOT. 

Also note, that after detecting the null, I tried calling getBody(String.class) - this also may return null. Sometimes it does return the proper XML. 

Route configuration that reproduces the problem (my input XML is about 2MB with about 500 article tags): 

        public void configure() throws Exception { 
          from(""file:D:/inbox"") 
            .split(new XPathBuilder(""//article"")) 
              .parallelProcessing()                     // remove this line getBody below never returns null 
              .process(new Processor() { 
                public void process(Exchange exchange) throws Exception { 
                  Message inMessage = exchange.getIn(); 
                  org.w3c.dom.Document domDocument = inMessage.getBody(org.w3c.dom.Document.class); 
                  if (domDocument == null) { 
                    log(""Null body""); 
                  } else { 
                    // process DOM here 
                  } 
                } 
              }) 
            .end() 
        } 
      }); 
",davsclaus,scrutinizer,Major,Closed,Fixed,05/May/10 04:57,17/Jan/11 16:50
Bug,CAMEL-2694,12487329,HttpEntityConverter should not create a InputStreamEntity by using the GZIPHelper.toGZIPInputStream(),"GZIPHelper.toGZIPInputStream() is used for uncompress the zip date and not for compress the data.
",njiang,njiang,Major,Closed,Fixed,05/May/10 13:06,17/Jan/11 16:50
Bug,CAMEL-2698,12487333,camel-netty - NettyProducer should detect write failures as its async operation,"It should do as MinaHelper

See nabble
http://old.nabble.com/Recovery-From-Netty-Connection-Drop-ts28467631.html",davsclaus,davsclaus,Major,Closed,Fixed,06/May/10 09:07,24/Apr/11 10:00
Bug,CAMEL-2700,12487336,cxfbean component should ignore the wsdlLocation in the POJO,"cxfbean component should ignore the wsdlLocation in the annotation in the POJO when starting the service. It should be using the JAXWS bean to create the service. Came across this bug when I (mistakenly) set the wsdlLocation to the http one published by the service, which of course wasn't up yet and I got the exception below. Fix for this coming soon.

For Googlers out there, this ticket *may* fix this error message you may be getting:
{code}
Caused by: org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: cxfbean://jaxwsBean due to: Failed to create service.
    at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:437)
    at org.apache.camel.util.CamelContextHelper.getMandatoryEndpoint(CamelContextHelper.java:46)
    at org.apache.camel.model.RouteDefinition.resolveEndpoint(RouteDefinition.java:154)
    at org.apache.camel.impl.DefaultRouteContext.resolveEndpoint(DefaultRouteContext.java:109)
    at org.apache.camel.impl.DefaultRouteContext.resolveEndpoint(DefaultRouteContext.java:115)
    at org.apache.camel.model.SendDefinition.resolveEndpoint(SendDefinition.java:63)
    at org.apache.camel.model.SendDefinition.createProcessor(SendDefinition.java:57)
    at org.apache.camel.model.ToDefinition.createProcessor(ToDefinition.java:87)
    at org.apache.camel.model.ProcessorDefinition.makeProcessor(ProcessorDefinition.java:286)
    at org.apache.camel.model.ProcessorDefinition.addRoutes(ProcessorDefinition.java:114)
    at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:640)
    ... 32 more
Caused by: org.apache.cxf.service.factory.ServiceConstructionException: Failed to create service.
    at org.apache.cxf.wsdl11.WSDLServiceFactory.<init>(WSDLServiceFactory.java:93)
    at org.apache.cxf.service.factory.ReflectionServiceFactoryBean.buildServiceFromWSDL(ReflectionServiceFactoryBean.java:396)
    at org.apache.cxf.service.factory.ReflectionServiceFactoryBean.initializeServiceModel(ReflectionServiceFactoryBean.java:521)
    at org.apache.cxf.service.factory.ReflectionServiceFactoryBean.create(ReflectionServiceFactoryBean.java:271)
    at org.apache.cxf.jaxws.support.JaxWsServiceFactoryBean.create(JaxWsServiceFactoryBean.java:177)
    at org.apache.cxf.frontend.AbstractWSDLBasedEndpointFactory.createEndpoint(AbstractWSDLBasedEndpointFactory.java:100)
    at org.apache.cxf.frontend.ServerFactoryBean.create(ServerFactoryBean.java:105)
    at org.apache.cxf.jaxws.JaxWsServerFactoryBean.create(JaxWsServerFactoryBean.java:167)
    at org.apache.camel.component.cxf.cxfbean.CxfBeanEndpoint.createServer(CxfBeanEndpoint.java:104)
    at org.apache.camel.component.cxf.cxfbean.CxfBeanEndpoint.init(CxfBeanEndpoint.java:86)
    at org.apache.camel.component.cxf.cxfbean.CxfBeanComponent.createEndpoint(CxfBeanComponent.java:48)
    at org.apache.camel.impl.DefaultComponent.createEndpoint(DefaultComponent.java:78)
    at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:416)
    ... 42 more
Caused by: javax.wsdl.WSDLException: WSDLException: faultCode=PARSER_ERROR: java.lang.IllegalArgumentException: InputSource must have a ByteStream or CharacterStream
    at org.apache.cxf.wsdl11.WSDLManagerImpl.loadDefinition(WSDLManagerImpl.java:226)
    at org.apache.cxf.wsdl11.WSDLManagerImpl.getDefinition(WSDLManagerImpl.java:179)
    at org.apache.cxf.wsdl11.WSDLServiceFactory.<init>(WSDLServiceFactory.java:91)
    ... 54 more
Caused by: java.lang.IllegalArgumentException: InputSource must have a ByteStream or CharacterStream
    at org.apache.cxf.staxutils.StaxUtils.createXMLStreamReader(StaxUtils.java:983)
    at org.apache.cxf.wsdl11.WSDLManagerImpl.loadDefinition(WSDLManagerImpl.java:217)
    ... 56 more 
{code}",janstey,janstey,Major,Closed,Fixed,06/May/10 13:29,24/Apr/11 10:01
Bug,CAMEL-2708,12487322,File name lost when it starts with the same characters as the relative directory on the endpoint,"When polling file from a directory using a relative file URI, the file name gets lost when it starts with the same characters as the directory name.

E.g. a directory 'orders' containing 'orders-1719.xml' and 'orders-1819.xml'

{code}
from(""file:orders"").process(new Processor() {
  public void process(Exchange exchange) {
    // there's no file name on the message here (exchange.getIn().getHeader(Exchange.FILE_NAME) returns null)
  }
});
{code}",gertvanthienen,gertvanthienen,Major,Closed,Fixed,11/May/10 05:55,24/Apr/11 10:01
Bug,CAMEL-2711,12487323,DefaultRestletBinding should not treat all the post request as a Web from,"We got a user bug report from the [Fuse Forums|http://fusesource.com/forums/thread.jspa?threadID=1862&tstart=0].
After digging the code, I found the DefaultRestletBinding treat the all the request as a web form .  
It should check the request entity's MediaType to decide if the request entity is a web from.",njiang,njiang,Major,Closed,Fixed,12/May/10 01:54,24/Apr/11 10:01
Bug,CAMEL-2716,12486121,Incomplete cleanup of jmx mbeans after context stop,"There is a fair amount of cleanup on context stop, but it is incomplete. Most of it has to do with JMX and if one creates and starts a new CamelContext she may end up with managed objects pointing nowhere or have duplicates or other minor side-effects. One of the nasty side effects is that we are pretty much forced to use ""<forkMode>pertest</forkMode>"" in our surefire plugin configuration, ensuring a clean slate for every test.

For long running apps in which you may have multiple CamelContext running at various times during the lifetime of the application we must properly cleanup when stopping the context. I have a series of patches fixing that which I hope to finish before 2.3. Once we're comfortable and tested with multiple jdk implementations and versions, I'd change the forkMode to ""once"". In my local copy this reduces the test time in core from 40+ mins to under 15 mins. Since interactions between tests is still possible and might lead us on a wild-goose we could add another profile that uses the safer yet time consuming ""pertest"" forkMode.",hadrian,hadrian,Major,Closed,Fixed,12/May/10 19:14,24/Apr/11 09:58
Bug,CAMEL-2720,12487314,XmlConverter omits XML declaration,"org.apache.camel.converter.jaxp.XmlConverter is hardcoded to omit XML declaration when converting DOM document to string. Character encoding information is lost that way.

Ideally there should be a way to override default output properties. 

See:
http://fisheye6.atlassian.com/browse/camel/trunk/camel-core/src/main/java/org/apache/camel/converter/jaxp/XmlConverter.java?r=HEAD",njiang,dragisak,Major,Closed,Fixed,13/May/10 19:23,24/Apr/11 10:01
Bug,CAMEL-2722,12487313,ManagedRoute should be unregister when the RouteDefinition is removed,"Here is the mailing thread which discusses about it.
http://old.nabble.com/Dynamically-removing-routes-tp28548051p28548051.html",njiang,njiang,Major,Closed,Fixed,14/May/10 02:23,24/Apr/11 10:01
Bug,CAMEL-2725,12487309,String to Source type converter is not found in TypeConverterRegistry,"This causes a problem when you need to convert a {{String}} payload to a {{javax.xml.transformation.Source}} such as when using the XSTL component.

",davsclaus,davsclaus,Minor,Closed,Fixed,15/May/10 08:05,24/Apr/11 10:00
Bug,CAMEL-2732,12487294,SMPP component should set the final status header for delivery notifications,"The camel-smpp comonent should set the final status that is provided by the delivery receipt from the SMSC.

Currently the status header is not set, but can easily be added by making a call to smscDeliveryReceipt.getFinalStatus() and setting the header CamelSmppStatus on the camel message (as documented on the site [http://camel.apache.org/smpp.html].

Patch provided. ",njiang,jacovt,Major,Closed,Fixed,18/May/10 15:12,24/Apr/11 10:01
Bug,CAMEL-2739,12487283,Problem with Spring NamespaceHandler class org.apache.camel.spring.handler.CamelNamespaceHandler ,"I contributed spring Namespace Handler from my custom eclipse plug-in as follows:
<extension point=""org.springframework.ide.eclipse.beans.core.namespaces"">
		<namespace name=""Camel Spring Namespace Handler extension""
			namespaceHandler=""org.apache.camel.spring.handler.CamelNamespaceHandler""
			uri=""http://camel.apache.org/schema/spring"">
		</namespace>
</extension>

After this, I create spring.xml file using some camel elements and I validated using spring validator . Now whenever I edit the spring.xml file and save it I am getting following error message in error log.

org.springframework.beans.factory.BeanDefinitionStoreException: Unexpected exception parsing XML document from file [C:/workspace locations/New Folder (10)/config/spring.xml]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'template': Not implemented
at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:420)
at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:342)
at org.springframework.ide.eclipse.beans.core.internal.model.BeansConfig$2.loadBeanDefinitions(BeansConfig.java:326)
at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:310)
at org.springframework.ide.eclipse.beans.core.internal.model.BeansConfig$3.call(BeansConfig.java:358)
at org.springframework.ide.eclipse.beans.core.internal.model.BeansConfig$3.call(BeansConfig.java:1)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
at java.util.concurrent.FutureTask.run(FutureTask.java:123)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:417)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
at java.util.concurrent.FutureTask.run(FutureTask.java:123)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:651)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:676)
at java.lang.Thread.run(Thread.java:595)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'template': Not implemented
at org.springframework.ide.eclipse.beans.core.DefaultBeanDefinitionRegistry.removeBeanDefinition(DefaultBeanDefinitionRegistry.java:189)
at org.apache.camel.spring.handler.CamelNamespaceHandler.autoRegisterBeanDefinition(CamelNamespaceHandler.java:411)
at org.apache.camel.spring.handler.CamelNamespaceHandler.registerTemplates(CamelNamespaceHandler.java:364)
at org.apache.camel.spring.handler.CamelNamespaceHandler$CamelContextBeanDefinitionParser.doParse(CamelNamespaceHandler.java:272)
at org.springframework.beans.factory.xml.AbstractSingleBeanDefinitionParser.parseInternal(AbstractSingleBeanDefinitionParser.java:84)
at org.springframework.beans.factory.xml.AbstractBeanDefinitionParser.parse(AbstractBeanDefinitionParser.java:56)
at org.springframework.beans.factory.xml.NamespaceHandlerSupport.parse(NamespaceHandlerSupport.java:69)
at org.springframework.beans.factory.xml.BeanDefinitionParserDelegate.parseCustomElement(BeanDefinitionParserDelegate.java:1297)
at org.springframework.beans.factory.xml.BeanDefinitionParserDelegate.parseCustomElement(BeanDefinitionParserDelegate.java:1287)
at org.apache.camel.component.cxf.transport.spring.AbstractCamelContextBeanDefinitionParser.doParse(AbstractCamelContextBeanDefinitionParser.java:55)
at org.springframework.beans.factory.xml.AbstractSingleBeanDefinitionParser.parseInternal(AbstractSingleBeanDefinitionParser.java:84)
at org.springframework.beans.factory.xml.AbstractBeanDefinitionParser.parse(AbstractBeanDefinitionParser.java:56)
at org.springframework.beans.factory.xml.NamespaceHandlerSupport.parse(NamespaceHandlerSupport.java:69)
at org.springframework.beans.factory.xml.BeanDefinitionParserDelegate.parseCustomElement(BeanDefinitionParserDelegate.java:1297)
at org.springframework.beans.factory.xml.BeanDefinitionParserDelegate.parseCustomElement(BeanDefinitionParserDelegate.java:1287)
at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:135)
at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.registerBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:92)
at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.registerBeanDefinitions(XmlBeanDefinitionReader.java:507)
at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:398)



There seems to problem with NamespaceHandler class. 
",davsclaus,tejash_p_shah,Major,Closed,Fixed,19/May/10 13:05,24/Apr/11 10:01
Bug,CAMEL-2740,12484698,Using static queue as a reply queue in InOut pattern causes memory leak,"I am running JBoss, ActiveMQ and Camel for my application.   In the InOut pattern,  I am using a predefined static queue as a reply queue.   After running for a while,  the memory usage of JBoss keeps growing and growing until it hits outOfMemory error and app server is totally hung.   I monitor the thread in jconsole,  I can see the jms connection/session keeps growing and growing.   

But once I switch to use temp queue for InOut pattern,  this problem goes away.

",davsclaus,qingyi,Major,Resolved,Fixed,19/May/10 18:22,09/Nov/11 17:29
Bug,CAMEL-2741,12487281,GZIP/ZIP marshal/unmarshal leaves input file open,"Gzip/zip marshal/unmarshal leaves input files open, relying on the garbage collector to close the file. The cause sporadic file deletion exceptions in the following route:
          from(""file:incoming?delete=true"")
            .marshal().gzip()
            .to(""file:outgoing?fileName=${file:name}.gz"");

",davsclaus,scrutinizer,Minor,Closed,Fixed,20/May/10 02:31,24/Apr/11 10:01
Bug,CAMEL-2742,12487276,camel-jms - Sending to WebSphereMQ must use specific setBooleanProperty methods to set JMS properties,"This code in JMSBinding
{code}
                // must encode to safe JMS header name before setting property on jmsMessage
                String key = jmsKeyFormatStrategy.encodeKey(headerName);
                jmsMessage.setObjectProperty(key, value);
{code}

Should detect the value type and use the jmsMessage.setBooleanProperty() and so on. Otherwise IBM thrown an exception.

See nabble
http://old.nabble.com/jmsbinding-problem-ts28620489.html",davsclaus,davsclaus,Minor,Closed,Fixed,20/May/10 12:52,24/Apr/11 10:01
Bug,CAMEL-2743,12487605,HttpProducer should not sending the Http headers with low case,"Here is the mail thread which discusses about it.
http://old.nabble.com/camel-http-converts-headers-to-lower-case-tp28620717p28620717.html",njiang,njiang,Major,Closed,Fixed,20/May/10 13:46,24/Apr/11 09:57
Bug,CAMEL-2744,12487257,AggregationStrategy returning null causes NPE,"When the AggregationStrategy's aggregate method returns null, the AggregateProcessor throws an NPE. While this is a result of user error, I think it would be easy to provide a more useful error indicating what the true problem is. This is the (not very helpful) error the user sees:
{noformat}
ERROR | Caused by: [org.apache.camel.RuntimeCamelException - java.lang.NullPointerException]
org.apache.camel.RuntimeCamelException: java.lang.NullPointerException
	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1117)
	at org.apache.camel.component.jms.EndpointMessageListener.onMessage(EndpointMessageListener.java:104)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:543)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:482)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:451)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:323)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:261)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:982)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:974)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:876)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
	at org.apache.camel.processor.aggregate.AggregateProcessor.doAggregation(AggregateProcessor.java:227)
	at org.apache.camel.processor.aggregate.AggregateProcessor.process(AggregateProcessor.java:182)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67)
	at org.apache.camel.processor.DelegateProcessor.processNext(DelegateProcessor.java:53)
	at org.apache.camel.processor.DelegateProcessor.proceed(DelegateProcessor.java:82)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:97)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67)
	at org.apache.camel.processor.RedeliveryErrorHandler.processExchange(RedeliveryErrorHandler.java:185)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:151)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:89)
	at org.apache.camel.processor.DefaultErrorHandler.process(DefaultErrorHandler.java:49)
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:228)
	at org.apache.camel.processor.UnitOfWorkProcessor.processNext(UnitOfWorkProcessor.java:70)
	at org.apache.camel.processor.DelegateProcessor.process(DelegateProcessor.java:48)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67)
	at org.apache.camel.component.jms.EndpointMessageListener.onMessage(EndpointMessageListener.java:84)
	... 9 more
{noformat}",davsclaus,gim,Minor,Closed,Fixed,20/May/10 16:37,24/Apr/11 10:00
Bug,CAMEL-2745,12487278,ExpressionDefinition toString  print the expression and expression value at the same time,"When I running the test which need to call the ExpressionDefinition toString method, I found the expression is
{code}
    simple{bodyAs(java.lang.String.class)bodyAs[java.lang.String]}
{code}
We need to avoid adding the expression when the ExpressionDefinition has the expression value.
",njiang,njiang,Minor,Closed,Fixed,21/May/10 06:59,24/Apr/11 10:01
Bug,CAMEL-2748,12487274,camel-cxf feature doesn't include all cxf needs bundles,"Current camel-cxf features doesn't include wss4j, saaj-imple, opensaml, fastinfoset bundle.
We need to add these bundle into camel-cxf features.",njiang,njiang,Major,Closed,Fixed,24/May/10 07:06,24/Apr/11 10:01
Bug,CAMEL-2750,12487250,org.apache.camel.component.bean.BeanInfo not working properly with mocked Interfaces,"The BeanInfo-Class shows the same behaviour as ClassUtils-Class from Spring 3.0.x (see Spring Issue 7066, https://jira.springsource.org/browse/SPR-7066).
When working with mock objects created by mockito using an interface the getSuperclass()-Method returns java.lang.Object and the method to called on the mock can't be found. 

While looking around for a solution I found the path applied in Spring (see https://fisheye.springsource.org/browse/spring-framework/trunk/org.springframework.core/src/main/java/org/springframework/util/ClassUtils.java?content-type=text/vnd.viewcvs-markup&r1=3227&r2=3228).

In method 
public static Class<?> getUserClass(Class<?> clazz) 

the code

return (clazz != null && clazz.getName().contains(CGLIB_CLASS_SEPARATOR) ?  clazz.getSuperclass() : clazz);

is replaced by 

if (clazz != null && clazz.getName().contains(CGLIB_CLASS_SEPARATOR)) {
    Class<?> superClass = clazz.getSuperclass();
    if (superClass != null && !Object.class.equals(superClass)) {
         return superClass;
    }
}
return clazz;

While waiting for a fix in BeanInfo class a workaround is to mock the concrete class not the interface, but this makes the test code more complicated if several classes implement the same interface.

A Wor",davsclaus,dgsoft,Major,Closed,Fixed,25/May/10 14:26,24/Apr/11 10:01
Bug,CAMEL-2751,12487258,Timer Component is not Restartable,"When the TimerComponent is stopped, it cancels all of the Timer instances it has created, which is good. However, TimerEndpoint keeps a local reference to the Timer instance, so if the TimerComponent is restarted, the TimerEndpoint will throw an exception as its timer is no longer usable as it has been canceled.

This patch provides a unit test, TimerRestartTest, and an update to TimerComponent that fixes this issue. The TimerComponent fix is to keep a list of all TimerEndpoints that have a reference to a Timer instance created by the TimerComponent. When TimerComponent.doStop is called, those TimerEndpoint references to the now canceled Timer instances are cleared.",hadrian,scranton,Major,Closed,Fixed,25/May/10 18:58,24/Apr/11 10:00
Bug,CAMEL-2755,12487241,VM endpoints with same name do not communicate if args don't match,"If you send to a VM endpoint from one route and consume from the same endpoint in another route, but include an argument on only one of the routes, Camel sees them as two different routes.  Therefore, the messages are never consumed.  For example:

<camelContext id=""sendNotifyContext""
	xmlns=""http://camel.apache.org/schema/spring""
	errorHandlerRef=""errorHandler"">

      <route id=""sendToNotify"">
            <from uri=""..."" />
            ....
            *<to uri=""vm:myNotify"" />*
      </route>
</camelContext>

<camelContext id=""receiveNotifyContext""
	xmlns=""http://camel.apache.org/schema/spring""
	errorHandlerRef=""errorHandler"">

      <route id=""receiveNotify"">
            *<from uri=""vm:myNotify?size=2500"" />*
            ....
            <to uri=""..."" />
      </route>
</camelContext>


The producer appears to send to vm:myNotify while the consumer is listening to a separate endpoint named vm:myNotify?size=2500, so the messages build up and are never received.",davsclaus,ndjensen,Major,Closed,Fixed,26/May/10 21:59,24/Apr/11 10:01
Bug,CAMEL-2758,12487245,OnCompletion - Should use pipes and filters to ensure IN is OUT from last step,"OnCompletion will route the Exchange directly as is which means if you have set an OUT the first step in the onCompletion route may not use this OUT but the IN instead.

And also add option {{useOriginalBody}} so you can do work based on the original input instead.",davsclaus,davsclaus,Major,Closed,Fixed,27/May/10 05:23,24/Apr/11 10:00
Bug,CAMEL-2760,12485785,@Consume should run in an unit of work,,davsclaus,davsclaus,Major,Closed,Fixed,27/May/10 13:19,24/Apr/11 10:01
Bug,CAMEL-2762,12487945,Default Tracer configuration eats Mina ByteBuffer responses,"It appears that the only way to get UDP responses out of Camel Mina without binary ( > 7 bit) getting garbled by internal conversions is to reply with a Mina ByteBuffer.

Unfortunately, if you have Trace on, the DefaultTraceFormatter ends up calling MinaConverter.toByteArray, which ""consumes"" the ByteBuffer, setting it to empty, which means no reply goes out. :)
{code}
27/05/10 07:02:41:DEBUG:org.apache.camel.component.mina.MinaConsumer:Writing body: DirectBuffer[pos=11 lim=11 cap=16: empty]
{code}
I'm not sure how to fix this one cleanly, so I don't have a patch.  I might also be doing something wrong, if so I'm all ears. :)

Here's a trace:
{code}
	  at org.apache.camel.component.mina.MinaConverter.toByteArray(MinaConverter.java:44)
	  at org.apache.camel.component.mina.MinaConverter.toString(MinaConverter.java:49)
	  at sun.reflect.GeneratedMethodAccessor228.invoke(Unknown Source:-1)
	  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	  at java.lang.reflect.Method.invoke(Method.java:597)
	  at org.apache.camel.util.ObjectHelper.invokeMethod(ObjectHelper.java:736)
	  at org.apache.camel.impl.converter.StaticMethodTypeConverter.convertTo(StaticMethodTypeConverter.java:50)
	  at org.apache.camel.impl.converter.DefaultTypeConverter.doConvertTo(DefaultTypeConverter.java:159)
	  at org.apache.camel.impl.converter.DefaultTypeConverter.convertTo(DefaultTypeConverter.java:85)
	  at org.apache.camel.impl.MessageSupport.getBody(MessageSupport.java:74)
	  at org.apache.camel.impl.MessageSupport.getBody(MessageSupport.java:48)
	  at org.apache.camel.util.MessageHelper.extractBodyAsString(MessageHelper.java:63)
	  at org.apache.camel.processor.interceptor.DefaultTraceFormatter.format(DefaultTraceFormatter.java:75)
	  at org.apache.camel.processor.interceptor.TraceInterceptor.format(TraceInterceptor.java:231)
{code}

Thanks!
",davsclaus,cott@internetstaff.com,Major,Closed,Fixed,27/May/10 16:07,21/Mar/19 20:10
Bug,CAMEL-2763,12487243,MailBinding can't handle the subject header with NO_ASCII code rightly.,"Here is the [mail thread|http://old.nabble.com/Camel-Mail%3A-Subject-cannot-handle-unicode-chars--tp28700124p28700124.html] which discusses about it.
",njiang,njiang,Major,Closed,Fixed,28/May/10 03:40,17/Jan/11 16:50
Bug,CAMEL-2772,12485929,camel-jetty can't deal with multiform data rightly,"You can't get the inputStream from the attachment when camel-jetty handle the request of MultiPartForm.
Here is the mail thread[1] which discusses about it.

[1] http://old.nabble.com/Unsupported-data-type-exception-with-Jetty-component-tp28730373p28731758.html",njiang,njiang,Major,Closed,Fixed,01/Jun/10 00:55,24/Apr/11 09:57
Bug,CAMEL-2773,12486511,Bindy - No @Section causes a null key being generated which causes a NumberFormatException,"See nabble
http://old.nabble.com/Bindy-CSV-not-Marshaling-ts28719942.html",cmoulliard,davsclaus,Minor,Closed,Fixed,01/Jun/10 06:33,24/Apr/11 10:00
Bug,CAMEL-2776,12486762,Cached stream file deletion causing file not found errors,"Cached streams larger than the threshold (64K by default) are almost impossible to work with since when a cached stream is closed, the cached file is deleted. This occurs as a side effect of converting the associated body to string which can occur frequently if tracing is turned on. The net result is file not found errors shown below. The only workaround is to increase the threshold to a large value to keep the body in memory.

It seems that if stream caching is to be useful, the cached files should be kept in a map and cleaned up and the end of the route, rather than upon close.


Work around (set threshold to 10M):
    camelContext.getProperties().put(CachedOutputStream.THRESHOLD, ""10000000"");


Error without work around:

16:09:49.278 [main] ERROR o.a.c.c.t.TimerConsumer - Error processing exchange. Exchange[Message: [Body is instance of java.io.InputStream]]. Caused by: [org.apache.camel.RuntimeCamelException - java.io.FileNotFoundException: C:\Users\ROLAND~1\AppData\Local\Temp\camel-tmp-594543\cos3643004935230268170.tmp (The system cannot find the file specified)]
org.apache.camel.RuntimeCamelException: java.io.FileNotFoundException: C:\Users\ROLAND~1\AppData\Local\Temp\camel-tmp-594543\cos3643004935230268170.tmp (The system cannot find the file specified)
	at org.apache.camel.converter.stream.FileInputStreamCache.reset(FileInputStreamCache.java:65)
	at org.apache.camel.util.MessageHelper.resetStreamCache(MessageHelper.java:106)
	at org.apache.camel.processor.RedeliveryErrorHandler.deliverToFailureProcessor(RedeliveryErrorHandler.java:344)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:111)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:89)
	at org.apache.camel.processor.DefaultErrorHandler.process(DefaultErrorHandler.java:49)
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:228)
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:75)
	at org.apache.camel.processor.UnitOfWorkProcessor.processNext(UnitOfWorkProcessor.java:80)
	at org.apache.camel.processor.DelegateProcessor.process(DelegateProcessor.java:48)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67)
	at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:44)
	at org.apache.camel.processor.SendProcessor$1.doInProducer(SendProcessor.java:106)
	at org.apache.camel.processor.SendProcessor$1.doInProducer(SendProcessor.java:104)
	at org.apache.camel.impl.ProducerCache.doInProducer(ProducerCache.java:189)
	at org.apache.camel.processor.SendProcessor.doProcess(SendProcessor.java:103)
	at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:87)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67)
	at org.apache.camel.processor.DelegateProcessor.processNext(DelegateProcessor.java:53)
	at org.apache.camel.processor.DelegateProcessor.proceed(DelegateProcessor.java:82)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:173)
	at org.apache.camel.processor.interceptor.StreamCachingInterceptor.process(StreamCachingInterceptor.java:52)
	at org.apache.camel.processor.interceptor.StreamCachingInterceptor.process(StreamCachingInterceptor.java:52)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67)
	at org.apache.camel.processor.RedeliveryErrorHandler.processExchange(RedeliveryErrorHandler.java:185)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:151)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:89)
	at org.apache.camel.processor.DefaultErrorHandler.process(DefaultErrorHandler.java:49)
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:228)
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:75)
	at org.apache.camel.processor.UnitOfWorkProcessor.processNext(UnitOfWorkProcessor.java:70)
	at org.apache.camel.processor.DelegateProcessor.process(DelegateProcessor.java:48)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67)
	at org.apache.camel.component.timer.TimerConsumer.sendTimerExchange(TimerConsumer.java:102)
	at org.apache.camel.component.timer.TimerConsumer$1.run(TimerConsumer.java:49)
	at java.util.TimerThread.mainLoop(Timer.java:512)
	at java.util.TimerThread.run(Timer.java:462)
Caused by: java.io.FileNotFoundException: C:\Users\ROLAND~1\AppData\Local\Temp\camel-tmp-594543\cos3643004935230268170.tmp (The system cannot find the file specified)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:106)
	at org.apache.camel.converter.stream.FileInputStreamCache.reset(FileInputStreamCache.java:63)
	... 36 common frames omitted
",njiang,scrutinizer,Major,Closed,Fixed,01/Jun/10 20:17,24/Apr/11 10:00
Bug,CAMEL-2778,12487011,Escaped characters in http4 URLs do not work,There is a bug in http4 HttpProducer that is unescaping URL query and path components too early. Patch fix attached.,njiang,scrutinizer,Major,Closed,Fixed,02/Jun/10 00:42,24/Apr/11 10:01
Bug,CAMEL-2793,12486722,Docs for camel-spring-security not clear on where the Authentication object should be placed,"The docs for the new camel-spring-security component (http://camel.apache.org/spring-security.html) do not specify where the Authentication object should be placed in order to use a secured route.  Initially, I thought that the Authentication object should be a property on the exchange since the constant for authentication is on the Exchange class, but after looking at the code, I found that the component only checks the in message headers.  It would be nice to specify this in the docs.",njiang,pegli,Major,Closed,Fixed,04/Jun/10 22:23,24/Apr/11 10:01
Bug,CAMEL-2798,12486739,Aggregation raises NullPointerException if last file in batch is not to be aggregated.,"When trying to aggregate files from a directory, it seems that camel-core raises a NullPointerException if the last file of the batch is a single file. That is it doesn't match the correlationExpression with any other files and therefore should just pass through. If such a file is the first file or in the middle of the files (alphabetically) this issue is not present.

See:
http://old.nabble.com/Aggregator-problem-with-files-(Camel-2.3)-td28778641.html#a28780522",njiang,ankelee,Critical,Closed,Fixed,08/Jun/10 08:33,24/Apr/11 10:00
Bug,CAMEL-2802,12486734,Camel-cxf does not properly populate MessageContentList for PAYLOAD mode,"Currently, CxfEndpoint.CamelCxfClientImpl does not leave place holder for SOAP header in MessageContentList.  It could be a problem when CXF tries to retrieve the values in the MessageContentList.  Without the placeholder in place, SOAP body part can be mistakenly retrieved as SOAP header.  This patch should fix the problem.  Note: The fix depends on CXF-2837 (Add null pointer check in SoapOutInterceptor) which will be delivered in CXF 2.2.10.

{code}
Index: src/main/java/org/apache/camel/component/cxf/CxfEndpoint.java
===================================================================
--- src/main/java/org/apache/camel/component/cxf/CxfEndpoint.java	(revision 43)
+++ src/main/java/org/apache/camel/component/cxf/CxfEndpoint.java	(working copy)
@@ -513,8 +513,9 @@
                 MessageContentsList content = new MessageContentsList();
                 int i = 0;
                 
-                for (MessagePartInfo partInfo : boi.getOperationInfo().getInput().getMessageParts()) {
-                    if (elements.size() > i) {
+                for (MessagePartInfo partInfo : boi.getOperationInfo().getInput().getMessageParts()) {                 
+                    if (elements.size() > i && partInfo.getConcreteName().getLocalPart()
+                        .equals(elements.get(i).getLocalName())) {
                         content.put(partInfo, elements.get(i++));
                     }
                 }

{code}",wtam,wtam,Major,Closed,Fixed,09/Jun/10 18:38,24/Apr/11 10:01
Bug,CAMEL-2804,12486735,Camel OSGi language resolver can't work rightly,"The OSGiResolver doesn't work rightly in OSGi platform.
Here is the mail [thread|http://old.nabble.com/camel-ftp-problems-with-ftps-implicit-mode-ts28777705.html#a28839339] to show it.",njiang,njiang,Major,Closed,Fixed,10/Jun/10 08:54,24/Apr/11 10:00
Bug,CAMEL-2805,12486729,It is impossible to put # sign in SQL statement in camel-sql,{{SqlProducer}} implementation turns all {{#}} characters into {{?}} making it impossible to have {{#}} in the SQL statement.,romkal,romkal,Major,Closed,Fixed,10/Jun/10 14:19,16/Jun/10 08:45
Bug,CAMEL-2806,12486512,camel-jetty  can't config the temp directory for the multi part form support rightly,You will get a ClassCastException if you try to set the temp directory from the camel property.,njiang,njiang,Major,Closed,Fixed,12/Jun/10 05:39,24/Apr/11 10:01
Bug,CAMEL-2809,12486641,OsgiFactoryFinder should be able to go through the bundle entry to check the META-INF of the Factory,"The refactor of OSGi CAMEL-2693 let the OSGiFactoryFinder just find the factory information based on the camel application context.
As camel core doesn't exports the its META-INF/...  package, we'd better to revert the change, and let OSGiFactoryFinder be able to find the factory class as usual.
",njiang,njiang,Major,Closed,Fixed,14/Jun/10 03:34,24/Apr/11 10:00
Bug,CAMEL-2810,12486454,camel-quartz component dosen't support to start with a new camelContext after the scheduler is shutdown ,"You will get the below error if you want to create a new camel context after the scheduler is shutdown.
 
{code}
org.quartz.SchedulerException: The Scheduler has been shutdown.
	at org.quartz.core.QuartzScheduler.validateState(QuartzScheduler.java:637)
	at org.quartz.core.QuartzScheduler.scheduleJob(QuartzScheduler.java:688)
	at org.quartz.impl.StdScheduler.scheduleJob(StdScheduler.java:265)
	at org.apache.camel.component.quartz.QuartzComponent.addJob(QuartzComponent.java:150)
	at org.apache.camel.component.quartz.QuartzEndpoint.addTrigger(QuartzEndpoint.java:77)
	at org.apache.camel.component.quartz.QuartzEndpoint.consumerStarted(QuartzEndpoint.java:190)
	at org.apache.camel.component.quartz.QuartzConsumer.doStart(QuartzConsumer.java:39)
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:56)
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:53)
	at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:1015)
	at org.apache.camel.component.quartz.QuartzTwoCamelContextTest.setUp(QuartzTwoCamelContextTest.java:47)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:592)
	at org.junit.internal.runners.MethodRoadie.runBefores(MethodRoadie.java:122)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:86)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

{code}",njiang,njiang,Major,Closed,Fixed,14/Jun/10 08:01,24/Apr/11 10:00
Bug,CAMEL-2811,12486705,CamelContexts seems to not correctly create their own instance of an imported routeContext.,"Issue: http://old.nabble.com/Direct-route-not-shared-across-contexts--td28766143.html

When two CamelContexts import the same routeContext they should instantiate their own route based on the definition in the routeContext. But something is not working correctly. One example is having a directqueue of the same name in each route-instance. Camel crashed with the exception:

org.apache.camel.RuntimeCamelException: java.lang.IllegalStateException: Endpoint direct://foo only allows 1 active consumer but you attempted to start a 2nd consumer.
	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1126)
	at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:103) ...

This is demonstrated in the sample file attached to this issue.

Obviously this makes other things than the direct-endpoints malfunction. But this should be enough to show the problem.",njiang,ankelee,Major,Closed,Fixed,14/Jun/10 15:28,24/Apr/11 10:01
Bug,CAMEL-2815,12486707,"camel-castor relies on specific, out of date version of Castor","camel-castor specifies a non-flexible version dependency on Castor 1.2.0.  Ideally should be changed to [1.2.0,2.0) to allow later versions of Castor to be deployed.  Castor 1.2 was released in Feb 2008, 1.3 was first released in Feb 2009",njiang,davisond,Major,Closed,Fixed,15/Jun/10 10:32,24/Apr/11 10:01
Bug,CAMEL-2817,12486703,renames (to .processed) are sometimes done even if the download failed.,"I have an ftp consumer endpoint URI like: 
{{ftp://conaxTest@localhost:2121/autreq/ok?passiveMode=true&amp;password=conaxTest&move=.processed&delay=5000}}

From my ftp server logs, I can see files being downloaded, and then renamed...
{quote}
[org.apache.ftpserver.command.impl.RETR:pool-2-thread-56] - <File downloaded /autreq/ok/vp006331.emm>
[org.apache.ftpserver.command.impl.RETR:pool-2-thread-56] - <File downloaded /autreq/ok/vp006332.emm>
[org.apache.ftpserver.impl.DefaultFtpHandler:pool-2-thread-57] - <Session idle, closing>
[org.apache.ftpserver.command.impl.RNTO:pool-2-thread-56] - <File rename from ""/autreq/ok/vp006331.emm"" to ""/autreq/ok/.processed/vp006331.emm"">
[org.apache.ftpserver.command.impl.RNTO:pool-2-thread-54] - <File rename from ""/autreq/ok/vp006332.emm"" to ""/autreq/ok/.processed/vp006332.emm"">
[org.apache.ftpserver.command.impl.RNTO:pool-2-thread-54] - <File rename from ""/autreq/ok/vp006333.emm"" to ""/autreq/ok/.processed/vp006333.emm"">
{quote}

Note, that vp006333.emm is renamed, but was never downloaded.  There's no other mention of the file in my logs.  On the camel ftp consumer side, I see that camel attempted to download the file, but ended up with a null...

{quote}
[is.vf.conan.conax.FileParser:Camel thread 7: seda://updateOk] - <handling: GenericFile[vp006331.emm]>
[is.vf.conan.ConanCore:Camel thread 7: seda://updateOk] - <updating operation txid:006331 with the results: OK>
[is.vf.conan.conax.FileParser:Camel thread 7: seda://updateOk] - <handling: GenericFile[vp006332.emm]>
[is.vf.conan.ConanCore:Camel thread 7: seda://updateOk] - <updating operation txid:006332 with the results: OK>
[is.vf.conan.conax.FileParser:Camel thread 7: seda://updateOk] - <handling: GenericFile[vp006333.emm]>
[is.vf.conan.conax.FileParser:Camel thread 7: seda://updateOk] - <Requested parse of an empty file!>
{quote}

The ""requested parse of an empty file"" is logged when {{ex.getIn().getBody(String.class)}} is empty or blank for the file object.

I had a look through the bugs fixed for 2.3.0, but I don't see anything that would be related to this at all.
",davsclaus,karlp,Major,Closed,Fixed,15/Jun/10 22:17,24/Apr/11 10:00
Bug,CAMEL-2821,12486700,camel-ftp - SFTP in fileExists should handle exception being thrown with id code stating no such file,"See nabble
http://camel.465427.n5.nabble.com/camel-ftp-cannot-create-directory-using-sftp-tp479092p479092.html",davsclaus,davsclaus,Minor,Closed,Fixed,16/Jun/10 04:14,24/Apr/11 10:01
Bug,CAMEL-2824,12486417,Deadlock in org.apache.camel.util.DefaultTimeoutMap,"After running a camel route with a camel Aggregator for a while, I get a deadlock in  org.apache.camel.util.DefaultTimeoutMap. A full processdump is attached to this bug.

I have also tried to recreate this as failing testcase, but without any luck so far. ",davsclaus,tarjei@scanmine.com,Major,Closed,Fixed,16/Jun/10 19:43,24/Apr/11 10:01
Bug,CAMEL-2825,12486443,onException - continued should not log exhausted just as handled does neither,Continued is in fact just like handled by will not break out but continue routing as if the exception didnt occur. So we should not log the exception by default.,davsclaus,davsclaus,Minor,Closed,Fixed,17/Jun/10 07:16,24/Apr/11 10:01
Bug,CAMEL-2826,12486438,Message content redelivered asynchronously by DLC is not re-readable,"When a route contains an exception handler clause, a stream based message being redelivered asynchronously after an error is not readable. The cached streams must be reset before redelivery so that they can be read again by the target endpoint. See CAMEL-1834 for a similar problem.
",njiang,rgavlin,Critical,Closed,Fixed,17/Jun/10 08:56,17/Jan/11 16:50
Bug,CAMEL-2829,12486381,"""Unconnected sockets not implemented"" exception in camel-ftp when using ftps","When using ftps with a secure data channel, camel-ftp cannot recover from a lost connection. This is due to a bug/flaw in the SFTPClient class in commons-net. Once a secure data channel has been established, SFTPClient replaces the connection factory with one that does not provide support for creating ""unconnected sockets"".

While waiting for a fix from the commons-net team (should they chosse to create one), a work-around in camel-ftp is to always create a new instance of SFTPClient for every connect attempt.

Attached are patches containing an attempt to implement the mentioned work-around. The classes, FtpsOperations, FtpsEndpoint and FtpOperations are involved.",davsclaus,rodehav,Major,Closed,Fixed,18/Jun/10 12:41,24/Apr/11 10:00
Bug,CAMEL-2843,12486123,camel-groovy - setting header causes exchange to lose message details,"See SMX4-417

This is the ticket to fix this issue.",davsclaus,davsclaus,Major,Closed,Fixed,23/Jun/10 11:25,24/Apr/11 09:57
Bug,CAMEL-2851,12486317,typo: ManagedBrowsableEndpoint qeue should be queue,"{code}
    @ManagedOperation(description = ""Current number of Exchanges in Queue"")
    public long qeueSize() {
        return endpoint.getExchanges().size();
    }
{code}

Should be 

{code}
    @ManagedOperation(description = ""Current number of Exchanges in Queue"")
    public long queueSize() {
        return endpoint.getExchanges().size();
    }
{code}",davsclaus,karlp,Trivial,Closed,Fixed,24/Jun/10 14:23,24/Apr/11 10:00
Bug,CAMEL-2854,12486289,"No consumers available on ""direct://xyz"" endpoint","The following warning in log4j log shows up with no predictable reason:

2010-06-24 14:07:01,090  WARN [DefaultMessageListenerContainer-7:DirectProducer.java:42] No consumers available on endpoint: Endpoint[direct://UserCancelJobRequest] to process: Exchange[JmsMessage: RunnerBase{locationToRunId='DV02YcABTE43HTRPATEGCBFk0m0', jobrun_id=1234, requestOrigId='null', secretKey='null', replyToQueueOrTopic='null', corrolationId='null', requestCreationTime=Thu Jun 24 14:07:01 PDT 2010}]

The endpoint [direct://UserCancelJobRequest] could be a number of different endpoints in our application and all of them are of ""direct:"" type.  When this happens, all messages that run through that endpoint will fail, and it will never recover unless the application is restarted.

Also, if I restart the application and use that route immediately, that warning will not occur and the request will finish successfully.  It seems to happen to routes that have not been used for a while after startup.

When this happens, I can find the exact Camel endpoint/route/processor in JConsole, and nothing seems to be wrong.  How can a ""direct:"" endpoint runs out of consumer?  Isn't the thread that product this warning THE thread to process the message?

We recently upgraded from 5.2 to 5.3.2 and this starts to show up randomly. It is a show stopper!

--Michael",davsclaus,mwc_tonesoft,Critical,Closed,Fixed,24/Jun/10 23:47,24/Apr/11 10:01
Bug,CAMEL-2858,12486216,useOriginalBody() still appears in fluent API; should be useOriginalMessage(),"The {{org.apache.camel.model.OnExceptionDefinition}} class defines {{useOriginalBody()}} in the fluent API. This should actually be {{useOriginalMessage()}}, in accordance with CAMEL-1820.",njiang,fbolton,Major,Closed,Fixed,25/Jun/10 11:06,24/Apr/11 10:01
Bug,CAMEL-2860,12486260,"camel-ftp - Should use absolute paths, eg the leading / should always be included","FTP Servers expect clients to use absolute paths, so camel-ftp clients should not clip any leading /

We could consider adding a flag to preserve the old behavior to have people if they rely on the old behavior be able to switch that flag on.",davsclaus,davsclaus,Major,Closed,Fixed,25/Jun/10 11:53,24/Apr/11 10:00
Bug,CAMEL-2867,12486303,convertBodyTo should handle null bodies,If the body is {{null}} and you use {{convertBodyTo}} you will get a no type converter exception due body is null. Instead it should accept the null body and continue routing.,davsclaus,davsclaus,Major,Closed,Fixed,28/Jun/10 07:03,24/Apr/11 10:01
Bug,CAMEL-2881,12486287,jms consumer should handle markRollbackOnly,If using markRollbackOnly to dente the route should rollback the Spring DMLC does not rollback despite its status has been told so. We are forced to thrown runtime exceptions.,davsclaus,davsclaus,Minor,Closed,Fixed,30/Jun/10 09:40,24/Apr/11 10:01
Bug,CAMEL-2893,12486052,(FromDefintion|ToDefinition).getUriOrRef tend to return null when they should return a URI,looks like a simple logic bug to me. Have a fix locally - just checking it works...,jstrachan,jstrachan,Major,Closed,Fixed,01/Jul/10 17:24,24/Apr/11 10:00
Bug,CAMEL-2894,12486234,DefaultPackageScanClassResolver.addClassLoader should check the UnsupportedOperationException,"When I was running some OSGi tests with Camel 2.4-SNAPSHOT, I got this UnsupportedOperationExcpetion,
This is stack trace
{code}
Caused by: java.lang.UnsupportedOperationException
	at java.util.AbstractCollection.add(AbstractCollection.java:221)
	at org.apache.camel.impl.DefaultPackageScanClassResolver.addClassLoader(DefaultPackageScanClassResolver.java:58)
	at org.apache.camel.spring.PackageScanRouteBuilderFinder.<init>(PackageScanRouteBuilderFinder.java:52)
	at org.apache.camel.spring.CamelContextFactoryBean.findRouteBuildersByPackageScan(CamelContextFactoryBean.java:183)

{code}

After digging the code , I found current OsgiPackageScanClassResolver only supports to turn the bundle classloader, so we need to check this kind of error.
",njiang,njiang,Major,Closed,Fixed,02/Jul/10 06:47,24/Apr/11 10:00
Bug,CAMEL-2895,12486160,OsgiPackageScanClassResolver can't find the class after refactoring,"After I applied the patch of CAMEL-2894, I can't get let camel load the router with package scan option.

As the Bundle.getResources(String name), can't work as the ClassLoader.getResources(String name).
So we have to use Bundle.findEntries() to check the package's resource.",njiang,njiang,Major,Closed,Fixed,02/Jul/10 06:54,24/Apr/11 10:00
Bug,CAMEL-2897,12485908,Splitting file using tokenizer should close the Scanner to avoid files not being able to be moved thereafter,,davsclaus,davsclaus,Major,Closed,Fixed,02/Jul/10 08:22,24/Apr/11 10:00
Bug,CAMEL-2899,12486190,out of heap space if remote FTP site has too many files to pick up,"2010-07-02 11:38:07,439 FATAL [org.apache.camel.component.file.remote.FtpConsumer:CamelThread 10]   - <Consumer Consumer[my_ftp_URI_here caused by: Java heap space>
java.lang.OutOfMemoryError: Java heap space

My remote FTP server has ~60k 100 byte files, and the camel endpoint consumer falls over and doesn't start again.  I can use JMX to stop/start the consumer, (it still has status ""started"") and it will log in to the remote server again, but then fall over with the out of heap space.

I can work around this by increasing the heap, or by moving some of the files aside,  but I don't think camel should care how many files there are, or at least, I think it should deal with it more gracefully.
",davsclaus,karlp,Major,Closed,Fixed,02/Jul/10 11:49,24/Apr/11 09:57
Bug,CAMEL-2901,12486249,Upgrade to HawtDB 1.1,"HawtDB 1.1 has been released.  Change log at: http://github.com/chirino/hawtdb/blob/master/changelog.md
We should upgrade to pick up the listed bug fixes:
{quote}
    * Fixing BTree node next linking.. It was possible that a next link would not properly get set in some conditions during a node removal.
    * You can add get callbacks when a commit gets flushed to disk.
    * Changed the way the journal was handling callback based write completed notifications. They are now delivered in batch form to a single JournalListener. This reduces thread contention and increases throughput.
    * Moved the built in predicate implementations into a Predicates class.
    * Added close method to the Transaction interface. Implementation now asserts it is no longer used after a close.
    * Making the appender's max write batch size configurable.
    * Revamped how Update and DefferedUpdates track shadow pages. A little easier to follow now. - changed the interface to PagedAccessor so that instead of removing the linked pages, it just needs to report what the linked pages are.
    * Got rid of the WriteKey wrapper class, updated logging.
    * Better looking printStrucuture BTree method
    * Added a few Logging classes to reduce the number of places we need to update if in case we decided to switch logging APIs.
    * Fixing free page allocation bug when using deferred updates.
    * Javadoc improvements
    * Expose a config property to control the read cache size.
    * Reworked how snapshot tracking was being done. Fixes errors that occurred during heavy concurrent access.
    * Added a non-blocking flush method to the TxPageFile
    * Read cache was not getting updated when a update batch was performed. Cached entries that were updated and flushed to disk continued returning stale data.
    * Fixed an recovery edge cases
    * Don't start the thread from the thread factory. that causes illegal state exceptions
    * Fixed journal bug where getting next location could return a the current location
    * Renamed EncoderDecoder to PagedAccessor
    * The util.buffer package has moved into it's own project at http://github.com/chirino/hawtbuf
    * Fixes #4 : Errors occur when you re-open an empty data file.
    * Extracted a SortedIndex interface from the Index interface to non sorted indexes having to deal with that leaky abstraction.
    * added a free() method to the Paged for symmetry with the alloc() method.
    * Improved website documentation
{quote}",chirino,chirino,Major,Closed,Fixed,02/Jul/10 13:12,24/Apr/11 10:01
Bug,CAMEL-2904,12485873,<proxy> doesn't support setting camelContextId,"Here is the mail thread which discusses about it.
http://camel.465427.n5.nabble.com/proxy-export-don-t-support-setting-camel-context-tp547161p547161.html",njiang,njiang,Major,Closed,Fixed,03/Jul/10 09:20,24/Apr/11 09:57
Bug,CAMEL-2909,12486039,"Oracle AQ does not support the JMSReplyTo property and therefore throws a ""JMS-102: Feature not supported"" exception when the JMS provider tries to get it.","Oracle AQ does not support the JMSReplyTo property and therefore throws a ""JMS-102: Feature not supported"" exception when the JMS provider tries to get it.",davsclaus,mmatveev,Major,Closed,Fixed,05/Jul/10 14:11,09/Jul/10 08:29
Bug,CAMEL-2912,12486171,"SFTP throws ResolveEndpointFailedException when ""ftpClient.connectionTimeout"" option is provided","Currently, SFTP throws an ResolveEndpointFailed exception if the option ""ftpClient.connectionTimeout"" is provided as a part of the URI. This could be fixed by calling the Session.connect(int timeout) method of JSCH. Some investigation reveals that this method eventually calls Socket.setSoTimeout(int timeout), which implies that the time unit of the timeout is milliseconds.

http://grepcode.com/file/repo1.maven.org/maven2/com.jcraft/jsch/0.1.42/com/jcraft/jsch/Session.java#Session.connect%28int%29


Example:

sftp://user@host/dir?password=secret&ftpClient.connectionTimeout=30000&disconnect=true&passiveMode=true

ResolveEndpointFailedException:
[...]
There are 1 parameters that couldn't be set on the endpoint. Check the uri if the parameters are spelt correctly and that they are properties of the endpoint. Unknown parameters=[{ftpClient.connectionTimeout=30000}] ",davsclaus,matsev,Major,Closed,Fixed,06/Jul/10 13:32,24/Apr/11 09:57
Bug,CAMEL-2922,12485960,XMPPConsumer does not remove the message which causes OOME with XMPP,,davsclaus,davsclaus,Major,Closed,Fixed,08/Jul/10 05:13,24/Apr/11 09:57
Bug,CAMEL-2924,12485790,httpClient.soTimeout property is not returned when getting HTTP endpoint URI,"getting the endpoint URI of an HTTP EndPoint with httpClient.soTimeout property set won't return the property

i.e: ""http://crive.optadev.com/valde/utils/posttest.php?httpClient.soTimeout=5000"" is returned as ""http://crive.optadev.com/valde/utils/posttest.php""

* I have noticed this using ExchangeHelper.resolveEndpoint:
String uri = ""http://crive.optadev.com/valde/utils/posttest.php?httpClient.soTimeout=5000"";
uri = ExchangeHelper.resolveEndpoint(exchange, uri.trim()).getEndpointUri();
//uri is now missing httpClient.soTimeout=5000

* it is also happening reading @Header(Exchange.TO_ENDPOINT)

this is not happening when using the soTimeout option in FTP component.


I think this is a major issue as recipientList is making use of ExchangeHelper and therefore the option will be ingored.

",davsclaus,crive,Major,Closed,Fixed,08/Jul/10 15:04,24/Apr/11 09:57
Bug,CAMEL-2935,12486038,Broken failure reporting via DefaultProucerTemplate.asyncCallback and Synchronization.onFailure,"The recent change in {{ProducerCache.send}}

{code:java|title=ProducerCache.java}
    public void send(Endpoint endpoint, Exchange exchange) {
        try {
            sendExchange(endpoint, null, null, exchange);
            // RECENT CHANGE HERE:
            // ensure that CamelExecutionException is always thrown
            if (exchange.getException() != null) {
                exchange.setException(wrapCamelExecutionException(exchange, exchange.getException()));
            }
        } catch (Exception e) {
            throw wrapCamelExecutionException(exchange, e);
        }
    }
{code}

that throws a {{CamelExecutionException}} if {{exchange.getException}} is not null, makes it impossible for {{DefaultProducerTemplate.asyncCallback}} to report failures (other than fault messages) asynchronously via {{Synchronization.onFailure}}

{code:java|title=DefaultProducerTemplate.java}
    public Future<Exchange> asyncCallback(final Endpoint endpoint, final Exchange exchange, final Synchronization onCompletion) {
        Callable<Exchange> task = new Callable<Exchange>() {
            public Exchange call() throws Exception {

                // FIXME: exception is thrown in Camel 2.4 where a normal return with answer.getException != null was done in Camel 2.3
                Exchange answer = send(endpoint, exchange);

                if (answer.isFailed()) {
                    onCompletion.onFailure(answer);
                } else {
                    // ...
                }
                return answer;
            }
        };
        // ...
    }
{code}

This was working in Camel 2.3 (but unfortunately there wasn't any test case for it). I attached a patch for {{DefaultProducerTemplateAsyncTest}} that demonstrates the problem. I didn't commit a fix yet because I'm unsure at the moment about the best way to fix that. Of course I tried a naive fix in the DefaultProducerTemplate.asyncCallback methods which causes the test (in the patch) to pass but I'd like to hear other opinions before I continue.",davsclaus,mrt1nz,Major,Closed,Fixed,10/Jul/10 10:01,24/Apr/11 09:57
Bug,CAMEL-2937,12486010,StreamProducer does not close stream in camel-stream,"Since the stream could be System.out or System.err, the producer does not close the stream ever. It should close all streams with the noted exceptions.",hadrian,hadrian,Major,Closed,Fixed,11/Jul/10 16:26,02/May/13 02:30
Bug,CAMEL-2940,12485952,org.apache.camel.component.test package is imported in camel-spring component - generates error on OSGI platform,"{code}
[  86] [Installed  ] [            ] [   60] camel-spring (2.5.0.SNAPSHOT)
karaf@root> start 86
Error executing command: Unresolved constraint in bundle org.apache.camel.camel-spring [86]: Unable to resolve 86.0: missing requirement [86.0] package; (&(package=org.ap
ache.camel.component.test)(version>=2.5.0)(!(version>=2.5.1))) - [86.0] package; (&(package=org.apache.camel.component.test)(version>=2.5.0)(!(version>=2.5.1)))
karaf@root> headers 86

camel-spring (86)
-----------------
Bnd-LastModified = 1278946400750
Build-Jdk = 1.6.0_20
Built-By = Charlesm
Bundle-Activator = org.apache.camel.osgi.Activator
Bundle-Description = Camel Spring support
Bundle-DocURL = http://www.apache.org/
Bundle-License = http://www.apache.org/licenses/LICENSE-2.0.txt
Bundle-ManifestVersion = 2
Bundle-Name = camel-spring
Bundle-SymbolicName = org.apache.camel.camel-spring
Bundle-Vendor = The Apache Software Foundation
Bundle-Version = 2.5.0.SNAPSHOT
Created-By = Apache Maven Bundle Plugin
Export-Package = org.apache.camel.spring.spi;uses:=""org.springframework.context,org.apache.camel.spi,org.springframework.beans.factory,org.apache.camel,org.springframewor
k.beans.factory.config,org.apache.camel.builder,org.springframework.transaction,org.apache.commons.logging,org.apache.camel.processor.exceptionpolicy,org.apache.camel.pro
cessor,org.apache.camel.util,org.apache.camel.model,org.springframework.transaction.support"";version=""2.5.0.SNAPSHOT"",org.apache.camel.spring.util;uses:=""org.apache.camel
,org.springframework.context,org.apache.camel.util,org.apache.commons.logging,org.springframework.beans.factory,org.springframework.util"";version=""2.5.0.SNAPSHOT"",org.apa
che.camel.spring.remoting;uses:=""org.apache.camel.util,org.springframework.remoting.support,org.apache.camel.component.bean,org.apache.camel,org.springframework.beans,org
.springframework.context,org.apache.camel.spi,org.apache.camel.spring.util,org.springframework.beans.factory"";version=""2.5.0.SNAPSHOT"",org.apache.camel.spring;uses:=""org.
apache.camel.impl,org.apache.camel.util,org.apache.commons.logging,org.apache.camel,org.springframework.beans,org.springframework.context,org.springframework.util,org.spr
ingframework.beans.factory.config,org.apache.camel.spring.util,javax.xml.bind.annotation,org.springframework.beans.factory,org.apache.camel.builder,org.apache.camel.model
.dataformat,org.springframework.context.event,org.apache.camel.model.config,org.apache.camel.model,org.apache.camel.spi,org.apache.camel.spring.spi,org.apache.camel.view,
org.springframework.context.support,org.apache.camel.spring.handler,javax.xml.bind,org.apache.camel.component.bean,org.apache.camel.component.event,org.springframework.tr
ansaction,org.springframework.transaction.support"";version=""2.5.0.SNAPSHOT"",org.apache.camel.spring.handler;uses:=""org.springframework.beans.factory.support,org.springfra
mework.core,org.springframework.util,org.springframework.beans.factory.xml,org.w3c.dom,org.springframework.beans,org.springframework.beans.factory.config,org.springframew
ork.beans.factory.parsing,org.apache.camel.model.dataformat,org.apache.camel.model.config,org.apache.camel.util,org.apache.camel.model,org.apache.camel.spring,javax.xml.b
ind,org.springframework.beans.factory,org.apache.camel.view,org.apache.commons.logging,org.apache.camel.model.loadbalancer,org.apache.camel.builder.xml,org.apache.camel.s
pring.remoting,org.apache.camel.spi,org.apache.camel,org.apache.camel.model.language,org.apache.camel.processor"";version=""2.5.0.SNAPSHOT"",org.apache.camel.osgi;uses:=""org
.osgi.framework,org.apache.camel.impl,org.springframework.osgi.context,org.apache.commons.logging,javax.xml.bind.annotation,org.springframework.context,org.apache.camel.s
pring,org.apache.camel.model,org.apache.camel.model.loadbalancer,org.apache.camel,org.apache.camel.model.language,org.springframework.beans.factory.xml,org.apache.camel.m
odel.dataformat,org.apache.camel.spring.handler,org.apache.camel.model.config,org.apache.camel.spi"";version=""2.5.0.SNAPSHOT"",org.apache.camel.component;uses:=""org.apache.
camel.impl,org.apache.commons.logging,org.springframework.core.io,org.apache.camel,org.apache.camel.converter,org.apache.camel.spi"";version=""2.5.0.SNAPSHOT"",org.apache.ca
mel.component.event;uses:=""org.apache.camel,org.springframework.context,org.apache.camel.impl,org.springframework.beans,org.apache.camel.util,org.apache.camel.processor.l
oadbalancer"";version=""2.5.0.SNAPSHOT"",org.apache.camel.component.test;uses:=""org.apache.camel.impl,org.apache.camel.util,org.apache.camel,org.apache.commons.logging,org.a
pache.camel.component.mock"";version=""2.5.0.SNAPSHOT"",org.apache.camel.component.validator;uses:=""javax.xml.transform,org.springframework.core.io,javax.xml.transform.strea
m,org.apache.camel.processor.validation,org.apache.camel.impl,org.apache.camel.component,org.apache.commons.logging,org.apache.camel"";version=""2.5.0.SNAPSHOT"",org.apache.
camel.component.xslt;uses:=""org.apache.camel.impl,org.apache.camel.component,org.apache.commons.logging,org.apache.camel,org.springframework.core.io,org.apache.camel.buil
der.xml,org.apache.camel.util,javax.xml.transform,org.apache.camel.converter.jaxp,org.apache.camel.spi"";version=""2.5.0.SNAPSHOT""
Ignore-Package = org.apache.camel.spring.remoting,org.apache.camel.component,org.apache.camel.component.validator,org.apache.camel.component.xslt,org.apache.camel.spring.
spi,org.apache.camel.spring,org.apache.camel.osgi,org.apache.camel.spring.handler,org.apache.camel.component.event,org.apache.camel.spring.util
Implementation-Title = Camel :: Spring
Implementation-Vendor = The Apache Software Foundation
Implementation-Vendor-Id = org.apache.camel
Implementation-Version = 2.5-SNAPSHOT
Import-Package = javax.xml.bind,javax.xml.bind.annotation,javax.xml.bind.annotation.adapters,javax.xml.transform,javax.xml.transform.stream,org.apache.camel;version=""[2.5
.0,2.5.1)"",org.apache.camel.builder;version=""[2.5.0,2.5.1)"",org.apache.camel.builder.xml;version=""[2.5.0,2.5.1)"",org.apache.camel.component.bean;version=""[2.5.0,2.5.1)"",o
rg.apache.camel.component.mock;version=""[2.5.0,2.5.1)"",org.apache.camel.component.properties;version=""[2.5.0,2.5.1)"",org.apache.camel.component.test;version=""[2.5.0,2.5.1
)"",org.apache.camel.converter;version=""[2.5.0,2.5.1)"",org.apache.camel.converter.jaxp;version=""[2.5.0,2.5.1)"",org.apache.camel.impl;version=""[2.5.0,2.5.1)"",org.apache.cam
el.impl.converter;version=""[2.5.0,2.5.1)"",org.apache.camel.management;version=""[2.5.0,2.5.1)"",org.apache.camel.model;version=""[2.5.0,2.5.1)"",org.apache.camel.model.config
;version=""[2.5.0,2.5.1)"",org.apache.camel.model.dataformat;version=""[2.5.0,2.5.1)"",org.apache.camel.model.language;version=""[2.5.0,2.5.1)"",org.apache.camel.model.loadbala
ncer;version=""[2.5.0,2.5.1)"",org.apache.camel.processor;version=""[2.5.0,2.5.1)"",org.apache.camel.processor.exceptionpolicy;version=""[2.5.0,2.5.1)"",org.apache.camel.proces
sor.interceptor;version=""[2.5.0,2.5.1)"",org.apache.camel.processor.loadbalancer;version=""[2.5.0,2.5.1)"",org.apache.camel.processor.validation;version=""[2.5.0,2.5.1)"",org.
apache.camel.spi;version=""[2.5.0,2.5.1)"",org.apache.camel.util;version=""[2.5.0,2.5.1)"",org.apache.camel.view;version=""[2.5.0,2.5.1)"",org.apache.commons.logging,org.osgi.f
ramework;version=""[1.3,2)"",org.osgi.util.tracker;version=""[1.3,2)"",org.springframework.beans;version=""[2.5,4)"",org.springframework.beans.factory;version=""[2.5,4)"",org.spr
ingframework.beans.factory.config;version=""[2.5,4)"",org.springframework.beans.factory.parsing;version=""[2.5,4)"",org.springframework.beans.factory.support;version=""[2.5,4)
"",org.springframework.beans.factory.xml;version=""[2.5,4)"",org.springframework.context;version=""[2.5,4)"",org.springframework.context.event;version=""[2.5,4)"",org.springfram
ework.context.support;version=""[2.5,4)"",org.springframework.core;version=""[2.5,4)"",org.springframework.core.io;version=""[2.5,4)"",org.springframework.osgi.context;version=
""[1.2,2)"",org.springframework.remoting.support;version=""[2.5,4)"",org.springframework.transaction;version=""[2.5,4)"",org.springframework.transaction.support;version=""[2.5,4
)"",org.springframework.util;version=""[2.5,4)"",org.w3c.dom
Manifest-Version = 1.0
Specification-Title = Camel :: Spring
Specification-Vendor = The Apache Software Foundation
Specification-Version = 2.5-SNAPSHOT
Tool = Bnd-0.0.357
{code}
",cmoulliard,cmoulliard,Major,Closed,Fixed,12/Jul/10 15:12,24/Apr/11 09:57
Bug,CAMEL-2942,12485961,CamelHttpTransportServlet.destroy causes java.lang.IllegalArgumentException: Cannot find the deployed servlet,"CamelHttpTransportServlet.destroy removes the servlet from a static map named CAMEL_SERVLET_MAP prior to stopping its associated Spring application context.  If that context defines a route using a servletEndpoint, the route is shutdown AFTER the servlet is removed from the map.  The shutdown code then looks for the servlet in the map by calling CamelHttpTransportServlet.getCamelServlet.  The servlet can't be found, and the ensuing exception is logged (copied below).

It seems to me that CamelHttpTransportServlet.destroy should be defined like this:
    
    public void destroy() {
        // Removal was here
        if (applicationContext != null) {
            applicationContext.stop();
        }
        // Moved the removal to here
        CAMEL_SERVLET_MAP.remove(servletName);
    }

so that the servlet is removed from the map after the context is stopped.

I know for sure that removal of the servlet from the map before the context is stopped causes this problem when my route is shutdown.  I DON'T know for sure that my overall configuration is correct, though I think it is valid.

If the above change is not obviously correct, or if there are further questions, please let me know, and I'll work up a sample that demonstrates the problem to help answer any questions.

--

2010-07-13 09:06:23,918 [Camel Thread 0 - ShutdownTask] WARN  org.apache.camel.impl.DefaultShutdownStrategy at org.apache.camel.impl.DefaultShutdownStrategy.shutdownNow(DefaultShutdownStrategy.java:219)
         Error occurred while shutting down route: Consumer[/relay]. This exception will be ignored.
java.lang.IllegalArgumentException: Cannot find the deployed servlet, please configure the ServletComponent or configure a org.apache.camel.component.servlet.CamelHttpTransportServlet servlet in web.xml 
	at org.apache.camel.component.servlet.ServletComponent.getCamelServlet(ServletComponent.java:55)
	at org.apache.camel.component.servlet.ServletComponent.disconnect(ServletComponent.java:116)
	at org.apache.camel.component.http.HttpEndpoint.disconnect(HttpEndpoint.java:152)
	at org.apache.camel.component.http.HttpConsumer.doStop(HttpConsumer.java:56)
	at org.apache.camel.impl.ServiceSupport.stop(ServiceSupport.java:86)
	at org.apache.camel.impl.ServiceSupport.stop(ServiceSupport.java:107)
	at org.apache.camel.util.ServiceHelper.stopService(ServiceHelper.java:100)
	at org.apache.camel.impl.DefaultShutdownStrategy.shutdownNow(DefaultShutdownStrategy.java:217)
	at org.apache.camel.impl.DefaultShutdownStrategy$ShutdownTask.run(DefaultShutdownStrategy.java:413)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)

",njiang,bstiles@bstiles.net,Major,Closed,Fixed,13/Jul/10 17:18,24/Apr/11 09:57
Bug,CAMEL-2944,12485941,StreamCache - File spooled stream cache does not work well on Windows due not closing file resource,"Windows is pesky about deleting files which may have an open stream hanging. While other OS is capable of that.

SplitterStreamCacheTest fails on windows due this issue.

The logic in CachedOutputStream should better leverage the Synchronization to be able to close the streams at the end.

Also various DOM converters will automatic closes input streams when converting to DOM. The only reliable mechanism is Camels synchronization which is invoked when the Exchange is done. And thus the time where we can cleanup.
",davsclaus,davsclaus,Major,Closed,Fixed,14/Jul/10 07:03,24/Apr/11 09:57
Bug,CAMEL-2947,12485934,CachedOutputStream supports not to close itself when the exchange is completed,"The patch of CAMEL-2944 just introduced a regression issue of CAMEL-2776.
We just need to find a way to avoid the CachedOutputStream close itself when the exchange is completed, as the input stream which is get from the CachedOutputStream will be used after that.",njiang,njiang,Major,Closed,Fixed,14/Jul/10 10:06,24/Apr/11 09:57
Bug,CAMEL-2949,12485832,"Attachment DataHander.getName() returns the name of the temporary storage file, not the MIME part name","When multipart file uploads are converted to message attachments in the DefaultHttpBinding class, the DataHandler that is created for the attachment returns the name of the temporary file created by Jetty, not the value of the name from the Content-Disposition header.  It would be useful to have the actual attachment name instead of the temporary filename in the DataHandler object for easier downstream processing.",njiang,pegli,Major,Closed,Fixed,14/Jul/10 17:43,24/Apr/11 09:58
Bug,CAMEL-2952,12485594,camel-ftp - Support polling from MVS file system,"The MVS file system requires to change directory to starting path and use CD to traverse file path instead of using listFile(path) as currently done in camel-ftp.

This means we should walk the path using code like:
0. remember path
1. cd path
2. list files()
3. loop files
4. if dir then goto 1
5. if file add file
6. when done cd back to ""remember path""

",davsclaus,davsclaus,Major,Closed,Fixed,15/Jul/10 13:19,24/Apr/11 09:58
Bug,CAMEL-2958,12485769,java.util.ConcurrentModificationException in Method org.apache.camel.util.CaseInsensitiveMap.putAll(),"Every now and then I'm facing the ConcurrentModificationException. It very hard to reproduce. This is my stack trace:

Failed delivery for exchangeId: ba969718-9044-4261-bc57-ca10aafb0a03. Exhausted after delivery attempt: 1 caught: java.util.ConcurrentModificationException
java.util.ConcurrentModificationException: null
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793) [na:1.6.0_20]
        at java.util.HashMap$KeyIterator.next(HashMap.java:828) [na:1.6.0_20]
        at org.apache.camel.util.CaseInsensitiveMap.putAll(CaseInsensitiveMap.java:86) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.impl.MessageSupport.copyFrom(MessageSupport.java:142) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.impl.DefaultMessage.copyFrom(DefaultMessage.java:52) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.util.ExchangeHelper.copyResults(ExchangeHelper.java:199) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:114) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.DelegateProcessor.processNext(DelegateProcessor.java:53) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.ChoiceProcessor.process(ChoiceProcessor.java:51) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.DelegateProcessor.processNext(DelegateProcessor.java:53) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.DelegateProcessor.proceed(DelegateProcessor.java:82) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:97) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.RedeliveryErrorHandler.processExchange(RedeliveryErrorHandler.java:185) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:151) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:89) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.DefaultErrorHandler.process(DefaultErrorHandler.java:49) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:228) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:75) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.UnitOfWorkProcessor.processNext(UnitOfWorkProcessor.java:70) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.processor.DelegateProcessor.process(DelegateProcessor.java:48) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:67) [camel-core-2.3.0.jar:2.3.0]
        at org.apache.camel.component.http.CamelServlet.service(CamelServlet.java:71) [camel-http-2.3.0.jar:2.3.0]
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:831) [javaee.jar:9.1]
...


Looks like CaseInsensitiveMap isn't enough thread save. But I have no idea what other thread is modifies the map. Most of the time everything is ok.",davsclaus,mouch,Major,Closed,Fixed,16/Jul/10 10:25,24/Apr/11 09:58
Bug,CAMEL-2962,12485649,camel-jms - disableReplyTo is not used in JmsProducer,The JmsProducer does not check the {{disableReplyTo}} option when sending the message.,davsclaus,davsclaus,Minor,Closed,Fixed,19/Jul/10 08:32,24/Apr/11 09:57
Bug,CAMEL-2963,12485733,dead letter channel - Should enforce the MEP to be InOnly,"When a message is moved into DLQ the MEP should be enforced to be InOnly as we should not expect a reply. For example if using a JMS queue as DLQ. To avoid JmsProducer will expect a reply and wait for it, which never comes and then timeout after 20 sec.",davsclaus,davsclaus,Minor,Closed,Fixed,19/Jul/10 08:40,24/Apr/11 09:57
Bug,CAMEL-2968,12485690,camel feature of spring2.5 is deployed with wrong name ,"Camel 2.4.0 karaf feature with spring 2.5 is deployed with spring3 name.
http://repo2.maven.org/maven2/org/apache/camel/karaf/apache-camel/2.4.0/apache-camel-2.4.0-features-spring3.xml
",njiang,njiang,Major,Closed,Fixed,20/Jul/10 08:33,24/Apr/11 09:57
Bug,CAMEL-2972,12485541,onException not working correctly when using routeContext,"When using a routeContext, the last (in the xml-file) defined route-scoped <onException> configuration is shared by the other routes in the context.

http://camel.465427.n5.nabble.com/possible-onException-bug-when-using-routeContext-td1616244.html#a1616244",davsclaus,ankelee,Major,Closed,Fixed,20/Jul/10 11:46,24/Apr/11 09:57
Bug,CAMEL-2978,12485603,java.net.SocketException: Too many open files with Apache Camel(Netty TCP) 2.4.0.,"I've got a unit test that works fine with Apache Camel 2.3.0, but as soon as I upgraded to 2.4.0 it consistently started to fail.

It performs a number of concurrent requests using this url:
netty:tcp://localhost:2048?sync=true
In both the client and server side of the unit test.

There's also a sister test which does the same thing with Netty directly and that works in isolation, so it would appear something has been broken in the transition to 2.4.0.  Previously this code was also using a beta version of Netty, but even updating that specific dependency has made no difference.",davsclaus,seanparsons,Major,Closed,Fixed,20/Jul/10 22:46,24/Apr/11 09:57
Bug,CAMEL-2979,12485630,FtpComponent: If login fails and disconnect=true another connection is opened.,"In a route such as below a second connection to the ftp server is opened if the login fails.

<route> 
        <from uri=""ftp:localhost/inbox/?username=usr&password=pwd&disconnect=true&consumer.delay=60s&maximumReconnectAttempts=0"" /> 
        <to uri=""file:test_data"" /> 
</route> 

Further description: http://camel.465427.n5.nabble.com/FTP-Try-login-once-and-disconnect-if-failure-td1692660.html#a1692660

",davsclaus,ankelee,Minor,Closed,Fixed,21/Jul/10 09:07,24/Apr/11 09:57
Bug,CAMEL-2980,12485488,camel-jpa doesn't use EntityManager.merge(entity) in the right way,"The EntityManager.merge(entity) method returns the merged entity. The exchange in message body should be updated with the merged entity (what camel-jpa currently not do).
I running into this problem, because after saving an entity in the database, the id field was still null. So, we don't know, which entity in the database is the corresponding to this entity... :-(

Have a look in the process method of [JpaProducer|http://svn.apache.org/viewvc/camel/trunk/components/camel-jpa/src/main/java/org/apache/camel/component/jpa/JpaProducer.java?view=markup] and the Java doc from the [EntityManager|http://download.oracle.com/docs/cd/E17477_01/javaee/5/api/javax/persistence/EntityManager.html#merge%28T%29].

Christian",muellerc,muellerc,Major,Closed,Fixed,21/Jul/10 14:55,24/Apr/11 09:57
Bug,CAMEL-2986,12485716,IllegalStateException in CamelContinuationServlet under heavy load,"Reason is a race between continuation.suspend() and continuation.resume(). It can occur that continuation.resume() is executed before continuation.suspend() leading to an IllegalStateException thrown by Jetty:

{noformat}
java.lang.IllegalStateException: DISPATCHED,initial
    at org.eclipse.jetty.server.AsyncContinuation.dispatch(AsyncContinuation.java:364)
    at org.eclipse.jetty.server.AsyncContinuation.resume(AsyncContinuation.java:769)
    at org.apache.camel.component.jetty.CamelContinuationServlet$1.done(CamelContinuationServlet.java:85)
    at org.apache.camel.management.InstrumentationProcessor$1.done(InstrumentationProcessor.java:78)
    at org.apache.camel.processor.UnitOfWorkProcessor$1.done(UnitOfWorkProcessor.java:104)
    at org.apache.camel.processor.DefaultChannel$1.done(DefaultChannel.java:262)
    at org.apache.camel.processor.RedeliveryErrorHandler$1.done(RedeliveryErrorHandler.java:302)
    at org.apache.camel.management.InstrumentationProcessor$1.done(InstrumentationProcessor.java:78)
    at org.apache.camel.management.InstrumentationProcessor$1.done(InstrumentationProcessor.java:78)
{noformat}

This is the case when an async completion callback is faster than the thread that is concurrently executing the {{if (continuation.isInitial())}} block. 

I'll commit a fix soon that also follows the recommendations in http://wiki.eclipse.org/Jetty/Feature/Continuations to always call continuation.suspend() before registering the continuation with a callback handler.
",mrt1nz,mrt1nz,Major,Closed,Fixed,22/Jul/10 19:19,24/Apr/11 09:58
Bug,CAMEL-2991,12485562,Restart CamelContext won't restart services on CxfBeanEndpoint,"A route that sends to a CxfBean destination is unusable after the CamelContext has been stopped and re-started.  The following is an example of a route that demonstrates the problem.

{code}
	<camelContext id=""camel"" xmlns=""http://camel.apache.org/schema/spring"">
		<route>
			<from uri=""jetty:http://localhost:9000?matchOnUriPrefix=true"" />
			<to uri=""cxfbean:customerServiceBean"" />
		</route>
	</camelContext>

	<util:list id=""customerServiceBean"">
		<bean class=""org.apache.camel.component.cxf.jaxrs.testbean.CustomerService"" />
	</util:list>
{code}

The follow issues have been identified.

1) SendProcessor should implement the Navigate interface so that its children (if they are Services) can be managed by the lifecycle methods.  This will address any destination endpoint that requires services to be re-started.

2) CxfBeanEndpoint should be a Service itself.  The reason why the start() on CxfBeanEndpoint gets called during the first start is because CxfBeanComponent.start() gets invoked during route definition initialization.   The problem is that re-starting doesn't trigger the CxfBeanComponent.start() method.  Therefore, putting the hook in CxfBeanComponent to start/stop CxfBeanEndpoint was a bug.

3) The default type converter which is a Service won't get ""re-started' as the (lazy-instantiation) getTypeConverter() method won't perform addService() on a created but ""stopped' typeConverter.  We need to discuss the correct remedy since I am not sure how to fix it.  It is kind of strange that the type converter is a service (which has the start/stop semantics) and yet it can be used even the CamelContext is stopped (or never started).  In the patch, I just set the type converter to null in DefaultCamelContext doStop() for now.",davsclaus,wtam,Major,Closed,Fixed,25/Jul/10 03:02,24/Apr/11 09:58
Bug,CAMEL-2994,12485607,camel-cxf - CxfClientCallback is invoked twice when only one was expected,"Run the test CXFWsdlOnlyTest

Because CXF invokes the CxfClientCallback 2 times there is a race condition with 2 threads wanting to complete routing the Exchange.

You can then end up with errors such as
{code}
>>>>>>>>>
2010-07-26 13:22:13,305 [default-workqueue-1                ] INFO  Logger                         - Exchange[Message: [Body is instance of java.io.InputStream]]
Exception in thread ""default-workqueue-1"" java.util.NoSuchElementException
	at java.util.AbstractList$Itr.next(AbstractList.java:350)
	at org.apache.camel.processor.Pipeline$1.done(Pipeline.java:153)
	at org.apache.camel.processor.DefaultChannel$1.done(DefaultChannel.java:262)
	at org.apache.camel.processor.RedeliveryErrorHandler$1.done(RedeliveryErrorHandler.java:302)
	at org.apache.camel.management.InstrumentationProcessor$1.done(InstrumentationProcessor.java:78)
	at org.apache.camel.management.InstrumentationProcessor$1.done(InstrumentationProcessor.java:78)
	at org.apache.camel.component.cxf.CxfClientCallback.handleException(CxfClientCallback.java:77)
	at org.apache.cxf.interceptor.ClientOutFaultObserver.onMessage(ClientOutFaultObserver.java:55)
	at org.apache.cxf.transport.http.HTTPConduit$WrappedOutputStream$1.run(HTTPConduit.java:2144)
	at org.apache.cxf.workqueue.AutomaticWorkQueueImpl$2.run(AutomaticWorkQueueImpl.java:253)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
2010-07-26 13:22:13,305 [default-workqueue-2                ] INFO  Logger                         - Exchange[Message: [Body is instance of java.io.InputStream]]
Exception in thread ""default-workqueue-2"" java.util.NoSuchElementException
	at java.util.AbstractList$Itr.next(AbstractList.java:350)
	at org.apache.camel.processor.Pipeline$1.done(Pipeline.java:153)
	at org.apache.camel.processor.DefaultChannel$1.done(DefaultChannel.java:262)
	at org.apache.camel.processor.RedeliveryErrorHandler$1.done(RedeliveryErrorHandler.java:302)
	at org.apache.camel.management.InstrumentationProcessor$1.done(InstrumentationProcessor.java:78)
	at org.apache.camel.management.InstrumentationProcessor$1.done(InstrumentationProcessor.java:78)
	at org.apache.camel.component.cxf.CxfClientCallback.handleException(CxfClientCallback.java:77)
	at org.apache.cxf.interceptor.ClientOutFaultObserver.onMessage(ClientOutFaultObserver.java:55)
	at org.apache.cxf.transport.http.HTTPConduit$WrappedOutputStream$1.run(HTTPConduit.java:2144)
	at org.apache.cxf.workqueue.AutomaticWorkQueueImpl$2.run(AutomaticWorkQueueImpl.java:253)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
{code}

CXF should only invoke the {{org.apache.cxf.endpoint.ClientCallback}} one time as we have only registered this once.",njiang,davsclaus,Major,Closed,Fixed,26/Jul/10 11:21,24/Apr/11 09:57
Bug,CAMEL-2995,12485476,"charset parser should cater for quotes, both single and double quotes","See nabble
http://camel.465427.n5.nabble.com/issue-with-encoding-when-using-HTTP-component-td2227887.html#a2227887

I bet many systems may report charset in different ways such as
{code}
Content-Type:text/xml;charset=""utf-8"" 
Content-Type:text/xml;charset='utf-8' 
Content-Type:text/xml;charset=utf-8 
{code}

We should ensure that we support all ways of setting this. And there may also be spaces between so we should trim and whatnot.

The code in 2.4 may have been improved. Just creating a ticket to be sure.",njiang,davsclaus,Major,Closed,Fixed,26/Jul/10 14:21,24/Apr/11 09:58
Bug,CAMEL-3001,12485530,Endpoints are recreated instead or restarted,"The change (r979549) made by CAMEL-2991 causes every single endpoint to be recreated instead of restarted which takes Camel a (quite big) step back.  It is bad for the following reasons:

1) recreating services associated could be expensive (the design should not assume recreating the services is cheap).
2) states (if any) in Endpoint/Service could not be preserved (stop does not mean destroy after all).   
3) it is unnecessary to recreate to the whole shebank and it is an unexpected behavior for the users.  (Users think stop but it is gone.)

In the code (DefaultCamelContext.doStartCamel()), one can clearly see the original intent of the author (who even bothered to put in a nice comment) is to avoid re-initialization the route definition after stopping the camel context.  (Well, things may change but not necessarily for the better.)

{code}
       // To avoid initiating the routeDefinitions after stopping the camel context
        if (!routeDefinitionInitiated) {
            startRouteDefinitions(routeDefinitions);
            routeDefinitionInitiated = true;
        }
{code}

But, the change simply reset the routeDefinitionInitiated flag in doStop() which is the exact opposite of the original intention and routeDefinitionInitiated is unless pretty much.  :-( 
",davsclaus,wtam,Major,Closed,Fixed,27/Jul/10 15:46,24/Apr/11 09:57
Bug,CAMEL-3007,12485225,Adding route from XML should honor its autoStartup flag,"If a route is adding after CamelContext has been started, then it's autoStartup flag should be honored. In case the flag is autoStartup=false, the route should not be auto started.",davsclaus,davsclaus,Minor,Closed,Fixed,29/Jul/10 04:01,24/Apr/11 09:57
Bug,CAMEL-3008,12485437,Starting CamelContext with autoStartup=false should startup JMX connector,"I recall some user saying something about JMX appears to not be loaded if he has autoStartup=false on the CamelContext.
It should only be the routes which are not started. The other stuff should start.

You may need to use JMX to start the routes there after :)",davsclaus,davsclaus,Minor,Closed,Fixed,29/Jul/10 04:03,24/Apr/11 09:58
Bug,CAMEL-3011,12485406,Using multiple CamelContext with quartz scheduler so keep track of the individual camel contexts,"The default will by default only register the first CamelContext. Which is used in non statefull jobs to lookup endpoints etc.

Instead the camel-quartz component should keep track of all the different camel contexts and use the id of the context on the job so it can lookup the context.",davsclaus,davsclaus,Minor,Closed,Fixed,29/Jul/10 10:03,24/Apr/11 09:58
Bug,CAMEL-3015,12485318,MailConsumer - Use OnCompletion for commit/rollback,"See nabble
http://camel.465427.n5.nabble.com/Mail-component-velocity-and-NullpointerException-td2256742.html#a2259340

This ensure the mail message is kept and we can do commit/rollback without impact of mail message being lost during the routing",davsclaus,davsclaus,Major,Closed,Fixed,30/Jul/10 09:21,24/Apr/11 09:57
Bug,CAMEL-3018,12487429,Building not possible with maven3,"maven3 is a way stricter with duplicated specified dependencies, so its not possible to compile camel with it. ",davsclaus,norman,Major,Closed,Fixed,02/Aug/10 11:49,24/Apr/11 09:57
Bug,CAMEL-3022,12485423,camel-cxf consumer should support take the fault message,"CxfConsumer will try to cast the fault message body to a Throwable, if  the fault message body is any other message, it will cause some trouble like this.
{code}
java.lang.ClassCastException: javax.xml.transform.dom.DOMSource cannot be cast to java.lang.Throwable
at org.apache.camel.component.cxf.CxfConsumer$1.invoke(CxfConsumer.java:99)
at org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:58)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at org.apache.cxf.workqueue.SynchronousExecutor.execute(SynchronousExecutor.java:37)
at org.apache.cxf.interceptor.ServiceInvokerInterceptor.handleMessage(ServiceInvokerInterceptor.java:106)
at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:243)
at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:110)
at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.serviceRequest(JettyHTTPDestination.java:312)
at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.doService(JettyHTTPDestination.java:276)
at org.apache.cxf.transport.http_jetty.JettyHTTPHandler.handle(JettyHTTPHandler.java:70)
at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
at org.mortbay.jetty.Server.handle(Server.java:322)
at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)
at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)
at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)
at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{code}",njiang,njiang,Major,Closed,Fixed,03/Aug/10 13:52,24/Apr/11 09:57
Bug,CAMEL-3031,12484839,Auto assigned CamelContext id should be unique in the JVM,"See nabble
http://camel.465427.n5.nabble.com/multiple-camel-contexts-as-viewed-from-JMX-tp2265902p2265902.html

Route ids are being unique, so we should be able to do that for CamelContext id as well. And we should reject starting an application if an existing CamelContext exists with the same id. This applies to JMX.",davsclaus,davsclaus,Major,Closed,Fixed,06/Aug/10 05:42,24/Apr/11 09:58
Bug,CAMEL-3036,12485044,camel-web seems a bit borked viewing an endpoint in tomcat,"e.g. try ""mvn tomcat:run"" then try navigate to an endpoint to try send a message to it.",jstrachan,jstrachan,Major,Closed,Fixed,09/Aug/10 10:23,24/Apr/11 09:57
Bug,CAMEL-3037,12484682,camel archetype complains about Java.lang.ClassNotFoundException: com.sun.xml.bind.v2.ContextFactory in JDK 1.5,"Here is the mail thread about it.
http://camel.465427.n5.nabble.com/Camel-2-4-0-NoClassDefFoundError-org-springframework-context-SmartLifecycle-appears-in-camel-in-acti-td2268328.html#a2268328",njiang,njiang,Major,Closed,Fixed,09/Aug/10 11:57,24/Apr/11 09:57
Bug,CAMEL-3040,12484712,The OSGiIntegrationSpringTest's setup method is called twice,"After checking the @Before annotation's java doc, you will find that the methods which are annotated with @Before will be called even it's in the superclasses.",njiang,njiang,Major,Closed,Fixed,11/Aug/10 03:33,24/Apr/11 09:57
Bug,CAMEL-3044,12484779,LoggingErrorHandler interfere with onException,"Looks like if you have an onException to catch and handle an exception, the logging error handler may interfere and cause the message to be logged as well, which causes the onException to kick in due the logging action is also a route process. (possible endless loop)",davsclaus,davsclaus,Major,Closed,Fixed,11/Aug/10 11:52,24/Apr/11 09:57
Bug,CAMEL-3045,12485142,Flatpack component : unable to set the delimiter parameter in spring config file,"It's not possible to set the delimiter parameter in spring configuration file or I don't find the way to do it !
For example : 
<route id=""routeA"">
   <from uri=""file:src/test/resources/csv?move=done/&amp;fileName=testfileRouteA.csv"" />
   <to uri=""flatpack:delim:META-INF/Delimited.pzmap.xml?delimiter=;"" />
</route>

I get the following error :
Exception in thread ""main"" org.apache.camel.RuntimeCamelException: org.apache.camel.FailedToCreateRouteException: Failed to create route routeA at: >>> To[flatpack:delim:META-INF/Delimited.pzmap.xml?ignoreFirstRecord=false&splitRows=true&delimiter=;] <<< in route: Route[[From[file:src/test/resources/csv?move=done/&fileName=... because of Failed to resolve endpoint: flatpack://delim:META-INF/Delimited.pzmap.xml?delimiter=%3B&ignoreFirstRecord=false&splitRows=true due to: Could not find a suitable setter for property: delimiter as there isn't a setter method with same type: java.lang.String nor type conversion possible: No type converter available to convert from type: java.lang.String to the required type: char with value ;
	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1126)
	at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:103)
	at org.apache.camel.spring.CamelContextFactoryBean.onApplicationEvent(CamelContextFactoryBean.java:231)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:97)
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:303)
	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:911)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:428)
	at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:139)
	at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:93)
	at org.apache.camel.spring.Main.createDefaultApplicationContext(Main.java:219)
	at org.apache.camel.spring.Main.doStart(Main.java:173)
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:56)
	at org.apache.camel.impl.MainSupport.run(MainSupport.java:114)
	at org.apache.camel.impl.MainSupport.run(MainSupport.java:291)
	at org.apache.camel.spring.Main.main(Main.java:97)

If it's a bug, to solve it, it's just necessary to change the class org.apache.camel.component.flatpack.DelimitedEndpoint as below :
- change the flied definition like this (and getter/setter to):
    private String delimiter = "","";
    private String textQualifier = ""\"""";

- and the the method createParser like this :
    public Parser createParser(Exchange exchange) throws InvalidPayloadException, IOException {
        Reader bodyReader = ExchangeHelper.getMandatoryInBody(exchange, Reader.class);
        Resource resource = getResource();
        if (delimiter == null || delimiter.equals(""""))
        	delimiter = "","";        
        if (textQualifier == null || textQualifier.equals(""""))
        	textQualifier = ""\"""";
        if (resource == null) {
            return getParserFactory().newDelimitedParser(bodyReader, delimiter.charAt(0), textQualifier.charAt(0));
        } else {
            return getParserFactory().newDelimitedParser(new InputStreamReader(resource.getInputStream()), bodyReader, delimiter.charAt(0), textQualifier.charAt(0), ignoreFirstRecord);
        }
    }

",davsclaus,rsinelle,Major,Closed,Fixed,11/Aug/10 12:38,24/Apr/11 09:58
Bug,CAMEL-3047,12487534,"JettyHttpComponent.doStop() shuts down all servers in the VM, not just those associated with the component","We are running several bundles in Karaf with separate Camel contexts, each of which uses the camel-jetty component to expose services over HTTP.  Each bundle has an assigned port and may listen on multiple URIs.  We noticed that when we updated or shut down one of these bundles, all of the Jetty servers in the other bundles would stop listening on their respective ports.

The problem is that the map of ConnectorRef objects in JettyHttpComponent is static, and therefore shared across the entire VM.  Changing this from static to an instance variable fixed the issue for us.  ",njiang,pegli,Major,Closed,Fixed,11/Aug/10 22:33,24/Apr/11 09:58
Bug,CAMEL-3049,12484678,Using custom global interceptor can cause routes to not entirely warmup,"See nabble
http://camel.465427.n5.nabble.com/Camel-2-4-InterceptStrategy-error-tp2473088p2473088.html",davsclaus,davsclaus,Major,Closed,Fixed,12/Aug/10 11:15,24/Apr/11 09:57
Bug,CAMEL-3050,12484692,RouteBuilderRef should work out of the box with Spring 3 and dependency injection,"Spring 3 changed internally how dependency resolution works. Its now worse as we have to play tricks in the Camel namespace handler to tell Camel the various pieces you can ref, should depend on Camel. Otherwise the dependency resolution in Spring is not working properly.

This used to work like a charm in Spring 2.5.6.",davsclaus,davsclaus,Major,Closed,Fixed,12/Aug/10 13:30,24/Apr/11 09:57
Bug,CAMEL-3056,12484954,dummyTrustManager does not work as described,"The documentation for the Mail component (http://camel.apache.org/mail.html) indicates that the {{dummyTrustManager}} option will allow you to skip over the certificate check. Using that option in a Mail component's URI results in:

{quote}
Caused by: java.io.IOException: Couldn't connect using SSL socket factory class null to host, port: my.email.server.com, -1; Exception: java.lang.ClassNotFoundException: org.apache.camel.component.mail.security.DummySSLSocketFactory
	at com.sun.mail.util.SocketFetcher.getSocket(SocketFetcher.java:216)
	at com.sun.mail.iap.Protocol.<init>(Protocol.java:109)
	at com.sun.mail.imap.protocol.IMAPProtocol.<init>(IMAPProtocol.java:104)
	at com.sun.mail.imap.IMAPStore.protocolConnect(IMAPStore.java:585)
	... 13 more
Caused by: java.lang.ClassNotFoundException: org.apache.camel.component.mail.security.DummySSLSocketFactory
{quote}

This is because of a decision made in CAMEL-1215 to move it to test. I think the documentation implies that this is a tool you could use (cautiously) in the development phase, and shouldn't be restricted to the unit tests. Either way, the class needs to be moved back into {{camel-mail.jar}} or the documentation needs to remove this option.",davsclaus,varnerac,Minor,Closed,Fixed,14/Aug/10 12:49,24/Apr/11 09:57
Bug,CAMEL-3060,12484657,Out of Heap memory issues with Camel-File processing large files,"Camel-File component throws heap memory issue when processing csv file which is about 45MB with 218k Lines. 

http://camel.465427.n5.nabble.com/Java-heap-space-issue-with-reading-large-CSV-file-tt2638903.html#a2638903",davsclaus,v_cheruvu@hotmail.com,Minor,Closed,Fixed,18/Aug/10 13:52,24/Apr/11 09:57
Bug,CAMEL-3064,12484640,Aggregator - Exception thrown from custom aggregation strategy could cause BatchSender thread to terminate,We should use try .. catch to catch custom exceptions being thrown from AggregationStrategy,davsclaus,davsclaus,Major,Closed,Fixed,19/Aug/10 14:03,17/Jan/11 16:50
Bug,CAMEL-3069,12484612,JMX statistics is incomplete for processors,"The statistics for routes is correct. However for processors the stats is wrong. For example {{ExchangesCompleted}} appears to be doubled.
",davsclaus,davsclaus,Major,Closed,Fixed,23/Aug/10 15:28,24/Apr/11 09:57
Bug,CAMEL-3077,12484607,Cache Component needs to check for null values during GET operations,"EhCache often has a bit of lag time when invalidating expired cache elements, first setting the Element value to null and then removing the key. If you are hitting a cache rapidly one often will run across a key that is present in the cache that still has a null element entry.

The logic for successfully key retrieval just needs to be slightly tweaked to check for null values.",davsclaus,deckerego,Major,Closed,Fixed,25/Aug/10 03:22,24/Apr/11 09:57
Bug,CAMEL-3078,12484613,"Cache Component configuration requires MemoryStoreEvictionPolicy instance, will not accept parameter as part of URI","In Spring, if one attempts to specify a cache endpoint as:
<endpoint id=""myCache"" uri=""cache://MyCache?memoryStoreEvictionPolicy=MemoryStoreEvictionPolicy.FIFO""/>
an exception will be thrown that the String ""MemoryStoreEvictionPolicy.FIFO"" was not a proper Java Object and no TypeConverter is available. This can be worked-around by manually creating a type converter that performs:
    String policyName = evictionPolicy.replace(""MemoryStoreEvictionPolicy."", """");
    return MemoryStoreEvictionPolicy.fromString(policyName);

Or one could just try to create a new instance from reflection. Above way is a bit more manageable however, since EhCache is taking care of the conversion for you.

",davsclaus,deckerego,Major,Closed,Fixed,25/Aug/10 03:29,24/Apr/11 09:58
Bug,CAMEL-3079,12484611,job rescheduling and clustering does not work properly ,"Currently stateful cron jobs are identified by their group name, job name and the cron expression. This prevents an easy rescheduling of cron jobs. For instance, stopping a camel context, rescheduling the cron job by editing the cron expressing and restart will end up in an exception. This will happen because the rescheduled job will be added as an additional job. The already existing job will produce an exception because the corresponding endpoint doesn't exist anymore. The previous solution deleting all triggers on shutdown doesn't work in a cluster scenario. 

I suggest to identify cron jobs only by their group and job name. On startup it will check if a trigger already exists and check if the cron expression has changed. If so it will be rescheduled.

Also the current explicit resuming of stateful jobs will produce an exception during startup, because the scheduler automatically finds and resumes stored triggers. 
",davsclaus,idueppe,Major,Closed,Fixed,25/Aug/10 15:26,24/Apr/11 09:58
Bug,CAMEL-3081,12487416,netty timeout exception,"eventhough message was received by netty procuder, it thows a timeout exception after 30 secs. here is the route and log. as this is a basic functionality I think this is critical priority.

		from(""netty:tcp://localhost:6205?textline=true&sync=true"").process(new Processor() {
		    int i=0;
			public void process(Exchange exchange) throws Exception {
		        String body = exchange.getIn().getBody(String.class);
				Thread.sleep(5000);
		        exchange.getOut().setBody(""Bye 1"" + body);
		       // if(i++%2==1) throw new IOException(""custom exception"");
		    }
		});


		from(""file:///test/test/response"")
		.convertBodyTo(String.class)
		.threads(1)
		.to(ExchangePattern.InOut,""netty:tcp://localhost:6205?textline=true&sync=true"")
		.to(""log:+++ reply++++"");

2010-08-25 11:33:29,963 [1 - file:///test/test/response] FileConsumer                   DEBUG Total 1 files to consume
2010-08-25 11:33:29,963 [1 - file:///test/test/response] FileConsumer                   DEBUG About to process file: GenericFile[C:\test\test\response\hello1.txt] using exchange: Exchange[GenericFileMessage with file: GenericFile[C:\test\test\response\hello1.txt]]
2010-08-25 11:33:31,510 [      Camel Thread 6 - Threads] SendProcessor                  DEBUG >>>> Endpoint[tcp://localhost:6205] Exchange[GenericFileMessage with body: hello1]
2010-08-25 11:33:31,682 [      Camel Thread 6 - Threads] NettyProducer                  DEBUG Creating connector to address: localhost:6205
2010-08-25 11:33:31,682 [      Camel Thread 6 - Threads] NettyProducer                  DEBUG Writing body: hello1

2010-08-25 11:33:31,697 [    New I/O client worker #1-1] NettyProducer                  DEBUG Operation complete org.jboss.netty.channel.DefaultChannelFuture@944dbd
2010-08-25 11:33:31,697 [    New I/O server worker #1-1] ServerChannelHandler           DEBUG Incoming message: hello1
2010-08-25 11:33:36,697 [    New I/O server worker #1-1] ServerChannelHandler           DEBUG Writing body: Bye 1hello1

2010-08-25 11:33:36,697 [    New I/O client worker #1-1] ClientChannelHandler           DEBUG Message received: Bye 1hello1
2010-08-25 11:33:36,697 [    New I/O client worker #1-1] SendProcessor                  DEBUG >>>> Endpoint[log://+++ reply++++] Exchange[GenericFileMessage with body: Bye 1hello1]
2010-08-25 11:33:36,713 [    New I/O client worker #1-1] +++ reply++++                  INFO  Exchange[ExchangePattern:InOut, BodyType:String, Body:Bye 1hello1]
2010-08-25 11:33:36,713 [    New I/O client worker #1-1] GenericFileOnCompletion        DEBUG Done processing file: GenericFile[C:\test\test\response\hello1.txt] using exchange: Exchange[GenericFileMessage with body: Bye 1hello1]
2010-08-25 11:33:36,713 [    New I/O client worker #1-1] FileUtil                       DEBUG Tried 1 to delete file: C:\test\test\response\hello1.txt.camelLock with result: true
2010-08-25 11:33:36,713 [    New I/O client worker #1-1] nericFileRenameProcessStrategy DEBUG Renaming file: GenericFile[C:\test\test\response\hello1.txt] to: GenericFile[C:\test\test\response\.camel\hello1.txt]
2010-08-25 11:33:36,713 [    New I/O client worker #1-1] FileUtil                       DEBUG Tried 1 to rename file: C:\test\test\response\hello1.txt to: C:\test\test\response\.camel\hello1.txt with result: true
2010-08-25 11:34:06,886 [         Hashed wheel timer #1] DefaultErrorHandler            DEBUG Failed delivery for exchangeId: f5eb372d-a214-4418-a60f-62c0557768d2. On delivery attempt: 0 caught: org.apache.camel.ExchangeTimedOutException: The OUT message was not received within: 30000 millis. Exchange[GenericFileMessage with body: Bye 1hello1]
2010-08-25 11:34:06,886 [         Hashed wheel timer #1] DefaultErrorHandler            ERROR Failed delivery for exchangeId: f5eb372d-a214-4418-a60f-62c0557768d2. Exhausted after delivery attempt: 1 caught: org.apache.camel.ExchangeTimedOutException: The OUT message was not received within: 30000 millis. Exchange[GenericFileMessage with body: Bye 1hello1]
org.apache.camel.ExchangeTimedOutException: The OUT message was not received within: 30000 millis. Exchange[GenericFileMessage with body: Bye 1hello1]
",davsclaus,skusma@arccorp.com,Major,Closed,Fixed,25/Aug/10 15:45,24/Apr/11 09:57
Bug,CAMEL-3082,12487421,BeanConveter didn't try to covert the bean invocation argument rightly,"Here is the mail thread about it.
http://camel.465427.n5.nabble.com/Bug-in-the-BeanConverter-class-td2653552.html#a2653552",njiang,njiang,Major,Closed,Fixed,26/Aug/10 07:00,24/Apr/11 09:57
Bug,CAMEL-3084,12487423,camel-jms - requestTimeout with 0 or negative value should mean no timeout,This only affects the refactored camel-jms component.,davsclaus,davsclaus,Minor,Closed,Fixed,26/Aug/10 12:11,24/Apr/11 09:57
Bug,CAMEL-3091,12487455,QuartzEndpoint does not set the job name properly,"Looks like the job.name argument is not se properly on the endpoint and defaults to the uri path.

Another nastier issue (for which I should have probably created a separate issue) is that the uri gets messed up if it does not have the double slashes after the scheme (as in {{""quartz:<uri>""}} vs {{""quartz://<uri>""}}) and the @cron argument contains '?'. The workaround (arguably) is to start the uri with {{""quartz://...""}}.

See [nabble thread|http://camel.465427.n5.nabble.com/Problem-with-cron-expression-configuration-in-camel-context-with-spring-tt2796958.html#a2796958]. Many thanks to SK for reporting this.
",hadrian,hadrian,Major,Closed,Fixed,31/Aug/10 01:58,24/Apr/11 09:57
Bug,CAMEL-3093,12487459,camel-smpp shoud honor the configured encoding,Currently camel-smpp doesn't honor the encoding option by creating the consumer/producer.,muellerc,muellerc,Major,Closed,Fixed,31/Aug/10 19:35,31/Aug/10 20:48
Bug,CAMEL-3100,12487482,${file:length} should return 0 instead of null if the file length is 0,"{code}
--- camel-core/src/main/java/org/apache/camel/component/file/GenericFile.java	(revision 991781)
+++ camel-core/src/main/java/org/apache/camel/component/file/GenericFile.java	(working copy)
@@ -122,7 +122,8 @@
             message.setHeader(""CamelFileRelativePath"", getRelativeFilePath());
             message.setHeader(Exchange.FILE_PARENT, getParent());
     
-            if (getFileLength() > 0) {
+            if (getFileLength() >= 0) {
                 message.setHeader(""CamelFileLength"", getFileLength());
             }
             if (getLastModified() > 0) {
{code}",njiang,njiang,Major,Closed,Fixed,02/Sep/10 03:48,24/Apr/11 09:57
Bug,CAMEL-3107,12487506,SmppBinding set the destination address npi instead of the source address npi,See discussion on the [user@list|http://camel.465427.n5.nabble.com/SMPP-sourceAddrNpi-bug-td2801906.html#a2801906],muellerc,muellerc,Major,Closed,Fixed,06/Sep/10 19:06,24/Apr/11 09:57
Bug,CAMEL-3108,12487541,ConsumerTemplate return body should complete UoW beforehand,Issue is discussed here: http://camel.465427.n5.nabble.com/ConsumerTemplate-not-finishing-td2642233.html#a2642233,davsclaus,ankelee,Minor,Closed,Fixed,07/Sep/10 08:50,24/Apr/11 09:58
Bug,CAMEL-3113,12487532,@QueryParam doesn't work for CxfBeans,"Charle report an issue[1], after tracing the code I found the DefaultCxfBeanBinding doesn't put the http query string into the cxf message.
So the @QueryParam will not take effect on the resource beans.
[1]http://camel.465427.n5.nabble.com/camel-cxfbean-JAX-Rs-QueryParam-td2827252.html#a2827252",njiang,njiang,Major,Closed,Fixed,10/Sep/10 01:17,24/Apr/11 09:57
Bug,CAMEL-3114,12487579,url encoding goes wrong in org.apache.camel.component.rss.RssComponent#afterConfiguration,"The method org.apache.camel.component.rss.RssComponent#afterConfiguration creates the Url that will be used to fetch the rss feed. We find that with some url's url encoding goes wrong.

consider this url: http://api.flickr.com/services/feeds/photos_public.gne?id=23353282@N05&tags=lowlands&lang=en-us&format=rss_200

AfterConfiguration() calls org.apache.camel.util.URISupport#createRemainingURI This method first calls org.apache.camel.util.URISupport#createQueryString, which is a method that will iterate over a map of request parameters, escape each param name and value using java.net.URLEncoder#encode, and put them together with all the & and = stuff to form the query string.
Then it calls org.apache.camel.util.URISupport#createURIWithQuery Which is a method that takes a URI (the base url) and the constructed query string, and simply creates a new URI with that, returning the toString() output from that.

So this is what the output of this procedure looks like: http://api.flickr.com/services/feeds/photos_public.gne?format=rss_200&id=23353282%2540N05&lang=en-us&tags=lowlands

1 the @ sign was escaped by org.apache.camel.util.URISupport#createQueryString, creating a query string like: id=23353282%40N05&tags=lowlands&lang=en-us&format=rss_200 (which is good)
2 the URI constructor then finds the % in %40 and escapes that again! creating a url like: http://api.flickr.com/services/feeds/photos_public.gne?format=rss_200&id=23353282%2540N05&lang=en-us&tags=lowlands 

Which predictably fails...

I did some tests with the URI constructor, and it seems it only escapes % chars, everything else is left alone.

I attach a groovy script that demonstrates the problem

regards,

Ernst Bunders",davsclaus,ebunders,Major,Closed,Fixed,10/Sep/10 10:53,24/Apr/11 09:58
Bug,CAMEL-3118,12487536,camel-spring causes wrong initialization-order of dependent beans,"Attached is a patch with a test that demonstrates the problem. The test uses a custom RouteBuilder ({{SampleIninitalizingRouteBuilder}}) and another bean ({{SampleIninitalizingBean}}) that both implement {{InitializingBean}}. When the beans' {{afterPropertiesSet()}} methods are called, these beans add their names to a shared list. When the {{SampleIninitalizingRouteBuilder.configure()}} method is called then ""configured"" is added to the shared list.

{code:java}
package  org.apache.camel.spring.issues

// imports omitted ...

public class SampleInitializingBean implements InitializingBean {
    private String name;
    private List<String> entries;

    public void setName(String name) {
        this.name = name;
    }

    public void setEntries(List<String> entries) {
        this.entries = entries;
    }

    public void afterPropertiesSet() {
        entries.add(name);
    }
}

public class SampleInitializingRouteBuilder extends RouteBuilder implements InitializingBean {
    private String name;
    private List<String> entries;

    public void setName(String name) {
        this.name = name;
    }

    public void setEntries(List<String> entries) {
        this.entries = entries;
    }

    public void afterPropertiesSet() {
        entries.add(name);
    }

    @Override
    public void configure() throws Exception {
        entries.add(""configured"");
    }
}
{code}

These beans are wired as follows:

{code:xml}
    <bean id=""entries1"" class=""java.util.ArrayList""/>

    <bean id=""sampleBean1""
          class=""org.apache.camel.spring.issues.SampleInitializingBean"">
        <property name=""name"" value=""test1a""/>
        <property name=""entries"" ref=""entries1""/>
    </bean>

    <bean id=""sampleRouteBuilder1""
          class=""org.apache.camel.spring.issues.SampleInitializingRouteBuilder"" depends-on=""sampleBean1"">
        <property name=""name"" value=""test1b""/>
        <property name=""entries"" ref=""entries1""/>
    </bean>

    <camelContext xmlns=""http://camel.apache.org/schema/spring"">
        <routeBuilder ref=""sampleRouteBuilder1""/>
    </camelContext>
{code}

Note the {{depends-on}} attribute on the {{sampleRouteBuilder1}} bean: it should ensure that {{sampleBean1}} is being initialized before {{sampleRouteBuilder1}} and the {{camelContext}}. 

Actual behaviour, however, is that the beans are initialized in the following order:

# {{sampleRouteBuilder1}}
# {{camelContext}}
# {{sampleBean1}}

which is definitely wrong. The shared list contains the entries

# {{test1b}}
# {{configured}}
# {{test1a}}

This differs from the expected order

# {{test1a}}
# {{test1b}}
# {{configured}}

which cannot be observed. After some debugging, it seems the problem is related to the {{CamelBeanPostProcessor.postProcessBeforeInitialization()}} method. It does a lookup of the {{camelContext}} (i.e. {{applicationContext.getBean(camelId))}}) *before* the application context finished initialization of dependent beans. The problem is that this lookup already triggers a {{SampleInitializingRouteBuilder.configure()}} method call.

Even worse, this behaviour depends on the declaration order of the beans in the application context XML file. When the {{camelContext}} bean is moved to the top, the bean initialization are done in the correct order.

To demonstrate that this is not a Spring-related problem, the attached test also contains another bean ({{SampleRouteBuilderContainer}}) that plays the role of the {{camelContext}} but does nothing else than calling {{configure()}} on the injected route builder within ({{afterPropertiesSet()}}). In this case, the bean initialization occur in the expected, correct order.

I didn't find a solution to this problem so far and need to dig in further (hope to find some time next week for that). If any of the committers (who are more familiar with camel-spring than I am) have already an idea how to solve that, I appreciate any hints.
",njiang,mrt1nz,Major,Closed,Fixed,11/Sep/10 14:51,24/Apr/11 09:57
Bug,CAMEL-3120,12487547,hawtdb - Should work in OSGi,"The {{decode}} method in {{ObjectCodec}} should wrap the causes stacktrace in the wrapped IOException.

For example OSGi frameworks may be pesky and we want to be able to see whatever stacktrace it may thrown on you.

{code}

    public T decode(DataInput dataIn) throws IOException {
        int size = dataIn.readInt();
        byte[] data = new byte[size];
        dataIn.readFully(data);
        ByteArrayInputStream bytesIn = new ByteArrayInputStream(data);
        ObjectInputStream objectIn = new ObjectInputStream(bytesIn);
        try {
            return (T) objectIn.readObject();
        } catch (ClassNotFoundException e) {
            throw new IOException(e.getMessage());
        }
    }
{code}

For being JDK 1.5 compatible you need to do it like
{code}
    public static IOException createIOException(String message, Throwable cause) {
        IOException answer = new IOException(message);
        answer.initCause(cause);
        return answer;
    }
{code}
",davsclaus,davsclaus,Minor,Closed,Fixed,14/Sep/10 08:43,24/Apr/11 09:57
Bug,CAMEL-3121,12487528,Splitter EIP - The sub exchanges should not contains on completions from original Exchange,"See nabble
http://camel.465427.n5.nabble.com/Camel-calling-commit-too-early-when-using-split-seda-file-endpoint-tp2830894p2830894.html",davsclaus,davsclaus,Minor,Closed,Fixed,14/Sep/10 11:39,24/Apr/11 09:57
Bug,CAMEL-3122,12487544,protobuf dataformat Spring DSL is not working,"There is the mail thread talks about it.
[1]http://camel.465427.n5.nabble.com/Problem-with-protobuf-example-Spring-DSL-to-unmarshal-to-protobuf-td2835112.html",njiang,njiang,Major,Closed,Fixed,14/Sep/10 13:24,24/Apr/11 09:57
Bug,CAMEL-3124,12487556,polling of feeds in FeedEntryPollingConsumer is broken.,"The FeedEntryPollingConsumer class implements the poll() method for the 'splitEntries' mode of the RssEndpoint is broken.

You can think of two ways that polling feeds could work:
1) A feed is created, then one item is processed, then the delay, then process another item. This way the feed is kept between calls to poll().
2) A feed is created, then all the items are processed, the feed is cleared, and then the delay.

But the way it presently works:
A feed is created, one items is processed, and the feed is cleared, then the delay, and again the feed is created and the next item is cleared.

This is clearly wrong. Feed entries can be missed, because the index of the next item to process is stored over polls but the list isn't. Also this creates a big network overhead when polling very active feeds such as twitter search...

This is easy to fix. In the below code:
{code}
public void poll() throws Exception {
        Object feed = createFeed();
        populateList(feed);   

        while (hasNextEntry()) {
            Object entry = list.get(entryIndex--);

            boolean valid = true;
            if (entryFilter != null) {
                valid = entryFilter.isValidEntry(endpoint, feed, entry);
            }
            if (valid) {
                Exchange exchange = endpoint.createExchange(feed, entry);
                getProcessor().process(exchange);
                // return and wait for the next poll to continue from last time (this consumer is stateful)
                return;
            }
        }
{code}

The return (at line 56 of org.apache.camel.component.feed.FeedEntryPollingConsumer) should be deleted.",davsclaus,ebunders,Minor,Closed,Fixed,14/Sep/10 13:59,24/Apr/11 09:57
Bug,CAMEL-3128,12487520,Using $ in endpoint uri causes thread name parser to fail,"See nabble
http://camel.465427.n5.nabble.com/How-to-specify-route-to-folder-with-in-actual-name-tp2839895p2839895.html",davsclaus,davsclaus,Minor,Closed,Fixed,16/Sep/10 13:41,24/Apr/11 09:58
Bug,CAMEL-3130,12487557,"Ref component causes consumer parameters to be cleared, such as delay and initialDelay","See nabble
http://camel.465427.n5.nabble.com/The-delay-option-seems-not-to-be-working-in-CAMEL-Java-DSL-mode-tp2840369p2840369.html",davsclaus,davsclaus,Minor,Closed,Fixed,16/Sep/10 16:33,24/Apr/11 09:57
Bug,CAMEL-3133,12487561,Camel soap dataformat does not work correctly if a method has no input or no output,"Currently the camel soap dataformat can not handle exachanges for a soap method that has no input or no output. This is a known limitation but we should support this.
",cschneider,cschneider,Minor,Closed,Fixed,17/Sep/10 12:49,25/Oct/11 11:35
Bug,CAMEL-3136,12487591,cxfbean creates another instance of the class instead of directly using the referenced bean,"... this makes it impossible to inject properties to the @WebService class

<camelContext>
        <route>
            <from uri=""...."" />
            <to uri=""cxfbean:handler"" />
        </route>
</camelContext>

<bean id=""handler"" class=""a.b.c.Handler"">
        <property name=""prop1"" value=""5"" />
</bean>

when ""handler"" is created by Spring, its 'prop1' is set to '5'

when the cxfbean:handler is triggered, the 'prop1' is null, because CXF created another instance of Handler instead of using the one created by Spring",davsclaus,di_m_an,Major,Closed,Fixed,19/Sep/10 13:18,24/Apr/11 09:57
Bug,CAMEL-3137,12487593,ftp login anonymous should send empty string as password instead of null parameter,"See nabble
http://camel.465427.n5.nabble.com/anonymous-FTP-login-fails-tp2846235p2846235.html",davsclaus,davsclaus,Minor,Closed,Fixed,20/Sep/10 07:01,24/Apr/11 09:57
Bug,CAMEL-3141,12487612,Route autoStartup configurable through Property,"Using Camel 2.4 I want to make the usage of a certain route configurable through properties. 


{code}
<camelContext id=""camel"" trace=""true""
	xmlns=""http://camel.apache.org/schema/spring"">
	
	<route id=""configurable_1"" autoStartup=""{{startup_1}}"">
		<from uri=""direct:start_1""/>
		<to uri=""mock:end_1""/>
	</route>
		
	<route id=""configurable_2"" autoStartup=""{{startup_2}}"">
		<from uri=""direct:start_2""/>
		<to uri=""mock:end_2""/>
	</route>

</camelContext>
{code}

But if I do use the PropertyPlaceholder on the CamelContext like below it works. 

{code}
<camelContext id=""camel"" trace=""true"" autoStartup=""{{startup_context}}""
{code}

So is this a bug or should this work this way?
",davsclaus,achim_nierbeck,Minor,Closed,Fixed,21/Sep/10 12:31,24/Apr/11 09:58
Bug,CAMEL-3143,12487627,OsgiDefaultCamelContext.getTypeConverterRegistry() returns null ,"The showed up when using dozer as shown in http://camel.apache.org/dozer-type-conversion.html, whose DozerTypeConverterLoader tries to get the type converter registry using: {{{TypeConverterRegistry registry = camelContext.getTypeConverterRegistry();}}}

Plausible error:
{code:title=OsgiDefaultCamelContext.java}
   @Override
    protected TypeConverter createTypeConverter() {
        return new OsgiTypeConverter(bundleContext, getInjector());
    }
{code}

{code:title=DefaultCamelContext.java}
public TypeConverterRegistry getTypeConverterRegistry() {
        if (typeConverterRegistry == null) {
            // init type converter as its lazy
            if (typeConverter == null) {
                getTypeConverter();
            }
            // type converter is usually the default one that also is the registry
            if (typeConverter instanceof DefaultTypeConverter) {
                typeConverterRegistry = (DefaultTypeConverter) typeConverter;
            }
        }
        return typeConverterRegistry;
    }
{code}

Error:
getTypeConverter() returns an OsgiTypeConverter 
OsgiTypeConverter does not inherit from DefaultTypeConverter, thus the instanceof returns false
=> null is returned

Solution:
Lots of different ways to do this, and it's getting late here. In this case, maybe it's OsgiDefaultCamelContext's responsibility to also override getTypeConverterRegistry with something along the lines of:
{code}
@Override
public TypeConverterRegistry getTypeConverterRegistry() {
        if (typeConverterRegistry == null) {
            // init type converter as its lazy
            if (typeConverter == null) {
                getTypeConverter();
            }
            // type converter is usually the default one that also is the registry
            if (typeConverter instanceof OsgiDefaultTypeConverter) {
                typeConverterRegistry = ((OsgiDefaultTypeConverter) typeConverter).getRegistry();
            }
        }
        return typeConverterRegistry;
    }
{code}

We've employed an (ugly) workaround in a local version of DozerTypeConverterLoader.java:
{code:java}
        TypeConverter typeConverter = camelContext.getTypeConverter();
        DefaultTypeConverter registry = null;
        if (typeConverter instanceof DefaultTypeConverter) {
            registry = (DefaultTypeConverter)typeConverter;
        } else if (typeConverter instanceof OsgiTypeConverter) {
            OsgiTypeConverter osgiTypeConverter = (OsgiTypeConverter)typeConverter;
            registry = osgiTypeConverter.getRegistry();
        }
{code}",davsclaus,cl4es,Major,Closed,Fixed,21/Sep/10 19:40,24/Apr/11 09:58
Bug,CAMEL-3144,12487632,camel-ftp: fileExist=Fail not honored when producer works in FTP root path,"when using a route with ftp producer such as :

 <to uri=""ftp://user@host/?fileExist=Fail&fileName=test&password=mypass"" />

The route will not correctly fail if the fileName already exists.

It will work in any sub-directory:

 <to uri=""ftp://user@host/mydir?fileExist=Fail&fileName=test&password=mypass"" />

The root cause of the bug is an incorrect test in org.apache.camel.util.FileUtil.onlyPath() in component camel-core.
This method returns null when the parameter string is ""/"". It should return ""/"".

The attached patch fixes the issue.
",hadrian,raphael@apache.org,Major,Closed,Fixed,21/Sep/10 19:59,24/Apr/11 09:57
Bug,CAMEL-3151,12487855,NullPointerException in CXF Producer if no type converter is available,"When I am trying to send the content of a file to an CXF endpoint in PAYLOAD format, I get a NullPointerException in line 603 of the CXFEndpoint class (in the current trunk). The offending coding is:
                CxfPayload<?> payload = (CxfPayload<?>)params[0];
                List<Element> elements = payload.getBody();

The params are set in line 282 of the CxfProducer class:
            params = new Object[1];
            // TODO: maybe it should be mandatory body?
            params[0] = exchange.getIn().getBody(CxfPayload.class);

The fix is most probably trivial (change to getMandatoryBody()) and add a throws InvalidPayloadException to the getParams() method of CxfProducer). After this is fixed (the same for the MESSAGE format two lines lower), the example will still not work, but I get a meaningful error message saying that there is no type converter between GenericFile and CxfPayload.

One could argue, that under some circumstances null payloads may be legal (I just don't see any), but in this case the CxfEndpoint class would have to be modified to avoid the NullPointerException. As the coding is a missing converter or a null payload will trigger NullPointerExceptions for CxfEndpoints unconditionally.",davsclaus,siano,Minor,Closed,Fixed,23/Sep/10 10:57,24/Apr/11 09:57
Bug,CAMEL-3158,12487650,PollingConsumerSupport.start() do not get called,"I have a subclass of PollingConsumerSupport and create it in a subclass of DefaultPollingEndpoint. The problem is that DefaultPollingEndpoint wraps PollingConsumer into DefaultScheduledPollConsumer and the latter does not call PollingConsumerSupport.start:

DefaultScheduledPollConsumer.java:

    @Override
    protected void doStart() throws Exception {
        pollingConsumer = getEndpoint().createPollingConsumer();
        super.doStart();
    }
",davsclaus,azarov,Major,Closed,Fixed,25/Sep/10 12:18,24/Apr/11 09:58
Bug,CAMEL-3170,12487692,camel-ftp - Setting password and username using UserInfo on uri does not work,"You should be able to set ftp endpoint uri as:
{code}
""ftp://scott@localhost:"" + getPort() + ""/deletefile?password=tiger&binary=false&delete=true""
{code}

And when using password and username in the userinfo part of the uri:
{code}
""ftp://tiger:scott@localhost:"" + getPort() + ""/deletefile?binary=false&delete=true""
{code}

The latter didn't work",davsclaus,davsclaus,Minor,Closed,Fixed,28/Sep/10 11:54,24/Apr/11 09:57
Bug,CAMEL-3174,12487792,camel-ftp - Change dir before retrieving file,"See nabble
http://camel.465427.n5.nabble.com/How-to-change-directory-while-using-sftp-component-tp2806817p2806817.html",davsclaus,davsclaus,Major,Closed,Fixed,29/Sep/10 06:38,24/Apr/11 09:57
Bug,CAMEL-3181,12487955,Confusing IllegalArgumentException when address attribute isn't specified on CXF endpoint.,"If you create CXF endpoint, like this, without the address attribute, 

{code:xml}
	<cxf:cxfEndpoint 
		id=""greeting"" 
		wsdlURL=""greeting.wsdl""
		serviceClass=""tutorial.hanbo.webservice.Greeting""
		>
	</cxf:cxfEndpoint>
{code} 

... then you get a really confusing error when you deploy the endpoint in ServiceMix: the error is 

{code}
Caused by: java.lang.IllegalArgumentException: endpointUri is not specified and org.apache.camel.component.cxf.CxfSpringEndpoint 
does not implement createEndpointUri() to create a default value
	at org.apache.camel.impl.DefaultEndpoint.getEndpointUri(DefaultEndpoint.java:83)
	at org.apache.camel.management.DefaultManagementLifecycleStrategy.onEndpointAdd(DefaultManagementLifecycleStrategy.java:205)
	at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:386)
{code} 

We find that if you explicitly set the address then the problem goes away (see below)

{code:xml} 
	<cxf:cxfEndpoint 
		id=""greeting"" 
		wsdlURL=""greeting.wsdl""
		address=""http://localhost:9000/GreeterContext/SOAPMessageService""
		serviceClass=""tutorial.hanbo.webservice.Greeting""
		>
	</cxf:cxfEndpoint>
{code} 

On camel-cxf web page, the 'address' attribute is not mentioned anywhere in the table of URI properties, so you might be lead to believe that it's not necessary. ",njiang,adrian.trenaman,Major,Closed,Fixed,30/Sep/10 09:13,24/Apr/11 09:57
Bug,CAMEL-3185,12487825,restlet producer - Should set status code,"See nabble
http://camel.465427.n5.nabble.com/Restlet-HTTP-status-and-message-tp3047023p3047023.html",davsclaus,davsclaus,Major,Closed,Fixed,01/Oct/10 08:24,24/Apr/11 09:57
Bug,CAMEL-3186,12487543,@EndpointInject not working in bean created by blueprint,"I am attaching a small test project that uses blueprint to create a camel context and a bean named Producer.  This project builds an OSGi bundle that I am deploying in FUSE ESB 4.3.0-fuse-01-00.  The Producer bean has an @EndpointInject annotation on a ProducerTemplate:

	@EndpointInject(ref = ""testEndpoint"")
	private ProducerTemplate testEndpoint;

The Producer bean's init method sets a timer that tries to call testEndpoint.sendBody() once per second.  This throws a null pointer exception because the testEndpoint did not get injected:

07:35:31,826 | WARN  | Producer         | Producer                         | org.aaron.camel.Producer           51 | 118 - org.aaron.camel - 1.0.0.SNAPSHOT | send exception
java.lang.NullPointerException
        at org.aaron.camel.Producer.timerPop(Producer.java:48)[118:org.aaron.camel:1.0.0.SNAPSHOT]
        at org.aaron.camel.Producer.access$000(Producer.java:11)[118:org.aaron.camel:1.0.0.SNAPSHOT]
        at org.aaron.camel.Producer$1.run(Producer.java:29)[118:org.aaron.camel:1.0.0.SNAPSHOT]
        at java.util.TimerThread.mainLoop(Timer.java:512)[:1.6.0_21]
        at java.util.TimerThread.run(Timer.java:462)[:1.6.0_21]

This same test works perfectly if I use Spring DM to create the camel context and Producer bean.
",gnodet,ariekenb,Major,Closed,Fixed,01/Oct/10 12:42,24/Apr/11 09:58
Bug,CAMEL-3187,12487834,PublishEventNotifier - Should not emit events during startup/shutdown and not spawn new events when processing event,"When an event is being send to an endpoint using PublishEventNotifier it may create new events, and so forth. This causes a flood of events.

Also further complications occur during start/shutdown of Camel when you send events to routes, which are then being graceful shutdown. Therefore this event notifier should *only* publish if camel is fully started and running.",davsclaus,davsclaus,Major,Closed,Fixed,01/Oct/10 12:51,24/Apr/11 09:57
Bug,CAMEL-3188,12487844,Concurrent consumers on seda endpoint can cause content routing to mismatch,"When consuming concurrently from a seda endpoint, when the route contains a content router based on the header, it will randomly route through the wrong choice.

In my specific case, I was consuming from an activemq queue, which would receive messages with a header that would then determine which route it would follow. It would randomly send messages down the wrong path. When I turned on tracing, it would behave itself. It also behaved itself when I limited it to only a single consumer. I was, however, able to duplicate it with the unit test below. Due to the concurrency issue, the test can occasionally pass, but run it a couple times and it should fail. It'll either receive 2 messages when it should have only gotten 1, or it will get no messages when it should have gotten 1.

{code:title=ConcurrencyTest.java|borderStyle=solid}
import org.apache.camel.EndpointInject;
import org.apache.camel.Produce;
import org.apache.camel.ProducerTemplate;
import org.apache.camel.builder.RouteBuilder;
import org.apache.camel.component.mock.MockEndpoint;
import org.apache.camel.test.CamelTestSupport;

public class ConcurrencyTest extends CamelTestSupport {

	@EndpointInject(uri = ""mock:result"")
	protected MockEndpoint resultEndpoint;
	
	@EndpointInject(uri = ""mock:otherResult"")
	protected MockEndpoint otherResultEndpoint;

	@Produce(uri = ""seda:start"")
	protected ProducerTemplate template;

	public void testSendMatchingMessage() throws Exception {
		String expectedBody = ""<matched/>"";
		
		resultEndpoint.expectedBodiesReceived(expectedBody);
		otherResultEndpoint.expectedBodiesReceived(expectedBody);

		template.sendBodyAndHeader(expectedBody, ""myDirection"", ""send"");
		template.sendBodyAndHeader(expectedBody, ""myDirection"", ""received"");

		resultEndpoint.assertIsSatisfied();
	}

	@Override
    protected RouteBuilder createRouteBuilder() {
        return new RouteBuilder() {
            public void configure() {
                from(""seda:start?concurrentConsumers=10"")
//                from(""seda:start?concurrentConsumers=1"")
                	.choice()
                		.when(header(""myDirection"").isEqualTo(""send"")).to(""mock:result"")
                		.when(header(""myDirection"").isEqualTo(""received"")).to(""mock:otherResult"");
            }
        };
    }
}
{code}",davsclaus,bfeaver,Major,Closed,Fixed,01/Oct/10 20:21,24/Apr/11 09:57
Bug,CAMEL-3189,12487838,Completed aggregated exchanges are never confirmed in the AggregationRepository,"Under certain circumstances, the completed exchanges from an aggregator would remain in the AggregationRepository and redeliver after a restart of Camel. These exchanges had already successfully completed their route, so this redelivery is in error.

My guess is that in the AggregationProcessor on line 374, the AggregateOnCompletion gets added to a UnitOfWork that doesn't ever get done() called on it... or something.

I seemed to be able to prevent the problem by changing my AggregationStrategy. The old version looked like this:
{code}
public Exchange aggregate (Exchange oldExchange, Exchange newExchange) {
  String body = """";
  if (oldExchange != null) {
    body = oldExchange.getIn().getBody(String.class);
  }
  body += newExchange.getIn().getBody(String.class);
 newExchange.getIn().setBody(body);
 return newExchange;
}
{code}

You can see that the exchanges are aggregated into the newExchange. I changed it to aggregate into the oldExchange:
{code}
public Exchange aggregate (Exchange oldExchange, Exchange newExchange) {
  String body = """";
  if (oldExchange != null) {
    body = oldExchange.getIn().getBody(String.class);
  } else {
    oldExchange = newExchange;
  }

  body += newExchange.getIn().getBody(String.class);
  oldExchange.getIn().setBody(body);
 return oldExchange;
}
{code}",davsclaus,gim,Major,Closed,Fixed,01/Oct/10 23:38,24/Apr/11 09:58
Bug,CAMEL-3193,12487430,request/reply over JMS using temporary queues - if connection lost the temporary queue is not re-created which causes the producer to not work anymore,"To recreate the problem you need a route with a jms inout endpoint that is configured to use a temporary destination for replies. Till now I was only able to show the problem with tibco ems.

- Start broker
- Start the jms consumer that replies to the request
- Start the route 
- Send a request/reoply exchange (should work)
- Stop the broker
- Wait long enough for the client to do a full reconnect (I used connectionFactory.setReconnAttemptCount(1) on the client so I do not have to wait so long)
- Start the broker -> The jms component will do a full new connect
- Send a request/reoply exchange -> Now a InvalidDestination Exception happens

The client route will not work anymore until a restart as it creates the temporary destination only once and it is invalid now.

",cschneider,cschneider,Major,Closed,Fixed,04/Oct/10 06:24,05/Oct/10 12:55
Bug,CAMEL-3198,12487448, DefaultCamelContext throws NPE in getRoute(String id)," DefaultCamelContext throws NPE in getRoute(String id) if no routes are deployed.
This happens because the ""route LinkedHashSet"" is not initialized.

Fix:
 don't do Lazy init of route  or add this to getRoute(String id):
 if (routes == null) {
	routes = new LinkedHashSet<Route>();
 } 


",hadrian,stefan_b,Minor,Closed,Fixed,05/Oct/10 15:29,24/Apr/11 09:57
Bug,CAMEL-3199,12487449,"Allow : and , inside quoted names for addresses","""Snell, Tracy"" <tjs@juicelabs.com> breaks with the current address parsing.",tjsnell,tjsnell,Minor,Closed,Fixed,05/Oct/10 15:57,24/Apr/11 09:58
Bug,CAMEL-3201,12487450,The CxfConsumer's getContinuation method might throw NPE,"Hello

When trying the latest CAMEL 2.5.0 SNAPSHOT, I got a NPE:

{noformat}
java.lang.NullPointerException
       at org.apache.camel.component.cxf.CxfConsumer$1.getContinuation(CxfConsumer.java:129)
       at org.apache.camel.component.cxf.CxfConsumer$1.invoke(CxfConsumer.java:65)
       at org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:58)
       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
...
{noformat}",hadrian,alitokmen,Major,Closed,Fixed,06/Oct/10 12:59,24/Apr/11 09:57
Bug,CAMEL-3203,12487457,Quartz routes are not started if quartz component is referenced after context was started,"Quartz routes are not active if added after camel context was already started.

Here is an elaborate description of the problem and a sample project that reproduces it: http://anydoby.com/jblog/en/java/1955
Sorry, no patch this time because I may not know enough about the internals of QuartzComponent, hesitate to offer anything but a boolean flag somewhere.",davsclaus,anydoby,Minor,Closed,Fixed,06/Oct/10 14:48,24/Apr/11 09:57
Bug,CAMEL-3216,12487475,ScheduledPollConsumer should avoid runnable thread to die to ensure its kept being scheduled,"scheduled consumers such as file / ftp uses ScheduledPollConsumer to poll. If a throwable is thrown then the thread may die due it throws that to the JDK.
We should avoid this and ensure to catch all exceptions, otherwise the thread may die, and the JDK will not re-schedule a new thread.

This cause it to stop polling.

See
http://fusesource.com/forums/thread.jspa?threadID=2320&tstart=0",davsclaus,davsclaus,Major,Closed,Fixed,11/Oct/10 15:25,24/Apr/11 09:57
Bug,CAMEL-3219,12487487,Bindy should not trim separate in case end users use tab separators or the likes,"See nabble
http://camel.465427.n5.nabble.com/Big-problem-with-csv-tab-separator-file-and-bindy-data-format-tp3207520p3207520.html",davsclaus,davsclaus,Minor,Closed,Fixed,12/Oct/10 05:23,24/Apr/11 09:57
Bug,CAMEL-3220,12487478,Threads - Should allow using 0 in core pool size,"See nabble
http://camel.465427.n5.nabble.com/ThreadPoolExecutor-configuration-tp3207845p3207845.html",davsclaus,davsclaus,Minor,Closed,Fixed,12/Oct/10 06:43,24/Apr/11 09:58
Bug,CAMEL-3223,12487504,GenericFileProducer.writeFile method creates instance of un-used InputStream,"org.apache.camel.component.file.GenericFileProducer
Method: writeFile

This method has the following statement:
InputStream payload = exchange.getIn().getBody(InputStream.class);

This internally results in calling a TypeConverter to convert an object into InputStream type. However this InputStream has not been used and is eventually closed in the finally block. 

In the same method calling method storeFile on FileOperations (boolean success = operations.storeFile(fileName, exchange);) also opens an InputStream on the same message.",davsclaus,unmarshall,Minor,Closed,Fixed,12/Oct/10 14:41,24/Apr/11 09:57
Bug,CAMEL-3231,12487500,JMX MBeans not registered for initiators,"registering the Initiator before it has been started will not register the sessions mbeans with JMX. So the acceptors are ther, but no initiators.",davsclaus,bglattfelder,Major,Closed,Fixed,13/Oct/10 14:34,24/Apr/11 09:57
Bug,CAMEL-3233,12487502,Eclipse setup targets generate invalid projects,"
The eclipse setups in the poms generates invalid  projects for camel-blueprint and camel-spring due to refering to source directories outside of the basedir.    ",hadrian,dkulp,Major,Closed,Fixed,14/Oct/10 17:17,24/Apr/11 09:57
Bug,CAMEL-3234,12487507,-Psetup.eclipse with Java6 doesn't use proper dependencies or JRE level,"
If you use -Psetup.eclipse or eclipse:eclipse to setup the projects, it sets the execution environment to J2SE-1.5.  HOWEVER, if using Java6, it doesn't include many of the dependencies that are required for Java 5, like JAXB and activation and such.

I'm attaching a patch that, when you use -Psetup.eclipse, changes the compiler plugin to 1.6 when using 1.6 ONLY for that profile.   This, in effect, sets the execution environment to workspace default (which will likely be 1.6 if using 1.6) and thus things build.   The projects then import correctly and build.

FYI: this was taken from CXF's build system.  :-)",hadrian,dkulp,Major,Closed,Fixed,14/Oct/10 18:31,24/Apr/11 09:57
Bug,CAMEL-3237,12487884,XmppEndPoint - setting login to false when creating an account results in no action,"{code}
XmppEndPoint myXmppEndPoint = new XmppEndPoint();
(...)
myXmppEndPoint.setCreateAccount(true);
myXmppEndPoint .setLogin(false);
(...)
{code}

This will result in ""no action"" in xmpp server, if setLogin true the account is created and the user stays online.

I believe it's because of the logic used in createConnection() method of XmppEndPoint that could be changed to:
{code}
if (!connection.isAuthenticated()) {
            if (user != null) {
                if (LOG.isDebugEnabled()) {
                    LOG.debug(""Logging in to XMPP as user: "" + user + "" on connection: "" + getConnectionMessage(connection));
                }
                if (password == null) {
                    LOG.warn(""No password configured for user: "" + user + "" on connection: "" + getConnectionMessage(connection));
                }

                if (createAccount) {
                    AccountManager accountManager = new AccountManager(connection);
                    accountManager.createAccount(user, password);
                }
                if(login){
                	if (resource != null) {
                		connection.login(user, password, resource);
                	} else {
                		connection.login(user, password);
                	}
                }
            } else {
                if (LOG.isDebugEnabled()) {
                    LOG.debug(""Logging in anonymously to XMPP on connection: ""  + getConnectionMessage(connection));
                }
                connection.loginAnonymously();
            }
{code}

",davsclaus,mdhomem,Minor,Closed,Fixed,15/Oct/10 16:01,24/Apr/11 09:57
Bug,CAMEL-3248,12487953,camel-exec not present in the feature,"camel-exec component is not present in the feature descriptor.

I'm gonna submit a patch for that component.",davsclaus,jb@nanthrax.net,Major,Closed,Fixed,18/Oct/10 12:18,24/Apr/11 09:57
Bug,CAMEL-3249,12487431,HawtDB file grows indefinitely over time,"When using an aggregator with the HawtDB persistent store, the file grows indefinitely until it fills the volume on which it's stored. This prevents the persistence from being used in any kind of long-running production deployment.",chirino,gim,Major,Closed,Fixed,18/Oct/10 23:19,24/Apr/11 09:57
Bug,CAMEL-3252,12487526,NPE log of the ConsumerTemplate,"When I try to run the CamelInAction code with Camel 2.5.0, I found this warning message.
{code}

[                          main] DefaultConsumerTemplate        WARN  Exception occurred during done UnitOfWork for Exchange: null. This exception will be ignored.
java.lang.NullPointerException
	at org.apache.camel.impl.DefaultConsumerTemplate.doneUoW(DefaultConsumerTemplate.java:229)[camel-core-2.5.0.jar:2.5.0]
	at org.apache.camel.impl.DefaultConsumerTemplate.receiveBody(DefaultConsumerTemplate.java:168)[camel-core-2.5.0.jar:2.5.0]
	at camelinaction.OrderCollectorBean.getOrders(OrderCollectorBean.java:17)[file:/Users/jiangning/work/camel/camelinaction/appendixC/consumer/target/classes/:]
	at camelinaction.ConsumerTemplateTest.testConsumerTemplate(ConsumerTemplateTest.java:46)[file:/Users/jiangning/work/camel/camelinaction/appendixC/consumer/target/test-classes/:]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)[:1.6.0_20]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)[:1.6.0_20]
	at java.lang.reflect.Method.invoke(Method.java:597)[:1.6.0_20]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)[junit-4.8.1.jar:]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)[junit-4.8.1.jar:]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)[junit-4.8.1.jar:]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)[junit-4.8.1.jar:]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)[junit-4.8.1.jar:]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)[junit-4.8.1.jar:]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)[junit-4.8.1.jar:]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)[junit-4.8.1.jar:]
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)[junit-4.8.1.jar:]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)[junit-4.8.1.jar:]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)[junit-4.8.1.jar:]
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)[junit-4.8.1.jar:]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)[junit-4.8.1.jar:]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)[junit-4.8.1.jar:]
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)[surefire-api-2.5.jar:2.5]
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)[surefire-api-2.5.jar:2.5]
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)[surefire-api-2.5.jar:2.5]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)[:1.6.0_20]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)[:1.6.0_20]
	at java.lang.reflect.Method.invoke(Method.java:597)[:1.6.0_20]
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)[surefire-booter-2.5.jar:2.5]
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)[surefire-booter-2.5.jar:2.5]

{code}",njiang,njiang,Minor,Closed,Fixed,19/Oct/10 10:56,24/Apr/11 09:58
Bug,CAMEL-3259,12487519,Dead links to JavaDocs items,"On the http://camel.apache.org/component.html, the link to the CamelContext class JavaDoc point to the old http://activemq.apache.org/camel/maven/camel-core/apidocs/org/apache/camel/CamelContext.html URL, which is a dead link. There some additional dead links to other JavaDoc items.

Same issue on the following pages :
  http://camel.apache.org/camelcontext.html
  http://camel.apache.org/error-handler.html
",davsclaus,hasalex,Minor,Closed,Fixed,20/Oct/10 13:42,24/Apr/11 09:57
Bug,CAMEL-3269,12487643,"CXF CamelTransport and ""Can't find input stream in message""","When using a JaxWS proxy with camel transport, to a route with only one processor, CXF fail with the message ""Can't find input stream in message"".

CamelConduit call CxfMessageHelper.getCxfInMessage which is looking for an Out part in the exchange. If the processor of the route doesn't use the Out message, or doesn't copy the in part to the out part (like the Pipeline processor), the call to the JaxWS proxy fail with ""Can't find input stream in message""",njiang,jeremie.brebec,Minor,Closed,Fixed,23/Oct/10 16:18,24/Apr/11 09:57
Bug,CAMEL-3271,12487615,packageScan does not work with camel-blueprint,"Using the following xml code does not activate the Java Camel routes that can be found in the given package;

<blueprint xmlns=""http://www.osgi.org/xmlns/blueprint/v1.0.0"">
	<camelContext xmlns=""http://camel.apache.org/schema/blueprint"">
		<packageScan>
			<package>eu.schuring.camel.blueprint.route</package>
		</packageScan>
	</camelContext>
</blueprint>

Attached is a usecase that should output messages from both a native blueprint DSL route and a Java DSL route activated by the xml section above. I've attached both the bundle and a source jar.",gnodet,cathodion,Minor,Closed,Fixed,24/Oct/10 08:02,24/Apr/11 09:57
Bug,CAMEL-3276,12487590,Multicast with pipeline may cause wrong aggregated exchange,"This is a problem when using 2 set of nested pipeline and doing a transform as the first processor in that pipeline
{code}
                from(""direct:start"").multicast(new SumAggregateBean())
                    .pipeline().transform(bean(IncreaseOne.class)).bean(new IncreaseTwo()).to(""log:foo"").end()
                    .pipeline().transform(bean(IncreaseOne.class)).bean(new IncreaseTwo()).to(""log:bar"").end()
                .end()
                .to(""mock:result"");
{code}

",davsclaus,davsclaus,Minor,Closed,Fixed,25/Oct/10 15:05,24/Apr/11 09:57
Bug,CAMEL-3277,12487595,Threading issue which lets converter loading abort,"The type converter loading is not threadsafe and fails sometimes. That will end up with around 17 type converters loaded only which will probably let your camel routes fail with conversion errors like:

{noformat}
07:04:31,112 | ERROR | qtp25205731-2783 | DefaultErrorHandler | rg.apache.camel.processor.Logger 248 | 68 - org.apache.camel.camel-core - 2.4.0.fuse-00-00 | Failed delivery for exchangeId: 53e23457-5551-4487-9fc4-87dd33687f0b. Exhausted after delivery attempt: 1 caught: java.lang.ClassCastException: org.apache.camel.converter.stream.InputStreamCache cannot be cast to org.w3c.dom.Node
java.lang.ClassCastException: org.apache.camel.converter.stream.InputStreamCache cannot be cast to org.w3c.dom.Node
at com.sun.org.apache.xpath.internal.jaxp.XPathExpressionImpl.eval(XPathExpressionImpl.java:116)[:1.6.0_17]
at com.sun.org.apache.xpath.internal.jaxp.XPathExpressionImpl.eval(XPathExpressionImpl.java:98)[:1.6.0_17]
at com.sun.org.apache.xpath.internal.jaxp.XPathExpressionImpl.evaluate(XPathExpressionImpl.java:180)[:1.6.0_17]
at org.apache.camel.builder.xml.XPathBuilder.doInEvaluateAs(XPathBuilder.java:598)[68:org.apache.camel.camel-core:2.4.0.fuse-00-00]
at org.apache.camel.builder.xml.XPathBuilder.evaluateAs(XPathBuilder.java:570)[68:org.apache.camel.camel-core:2.4.0.fuse-00-00]
at org.apache.camel.builder.xml.XPathBuilder.matches(XPathBuilder.java:122)[68:org.apache.camel.camel-core:2.4.0.fuse-00-00]
{noformat}

Tracking the issue down showed that several type converter loaders share the same registry object and fail to load all converters every once a while. That can be fixed by making the load method synchronized...see attached patch.
",davsclaus,lhein,Major,Closed,Fixed,25/Oct/10 19:46,23/May/11 22:17
Bug,CAMEL-3281,12487609,RouteBuilder - Let if fail if end user is configuring onException etc after routes,"All such cross cutting concerns must be defined before routes.

We should throw an exception if end user has configured them after routes, which is currently not supported in the DSL.",davsclaus,davsclaus,Major,Closed,Fixed,27/Oct/10 06:28,24/Apr/11 09:57
Bug,CAMEL-3286,12487616,camel-jms - Setting TaskExecutor should support both Spring 2.5 and 3.0,"See nabble
http://camel.465427.n5.nabble.com/camel-2-4-with-spring-2-5-6-TaskExecutor-issue-tp3237897p3237897.html

We need to use reflection when invoking setTaskExecutor on the spring JMS stuff.
Because at compile time it would be linked to the Spring 3.0 method signature.
And in Spring 2.5 the method signature is different.

Using reflection should allow us to support both cases. And this is only done on startup so no runtime overhead.",davsclaus,davsclaus,Minor,Closed,Fixed,28/Oct/10 04:20,25/Oct/11 11:36
Bug,CAMEL-3295,12487647,camel-blueprint - Dependency Injection seems not working,"This is just a placeholder - things like this should work.

{code}
<?xml version=""1.0"" encoding=""UTF-8""?>
<blueprint xmlns=""http://www.osgi.org/xmlns/blueprint/v1.0.0""
           xmlns:cm=""http://aries.apache.org/blueprint/xmlns/blueprint-cm/v1.0.0""
           xmlns:ext=""http://aries.apache.org/blueprint/xmlns/blueprint-ext/v1.0.0""
           xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
           xsi:schemaLocation=""http://www.osgi.org/xmlns/blueprint/v1.0.0
            http://www.osgi.org/xmlns/blueprint/v1.0.0/blueprint.xsd"">


    <camelContext xmlns=""http://camel.apache.org/schema/blueprint"" id=""camelBlueprint"">
        <route>
            <from uri=""jms:queue""/>
            <to uri=""mock:result""/>
        </route>
    </camelContext>

    <bean id=""jms"" class=""org.apache.camel.component.jms.JmsComponent"">
        <property name=""connectionFactory"" ref=""jmsConnectionPool""/>
    </bean>

    <reference id=""jmsConnectionPool"" interface=""javax.jms.ConnectionFactory""/>

</blueprint>
{code}

Currently the jmsConnectionPool is not at all passed to the JmsComponent.
The usage of JmsTemplate as debated on the mailinglists I think is of a (currently) much lesser concern.
Especially comparing a little to the ProducerCode in the servicemix-jms components.
",gnodet,joed,Minor,Closed,Fixed,30/Oct/10 05:02,24/Apr/11 09:57
Bug,CAMEL-3298,12487495,XmppPrivateChatProducer should close the connection when it is stopped.,"Here is a the mail thread[1] which is discussing about it.
http://camel.465427.n5.nabble.com/XMPP-communication-not-closed-and-new-message-rejected-td3236657.html#a3236657 ",njiang,njiang,Major,Closed,Fixed,01/Nov/10 08:38,24/Apr/11 09:58
Bug,CAMEL-3299,12487652,BeanInvocation handling LinkedHashMap cannot be converted to java.util.Map,"I'm calling a Camel proxy and passing it a single argument of type LinkedHashMap.
On the service side I'm waiting for a java.util.Map, so the converter BeanInvocation -> java.util.Map is called.
Finally it comes to BeanConverter:convertTo with type=java.util.Map and value class=BeanInvocation

then it goes to
            // maybe from is already the type we want
            if (from.isAssignableFrom(type)) {
                return body;
            }
where from=LinkedHashMap
and... the condition is false!

The LinkedHashMap is not assignable from java.util.Map,
but java.util.Map is assignable from LinkedHashMap and, I guess, that is what we want.

Please fix?
- if (from.isAssignableFrom(type)) {
+ if (type.isAssignableFrom(from)) {",njiang,di_m_an,Major,Closed,Fixed,02/Nov/10 00:12,24/Apr/11 09:57
Bug,CAMEL-3302,12487851,OsgiPackageScanClassResolver should have Non-OSGi classloader check fallback,"This is necessary when use JBI packaging for servicemix-camel ServiceUnit  so that we get chance to use SU classloader to scan packages in the ServiceUnit
",njiang,ffang,Major,Closed,Fixed,02/Nov/10 12:02,24/Apr/11 09:57
Bug,CAMEL-3303,12487841,camel-cxf bundle should not export META-INF.cxf which is also exported by cxf-bundle.,"Here is the mailing thread[1] about it.

[1] http://camel.465427.n5.nabble.com/CXF-Startup-issue-with-Camel-in-OSGI-runtime-tp3241913p3247484.html
",njiang,njiang,Major,Closed,Fixed,03/Nov/10 02:32,24/Apr/11 09:58
Bug,CAMEL-3306,12487897,Transfer-Encoding chunking implementation leaves a loop hole for error,"The description of this issue is at [1].

[1] http://camel.465427.n5.nabble.com/CXF-http-conduit-AllowChunking-does-not-work-td3247495.html#a3248727",njiang,unmarshall,Major,Closed,Fixed,03/Nov/10 16:33,24/Apr/11 09:57
Bug,CAMEL-3310,12487865,Update camel-soap to use 2.3.0 tooling,"
The camel-soap component does not build with the update to CXF 2.3.   patch will be supplied.",njiang,dkulp,Major,Closed,Fixed,04/Nov/10 15:42,24/Apr/11 09:57
Bug,CAMEL-3314,12487858,Property resolve in EIP does not work when in a sub route.,"The 2.5 feature: ""The EIP now supports property placeholders in the String based options (a few spots in Java DSL where its not possible). For example: 
<convertBodyTo type=""String"" charset=""{{foo.myCharset}}""/>"" does not work correctly when ie nested in a <choice> tag.

See discussion: http://camel.465427.n5.nabble.com/Camel-2-5-Propertyplaceholders-and-Spring-DSL-still-not-working-td3251608.html#a3251608

Example route:

This works: 
<route> 
        <from uri=""direct:in"" /> 
        <convertBodyTo type=""String"" charset=""{{charset.external}}"" />	
        <log message=""Charset: {{charset.external}}"" /> 
        <to uri=""mock:out"" /> 
</route> 

This fails: 
<route> 
        <from uri=""direct:in"" /> 
        <choice> 
                <when> 
                        <constant>true</constant> 
                        <convertBodyTo type=""String"" charset=""{{charset.external}}"" />	
                </when> 
        </choice> 
        <to uri=""mock:out"" /> 
</route> ",davsclaus,ankelee,Major,Closed,Fixed,05/Nov/10 13:29,24/Apr/11 09:57
Bug,CAMEL-3319,12487771,cxfEndpoint of camel-cxf doesn't support  the shcemaLocation element,"Here is the mail thread[1] which discusses about it.
[1]http://camel.465427.n5.nabble.com/camel-cxf-exception-when-parsing-cxf-schemaLocation-element-td3253254.html#a3253254",njiang,njiang,Major,Closed,Fixed,07/Nov/10 04:23,24/Apr/11 09:58
Bug,CAMEL-3321,12487864,SmppBinding raises NullPointerException when an SMSC insert the Short Message Data inside the message_payload field,"When an SMSC sends the DeliverSm with the short message data inside the message_payload field (in the OptionalParameter) the method
createSmppMessage(DeliverSm deliverSm) in the SmppBinding class raises a nullPointerException at the following line:

else {
            smppMessage.setBody(String.valueOf(new String(deliverSm.getShortMessage(),  
                    
this happen because deliverySm.getShortMessage return null

",muellerc,vincenzo.cacurri,Major,Closed,Fixed,08/Nov/10 11:29,24/Apr/11 09:58
Bug,CAMEL-3328,12487883,NPE on Request-Reply InOut (Test attached),"simply request reply with ActiveMQ 5.4.1 using a queue is failing in v2.5 instead is running in v.2.4

{code:java} 
import static org.junit.Assert.assertTrue;

import org.apache.activemq.ActiveMQConnectionFactory;
import org.apache.camel.Endpoint;
import org.apache.camel.Exchange;
import org.apache.camel.ExchangePattern;
import org.apache.camel.Processor;
import org.apache.camel.Producer;
import org.apache.camel.builder.RouteBuilder;
import org.apache.camel.component.jms.JmsComponent;
import org.apache.camel.impl.DefaultCamelContext;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

public class RequestReplyFailureTest
{
	private Endpoint			endpoint;
	private Producer			producer;
	private DefaultCamelContext	camel;

	@Before
	public void before() throws Exception
	{
		String brokerUrl = ""tcp://localhost:61616"";
		camel = new DefaultCamelContext();
		camel.addComponent(""jms"", JmsComponent.jmsComponentAutoAcknowledge(new ActiveMQConnectionFactory(brokerUrl)));

		final String url = ""jms:queue:test"";
		endpoint = camel.getEndpoint(url);

		camel.addRoutes(new RouteBuilder()
		{
			@Override
			public void configure() throws Exception
			{
				from(url).process(new Processor()
				{
					@Override
					public void process(Exchange exchange) throws Exception
					{
						// simply set 'pong' as response
						exchange.getOut().setBody(""pong"");
					}
				});
			}
		});

		System.out.println(camel.getVersion());
		camel.start();

		producer = endpoint.createProducer();
	}

	@After
	public void after() throws Exception
	{
		camel.stop();
	}

	/**
	 * @throws Exception
	 */
	@Test
	public void testInOut() throws Exception
	{
		Exchange exchange = endpoint.createExchange(ExchangePattern.InOut);
		exchange.getIn().setBody(""ping"");
		producer.process(exchange);

		assertTrue(""pong"".equals(exchange.getOut().getBody()));
	}
{code} ",davsclaus,lkwg,Major,Closed,Fixed,11/Nov/10 14:25,24/Apr/11 09:58
Bug,CAMEL-3333,12487887,Nested multicast in splitter EIP and issue with UseOriginalAggregationStrategy,"See nabble
http://camel.465427.n5.nabble.com/Multicast-inside-splitter-tp3261288p3261288.html",davsclaus,davsclaus,Major,Closed,Fixed,12/Nov/10 08:30,24/Apr/11 09:57
Bug,CAMEL-3334,12487881,BeanShell Expressions Don't Work,"BeanShell 2.0b5 has the JSR-223 integration, but it implements Compilable, when in fact it throws an exception if you call compile.

Camel calls compile for any language that implements Compilable, therefore you get an exception every time your route has BeanShell in it.

I notice the BeanShell tests are commented-out in camel-script.

Also, the Camel Wiki page for BeanShell is stupendously vague on actual usage.",davsclaus,ammulder,Minor,Closed,Fixed,12/Nov/10 08:48,23/Nov/12 13:58
Bug,CAMEL-3335,12487880,NPE occurs without @Entity annotation,"Here is code from the org.apache.camel.component.jpa.JpaConsumer class

protected String getEntityName(Class<?> clazz) {
        
    Entity entity = clazz.getAnnotation(Entity.class);
        
    // Check if the property name has been defined for Entity annotation
    if (!entity.name().equals("""")) { // line 307
        return entity.name();
    } else {
        return null;
    }
 
}

When trying to consume entities from jpa endpoint exception occurs

java.lang.NullPointerException
	at org.apache.camel.component.jpa.JpaConsumer.getEntityName(JpaConsumer.java:307)
	at org.apache.camel.component.jpa.JpaConsumer.createQueryFactory(JpaConsumer.java:287)
	at org.apache.camel.component.jpa.JpaConsumer.getQueryFactory(JpaConsumer.java:196)
	at org.apache.camel.component.jpa.JpaConsumer$1.doInJpa(JpaConsumer.java:83)
	at org.apache.camel.component.jpa.JpaTemplateTransactionStrategy$1$1.doInJpa(JpaTemplateTransactionStrategy.java:82)
	at org.springframework.orm.jpa.JpaTemplate.execute(JpaTemplate.java:183)
	at org.springframework.orm.jpa.JpaTemplate.execute(JpaTemplate.java:146)
	at org.apache.camel.component.jpa.JpaTemplateTransactionStrategy$1.doInTransaction(JpaTemplateTransactionStrategy.java:80)
	at org.springframework.transaction.support.TransactionTemplate.execute(TransactionTemplate.java:130)
	at org.apache.camel.component.jpa.JpaTemplateTransactionStrategy.execute(JpaTemplateTransactionStrategy.java:78)
	at org.apache.camel.component.jpa.JpaConsumer.poll(JpaConsumer.java:79)
	at org.apache.camel.impl.ScheduledPollConsumer.run(ScheduledPollConsumer.java:98)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

This is because there is no @Entity annotation and persistence is configured by means of orm.xml.

To fix the problem change line 307 to
if (entity != null && !entity.name().equals(""""))",davsclaus,szhemzhitsky,Major,Closed,Fixed,12/Nov/10 09:12,24/Apr/11 09:58
Bug,CAMEL-3341,12487904,ProxyHelper and JMS component - need to start producer or fails with NPE,"

I've a very simple unit test (based on http://camel.apache.org/using-camelproxy.html) that works fine when using endpoint uri ""direct:start"" but fails with the following exception when using ""activemq:somequeue""

java.lang.NullPointerException
	at org.apache.camel.component.jms.JmsProducer.processInOut(JmsProducer.java:140)
	at org.apache.camel.component.jms.JmsProducer.process(JmsProducer.java:90)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:91)
	at org.apache.camel.impl.DefaultAsyncProducer.process(DefaultAsyncProducer.java:37)
	at org.apache.camel.component.bean.CamelInvocationHandler.invoke(CamelInvocationHandler.java:65)
	at $Proxy7.sayWordTo(Unknown Source)
	at net.earcam.cdosgi.remoteserviceadmin.CamelProxyTest.simpleArgTest(CamelProxyTest.java:93)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)



Looks like the problem is simply that the producer isn't started so never creates a UuidGenerator (needed for the JMS Reply-To)

org.apache.camel.component.bean.ProxyHelper - lines 45 - 51, needs to start producer (for JmsProducer)
    /**
     * Creates a Proxy which sends PojoExchange to the endpoint.
     */
    @SuppressWarnings(""unchecked"")
    public static <T> T createProxy(Endpoint endpoint, ClassLoader cl, Class[] interfaces, MethodInfoCache methodCache) throws Exception {
        return (T) createProxyObject(endpoint, endpoint.createProducer(), cl, interfaces, methodCache);
    }


As a workaround, simply create the producer, start it and use the overloaded method createProxy that takes a producer parameter.
",davsclaus,earcam,Major,Closed,Fixed,17/Nov/10 12:01,24/Apr/11 09:57
Bug,CAMEL-3343,12487905,CxfRsInvoker produces NullpointerException when no ContinuationProvider is set in the InMessage,"In my usage of CXF-RS with Camel through the camel-cxf component I ran into a NullpointerException, the specific case is described in some detail on StackOverflow: http://stackoverflow.com/questions/4198461/why-do-i-get-a-nullpointerexception-when-invoking-the-cxf-rs-endpoint-of-a-camel 

Specifically I run into a NullpointerException:

{code:java}
Caused by: java.lang.NullPointerException
        at org.apache.camel.component.cxf.jaxrs.CxfRsInvoker.getContinuation(CxfRsInvoker.java:63)
        at org.apache.camel.component.cxf.jaxrs.CxfRsInvoker.performInvocation(CxfRsInvoker.java:52)
        at org.apache.cxf.service.invoker.AbstractInvoker.invoke(AbstractInvoker.java:89)
        ... 33 more
{code}

I was assuming I was misconfiguring something but didn't find any alternative way so I started digging in the camel-cxf source code.

I found that such a ""getContinuation"" method also exists for the {{org.apache.camel.component.cxf.CxfConsumer}} as well as for the CxfRsInvoker (where it was failing for me). However in the case of the CxfConsumer there is a guard on the ContinuationProvider so that if it is null, then null is returned:

{code:java}
return provider == null ? null : provider.getContinuation();
{code}

CxfRsInvoker does not have this guard, but it does have code to deal with a ""null"" Continuation at the calling site (line 52, in performInvocation). Therefore I assumed this was a bug and patched the guard into the CxfRsInvoker as well.

My program now seems to work correctly. I assume this is a bug.

For completeness sake, this is what getContinuation now looks like for me:

{code:java}
    private Continuation getContinuation(Exchange cxfExchange) {
        ContinuationProvider provider = 
            (ContinuationProvider)cxfExchange.getInMessage().get(ContinuationProvider.class.getName());
        return provider == null ? null : provider.getContinuation();
    }
{code}",njiang,boristerzic,Major,Closed,Fixed,17/Nov/10 21:16,25/Oct/11 11:36
Bug,CAMEL-3344,12487907,RedeliveryPolicy does not honor MaximumRedeliveryDelay,"When using exponential retry back-off with a maximumRedeliveryDelay, the delay is not honored.

The bug is in RedeliveryPolicy.java's calculateRedeliveryDelay method:

        if (maximumRedeliveryDelay > 0 && redeliveryDelay > maximumRedeliveryDelay) {
            redeliveryDelayResult = maximumRedeliveryDelay;
        }

redeliveryDelay is the initial delay and never increases, so the max is never applied. It needs to compare against redeliveryDelayResult instead.",davsclaus,lorrin,Major,Closed,Fixed,18/Nov/10 05:48,24/Apr/11 09:58
Bug,CAMEL-3348,12487910,DefaultShutdownStrategy and ShutdownAware (SedaConsumer) losing exchange,"There's a problem when we shutdown the camel context with a seda endpoint.

In the SedaConsumer, the exchange is removed from the queue and then, later, is added to the InflightRepository as shown in the following code (I put comments where it is done):

{code}
public void run() {
        BlockingQueue<Exchange> queue = endpoint.getQueue();
        while (queue != null && isRunAllowed()) {
            final Exchange exchange;
            try {
                exchange = queue.poll(1000, TimeUnit.MILLISECONDS); // The exchange is removed here from the queue
            } catch (InterruptedException e) {
                if (LOG.isDebugEnabled()) {
                    LOG.debug(""Sleep interrupted, are we stopping? "" + (isStopping() || isStopped()));
                }
                continue;
            }
            if (exchange != null) {
                if (isRunAllowed()) {
                    try {
                        sendToConsumers(exchange); // Call to sendToConsumers detailed below

                        if (exchange.getException() != null) {
                            getExceptionHandler().handleException(""Error processing exchange"", exchange, exchange.getException());
                        }
                    } catch (Exception e) {
                        getExceptionHandler().handleException(""Error processing exchange"", exchange, e);
                    }
                } else {
                    if (LOG.isWarnEnabled()) {
                        LOG.warn(""This consumer is stopped during polling an exchange, so putting it back on the seda queue: "" + exchange);
                    }
                    try {
                        queue.put(exchange);
                    } catch (InterruptedException e) {
                        if (LOG.isDebugEnabled()) {
                            LOG.debug(""Sleep interrupted, are we stopping? "" + (isStopping() || isStopped()));
                        }
                        continue;
                    }
                }
            }
        }
    }

    protected void sendToConsumers(Exchange exchange) throws Exception {
        int size = endpoint.getConsumers().size();

        if (size > 1) {

            if (LOG.isDebugEnabled()) {
                LOG.debug(""Multicasting to "" + endpoint.getConsumers().size() + "" consumers for Exchange: "" + exchange);
            }
           
            MulticastProcessor mp = endpoint.getConumserMulticastProcessor();

            AsyncProcessorHelper.process(mp, exchange, new AsyncCallback() {
                public void done(boolean doneSync) {
                }
            });
        } else {
            AsyncProcessorHelper.process(processor, exchange, new AsyncCallback() { // This line will create the UnitOfWork (in UnitOfWorkProcessor) which will put the exchange in the InflightRepository
                public void done(boolean doneSync) {

                }
            });
        }
    }
{code}

If the shutdown occurs between these two actions, the DefaultShutdownStrategy will shutdown the route even if there is a message in progress. And the message will be lost.

Here is the code of ShutdownTask in DefaultShutdownStrategy which cause the shutdown even if there is some messages still in progress. (I put comments in it to show the state of the seda queue and InflightRepository if it is called between the queue.poll() and the InflightRepository.add())

{code}
for (Consumer consumer : order.getInputs()) {
                        int inflight = context.getInflightRepository().size(consumer.getEndpoint()); // check the number of inflight exchanges which is 0 because the UnitOfWork is not created

                        if (consumer instanceof ShutdownAware) {
                            inflight += ((ShutdownAware) consumer).getPendingExchangesSize(); // check the number of exchange in the seda queue which is 0 because the message is already removed
                        }
                        if (inflight > 0) {
                            size += inflight;
                            if (LOG.isDebugEnabled()) {
                                LOG.debug(inflight + "" inflight and pending exchanges for consumer: "" + consumer);
                            }
                        }
                    }
{code}
You can reproduce it by putting a breakpoint in the method {code}protected void sendToConsumers(Exchange exchange){code} in SedaConsumer and calling stop() on the CamelContext while the thread is suspended by the breakpoint.

We caught the problem in a unit test where we were testing the shutdown and when our test server was under heavy load.
",davsclaus,ddelautre,Major,Closed,Fixed,19/Nov/10 22:02,24/Apr/11 09:57
Bug,CAMEL-3349,12487911,Race condition found in CxfRsEndpoint while getting the endpoint binding under load and performing sync and async invocation,"The CxfRsEndpoint's getBinding method is not thread safe. At a customer site, I ran into an issue at startup if 2 threads raced to perform sync and async invocation, the code for getBinding (given below) would react in the following way.
      - Thread 1 would proceed to create a binding object 
      - Thread 2 would mean while still find the  binding to be null and proceed to create a new binding
      - Meanwhile thread one would have its binding and set the Atomic boolean for binding initialized and proceed to set the HeaderStrategy.
      - Thread 2 meanwhile would overwrite the original binding object and find that Atomic boolean already set and would have no way to associate a HeaderFilterStrategy object since the flag is up.
      - In the absence of a HeaderFilterStrategy, copying of ProtocolHeaders etc will throw exceptions on every following request/invocation.

--------------------------------------------------
    public CxfRsBinding getBinding() {
        if (binding == null) {
            binding = new DefaultCxfRsBinding();
            if (LOG.isDebugEnabled()) {
                LOG.debug(""Create default CXF Binding "" + binding);
            }
        } 
        
        if (!bindingInitialized.getAndSet(true) && binding instanceof HeaderFilterStrategyAware) {
            ((HeaderFilterStrategyAware)binding).setHeaderFilterStrategy(getHeaderFilterStrategy());
        }
        
        return binding;
    }
------------------------------------------------",akarpe,akarpe,Critical,Closed,Fixed,19/Nov/10 22:32,24/Apr/11 09:57
Bug,CAMEL-3351,12487913,camel-irc component silently fails on nick collision,When the camel-irc component connects to an irc server and there's a nick collision it silently fails. Also note there is no camel-irc component in Jira.,njiang,tjsnell,Minor,Closed,Fixed,21/Nov/10 13:54,24/Apr/11 09:57
Bug,CAMEL-3352,12487906,Multicast with UseLatestAggregationStrategy fails to propagated exceptions which has been handled by onException,"See nabble
http://camel.465427.n5.nabble.com/Multicast-Behaviour-with-Exceptions-tp3270272p3270272.html",davsclaus,davsclaus,Major,Closed,Fixed,22/Nov/10 13:58,24/Apr/11 09:57
Bug,CAMEL-3353,12487912,CxfRsInvoker silently swallows exceptions,"If you have a route with a CXF consuming endpoint in the beginning and any component afterwards that can produce Exceptions that are not RuntimeCamelExceptions or WebApplicationException then the CxfRsInvoker will swallow the exception and return a HTTP 204 (all fine but no content to return) response.

For example in the following route:
{code:java}
 from(""cxfrs://bean://fooServer"")
            .convertBodyTo(Foo.class)
            .to(""bean-validator://x"")
            .to(""jms:queue:foos"").inOnly();
{code}

The bean validator component can throw BeanValidationException when the Foo instance has errors. This exception will be ignored by the CxfRsInvoker.

This causes important exceptions to become invisible by default which seems wrong to me. The docs and Camel in Action additionally talk about how the DefaultErrorHandler has a strategy of returning exceptions to the caller and this is also not happening here.

My local fix is a patched version of camel-cxf that converts any unknown exception (i.e. not CamelRuntimeException or WebApplicationException) to a WebApplicationException with the original exception as a constructor parameter. This is then effectively an HTTP 500 Exception and will be returned as such to the caller.

However my knowledge of camel and camel-cxf is not sufficient to ascertain whether this is the right approach, it seems to me that the CamelRuntimeException should also be treated this way since in the current code that will also be swallowed (as far as I can tell).",njiang,boristerzic,Major,Closed,Fixed,22/Nov/10 19:25,25/Oct/11 11:36
Bug,CAMEL-3355,12487915,"ConcurrentModifictionException on UoW done, when under heavy load","See nabble
http://camel.465427.n5.nabble.com/Errors-when-under-load-tp3276259p3276259.html",davsclaus,davsclaus,Major,Closed,Fixed,23/Nov/10 09:15,24/Apr/11 09:58
Bug,CAMEL-3356,12487890,Hard coded locale in Bindy DatePatternFormat,"As reported by a user from the community (http://camel.465427.n5.nabble.com/Bindy-Dateformat-Parsing-exception-td3276981.html), the locale of SimpeDateFormat in bindy's DatePatternFormat class is hard coded to FRANCE.",hadrian,rkettelerij,Minor,Closed,Fixed,23/Nov/10 20:39,24/Apr/11 09:58
Bug,CAMEL-3358,12487919,Consumer template ignores options in uri for file consumer,"I have an OSGi bundle:

My camel-context.xml:
------------
	<osgi:camelContext xmlns=""http://camel.apache.org/schema/spring"">					
		<template id=""producer"" />
		<consumerTemplate id=""consumer"" />
		<routeBuilder ref=""routeBuilder""/>
	</osgi:camelContext>

	<bean id=""routeBuilder"" class=""com.proiam.postmarque.customer.RouteBuilder"">	
	</bean>
	
	<bean id=""customerFilePolling"" class=""com.proiam.postmarque.customer.CustomerFilePolling"">
		<property name=""pullAddress"" value=""${pullAddress}"" />
	</bean>
------------------
RouteBuilder:
------------------
		from(""quartz://customer4pxfilein?cron="" + cronTime)		
		.beanRef(""customer4PXFilePolling"", ""pollFiles"");
-----------------
pollFiles:
-----------------
public class CustomerFilePolling {
	//@AutoWired gives me NPE
	@EndpointInject
	ConsumerTemplate consumer;
	//@AutoWired gives me NPE
	@EndpointInject
	ProducerTemplate producer;

        public void pollFiles() {
		while (true) {
			//System.out.println(""Prepare to comsume"");
			Exchange exchange = consumer.receive(""file:///tmp/test/in?move=../done&readLock=changed"", 2000);
			if (exchange == null) {
				break;
			}
			Exchange result = producer.send(""activemq:queue:CustomerFileInBody"", exchange);
			if (result.isFailed()) {
				System.out.println(""pollFiles processor raised an error!"");
			}
		}
	}
}

--------------
Everything is fine except that after being processed, the files are still there together with their .camelLock ",davsclaus,ngochai,Major,Closed,Fixed,24/Nov/10 06:48,24/Apr/11 09:57
Bug,CAMEL-3373,12491485,markRollbackOnlyLast should remove caused exception to avoid it affecting outer transaction,"The {{markRollbackOnlyLast()}} should remove any caused exception because it should not affect outer transactions.

",davsclaus,davsclaus,Major,Closed,Fixed,28/Nov/10 13:23,24/Apr/11 09:57
Bug,CAMEL-3377,12491521,spring-integration - Should have better configuration validation,"See nabble
http://camel.465427.n5.nabble.com/Integration-camel-and-spring-integration-tp3284105p3284105.html",davsclaus,davsclaus,Minor,Closed,Fixed,29/Nov/10 09:40,24/Apr/11 09:58
Bug,CAMEL-3388,12492079,@OutHeaders in bean binding issue with InOnly MEP,"When you invoke a bean with a method signature like this in Camel 2.5.0/HEAD, the in and out message both are null (the ""Hello!"" value just disappears):

{code:java}
    public String doTest(@Body Object body, @Headers Map headers, @OutHeaders Map outHeaders) {
        return ""Hello!"";
    }
{code}

The same thing without the headers works OK:

{code:java}
    public String doTest(@Body Object body) {
        return ""Hello!"";
    }
{code}
See camel-core/src/test/java/org/apache/camel/component/bean/BeanWithHeadersAndBodyInject3Test.java",davsclaus,ammulder,Major,Closed,Fixed,04/Dec/10 22:32,24/Apr/11 09:57
Bug,CAMEL-3389,12492091,PackageHelper - issue with version numbers which contains non decimal,"See more here
http://fusesource.com/forums/thread.jspa?threadID=2447&tstart=0",davsclaus,davsclaus,Minor,Closed,Fixed,05/Dec/10 09:04,24/Apr/11 09:57
Bug,CAMEL-3390,12492093,routeBuilder ref not working in camel 2.5.0," I amended the camel-example-osgi project so as to use the Java RouteBuilder, the MyRouteBuilder class in that project

I changed the blueprint.xml file to the following

	<bean id=""routeBuilder"" class=""org.apache.camel.example.osgi.MyRouteBuilder"" />
	<camelContext xmlns=""http://camel.apache.org/schema/blueprint"">
			<routeBuilder ref=""routeBuilder""/>
	</camelContext>
 
However nothing happens, no error message is displayed either.
If I leave the example in its original state, in other words using the xml-based dsl, then it works.
Maybe this is somehow related to the fact that packageScan does not work???

Regards 
Ivanhoe

",gnodet,ivanhoe.abrahams,Major,Closed,Fixed,05/Dec/10 09:40,24/Apr/11 09:57
Bug,CAMEL-3394,12492149,Splitter and Multicast EIP marks exchange as exhausted to early if exception was thrown from an evaluation,"See nabble
http://camel.465427.n5.nabble.com/Cannot-handle-Exception-thrown-from-Splitter-Expression-tp3286043p3286043.html",davsclaus,davsclaus,Minor,Closed,Fixed,06/Dec/10 12:59,23/Nov/12 13:52
Bug,CAMEL-3395,12492164,Splitter - Exchange.CORRELATION_ID should point back to parent Exchange id,"See nabble
http://camel.465427.n5.nabble.com/Splitted-exchange-has-incorrect-correlation-ID-tp3289354p3289354.html",davsclaus,davsclaus,Minor,Closed,Fixed,06/Dec/10 15:02,24/Apr/11 09:57
Bug,CAMEL-3398,12492550,"Automatically imported beans are not taken into account in camel-blueprint (InterceptStrategy, LifecycleStrategy, etc...)",,gnodet,gnodet,Major,Closed,Fixed,06/Dec/10 22:20,24/Apr/11 09:57
Bug,CAMEL-3401,12492632,camel-jetty component does not work when authentication is enable,"I try to use camel-jetty in a camel route usign LDAP authentication. The authentication mechanisms of Jetty works fine as I'm able to authenticate a LDAP user from the web page but the camel route is not called.

Here is the camel route

{code}
    <camelContext trace=""true"" xmlns=""http://camel.apache.org/schema/spring"">

        <route>
            <from uri=""jetty:http://localhost:8080/services?handlers=securityHandler&amp;matchOnUriPrefix=true""/>
            <onException>
                <exception>java.lang.Exception</exception>
                <!-- we can set the redelivery policy here as well -->
                <redeliveryPolicy maximumRedeliveries=""1""/>
                <handled>
                    <constant>true</constant>
                </handled>
                <transform>
                    <simple>Due to something went wrong</simple>
                </transform>
                <setHeader headerName=""Exchange.HTTP_RESPONSE_CODE"">
                    <simple>500</simple>
                </setHeader>
            </onException>

            <bean ref=""responseBean""/>
        </route>
    </camelContext>
{code}

REMARK : the camel route works fine when authentication is disabled

and the error

{code}
14:56:31,587 | WARN  | 0-98 - /services | log                              | .eclipse.jetty.util.log.Slf4jLog   40 | 61 - org.eclipse.jetty.util - 7.1.6.v20100715 | /services: java.lang.NullPointerException
14:56:31,587 | DEBUG | 0-98 - /services | log                              | .eclipse.jetty.util.log.Slf4jLog   80 | 61 - org.eclipse.jetty.util - 7.1.6.v20100715 | EXCEPTION 
java.lang.NullPointerException
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:489)[68:org.eclipse.jetty.security:7.1.6.v20100715]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:113)[67:org.eclipse.jetty.server:7.1.6.v20100715]
	at org.eclipse.jetty.server.Server.handle(Server.java:347)[67:org.eclipse.jetty.server:7.1.6.v20100715]

{code}",,cmoulliard,Major,Closed,Fixed,07/Dec/10 15:53,28/Oct/11 13:23
Bug,CAMEL-3403,12492706,InterceptSendToEndpoint should check for if stop DSL has been used,"See nabble
http://camel.465427.n5.nabble.com/InterceptSendToEndpoint-Http-Component-tp3296620p3296620.html",davsclaus,davsclaus,Minor,Closed,Fixed,08/Dec/10 07:44,24/Apr/11 09:58
Bug,CAMEL-3413,12492759,Using java based RouteBuilder reference in blueprint.xml CameContext definition failing,"Hi 

I am trying to get the Java based RouteBuilder to work using the simple ""camel-osgi-example"" project.
I have downloaded and installed karaf 2.1.2
I also downloaded the latest 2.6-SNAPSHOT build of Camel (8 Dec 2010).

I proceeded to unzip the camel download an in Karaf I installed the ""camel-core"" and ""camel-blueprint"" bundles, (I also installed the servicemix jaxb-impl jar)

I then amended the ""camel-osgi-example"" project blueprint.xml file, so that it creates the example RouteBuilder as a bean, and the references the bean from the CamelContext.

Below is the bluerint.xml file contents,

<?xml version=""1.0"" encoding=""UTF-8""?>

<blueprint xmlns=""http://www.osgi.org/xmlns/blueprint/v1.0.0""
	xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:jpa=""http://aries.apache.org/xmlns/jpa/v1.0.0""
	xmlns:tx=""http://aries.apache.org/xmlns/transactions/v1.0.0""
	xmlns:camel=""http://camel.apache.org/schema/blueprint""
	default-activation=""eager"" default-availability=""optional"">


	<bean id=""routeBuilder"" class=""org.apache.camel.example.osgi.MyRouteBuilder"" />
	<camelContext xmlns=""http://camel.apache.org/schema/blueprint"">
			<camel:routeBuilder ref=""routeBuilder""/>
<!--		<camel:route>-->
<!--			<camel:from uri=""timer://myTimer?fixedRate=true&amp;period=2000"" />-->
<!--			<camel:bean ref=""myTransform"" method=""transform"" />-->
<!--			<camel:to uri=""log:ExampleRouter"" />-->
<!--		</camel:route>-->
	</camelContext>

	<bean id=""myTransform"" class=""org.apache.camel.example.osgi.MyTransform"">
		<property name=""prefix"" value=""SpringDSL"" />
	</bean>

</blueprint>
 

Now when I deploy this bundle in Karaf I encounter the following exception.

org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to initialize camel context factory
	at org.apache.camel.blueprint.handler.CamelNamespaceHandler$CamelDependenciesFinder.process(CamelNamespaceHandler.java:271)[52:org.apache.camel.camel-blueprint:2.6.0.SNAPSHOT]
	at org.apache.aries.blueprint.container.BlueprintContainerImpl.processProcessors(BlueprintContainerImpl.java:479)[7:org.apache.aries.blueprint:0.2.0.incubating]
	at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:299)[7:org.apache.aries.blueprint:0.2.0.incubating]
	at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:213)[7:org.apache.aries.blueprint:0.2.0.incubating]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)[:1.6.0_21]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)[:1.6.0_21]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)[:1.6.0_21]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)[:1.6.0_21]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207)[:1.6.0_21]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)[:1.6.0_21]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)[:1.6.0_21]
	at java.lang.Thread.run(Thread.java:619)[:1.6.0_21]
Caused by: org.apache.camel.FailedToCreateRouteException: Failed to create route route1 at: >>> Bean[org.apache.camel.example.osgi.MyTransform@7439aca7] <<< in route: Route[[From[timer://myTimer?fixedRate=true&period=2000]] -> ... because of org.osgi.service.blueprint.container.NoSuchComponentException: No component with id 'CamelBeanParameterMappingStrategy' could be found
	at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:773)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:174)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.impl.DefaultCamelContext.startRoute(DefaultCamelContext.java:654)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.impl.DefaultCamelContext.startRouteDefinitions(DefaultCamelContext.java:1559)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.impl.DefaultCamelContext.doStartCamel(DefaultCamelContext.java:1348)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.impl.DefaultCamelContext.doStart(DefaultCamelContext.java:1257)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:1235)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.blueprint.BlueprintCamelContext.maybeStart(BlueprintCamelContext.java:79)[52:org.apache.camel.camel-blueprint:2.6.0.SNAPSHOT]
	at org.apache.camel.blueprint.BlueprintCamelContext.init(BlueprintCamelContext.java:72)[52:org.apache.camel.camel-blueprint:2.6.0.SNAPSHOT]
	at org.apache.camel.blueprint.handler.CamelNamespaceHandler$CamelDependenciesFinder.process(CamelNamespaceHandler.java:269)[52:org.apache.camel.camel-blueprint:2.6.0.SNAPSHOT]
	... 11 more
Caused by: org.apache.camel.RuntimeCamelException: org.osgi.service.blueprint.container.NoSuchComponentException: No component with id 'CamelBeanParameterMappingStrategy' could be found
	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1140)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.impl.PropertyPlaceholderDelegateRegistry.lookup(PropertyPlaceholderDelegateRegistry.java:56)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.component.bean.BeanInfo.createParameterMappingStrategy(BeanInfo.java:115)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.component.bean.BeanProcessor.<init>(BeanProcessor.java:60)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.model.BeanDefinition.createProcessor(BeanDefinition.java:170)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.model.ProcessorDefinition.makeProcessor(ProcessorDefinition.java:403)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.model.ProcessorDefinition.addRoutes(ProcessorDefinition.java:174)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:770)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	... 22 more
Caused by: org.osgi.service.blueprint.container.NoSuchComponentException: No component with id 'CamelBeanParameterMappingStrategy' could be found
	at org.apache.aries.blueprint.container.DependencyGraph.getSortedRecipes(DependencyGraph.java:51)[7:org.apache.aries.blueprint:0.2.0.incubating]
	at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:214)[7:org.apache.aries.blueprint:0.2.0.incubating]
	at org.apache.aries.blueprint.container.BlueprintRepository.createInstance(BlueprintRepository.java:198)[7:org.apache.aries.blueprint:0.2.0.incubating]
	at org.apache.aries.blueprint.container.BlueprintRepository.create(BlueprintRepository.java:137)[7:org.apache.aries.blueprint:0.2.0.incubating]
	at org.apache.aries.blueprint.container.BlueprintContainerImpl.getComponentInstance(BlueprintContainerImpl.java:702)[7:org.apache.aries.blueprint:0.2.0.incubating]
	at org.apache.camel.blueprint.BlueprintContainerRegistry.lookup(BlueprintContainerRegistry.java:41)[52:org.apache.camel.camel-blueprint:2.6.0.SNAPSHOT]
	at org.apache.camel.impl.CompositeRegistry.lookup(CompositeRegistry.java:47)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	at org.apache.camel.impl.PropertyPlaceholderDelegateRegistry.lookup(PropertyPlaceholderDelegateRegistry.java:54)[51:org.apache.camel.camel-core:2.6.0.SNAPSHOT]
	... 28 more


Regards 
Ivanhoe
 ",gnodet,ivanhoe.abrahams,Major,Closed,Fixed,08/Dec/10 19:28,09/Dec/10 17:04
Bug,CAMEL-3419,12492825,timestamp property in JpaTraceEventMessage does not specify a temporal type,"The timestamp property in the JpaTraceEventMessage does not specify a temporal type. This results in an exception using EclipseLink 2.1.0 (and maybe using other JPA frameworks, too). 

Using the annotation  @Temporal(TemporalType.TIMESTAMP) on timestamp should solve this issue.",davsclaus,marcozapletal,Major,Closed,Fixed,09/Dec/10 12:00,24/Apr/11 09:58
Bug,CAMEL-3425,12493164,CamelContext is started twice when using camel-blueprint,"For example if you run the {{testRouteWithAllComponents}} test in {{CamelBlueprintTest}} in tests/camel-itest-osgi you will see _a lot_ of logging. But notice
{code}
[RMI TCP Connection(1)-10.0.1.2] INFO org.apache.camel.core.osgi.OsgiDefaultCamelContext - Apache Camel 2.6-SNAPSHOT (CamelContext: 67-camel-2) is starting
[RMI TCP Connection(1)-10.0.1.2] INFO org.apache.camel.core.osgi.OsgiDefaultCamelContext - Apache Camel 2.6-SNAPSHOT (CamelContext: 67-camel-2) started in 0.147 seconds
{code}

Then blueprint does something
{code}
[Blueprint Extender: 3] INFO org.apache.camel.blueprint.BlueprintCamelContext - Apache Camel 2.6-SNAPSHOT (CamelContext: 4-camel-5) is starting
[Blueprint Extender: 3] INFO org.apache.camel.blueprint.BlueprintCamelContext - Apache Camel 2.6-SNAPSHOT (CamelContext: 4-camel-5) started in 0.071 seconds
{code}

And likewise both CamelContext's is shutdown when the test completes. There are logging events for that as well.

Pay attention to the name of the 1st CamelContext {{67-camel-2}}. This is the *correct* name as its based on the bundle id, our end user Camel application is given. This ensures that the CamelContext from his application has a name which is unique and refers to the bundle id as well.

Now if you look at the 2nd CamelContext being started its given another name {{4-camel-5}}. That id is most likely using a shared bundle id with a low number. For example camel-blueprint or camel-core bundle, or something like that. 

In essence camel-blueprint should only create *one* CamelContext and it should use the {{67-camel-2}} as the name of the CamelContext (eg. include the bundle id of the end user application).",gnodet,davsclaus,Critical,Closed,Fixed,14/Dec/10 10:36,24/Apr/11 09:57
Bug,CAMEL-3426,12493187,CxfProducer doesn't callback.done when the operation is oneway.,"There is a mail thread[1] describes the whole story.
[1]http://camel.465427.n5.nabble.com/file-to-oneway-cxf-service-scenario-does-not-work-with-camel-2-5-0-td3303263.html",njiang,njiang,Major,Closed,Fixed,14/Dec/10 14:02,24/Apr/11 09:57
Bug,CAMEL-3428,12493240,"DefaultCamelContext.getEndpoint(String name, Class<T> endpointType) throws Nullpointer for unknown endpoint","The method getEndpoint throws an NullPointerException when it's called with an unknown endpoint name:

java.lang.NullPointerException
	at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:480)
	at org.apache.camel.impl.DefaultCamelContextTest.testGetEndPointByTypeUnknown(DefaultCamelContextTest.java:95)

The patch is attached.
",hadrian,blob79,Minor,Closed,Fixed,14/Dec/10 21:05,24/Apr/11 09:58
Bug,CAMEL-3430,12493273,InterceptSendToEndpoint has issues with interception http endpoints which has multiple parameters,"See nabble
http://camel.465427.n5.nabble.com/interceptSendToEndpoint-with-dynamic-endpoint-tp3301978p3301978.html

The issue is when any endpoints have parameters which may be re-ordered when the endpoint is normalized.",davsclaus,davsclaus,Major,Closed,Fixed,15/Dec/10 08:05,17/Mar/14 21:08
Bug,CAMEL-3433,12493282,Undefined header results in Nullpointer when expression is evaluated,"If you define a filter for a header that is not defined like

from(""p:a"").filter(header(""header"").in(""value"")).to(""p:b"");

it results in a NullPointerException:

{code}
2010-12-15 10:07:45,920 [main] ERROR DefaultErrorHandler            - 
Failed delivery for exchangeId: 0215-1237-1292404064936-0-2. 
Exhausted after delivery attempt: 1 caught: java.lang.NullPointerException
	at org.apache.camel.builder.ExpressionBuilder$40.evaluate(ExpressionBuilder.java:955)
	at org.apache.camel.impl.ExpressionAdapter.evaluate(ExpressionAdapter.java:36)
	at org.apache.camel.builder.BinaryPredicateSupport.matches(BinaryPredicateSupport.java:54)
	at org.apache.camel.builder.PredicateBuilder$5.matches(PredicateBuilder.java:127)
	at org.apache.camel.processor.FilterProcessor.process(FilterProcessor.java:46)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)
{code}

This test reproduces the problem:
{code}
public void testExpressionForUndefinedHeader(){
    Expression type = ExpressionBuilder.headerExpression(""header"");
    Expression expression = ExpressionBuilder.constantExpression(""value"");
    Expression convertToExpression = ExpressionBuilder.convertToExpression(expression, type);
    convertToExpression.evaluate(exchange, Object.class);
}
{code}",davsclaus,blob79,Minor,Closed,Fixed,15/Dec/10 09:17,24/Apr/11 09:58
Bug,CAMEL-3438,12493539,JAXBDataFormatter should using Spring ApplicationContext's classLoader explicitly,"JAXBDataFormatter now using JAXBContext.newInstance(path) to create JAXBContext,
but this will using Thread's context classLoader.
this may causing un-expected class or resource not found exceptions;",hadrian,ext2xhb,Major,Closed,Fixed,17/Dec/10 17:35,25/Oct/11 11:35
Bug,CAMEL-3442,12493603,JBI ClassLoading issue in SMX 4.x in OsgiPackageScanClassResolver,"CAMEL-3302 introduced a fallback when using JBI in Apache ServiceMix 4.x.

However it may lead to an issue with ConcurrentModificationException when traversing the list of classloaders.

{code}
            for (ClassLoader classLoader : super.getClassLoaders()) {
                if (!isOsgiClassloader(classLoader)) {
                    find(test, packageName, classLoader, classes);
                }
            }  
{code}

The for loop is line 62 which causes the exception.

Issue reported here
http://camel.465427.n5.nabble.com/ServiceMix-4-Fuse-4-3-0-fuse-03-00-Problems-running-JBI-example-examples-camel-td3309088.html

",davsclaus,davsclaus,Major,Closed,Fixed,18/Dec/10 18:56,24/Apr/11 09:57
Bug,CAMEL-3446,12493665,NPE in camel-printer when not setting media size or omitting sides attribute,"When specifying a camel-printer configuration that does not include config properties for sides or mediaSize, the camel route will fail to start up with a NullPointerException. E.g. this route

{code:java}
from(""file://target/incoming?delete=true"")
	.to(""lpr://localhost/default"");
{code}

will raise an NPE at route startup time.


",davsclaus,tmielke,Major,Closed,Fixed,20/Dec/10 10:50,24/Apr/11 09:57
Bug,CAMEL-3448,12493680,Route scoped onException may pick onException from another route if they are the same class type,"If you have a clash with route scoped onException and have the exact same class, then the key in the map isn't catering for this. And thus a 2nd route could override the 1st route onException definition.

For example:

from X route A
  onException IOException

from Y route B
  onException IOException

The map should contain 2 entries, but unfortunately it only contain 1. This only happens when its an exact type match.
",davsclaus,davsclaus,Critical,Closed,Fixed,20/Dec/10 14:56,24/Apr/11 09:57
Bug,CAMEL-3458,12493986,Bindy should support clipping fields if they exceed maximum length allowed when using fixed length,"Adding a new option to bindy annotation

{code}

    /**
     * Indicates to clip data in the field if it exceeds the allowed length when using fixed length.
     */
    boolean clip() default false;
{code}

Then if enabled it will clip the data so it can fit the length.
Also now Camel throws an exception if the data is too long and you have clip set as false.",davsclaus,davsclaus,Minor,Closed,Fixed,23/Dec/10 16:19,24/Apr/11 09:57
Bug,CAMEL-3464,12494043,Wiki Page for Google App Engine example needs improving,"[11:26am] MartinCleaver: ok, well I did build it against trunk, I used ""svn co http://svn.apache.org/repos/asf/camel/trunk/examples/camel-example-gae camel-example-gae""
[11:26am] MartinCleaver: but the pom.xml didn't work for me
[11:27am] MartinCleaver: maybe I installed google app engine incorrectly, the instructions felt vague
[11:27am] joed: You are buidling a snapshot, you don't have any of the parent pom deps, you'll need the whole source tree most likely.
[11:29am] MartinCleaver: ah - is this mentioned on http://camel.apache.org/tutorial-for-camel-on-google-app-engine.html ? I should have checked out google app engine rather than just a ""Install the http://code.google.com/appengine/downloads.html""
[11:31am] MartinCleaver: and, did installing google app engine not install the parent pom deps?
[11:33am] joed: I just tried that too and that also works fine with mvn clean install
[11:33am] MartinCleaver: ok, so I've screwed up somewhere.
[11:33am] tjsnell: you have snapshot in your repo already though
[11:33am] tjsnell: hrmm
[11:33am] joed: True.
[11:34am] joed: One sec.
[11:34am] tjsnell: trying a clean repo?
[11:34am] MartinCleaver: I was surprised I needed to edit the pom.xml file, but perhaps that's just my naivety
[11:35am] joed: Yeah, those instructions won't work.
[11:35am] MartinCleaver: ah!
[11:36am]  MartinCleaver feels a little vindicated, but mostly thankful
[11:36am] tjsnell: heh
[11:36am] joed: MartinCleaver: you are right, since you followed the instructions - the example gets the null: basically because you have the project only and no the parent poms.
[11:36am] joed: not*
[11:36am] tjsnell: can he set the version to a realeased version not snapshot?
[11:37am] tjsnell: should pull the poms then right?
[11:38am] joed: If you were to build against trunk, it'll work fine
[11:38am] joed: tjsnell: Don't think so, since properties/version/parent/etc still is missing.
[11:39am] tjsnell: ahh
[11:39am] tjsnell: yeah
[11:39am] joed: MartinCleaver: the quickest fix is this : svn co http://svn.apache.org/repos/asf/camel/trunk camel ; cd camel/examples/camel-example-gae ; mvn clean install
[11:40am] joed: We need to improve the wiki page though.",joed,mrjcleaver,Major,Closed,Fixed,24/Dec/10 16:53,24/Apr/11 09:57
Bug,CAMEL-3473,12494248,register several cacheEnpoint with different name,"When you declare in camel context

<camel:endpoint id=""cache1"" uri=""cache:cache1"" />
<camel:endpoint id=""cache2"" uri=""cache:cache2"" />
<camel:endpoint id=""cache3"" uri=""cache:cache3"" />

CamelCacheProducer produce systematically  in same cache because in CacheComponent, CacheConfiguration is modified by method createEndpoint

{code:title=CamelCacheProducer.java|borderStyle=solid}

public void process(Exchange exchange) throws Exception {
        if (LOG.isTraceEnabled()) {
            LOG.trace(""Cache Name: "" + config.getCacheName());
        }

        if (cacheManager.cacheExists(config.getCacheName())) {
            if (LOG.isTraceEnabled()) {
                LOG.trace(""Found an existing cache: "" + config.getCacheName());
                LOG.trace(""Cache "" + config.getCacheName() + "" currently contains ""
                        + cacheManager.getCache(config.getCacheName()).getSize() + "" elements"");
            }
            cache = cacheManager.getCache(config.getCacheName());
        } else {
            cache = new Cache(config.getCacheName(),
                    config.getMaxElementsInMemory(),
                    config.getMemoryStoreEvictionPolicy(),
                    config.isOverflowToDisk(),
                    config.getDiskStorePath(),
                    config.isEternal(),
                    config.getTimeToLiveSeconds(),
                    config.getTimeToIdleSeconds(),
                    config.isDiskPersistent(),
                    config.getDiskExpiryThreadIntervalSeconds(),
                    null);
            cacheManager.addCache(cache);
            if (LOG.isDebugEnabled()) {
                LOG.debug(""Added a new cache: "" + cache.getName());
            }
        }

        String key = exchange.getIn().getHeader(CacheConstants.CACHE_KEY, String.class);
        String operation = exchange.getIn().getHeader(CacheConstants.CACHE_OPERATION, String.class);

        if (operation == null) {
            throw new CacheException(""Operation not specified in the message header ["" + CacheConstants.CACHE_KEY + ""]"");
        }
        if ((key == null) && (!operation.equalsIgnoreCase(CacheConstants.CACHE_OPERATION_DELETEALL))) {
            throw new CacheException(""Cache Key is not specified in message header header or endpoint URL."");
        }

        performCacheOperation(exchange, operation, key);
    }
{code} 

{code:title=CacheComponent.java|borderStyle=solid}

   public class CacheComponent extends DefaultComponent {
    private CacheConfiguration config;
    private CacheManagerFactory cacheManagerFactory = new CacheManagerFactory();
    
    public CacheComponent() {
        config = new CacheConfiguration();
    }

    public CacheComponent(CamelContext context) {
        super(context);
        config = new CacheConfiguration();
    }

    @Override
    @SuppressWarnings(""unchecked"")
    protected Endpoint createEndpoint(String uri, String remaining, Map parameters) throws Exception {
        config.parseURI(new URI(uri));
        
        CacheEndpoint cacheEndpoint = new CacheEndpoint(uri, this, config, cacheManagerFactory);
        setProperties(cacheEndpoint.getConfig(), parameters);
        return cacheEndpoint;
    }

    public CacheManagerFactory getCacheManagerFactory() {
        return cacheManagerFactory;
    }

    public void setCacheManagerFactory(CacheManagerFactory cacheManagerFactory) {
        this.cacheManagerFactory = cacheManagerFactory;
    }

    @Override
    protected void doStart() throws Exception {
        super.doStart();
        ServiceHelper.startService(cacheManagerFactory);
    }

    @Override
    protected void doStop() throws Exception {
        ServiceHelper.stopService(cacheManagerFactory);
        super.doStop();
    }
}
{code} 

The resolution could be in CacheComponent

{code:title=CacheComponent.java|borderStyle=solid}
    @Override
    @SuppressWarnings(""unchecked"")
    protected Endpoint createEndpoint(String uri, String remaining, Map parameters) throws Exception {
        CacheConfiguration   config = new CacheConfiguration();
        config.parseURI(new URI(uri));
        
        CacheEndpoint cacheEndpoint = new CacheEndpoint(uri, this, config, cacheManagerFactory);
        setProperties(cacheEndpoint.getConfig(), parameters);
        return cacheEndpoint;
    }
{code} ",tjsnell,skydjol,Minor,Closed,Fixed,29/Dec/10 14:33,25/Oct/11 11:36
