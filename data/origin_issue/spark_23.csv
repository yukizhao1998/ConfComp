Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Completes),Inward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Potential for incorrect results or NPE when full outer USING join has null key value,SPARK-44251,13541996,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,29/Jun/23 19:53,11/Jul/23 03:22,13/Jul/23 08:42,11/Jul/23 03:21,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,SQL,,,0,correctness,"The following query produces incorrect results:
{noformat}
create or replace temp view v1 as values (1, 2), (null, 7) as (c1, c2);
create or replace temp view v2 as values (2, 3) as (c1, c2);

select explode(array(c1)) as x
from v1
full outer join v2
using (c1);

-1   <== should be null
1
2
{noformat}
The following query fails with a {{NullPointerException}}:
{noformat}
create or replace temp view v1 as values ('1', 2), (null, 7) as (c1, c2);
create or replace temp view v2 as values ('2', 3) as (c1, c2);

select explode(array(c1)) as x
from v1
full outer join v2
using (c1);

23/06/25 17:06:39 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID 11)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.smj_consumeFullOuterJoinRow_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.wholestagecodegen_findNextJoinRows_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
...
{noformat}
",,bersprockets,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 11 03:21:22 UTC 2023,,,,,,,,,,"0|z1ivsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 19:57;bersprockets;This is similar to, but not quite the same as SPARK-43718, and the fix will be similar too.

I will make a PR shortly.
 ;;;","30/Jun/23 17:17;bersprockets;PR can be found here: https://github.com/apache/spark/pull/41809;;;","11/Jul/23 03:21;yumwang;Issue resolved by pull request 41809
[https://github.com/apache/spark/pull/41809];;;",,,,,,,,
Kafka Source v2 should return preferred locations,SPARK-44248,13541987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,siying,siying,siying,29/Jun/23 18:21,29/Jun/23 21:50,13/Jul/23 08:43,29/Jun/23 21:50,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Structured Streaming,,,0,,"DSv2 Kafka streaming source seems to miss setting the preferred location, which may destroy the purpose of cache for Kafka consumer (connection) & fetched data.

For DSv1, we have set the preferred location in RDD.

For DSv2, we should provide the info. in input partition, but we don't add the information into KafkaBatchInputPartition.",,kabhwan,siying,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 21:50:39 UTC 2023,,,,,,,,,,"0|z1ivqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 21:50;kabhwan;Issue resolved by pull request 41790
[https://github.com/apache/spark/pull/41790];;;",,,,,,,,,,
pyspark.sql.dataframe doctests can behave differently,SPARK-44245,13541919,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cdkrot,cdkrot,cdkrot,29/Jun/23 09:57,04/Jul/23 23:49,13/Jul/23 08:43,04/Jul/23 23:49,3.4.1,,,,,,,,,,,,,,,3.5.0,,,,PySpark,,,0,,,,cdkrot,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 23:49:10 UTC 2023,,,,,,,,,,"0|z1ivbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Jul/23 23:49;gurwls223;Issue resolved by pull request 41787
[https://github.com/apache/spark/pull/41787];;;",,,,,,,,,,
Set io.connectionTimeout/connectionCreationTimeout to zero or negative will cause executor incessantes cons/destructions,SPARK-44241,13541900,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,29/Jun/23 08:03,30/Jun/23 10:34,13/Jul/23 08:43,30/Jun/23 10:34,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Spark Core,,,0,,"{code:java}
2023-06-28 14:57:23 CST Bootstrap WARN - Failed to set channel option 'CONNECT_TIMEOUT_MILLIS' with value '-1000' for channel '[id: 0xf4b54a73]'
java.lang.IllegalArgumentException: connectTimeoutMillis : -1000 (expected: >= 0)
	at io.netty.util.internal.ObjectUtil.checkPositiveOrZero(ObjectUtil.java:144) ~[netty-common-4.1.74.Final.jar:4.1.74.Final] {code}",,githubbot,Qin Yao,yao,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 10:34:59 UTC 2023,,,,,,,,,,"0|z1iv80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 09:23;githubbot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/41785;;;","30/Jun/23 10:34;Qin Yao;Issue resolved by pull request 41785
[https://github.com/apache/spark/pull/41785];;;",,,,,,,,,
Client receives zero number of chunks in merge meta response which doesn't trigger fallback to unmerged blocks,SPARK-44215,13541609,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csingh,csingh,csingh,27/Jun/23 16:55,06/Jul/23 01:52,13/Jul/23 08:43,04/Jul/23 23:48,3.2.0,,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Shuffle,,,0,,"We still see instances of the server returning 0 {{numChunks}} in {{mergedMetaResponse}} which causes the executor to fail with {{ArithmeticException}}. 
{code}
java.lang.ArithmeticException: / by zero
	at org.apache.spark.storage.PushBasedFetchHelper.createChunkBlockInfosFromMetaResponse(PushBasedFetchHelper.scala:128)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1047)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:90)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
{code}
Here the executor doesn't fallback to fetch un-merged blocks and this also doesn't result in a {{FetchFailure}}. So, the application fails.",,csingh,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 01:52:31 UTC 2023,,,,,,,,,,"0|z1itfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Jul/23 05:00;csingh;PR to backport the change to 3.3
https://github.com/apache/spark/pull/41859;;;","06/Jul/23 01:52;mridulm80;Issue resolved by pull request 41762
https://github.com/apache/spark/pull/41762;;;",,,,,,,,,
Add missing recordHiveCall for getPartitionNames,SPARK-44204,13541495,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,chengpan,chengpan,chengpan,27/Jun/23 03:25,27/Jun/23 08:42,13/Jul/23 08:43,27/Jun/23 08:42,3.3.2,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,chengpan,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 08:42:52 UTC 2023,,,,,,,,,,"0|z1isqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Jun/23 08:42;LuciferYang;Issue resolved by pull request 41756
[https://github.com/apache/spark/pull/41756];;;",,,,,,,,,,
CacheManager refreshes the fileIndex unnecessarily,SPARK-44199,13541463,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vihangk1,vihangk1,vihangk1,26/Jun/23 18:30,03/Jul/23 23:08,13/Jul/23 08:43,03/Jul/23 23:08,3.4.1,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,0,,"The CacheManager on this line [https://github.com/apache/spark/blob/680ca2e56f2c8fc759743ad6755f6e3b1a19c629/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala#L372] uses a prefix based matching to decide which file index needs to be refreshed. However, that can be incorrect if the users have paths which are not subdirectories but share prefixes. For example, in the function below:

 
{code:java}
  private def refreshFileIndexIfNecessary(
      fileIndex: FileIndex,
      fs: FileSystem,
      qualifiedPath: Path): Boolean = {
    val prefixToInvalidate = qualifiedPath.toString
    val needToRefresh = fileIndex.rootPaths
      .map(_.makeQualified(fs.getUri, fs.getWorkingDirectory).toString)
      .exists(_.startsWith(prefixToInvalidate))
    if (needToRefresh) fileIndex.refresh()
    needToRefresh
  } {code}
{{If the prefixToInvalidate is s3://bucket/mypath/table_dir and the file index has one of the root paths as s3://bucket/mypath/table_dir_2/part=1, then the needToRefresh will be true and the file index gets refreshed unnecessarily. This is not just wasted CPU cycles but can cause query failures as well, if there are access restrictions to the path being refreshed.}}",,ignitetcbot,vihangk1,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 16:52:55 UTC 2023,,,,,,,,,,"0|z1isjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 16:52;ignitetcbot;User 'vihangk1' has created a pull request for this issue:
https://github.com/apache/spark/pull/41749;;;",,,,,,,,,,
Inconsistent path qualifying between catalog and data operations,SPARK-44185,13541343,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,26/Jun/23 03:49,03/Jul/23 09:27,13/Jul/23 08:43,03/Jul/23 09:27,3.2.4,3.3.2,3.4.1,3.5.0,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"For example
 * CREATE TABLE statement with relative LOCATION will infer schema from files from the directory relative to the current working directory and store the directory relative to the warehouse path. 
 * CTAS statement with relative LOCATION cannot assert empty root path as it checks the wrong path it will finally use.
 * DataframeWriter does not qualify the path before checking",,yao,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 09:27:20 UTC 2023,,,,,,,,,,"0|z1irso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Jul/23 09:27;yumwang;Issue resolved by pull request https://github.com/apache/spark/pull/41733;;;",,,,,,,,,,
Remove a wrong doc about ARROW_PRE_0_15_IPC_FORMAT,SPARK-44184,13541334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,25/Jun/23 22:21,26/Jun/23 01:53,13/Jul/23 08:43,26/Jun/23 01:53,3.0.3,3.1.3,3.2.4,3.3.2,3.4.1,3.5.0,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Documentation,PySpark,,0,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 01:53:57 UTC 2023,,,,,,,,,,"0|z1irqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Jun/23 01:53;dongjoon;Issue resolved by pull request 41730
[https://github.com/apache/spark/pull/41730];;;",,,,,,,,,,
Row as UDF inputs causes encoder errors,SPARK-44161,13541218,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhenli,zhenli,zhenli,23/Jun/23 21:20,28/Jun/23 01:07,13/Jul/23 08:43,28/Jun/23 01:07,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,0,,Ensure row inputs to udfs can be handled correctly.,,zhenli,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-23 21:20:29.0,,,,,,,,,,"0|z1ir14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unused `spark.kubernetes.executor.lostCheck.maxAttempts`,SPARK-44158,13541205,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,23/Jun/23 18:08,23/Jun/23 20:49,13/Jul/23 08:43,23/Jun/23 20:49,2.4.8,3.0.3,3.1.3,3.2.4,3.3.3,3.4.1,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Kubernetes,,,0,,,,dongjoon,,,,,,,,,,,,,,,,,SPARK-24248,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 23 20:49:57 UTC 2023,,,,,,,,,,"0|z1iqy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Jun/23 20:49;dongjoon;Issue resolved by pull request 41713
[https://github.com/apache/spark/pull/41713];;;",,,,,,,,,,
"Utility to convert python types to spark types compares Python ""type"" object rather than user's ""tpe"" for categorical data types",SPARK-44142,13541026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tedjenks,tedjenks,tedjenks,22/Jun/23 12:05,23/Jun/23 00:11,13/Jul/23 08:43,23/Jun/23 00:09,3.4.0,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,PySpark,,,0,,"In the typehints utility that converts python types to spark types, the line:
{code:java}
    # categorical types
    elif isinstance(tpe, CategoricalDtype) or (isinstance(tpe, str) and type == ""category""):
        return types.LongType() {code}
uses Python's 'type' keyword in the comparison. Hence, it will always be false. Here, the user's type is actually stored in the variable 'tpe'.

 

 

See line [here|https://github.com/apache/spark/blob/1b4048bf62dddae7d324c4b12aa409a1bd456dc5/python/pyspark/pandas/typedef/typehints.py#LL217C7-L217C7].",,gurwls223,tedjenks,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 23 00:09:52 UTC 2023,,,,,,,,,,"0|z1ipuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Jun/23 12:09;tedjenks;https://github.com/apache/spark/pull/41697;;;","23/Jun/23 00:09;gurwls223;Issue resolved by pull request 41697
[https://github.com/apache/spark/pull/41697];;;",,,,,,,,,
StateManager may get materialized in executor instead of driver in FlatMapGroupsWithStateExec,SPARK-44136,13540939,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dlgaobo,dlgaobo,dlgaobo,21/Jun/23 21:08,22/Jun/23 00:26,13/Jul/23 08:43,22/Jun/23 00:26,3.3.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Structured Streaming,,,0,,StateManager may get materialized in executor instead of driver in FlatMapGroupsWithStateExec because of a previous change https://issues.apache.org/jira/browse/SPARK-40411,,dlgaobo,gurwls223,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 22 00:26:59 UTC 2023,,,,,,,,,,"0|z1ipbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Jun/23 23:07;kabhwan;PR: https://github.com/apache/spark/pull/41693;;;","22/Jun/23 00:26;gurwls223;Issue resolved by pull request 41693
[https://github.com/apache/spark/pull/41693];;;",,,,,,,,,
Can't set resources (GPU/FPGA) to 0 when they are set to positive value in spark-defaults.conf,SPARK-44134,13540908,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,21/Jun/23 14:14,25/Jun/23 17:43,13/Jul/23 08:43,23/Jun/23 00:41,3.2.0,,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Spark Core,,,0,,"With resource aware scheduling, if you specify a default value in the spark-defaults.conf, a user can't override that to set it to 0.

Meaning spark-defaults.conf has something like:
{{spark.executor.resource.\{resourceName}.amount=1}}

{{spark.task.resource.\{resourceName}.amount}} =1

If the user tries to override when submitting an application with {{{}spark.executor.resource.\{resourceName}.amount{}}}=0 and {{spark.task.resource.\{resourceName}.amount}} =0, it gives the user an error:

 
{code:java}
23/06/21 09:12:57 ERROR Main: Failed to initialize Spark session.
org.apache.spark.SparkException: No executor resource configs were not specified for the following task configs: gpu
        at org.apache.spark.resource.ResourceProfile.calculateTasksAndLimitingResource(ResourceProfile.scala:206)
        at org.apache.spark.resource.ResourceProfile.$anonfun$limitingResource$1(ResourceProfile.scala:139)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.resource.ResourceProfile.limitingResource(ResourceProfile.scala:138)
        at org.apache.spark.resource.ResourceProfileManager.addResourceProfile(ResourceProfileManager.scala:95)
        at org.apache.spark.resource.ResourceProfileManager.<init>(ResourceProfileManager.scala:49)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:455)
        at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2704)
        at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953){code}
This used to work, my guess is this may have gotten broken with the stage level scheduling feature.",,dongjoon,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 25 17:43:25 UTC 2023,,,,,,,,,,"0|z1ip4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Jun/23 14:14;tgraves;I'm working on a fix for this;;;","23/Jun/23 00:41;dongjoon;Issue resolved by pull request 41703
[https://github.com/apache/spark/pull/41703];;;","25/Jun/23 17:43;dongjoon;Oh, thank you for updating the fix versions, [~tgraves].;;;",,,,,,,,
"Use ""3.5.0"" for `master` branch until creating `branch-3.5`",SPARK-44129,13540831,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,dongjoon,dongjoon,21/Jun/23 01:29,21/Jun/23 02:20,13/Jul/23 08:43,21/Jun/23 02:20,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Project Infra,,,0,,,,dongjoon,gurwls223,,,,,,,,,,,,,,,,,,,,SPARK-44130,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 21 02:20:39 UTC 2023,,,,,,,,,,"0|z1ionc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Jun/23 01:47;gurwls223;Issue resolved by pull request 41682
[https://github.com/apache/spark/pull/41682];;;","21/Jun/23 02:13;gurwls223;Reverted in https://github.com/apache/spark/commit/5a55061df25a9f9c1c35c272b1563705d957eb84;;;","21/Jun/23 02:20;dongjoon;Issue resolved by pull request 41684
[https://github.com/apache/spark/pull/41684];;;",,,,,,,,
Upgrade Apache Arrow to 12.0.1,SPARK-44094,13540511,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,18/Jun/23 08:43,18/Jun/23 20:27,13/Jul/23 08:43,18/Jun/23 20:27,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Build,,,0,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 18 20:27:48 UTC 2023,,,,,,,,,,"0|z1imog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Jun/23 20:27;dongjoon;Issue resolved by pull request 41650
[https://github.com/apache/spark/pull/41650];;;",,,,,,,,,,
Json reader crashes when a different schema is present,SPARK-44079,13540407,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,charlottevdscheun,charlottevdscheun,16/Jun/23 13:20,29/Jun/23 13:38,13/Jul/23 08:43,29/Jun/23 06:28,3.4.0,,,,,,,,,,,,,,,3.4.2,3.5.0,,,SQL,,,0,,"When using pyspark 3.4, we noticed that when reading a json file with a corrupted record the reader crashes. In pyspark 3.3 this worked fine.

{*}Code{*}:
{code:java}
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
import json


data = """"""[{""a"": ""incorrect"", ""b"": ""correct""}]""""""
schema = StructType([StructField('a', IntegerType(), True), StructField('b', StringType(), True), StructField('_corrupt_record', StringType(), True)])


spark.read.option(""mode"", ""PERMISSIVE"").option(""multiline"",""true"").schema(schema).json(spark.sparkContext.parallelize([data])).show(truncate=False){code}
*Used packages:*
 * Pyspark==3.4.0
 * python==3.10.0
 * delta-spark==2.4.0

 
spark_jars=(
  ""org.apache.spark:spark-avro_2.12:3.4.0""
  "",io.delta:delta-core_2.12:2.4.0""
  "",com.databricks:spark-xml_2.12:0.16.0""
)
 

{*}Expected behaviour{*}:
|a|b|_corrupt_record|
|null|null|[\\{""a"": ""incorrect"", ""b"": ""correct""}]|

 

{*}Actual behaviour{*}:
{code:java}
 
*** py4j.protocol.Py4JJavaError: An error occurred while calling o104.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 2.0 failed 1 times, most recent failure: Lost task 4.0 in stage 2.0 (TID 9) (charlottesmbp2.home executor driver): java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:201)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getAs(rows.scala:35)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get$(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.get(rows.scala:195)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$2(FailureSafeParser.scala:47)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$1(FailureSafeParser.scala:47)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:64)
        at org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:431)
        at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
        at org.apache.spark.scheduler.Task.run(Task.scala:139)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        at java.base/java.lang.Thread.run(Thread.java:1589)
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
        at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
        at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
        at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
        at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
        at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
        at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
        at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
        at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
        at java.base/java.lang.reflect.Method.invoke(Method.java:578)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:1589)
Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:201)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getAs(rows.scala:35)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get$(rows.scala:37)
        at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.get(rows.scala:195)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$2(FailureSafeParser.scala:47)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$1(FailureSafeParser.scala:47)
        at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:64)
        at org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:431)
        at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
        at org.apache.spark.scheduler.Task.run(Task.scala:139)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        ... 1 more {code}",,charlottevdscheun,fanjia,githubbot,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 06:28:28 UTC 2023,,,,,,,,,,"0|z1im28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Jun/23 09:15;githubbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41662;;;","29/Jun/23 06:28;maxgekk;Issue resolved by pull request 41662
[https://github.com/apache/spark/pull/41662];;;",,,,,,,,,
`Logging plan changes for execution` test failed,SPARK-44074,13540299,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,16/Jun/23 03:12,20/Jun/23 04:40,13/Jul/23 08:43,20/Jun/23 04:40,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,0,,"run {{build/sbt clean ""sql/test"" -Dtest.exclude.tags=org.apache.spark.tags.ExtendedSQLTest,org.apache.spark.tags.SlowSQLTest}}

{{}}
{code:java}
2023-06-15T19:58:34.4105460Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32mQueryExecutionSuite:�[0m�[0m
2023-06-15T19:58:34.5395268Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to a file (77 milliseconds)�[0m�[0m
2023-06-15T19:58:34.5856902Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to an existing file (49 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6099849Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to non-existing folder (25 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6136467Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info by invalid path (4 milliseconds)�[0m�[0m
2023-06-15T19:58:34.6425071Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- dumping query execution info to a file - explainMode=formatted (28 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7084916Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- limit number of fields by sql config (66 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7432299Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- check maximum fields restriction (34 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7554546Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- toString() exception/error handling (11 milliseconds)�[0m�[0m
2023-06-15T19:58:34.7621424Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[32m- SPARK-28346: clone the query plan between different stages (6 milliseconds)�[0m�[0m
2023-06-15T19:58:34.8001412Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[31m- Logging plan changes for execution *** FAILED *** (12 milliseconds)�[0m�[0m
2023-06-15T19:58:34.8007977Z �[0m[�[0m�[0minfo�[0m] �[0m�[0m�[31m  testAppender.loggingEvents.exists(((x$10: org.apache.logging.log4j.core.LogEvent) => x$10.getMessage().getFormattedMessage().contains(expectedMsg))) was false (QueryExecutionSuite.scala:232)�[0m�[0m 

{code}
 

but run {{build/sbt ""sql/testOnly *QueryExecutionSuite""}} not this issue, need to investigate. ",,dongjoon,LuciferYang,snoot,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 20 04:40:37 UTC 2023,,,,,,,,,,"0|z1ile8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Jun/23 03:57;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/41663;;;","20/Jun/23 04:40;dongjoon;Issue resolved by pull request 41663
[https://github.com/apache/spark/pull/41663];;;",,,,,,,,,
Maven test `ProductAggSuite` aborted,SPARK-44064,13540138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,15/Jun/23 05:43,25/Jun/23 23:52,13/Jul/23 08:43,25/Jun/23 23:52,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"run 

 
{code:java}
 ./build/mvn  -DskipTests -Pyarn -Pmesos -Pkubernetes -Pvolcano -Phive -Phive-thriftserver -Phadoop-cloud -Pspark-ganglia-lgpl  clean install
 build/mvn test -pl sql/catalyst     {code}
aborted

 
{code:java}
ProductAggSuite:
*** RUN ABORTED ***
  java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$
  at org.apache.spark.sql.catalyst.expressions.codegen.JavaCode$.variable(javaCode.scala:64)
  at org.apache.spark.sql.catalyst.expressions.codegen.JavaCode$.isNullVariable(javaCode.scala:77)
  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:200)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:196)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.$anonfun$create$1(GenerateSafeProjection.scala:156)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:153)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:39)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1369) {code}
 ",,dongjoon,LuciferYang,snoot,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 25 23:52:11 UTC 2023,,,,,,,,,,"0|z1ikeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Jun/23 04:17;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/41654;;;","25/Jun/23 23:52;dongjoon;Issue resolved by pull request 41654
[https://github.com/apache/spark/pull/41654];;;",,,,,,,,,
Update ORC to 1.8.4,SPARK-44053,13540006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Guiyankuang,Guiyankuang,Guiyankuang,14/Jun/23 08:25,14/Jun/23 17:56,13/Jul/23 08:43,14/Jun/23 16:37,3.4.1,3.5.0,,,,,,,,,,,,,,3.4.1,3.5.0,,,Build,,,0,,,,dongjoon,Guiyankuang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 14 17:56:09 UTC 2023,,,,,,,,,,"0|z1ijl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Jun/23 08:43;Guiyankuang;Our plan is to
spark 3.4.1 upgrade to ORC 1.8.4
spark 3.5.0 upgrade to ORC 1.9.0
So I set the affected version to 3.4.1

[~yumwang]  :);;;","14/Jun/23 16:37;dongjoon;Issue resolved by pull request 41593
[https://github.com/apache/spark/pull/41593];;;","14/Jun/23 17:56;dongjoon;Apache ORC 1.9.0 PR will arrive soon in this month.;;;",,,,,,,,
Incorrect result after count distinct,SPARK-44040,13539864,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yumwang,boltonidze,boltonidze,13/Jun/23 13:42,16/Jun/23 04:07,13/Jul/23 08:43,16/Jun/23 04:07,3.3.2,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,Spark Core,,,0,,"When i try to call count after distinct function for Decimal null field, spark return incorrect result starting from spark 3.4.0.
A minimal example to reproduce:

import org.apache.spark.sql.types._
import org.apache.spark.sql.\{Column, DataFrame, Dataset, Row, SparkSession}
import org.apache.spark.sql.types.\{StringType, StructField, StructType}
val schema = StructType( Array(
StructField(""money"", DecimalType(38,6), true),
StructField(""reference_id"", StringType, true)
))

val payDf = spark.createDataFrame(sc.emptyRDD[Row], schema)

val aggDf = payDf.agg(sum(""money"").as(""money"")).withColumn(""name"", lit(""df1""))
val aggDf1 = payDf.agg(sum(""money"").as(""money"")).withColumn(""name"", lit(""df2""))
val unionDF: DataFrame = aggDf.union(aggDf1)
unionDF.select(""money"").distinct.show // return correct result
unionDF.select(""money"").distinct.count // return 2 instead of 1
unionDF.select(""money"").distinct.count == 1 // return false


This block of code returns some assertion error and after that an incorrect count (in spark 3.2.1 everything works fine and i get correct result = 1):

*scala> unionDF.select(""money"").distinct.show // return correct result*
java.lang.AssertionError: assertion failed:
Decimal$DecimalIsFractional
while compiling: <console>
during phase: globalPhase=terminal, enteringPhase=jvm
library version: version 2.12.17
compiler version: version 2.12.17
reconstructed args: -classpath /Users/aleksandrov/.ivy2/jars/org.apache.spark_spark-connect_2.12-3.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-storage-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar:/Users/aleksandrov/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar -Yrepl-class-based -Yrepl-outdir /private/var/folders/qj/_dn4xbp14jn37qmdk7ylyfwc0000gr/T/spark-f37bb154-75f3-4db7-aea8-3c4363377bd8/repl-350f37a1-1df1-4816-bd62-97929c60a6c1

last tree to typer: TypeTree(class Byte)
tree position: line 6 of <console>
tree tpe: Byte
symbol: (final abstract) class Byte in package scala
symbol definition: final abstract class Byte extends (a ClassSymbol)
symbol package: scala
symbol owners: class Byte
call site: constructor $eval in object $eval in package $line19

== Source file context for tree position ==

3
4object $eval {
5lazyval $result = $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.res0
6lazyval $print: {_}root{_}.java.lang.String = {
7 $line19.$read.INSTANCE.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw.$iw
8
9""""
at scala.reflect.internal.SymbolTable.throwAssertionError(SymbolTable.scala:185)
at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1525)
at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
at scala.reflect.internal.Symbols$Symbol.flatOwnerInfo(Symbols.scala:2353)
at scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala:3346)
at scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:3348)
at scala.reflect.internal.Symbols$ModuleClassSymbol.sourceModule(Symbols.scala:3487)
at scala.reflect.internal.Symbols.$anonfun$forEachRelevantSymbols$1$adapted(Symbols.scala:3802)
at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
at scala.reflect.internal.Symbols.markFlagsCompleted(Symbols.scala:3799)
at scala.reflect.internal.Symbols.markFlagsCompleted$(Symbols.scala:3805)
at scala.reflect.internal.SymbolTable.markFlagsCompleted(SymbolTable.scala:28)
at scala.reflect.internal.pickling.UnPickler$Scan.finishSym$1(UnPickler.scala:324)
at scala.reflect.internal.pickling.UnPickler$Scan.readSymbol(UnPickler.scala:342)
at scala.reflect.internal.pickling.UnPickler$Scan.readSymbolRef(UnPickler.scala:645)
at scala.reflect.internal.pickling.UnPickler$Scan.readType(UnPickler.scala:413)
at scala.reflect.internal.pickling.UnPickler$Scan.$anonfun$readSymbol$10(UnPickler.scala:357)
at scala.reflect.internal.pickling.UnPickler$Scan.at(UnPickler.scala:188)
at scala.reflect.internal.pickling.UnPickler$Scan.readSymbol(UnPickler.scala:357)
at scala.reflect.internal.pickling.UnPickler$Scan.$anonfun$run$1(UnPickler.scala:96)
at scala.reflect.internal.pickling.UnPickler$Scan.run(UnPickler.scala:88)
at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:47)
at scala.tools.nsc.symtab.classfile.ClassfileParser.unpickleOrParseInnerClasses(ClassfileParser.scala:1173)
at scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfileParser.scala:467)
at scala.tools.nsc.symtab.classfile.ClassfileParser.$anonfun$parse$2(ClassfileParser.scala:160)
at scala.tools.nsc.symtab.classfile.ClassfileParser.$anonfun$parse$1(ClassfileParser.scala:146)
at scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.scala:129)
at scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoaders.scala:343)
at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:250)
at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scala:269)
at scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:1104)
at scala.reflect.internal.Symbols$Symbol.toOption(Symbols.scala:2609)
at scala.tools.nsc.interpreter.IMain.translateSimpleResource(IMain.scala:340)
at scala.tools.nsc.interpreter.IMain$TranslatingClassLoader.findAbstractFile(IMain.scala:354)
at scala.reflect.internal.util.AbstractFileClassLoader.findResource(AbstractFileClassLoader.scala:76)
at java.base/java.lang.ClassLoader.getResource(ClassLoader.java:1401)
at java.base/java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1737)
at scala.reflect.internal.util.RichClassLoader$.classAsStream$extension(ScalaClassLoader.scala:89)
at scala.reflect.internal.util.RichClassLoader$.classBytes$extension(ScalaClassLoader.scala:81)
at scala.reflect.internal.util.ScalaClassLoader.classBytes(ScalaClassLoader.scala:131)
at scala.reflect.internal.util.ScalaClassLoader.classBytes$(ScalaClassLoader.scala:131)
at scala.reflect.internal.util.AbstractFileClassLoader.classBytes(AbstractFileClassLoader.scala:41)
at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:70)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:576)
at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)
at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:75)
at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:317)
at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8895)
at org.codehaus.janino.UnitCompiler.reclassifyName(UnitCompiler.java:9115)
at org.codehaus.janino.UnitCompiler.reclassifyName(UnitCompiler.java:8806)
at org.codehaus.janino.UnitCompiler.reclassify(UnitCompiler.java:8667)
at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7194)
at org.codehaus.janino.UnitCompiler.access$18100(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$26.visitAmbiguousName(UnitCompiler.java:6785)
at org.codehaus.janino.UnitCompiler$26.visitAmbiguousName(UnitCompiler.java:6784)
at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4603)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6784)
at org.codehaus.janino.UnitCompiler.access$15100(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$25.visitLvalue(UnitCompiler.java:6745)
at org.codehaus.janino.UnitCompiler$25.visitLvalue(UnitCompiler.java:6742)
at org.codehaus.janino.Java$Lvalue.accept(Java.java:4528)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6742)
at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$23.visitRvalue(UnitCompiler.java:6690)
at org.codehaus.janino.UnitCompiler$23.visitRvalue(UnitCompiler.java:6681)
at org.codehaus.janino.Java$Rvalue.accept(Java.java:4495)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6681)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9392)
at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7486)
at org.codehaus.janino.UnitCompiler.access$16100(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$25.visitMethodInvocation(UnitCompiler.java:6756)
at org.codehaus.janino.UnitCompiler$25.visitMethodInvocation(UnitCompiler.java:6742)
at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5470)
at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6742)
at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9590)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9475)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9391)
at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5232)
at org.codehaus.janino.UnitCompiler.access$9300(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4735)
at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4711)
at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5470)
at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4711)
at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5854)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:4101)
at org.codehaus.janino.UnitCompiler.access$6300(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:4057)
at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:4040)
at org.codehaus.janino.Java$Assignment.accept(Java.java:4864)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:4040)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2523)
at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1580)
at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:3209)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1646)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1579)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1575)
at org.codehaus.janino.Java$Block.accept(Java.java:3115)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2659)
at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1581)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$IfStatement.accept(Java.java:3284)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1646)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1579)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1575)
at org.codehaus.janino.Java$Block.accept(Java.java:3115)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2637)
at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1581)
at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$IfStatement.accept(Java.java:3284)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1646)
at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1579)
at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1575)
at org.codehaus.janino.Java$Block.accept(Java.java:3115)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2001)
at org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1584)
at org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1575)
at org.codehaus.janino.Java$WhileStatement.accept(Java.java:3389)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3658)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3329)
at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1447)
at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1420)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:829)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1026)
at org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$3.visitMemberClassDeclaration(UnitCompiler.java:425)
at org.codehaus.janino.UnitCompiler$3.visitMemberClassDeclaration(UnitCompiler.java:418)
at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1533)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:418)
at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1397)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:864)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:442)
at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$3.visitPackageMemberClassDeclaration(UnitCompiler.java:422)
at org.codehaus.janino.UnitCompiler$3.visitPackageMemberClassDeclaration(UnitCompiler.java:418)
at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1688)
at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:418)
at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:392)
at org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:236)
at org.codehaus.janino.UnitCompiler$2.visitCompilationUnit(UnitCompiler.java:363)
at org.codehaus.janino.UnitCompiler$2.visitCompilationUnit(UnitCompiler.java:361)
at org.codehaus.janino.Java$CompilationUnit.accept(Java.java:371)
at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:361)
at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:264)
at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:294)
at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:288)
at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:267)
at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:82)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1496)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1586)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1583)
at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1443)
at org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:726)
at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:725)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
at org.apache.spark.sql.execution.UnionExec.$anonfun$doExecute$5(basicPhysicalOperators.scala:699)
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
at scala.collection.TraversableLike.map(TraversableLike.scala:286)
at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
at scala.collection.AbstractTraversable.map(Traversable.scala:108)
at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:699)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs(AggregateCodegenSupport.scala:89)
at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.inputRDDs$(AggregateCodegenSupport.scala:88)
at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:47)
at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:140)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)
at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:181)
at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:181)
at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:183)
at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:266)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:264)
at scala.collection.Iterator.foreach(Iterator.scala:943)
at scala.collection.Iterator.foreach$(Iterator.scala:943)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
at scala.collection.IterableLike.foreach(IterableLike.scala:74)
at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:264)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:381)
at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)
at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
at org.apache.spark.sql.Dataset.show(Dataset.scala:809)
at org.apache.spark.sql.Dataset.show(Dataset.scala:768)
at org.apache.spark.sql.Dataset.show(Dataset.scala:777)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:33)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:35)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:37)
at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:39)
at $line19.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:41)
at $line19.$read$$iw$$iw$$iw$$iw.<init>(<console>:43)
at $line19.$read$$iw$$iw$$iw.<init>(<console>:45)
at $line19.$read$$iw$$iw.<init>(<console>:47)
at $line19.$read$$iw.<init>(<console>:49)
at $line19.$read.<init>(<console>:51)
at $line19.$read$.<init>(<console>:55)
at $line19.$read$.<clinit>(<console>)
at $line19.$eval$.$print$lzycompute(<console>:7)
at $line19.$eval$.$print(<console>:6)
at $line19.$eval.$print(<console>)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
at org.apache.spark.repl.Main$.doMain(Main.scala:78)
at org.apache.spark.repl.Main$.main(Main.scala:58)
at org.apache.spark.repl.Main.main(Main.scala)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:566)
at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1020)
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
error: error while loading Decimal, class file '/Users/aleksandrov/Projects/apache/spark-3.4.0-bin-hadoop3/jars/spark-catalyst_2.12-3.4.0.jar(org/apache/spark/sql/types/Decimal.class)' is broken
(class java.lang.RuntimeException/error reading Scala signature of Decimal.class: assertion failed:
Decimal$DecimalIsFractional
while compiling: <console>
during phase: globalPhase=terminal, enteringPhase=jvm
library version: version 2.12.17
compiler version: version 2.12.17
reconstructed args: -classpath /Users/aleksandrov/.ivy2/jars/org.apache.spark_spark-connect_2.12-3.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/io.delta_delta-storage-2.4.0.jar:/Users/aleksandrov/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar:/Users/aleksandrov/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar -Yrepl-class-based -Yrepl-outdir /private/var/folders/qj/_dn4xbp14jn37qmdk7ylyfwc0000gr/T/spark-f37bb154-75f3-4db7-aea8-3c4363377bd8/repl-350f37a1-1df1-4816-bd62-97929c60a6c1

last tree to typer: TypeTree(class Byte)
tree position: line 6 of <console>
tree tpe: Byte
symbol: (final abstract) class Byte in package scala
symbol definition: final abstract class Byte extends (a ClassSymbol)
symbol package: scala
symbol owners: class Byte
call site: constructor $eval in object $eval in package $line19

== Source file context for tree position ==

3
4object $eval {
5lazyval $result = res0
6lazyval $print: {_}root{_}.java.lang.String = {
7 $iw
8
9"""" )
+-----+
|money|

+-----+
|null|

+-----+

*scala> unionDF.select(""money"").distinct.count // return 2 instead of 1*
res1: Long = 2 

*scala> unionDF.select(""money"").distinct.count == 1 // return False*
res2: Boolean = false",,bersprockets,boltonidze,githubbot,yumwang,,,,,,,,,,,,,,,,,,SPARK-38162,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 04:07:40 UTC 2023,,,,,,,,,,"0|z1iipk:",9223372036854775807,,,,,,,,,,,,,3.4.1,,,,,,,,,"13/Jun/23 16:15;yumwang;Thanks for reporting this bug. We will fix it soon.;;;","13/Jun/23 16:29;bersprockets;It seems this can be reproduced in {{spark-sql}} as well.

Interestingly, turning off AQE seems to fix the issue (for both the above dataframe version and the below SQL version):
{noformat}
spark-sql (default)> create or replace temp view v1 as
select 1 as c1 limit 0;
Time taken: 0.959 seconds
spark-sql (default)> create or replace temp view agg1 as
select sum(c1) as c1, ""agg1"" as name
from v1;
Time taken: 0.16 seconds
spark-sql (default)> create or replace temp view agg2 as
select sum(c1) as c1, ""agg2"" as name
from v1;
Time taken: 0.035 seconds
spark-sql (default)> create or replace temp view union1 as
select * from agg1
union
select * from agg2;
Time taken: 0.088 seconds
spark-sql (default)> -- the following incorrectly produces 2 rows
select distinct c1 from union1;
NULL
NULL
Time taken: 1.649 seconds, Fetched 2 row(s)
spark-sql (default)> set spark.sql.adaptive.enabled=false;
spark.sql.adaptive.enabled	false
Time taken: 0.019 seconds, Fetched 1 row(s)
spark-sql (default)> -- the following correctly produces 1 row
select distinct c1 from union1;
NULL
Time taken: 1.372 seconds, Fetched 1 row(s)
spark-sql (default)> 
{noformat};;;","13/Jun/23 16:35;yumwang;https://github.com/apache/spark/pull/41576;;;","14/Jun/23 09:16;githubbot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/41576;;;","16/Jun/23 04:07;yumwang;Issue resolved by pull request 41576
[https://github.com/apache/spark/pull/41576];;;",,,,,,
Artifacts with name as an absolute path may overwrite other files ,SPARK-44016,13539422,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,09/Jun/23 12:23,13/Jun/23 17:05,13/Jul/23 08:43,13/Jun/23 17:05,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,0,,"In `SparkConnectAddArtifactsHandler`, an artifact being moved to a staging location may overwrite another file when the `name`/`path` of the artifact is an `absolute` path. 

This happens when the [stagedPath|https://github.com/apache/spark/blob/master/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala#L172] is being computed with the help of the `.resolve(...)` method where the `resolve` method returns the `other` path (in this case, the name of the artifact) if the `other` path is an absolute path.",,vicennial,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-09 12:23:42.0,,,,,,,,,,"0|z1ifzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python StreamingQueryProgress rowsPerSecond type fix,SPARK-44010,13539351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,WweiL,WweiL,WweiL,09/Jun/23 01:51,09/Jun/23 04:11,13/Jul/23 08:43,09/Jun/23 04:11,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,PySpark,Structured Streaming,,0,,,,gurwls223,WweiL,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 04:11:47 UTC 2023,,,,,,,,,,"0|z1ifjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Jun/23 04:11;gurwls223;Issue resolved by pull request 41522
[https://github.com/apache/spark/pull/41522];;;",,,,,,,,,,
SparkConnectArtifactStatusesHandler freezes on second request,SPARK-44002,13539145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,07/Jun/23 20:56,07/Jun/23 23:47,13/Jul/23 08:43,07/Jun/23 23:47,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,SparkConnectArtifactStatusesHandler freezes on the second request when a cache exists. Need to free a lock on the block.,,gurwls223,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 07 23:47:21 UTC 2023,,,,,,,,,,"0|z1ie9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Jun/23 23:47;gurwls223;Issue resolved by pull request 41500
[https://github.com/apache/spark/pull/41500];;;",,,,,,,,,,
Spark protobuf enums.as.ints raises exception on repeated enum types,SPARK-43985,13538947,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,justaparth,justaparth,justaparth,06/Jun/23 15:16,07/Jun/23 03:06,13/Jul/23 08:43,07/Jun/23 03:06,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Protobuf,,,0,,"For repeated enum types, the `enums.as.ints` being enabled currently raises an exception when trying to deserialize repeated enum fields. We should fix this behavior so that repeated enum fields work correctly.",,gurwls223,justaparth,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 07 03:06:29 UTC 2023,,,,,,,,,,"0|z1id1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Jun/23 03:06;gurwls223;Issue resolved by pull request 41481
[https://github.com/apache/spark/pull/41481];;;",,,,,,,,,,
bad case of connect-jvm-client-mima-check,SPARK-43977,13538852,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,06/Jun/23 03:46,06/Jun/23 14:46,13/Jul/23 08:43,06/Jun/23 14:46,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Tests,,,0,,"run 

```

build/sbt ""protobuf/clean""

dev/connect-jvm-client-mima-check

```
{code:java}
Using SPARK_LOCAL_IP=localhost
Using SPARK_LOCAL_IP=localhost
Do connect-client-jvm module mima check ...
Failed to find the jar: spark-protobuf-assembly(.*).jar or spark-protobuf(.*)3.5.0-SNAPSHOT.jar inside folder: /Users/yangjie01/SourceCode/git/spark-mine-sbt/connector/protobuf/target. This file can be generated by similar to the following command: build/sbt package|assembly
finish connect-client-jvm module mima check ...
connect-client-jvm module mima check passed.
 {code}
The check result is wrong,  there are both error messages and checks successful

 ",,LuciferYang,snoot,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 14:46:34 UTC 2023,,,,,,,,,,"0|z1icgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Jun/23 03:50;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/41473;;;","06/Jun/23 14:46;LuciferYang;Issue resolved by pull request 41473
[https://github.com/apache/spark/pull/41473];;;",,,,,,,,,
Handle the case where modifiedConfigs doesn't exist in event logs,SPARK-43976,13538850,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,06/Jun/23 03:08,06/Jun/23 16:35,13/Jul/23 08:43,06/Jun/23 16:35,3.3.2,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,Spark Core,,,0,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 16:35:40 UTC 2023,,,,,,,,,,"0|z1icg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Jun/23 16:35;dongjoon;Issue resolved by pull request 41472
[https://github.com/apache/spark/pull/41472];;;",,,,,,,,,,
Structured Streaming UI should display failed queries correctly,SPARK-43973,13538832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,05/Jun/23 22:39,06/Jun/23 05:08,13/Jul/23 08:43,06/Jun/23 05:08,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,,3.4.1,3.5.0,,,Web UI,,,0,,"The Structured Streaming UI is designed to be able to show a query's status (active/finished/failed) and if failed, the error message.
Due to a bug in the implementation, the error message in {{QueryTerminatedEvent}} isn't being tracked by the UI data, so in turn the UI always shows failed queries as ""finished"".

Example:
{code:scala}
implicit val ctx = spark.sqlContext
import org.apache.spark.sql.execution.streaming.MemoryStream

spark.conf.set(""spark.sql.ansi.enabled"", ""true"")

val inputData = MemoryStream[(Int, Int)]

val df = inputData.toDF().selectExpr(""_1 / _2 as a"")

inputData.addData((1, 2), (3, 4), (5, 6), (7, 0))
val testQuery = df.writeStream.format(""memory"").queryName(""kristest"").outputMode(""append"").start
testQuery.processAllAvailable()
{code}

Here we intentionally fail a query, but the Spark UI's Structured Streaming tab would show this as ""FINISHED"" without any errors, which is wrong.",,Gengliang.Wang,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 05:08:03 UTC 2023,,,,,,,,,,"0|z1icc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Jun/23 05:08;Gengliang.Wang;Issue resolved by pull request 41468
[https://github.com/apache/spark/pull/41468];;;",,,,,,,,,,
"Improve error messages: CANNOT_DECODE_URL, CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE, CANNOT_PARSE_DECIMAL, CANNOT_READ_FILE_FOOTER, CANNOT_RECOGNIZE_HIVE_TYPE.",SPARK-43962,13538726,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,05/Jun/23 04:34,06/Jun/23 07:25,13/Jul/23 08:43,06/Jun/23 07:25,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,Improve error message for usability.,,itholic,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 07:25:36 UTC 2023,,,,,,,,,,"0|z1iboo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Jun/23 07:25;maxgekk;Issue resolved by pull request 41455
[https://github.com/apache/spark/pull/41455];;;",,,,,,,,,,
DataFrameConversionTestsMixin is not tested properly.,SPARK-43960,13538693,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,04/Jun/23 09:51,04/Jun/23 23:34,13/Jul/23 08:43,04/Jun/23 23:34,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Pandas API on Spark,Tests,,0,,DataFrameConversionTestsMixin is not tested properly. We should fix it.,,gurwls223,itholic,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 04 23:34:20 UTC 2023,,,,,,,,,,"0|z1ibhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Jun/23 23:34;gurwls223;Issue resolved by pull request 41450
[https://github.com/apache/spark/pull/41450];;;",,,,,,,,,,
"Fix the bug doesn't display column's sql for Percentile[Cont|Disc]",SPARK-43956,13538665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,03/Jun/23 05:13,04/Jun/23 00:31,13/Jul/23 08:43,03/Jun/23 05:20,3.3.0,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,0,,"Last year, I committed Percentile[Cont|Disc] functions for Spark SQL.
Recently, I found the sql method of Percentile[Cont|Disc] doesn't display column's sql suitably.",,beliefer,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 04 00:31:30 UTC 2023,,,,,,,,,,"0|z1ibb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Jun/23 06:58;beliefer;resolved by https://github.com/apache/spark/pull/41436;;;","04/Jun/23 00:31;dongjoon;This is backported to branch-3.3 via https://github.com/apache/spark/pull/41446;;;",,,,,,,,,
RocksDB state store can become corrupt on task retries,SPARK-43951,13538603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kimahriman,kimahriman,02/Jun/23 13:07,02/Jun/23 13:09,13/Jul/23 08:43,02/Jun/23 13:09,3.4.0,,,,,,,,,,,,,,,,,,,SQL,,,0,,"A couple of our streaming jobs have failed since upgrading to Spark 3.4 with an error such as:

org.rocksdb.RocksDBException: Mismatch in unique ID on table file ###. Expected: [###,###} Actual\{###,###} in file ..../MANIFEST-####

This is due to the change from [https://github.com/facebook/rocksdb/commit/6de7081cf37169989e289a4801187097f0c50fae] that enabled unique ID checks by default, and I finally tracked down the exact sequence of steps that leads to this failure in the way RocksDB state store is used.
 # A task fails after uploading the checkpoint to HDFS. Lets say it uploaded 11.zip to version 11 of the table, but the task failed before it could finish after successfully uploading the checkpoint.
 # The same task is retried and goes back to load version 10 of the table as expected.
 # Cleanup/maintenance is called for this partition, which looks in HDFS for persisted versions and sees up through version 11 since that zip file was successfully uploaded on the previous task.
 # As part of resolving what SST files are part of each table version, versionToRocksDBFiles.put(version, newResolvedFiles) is called for version 11 with its SST files that were uploaded in the first failed task.
 # The second attempt at the task commits and goes to sync its checkpoint to HDFS.
 # versionToRocksDBFiles contains the SST files to upload from step 4, and these files are considered ""the same"" as what's in the local working dir because the name and file size match.
 # No SST files are uploaded because they matched above, but in reality the unique ID inside the SST files is different (presumably this is just randomly generated and inserted into each SST file?), it just doesn't affect the size.
 # A new METADATA file is uploaded which has the new unique IDs listed inside.
 # When version 11 of the table is read during the next batch, the unique IDs in the METADATA file don't match the unique IDS in the SST files, which causes the exception.

 

This is basically a ticking time bomb for anyone using RocksDB. Thoughts on possible fixes would be:
 * Disable unique ID verification. I don't currently see a binding for this in the RocksDB java wrapper, so that would probably have to be added first.
 * Disable checking if files are already uploaded with the same size, and just always upload SST files no matter what.
 * Update the ""same file"" check to also be able to do some kind of CRC comparison or something like that.
 * Update the mainteance/cleanup to not update the versionToRocksDBFiles map.",,kimahriman,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 13:09:07 UTC 2023,,,,,,,,,,"0|z1iaxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Jun/23 13:09;kimahriman;Of course as soon as I finish figuring all this out I found https://github.com/apache/spark/pull/41089;;;",,,,,,,,,,
Upgrade Cloudpickle to 2.2.1,SPARK-43949,13538540,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,02/Jun/23 08:16,02/Jun/23 10:40,13/Jul/23 08:43,02/Jun/23 10:40,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,3.4.1,3.5.0,,,PySpark,,,0,,Cloudpickle 2.2.1 has a fix for named tuple issue (https://github.com/cloudpipe/cloudpickle/issues/460). PySpark relies on namedtuple heavily especially for RDD. We should upgrade and fix it.,,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 10:40:53 UTC 2023,,,,,,,,,,"0|z1iajc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Jun/23 10:40;gurwls223;Fixed in https://github.com/apache/spark/pull/41433;;;",,,,,,,,,,
Fix bug for `SQLQueryTestSuite` when run on local env,SPARK-43945,13538527,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,02/Jun/23 07:02,02/Jun/23 16:36,13/Jul/23 08:43,02/Jun/23 16:36,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,0,,,,hudson,maxgekk,panbingkun,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 16:36:24 UTC 2023,,,,,,,,,,"0|z1iagg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Jun/23 07:05;hudson;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41431;;;","02/Jun/23 16:36;maxgekk;Issue resolved by pull request 41431
[https://github.com/apache/spark/pull/41431];;;",,,,,,,,,
Fix bug for toSQLId,SPARK-43936,13538498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,02/Jun/23 02:31,02/Jun/23 05:38,13/Jul/23 08:43,02/Jun/23 05:38,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"After SPARK-43910, {{__auto_generated_subquery_name}} from ids in errors should remove, but when the type of {{parts}} is ArrayBuffer, match will fail. causing unexpected behavior.",,LuciferYang,panbingkun,snoot,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 05:38:58 UTC 2023,,,,,,,,,,"0|z1iaa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Jun/23 02:39;panbingkun;h4. Hi，[~maxgekk] 

Do you have time to help review this PR?;;;","02/Jun/23 04:44;snoot;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41430;;;","02/Jun/23 05:38;LuciferYang;Issue resolved by pull request 41430
[https://github.com/apache/spark/pull/41430];;;",,,,,,,,
Add check to see if column name is legal for __dir__,SPARK-43889,13538161,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jarviscao,jarviscao,jarviscao,30/May/23 20:55,31/May/23 07:00,13/Jul/23 08:43,31/May/23 07:00,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,PySpark,,,0,,"In SPARK-43270, we add a support for the autocomplete suggest for {{df.|}} , which also suggest column_name for dataframe. 

However, we found out later that some dataframe can have column which has illegal variable name (e.g: name?1, name 1, 2name etc.) These variable name should be filtered out and this will be consistent with pandas behavior now",,gurwls223,jarviscao,snoot,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 31 07:00:03 UTC 2023,,,,,,,,,,"0|z1i87s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/May/23 22:16;jarviscao;Submit a pr about this https://github.com/apache/spark/pull/41393;;;","31/May/23 03:47;snoot;User 'BeishaoCao-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/41393;;;","31/May/23 07:00;gurwls223;Issue resolved by pull request 41393
[https://github.com/apache/spark/pull/41393];;;",,,,,,,,
Upgrade `gcs-connector` to 2.2.14,SPARK-43842,13537936,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,28/May/23 21:25,29/May/23 00:19,13/Jul/23 08:43,29/May/23 00:19,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Build,,,0,,,,dongjoon,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 29 00:19:35 UTC 2023,,,,,,,,,,"0|z1i6u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/May/23 00:19;gurwls223;Issue resolved by pull request 41352
[https://github.com/apache/spark/pull/41352];;;",,,,,,,,,,
Non-existent column in projection of full outer join with USING results in StringIndexOutOfBoundsException,SPARK-43841,13537934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,28/May/23 18:48,29/May/23 07:05,13/Jul/23 08:43,29/May/23 07:05,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"The following query throws a {{StringIndexOutOfBoundsException}}:
{noformat}
with v1 as (
 select * from values (1, 2) as (c1, c2)
),
v2 as (
  select * from values (2, 3) as (c1, c2)
)
select v1.c1, v1.c2, v2.c1, v2.c2, b
from v1
full outer join v2
using (c1);
{noformat}
The query should fail anyway, since {{b}} refers to a non-existent column. But it should fail with a helpful error message, not with a {{StringIndexOutOfBoundsException}}.

The issue seems to be in {{StringUtils#orderSuggestedIdentifiersBySimilarity}}. {{orderSuggestedIdentifiersBySimilarity}} assumes that a list of candidate attributes with a mix of prefixes will never have an attribute name with an empty prefix. But in this case it does ({{c1}} from the {{coalesce}} has no prefix, since it is not associated with any relation or subquery):
{noformat}
+- 'Project [c1#5, c2#6, c1#7, c2#8, 'b]
   +- Project [coalesce(c1#5, c1#7) AS c1#9, c2#6, c2#8] <== c1#9 has no prefix, unlike c2#6 (v1.c2) or c2#8 (v2.c2)
      +- Join FullOuter, (c1#5 = c1#7)
         :- SubqueryAlias v1
         :  +- CTERelationRef 0, true, [c1#5, c2#6]
         +- SubqueryAlias v2
            +- CTERelationRef 1, true, [c1#7, c2#8]
{noformat}
Because of this, {{orderSuggestedIdentifiersBySimilarity}} returns a sorted list of suggestions like this:
{noformat}
ArrayBuffer(.c1, v1.c2, v2.c2)
{noformat}
{{UnresolvedAttribute.parseAttributeName}} chokes on an attribute name that starts with a namespace separator ('.').
",,bersprockets,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 29 07:05:07 UTC 2023,,,,,,,,,,"0|z1i6tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/May/23 23:38;bersprockets;PR at https://github.com/apache/spark/pull/41353;;;","29/May/23 07:05;maxgekk;Issue resolved by pull request 41353
[https://github.com/apache/spark/pull/41353];;;",,,,,,,,,
unbase64 and unhex codegen are invalid with failOnError,SPARK-43802,13537722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,25/May/23 21:45,27/May/23 02:31,13/Jul/23 08:43,26/May/23 17:37,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,0,,"to_binary with hex and base64 generate invalid codegen:

{{spark.range(5).selectExpr('to_binary(base64(cast(id as binary)), ""BASE64"")').show()}}

results in

{{Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: Unknown variable or type ""BASE64""}}

because this is the generated code:



/* 107 */         if (!org.apache.spark.sql.catalyst.expressions.UnBase64.isValidBase64(project_value_1)) {

/* 108 */           throw QueryExecutionErrors.invalidInputInConversionError(

/* 109 */             ((org.apache.spark.sql.types.BinaryType$) references[1] /* to */),

/* 110 */             project_value_1,

/* 111 */             BASE64,

/* 112 */             ""try_to_binary"");

/* 113 */         }",,dongjoon,kimahriman,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 27 02:31:03 UTC 2023,,,,,,,,,,"0|z1i5io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/May/23 17:37;maxgekk;Issue resolved by pull request 41317
[https://github.com/apache/spark/pull/41317];;;","27/May/23 02:31;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/41334;;;",,,,,,,,,
Fix bug in AvroSuite for 'reading from invalid path throws exception',SPARK-43767,13537414,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,24/May/23 02:10,24/May/23 04:10,13/Jul/23 08:43,24/May/23 04:10,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,0,,,,dongjoon,panbingkun,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 04:10:39 UTC 2023,,,,,,,,,,"0|z1i3mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/May/23 04:10;dongjoon;Issue resolved by pull request 41289
[https://github.com/apache/spark/pull/41289];;;",,,,,,,,,,
Incorrect attribute nullability after RewriteCorrelatedScalarSubquery leads to incorrect query results,SPARK-43760,13537399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gubichev,gubichev,gubichev,24/May/23 00:14,01/Jun/23 07:12,13/Jul/23 08:43,31/May/23 00:29,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,0,,"The following query:

 
{code:java}
select * from (
 select t1.id c1, (
  select t2.id c from range (1, 2) t2
  where t1.id = t2.id  ) c2
 from range (1, 3) t1 ) t
where t.c2 is not null
-- !query schema
struct<c1:bigint,c2:bigint>
-- !query output
1	1
2	NULL
 {code}
 
should return 1 row, because the second row is supposed to be removed by IsNotNull predicate. However, due to a wrong nullability propagation after subquery decorrelation, the output of the subquery is declared as not-nullable (incorrectly), so the predicate is constant folded into True.",,bersprockets,cloud_fan,gubichev,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 31 00:29:59 UTC 2023,,,,,,,,,,"0|z1i3j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/May/23 00:29;cloud_fan;Issue resolved by pull request 41287
[https://github.com/apache/spark/pull/41287];;;",,,,,,,,,,
Expose TimestampNTZType in pyspark.sql.types,SPARK-43759,13537396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,23/May/23 23:59,04/Jul/23 02:40,13/Jul/23 08:43,24/May/23 04:59,3.5.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,PySpark,,,0,,{{TimestampNTZType}} is missing in {{__all__}} list in {{pyspark.sql.types}}.,,podongfeng,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 04:59:38 UTC 2023,,,,,,,,,,"0|z1i3ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/May/23 04:59;podongfeng;Issue resolved by pull request 41286
[https://github.com/apache/spark/pull/41286];;;",,,,,,,,,,
Upgrade snappy-java to 1.1.10.0,SPARK-43758,13537392,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,23/May/23 22:47,24/May/23 02:51,13/Jul/23 08:43,24/May/23 02:51,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Build,,,0,,Update {{snappy-java}} to 1.1.10.0,,csun,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 02:51:48 UTC 2023,,,,,,,,,,"0|z1i3hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/May/23 02:51;dongjoon;Issue resolved by pull request 41285
[https://github.com/apache/spark/pull/41285];;;",,,,,,,,,,
Change CheckConnectJvmClientCompatibility to deny list to increase the API check coverage,SPARK-43757,13537385,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhenli,zhenli,zhenli,23/May/23 21:37,28/Jun/23 14:38,13/Jul/23 08:43,28/Jun/23 14:38,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,0,,"The current compatibility check only checks selected classes. So when adding a new class, if a developer forgets to add this class into the checklist, then this API is not covered in compatibility tests. Thus we should change this API check to always include all APIs by default.",,hudson,zhenli,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 22:11:56 UTC 2023,,,,,,,,,,"0|z1i3g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/May/23 22:11;hudson;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/41284;;;","23/May/23 22:11;hudson;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/41284;;;",,,,,,,,,
Port HIVE-12188: DoAs does not work properly in non-kerberos secured HS2,SPARK-43743,13537295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,23/May/23 08:55,23/May/23 10:35,13/Jul/23 08:43,23/May/23 10:34,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,Please see: https://issues.apache.org/jira/browse/HIVE-12188,,dongjoon,githubbot,yumwang,,,,,,,,,,,,,,,,,,,HIVE-12188,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 10:34:23 UTC 2023,,,,,,,,,,"0|z1i2w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/May/23 09:15;githubbot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/41276;;;","23/May/23 10:34;dongjoon;Issue resolved by pull request 41276
[https://github.com/apache/spark/pull/41276];;;",,,,,,,,,
refactor default column value resolution,SPARK-43742,13537284,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,23/May/23 07:50,28/May/23 23:28,13/Jul/23 08:43,28/May/23 23:28,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,cloud_fan,dongjoon,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 28 23:28:30 UTC 2023,,,,,,,,,,"0|z1i2tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/May/23 09:04;githubbot;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/41262;;;","28/May/23 23:28;dongjoon;Issue resolved by pull request 41262
[https://github.com/apache/spark/pull/41262];;;",,,,,,,,,
Handle missing row.excludedInStages field,SPARK-43719,13537224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,22/May/23 22:51,23/May/23 05:17,13/Jul/23 08:43,23/May/23 05:17,3.3.2,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,Spark Core,,,0,,,,dongjoon,snoot,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 05:17:46 UTC 2023,,,,,,,,,,"0|z1i2g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/May/23 04:23;snoot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41266;;;","23/May/23 05:17;dongjoon;Issue resolved by pull request 41266
[https://github.com/apache/spark/pull/41266];;;",,,,,,,,,
References to a specific side's key in a USING join can have wrong nullability,SPARK-43718,13537216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,22/May/23 20:48,23/May/23 04:48,13/Jul/23 08:43,23/May/23 04:48,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,0,correctness,"Assume this data:
{noformat}
create or replace temp view t1 as values (1), (2), (3) as (c1);
create or replace temp view t2 as values (2), (3), (4) as (c1);
{noformat}
The following query produces incorrect results:
{noformat}
spark-sql (default)> select explode(array(t1.c1, t2.c1)) as x1
from t1
full outer join t2
using (c1);
1
-1      <== should be null
2
2
3
3
-1      <== should be null
4
Time taken: 0.663 seconds, Fetched 8 row(s)
spark-sql (default)> 
{noformat}
Similar issues occur with right outer join and left outer join.

{{t1.c1}} and {{t2.c1}} have the wrong nullability at the time the array is resolved, so the array's {{containsNull}} value is incorrect.

Queries that don't use arrays also can get wrong results. Assume this data:
{noformat}
create or replace temp view t1 as values (0), (1), (2) as (c1);
create or replace temp view t2 as values (1), (2), (3) as (c1);
create or replace temp view t3 as values (1, 2), (3, 4), (4, 5) as (a, b);
{noformat}
The following query produces incorrect results:
{noformat}
select t1.c1 as t1_c1, t2.c1 as t2_c1, b
from t1
full outer join t2
using (c1),
lateral (
  select b
  from t3
  where a = coalesce(t2.c1, 1)
) lt3;
1	1	2
NULL	3	4
Time taken: 2.395 seconds, Fetched 2 row(s)
spark-sql (default)> 
{noformat}
The result should be the following:
{noformat}
0	NULL	2
1	1	2
NULL	3	4
{noformat}

",,bersprockets,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 04:48:12 UTC 2023,,,,,,,,,,"0|z1i2eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/May/23 20:56;bersprockets;I think I have a handle on this. I will submit in a PR in the coming days.;;;","23/May/23 00:46;bersprockets;PR here: https://github.com/apache/spark/pull/41267;;;","23/May/23 04:48;dongjoon;Issue resolved by pull request 41267
[https://github.com/apache/spark/pull/41267];;;",,,,,,,,
Scala Client Dataset#reduce failed to handle null partitions for scala primitive types,SPARK-43717,13537211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhenli,zhenli,zhenli,22/May/23 19:32,07/Jun/23 06:31,13/Jul/23 08:43,07/Jun/23 06:31,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,0,,"Scala client failed with NPE when running:

assert(spark.range(0, 5, 1, 10).as[Long].reduce(_ + _) == 10)",,LuciferYang,snoot,zhenli,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 07 06:31:05 UTC 2023,,,,,,,,,,"0|z1i2dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/May/23 04:22;snoot;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/41264;;;","07/Jun/23 06:31;LuciferYang;Issue resolved by pull request 41264
[https://github.com/apache/spark/pull/41264];;;",,,,,,,,,
"Maven test failed: `interrupt all - background queries, foreground interrupt` and `interrupt all - foreground queries, background interrupt`",SPARK-43648,13537062,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,22/May/23 07:47,09/Jun/23 11:09,13/Jul/23 08:43,09/Jun/23 11:09,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,0,,"run 
{code:java}
build/mvn clean install -DskipTests -Phive
build/mvn test -pl connector/connect/client/jvm -Dtest=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite {code}
`interrupt all - background queries, foreground interrupt` and `interrupt all - foreground queries, background interrupt` failed as follows:
{code:java}
23/05/22 15:44:11 ERROR SparkConnectService: Error during: execute. UserId: . SessionId: 0f4013ca-3af9-443b-a0e5-e339a827e0cf.
java.lang.NoClassDefFoundError: org/apache/spark/sql/connect/client/SparkResult
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.getDeclaredMethod(Class.java:2128)
	at java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1643)
	at java.io.ObjectStreamClass.access$1700(ObjectStreamClass.java:79)
	at java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:520)
	at java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:494)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:494)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:391)
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:681)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2005)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1852)
	at java.io.ObjectInputStream.readClass(ObjectInputStream.java:1815)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1640)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
	at org.apache.spark.util.Utils$.deserialize(Utils.scala:148)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.org$apache$spark$sql$connect$planner$SparkConnectPlanner$$unpackUdf(SparkConnectPlanner.scala:1353)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner$TypedScalaUdf$.apply(SparkConnectPlanner.scala:761)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformTypedMapPartitions(SparkConnectPlanner.scala:531)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformMapPartitions(SparkConnectPlanner.scala:495)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:143)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handlePlan(SparkConnectStreamHandler.scala:100)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$2(SparkConnectStreamHandler.scala:87)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$1(SparkConnectStreamHandler.scala:53)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:209)
	at org.apache.spark.sql.connect.artifact.SparkConnectArtifactManager$.withArtifactClassLoader(SparkConnectArtifactManager.scala:178)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handle(SparkConnectStreamHandler.scala:48)
	at org.apache.spark.sql.connect.service.SparkConnectService.executePlan(SparkConnectService.scala:166)
	at org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:611)
	at org.sparkproject.connect.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
	at org.sparkproject.connect.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:352)
	at org.sparkproject.connect.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:866)
	at org.sparkproject.connect.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.sparkproject.connect.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.connect.client.SparkResult
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	... 56 more {code}",,gurwls223,juliuszsompolski,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-43745,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 11:09:51 UTC 2023,,,,,,,,,,"0|z1i1g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/May/23 09:31;juliuszsompolski;Duplicated by https://issues.apache.org/jira/browse/SPARK-43744; closing the duplicate.

See [https://github.com/apache/spark/pull/41005/files/35e500d4cb72f8d3bee21a7f86ee16cbbc8a936c#r1200551487] thread for some debugging info.
It seems it may be related to https://issues.apache.org/jira/browse/SPARK-43227;;;","09/Jun/23 11:09;gurwls223;Issue resolved by pull request 41487
[https://github.com/apache/spark/pull/41487];;;",,,,,,,,,
Maven test failed in ClientE2ETestSuite/CatalogSuite/StreamingQuerySuite without -Phive,SPARK-43647,13537060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,22/May/23 07:28,26/May/23 01:31,13/Jul/23 08:43,26/May/23 01:31,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,0,," 
{code:java}
build/mvn clean install -DskipTests
build/mvn test -pl connector/connect/client/jvm{code}
 

13 test failed with similar reasons:

 
{code:java}
- read and write *** FAILED ***
  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated
  at io.grpc.Status.asRuntimeException(Status.java:535)
  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)
  at scala.collection.Iterator.toStream(Iterator.scala:1417)
  at scala.collection.Iterator.toStream$(Iterator.scala:1416)
  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)
  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)
  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)
  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)
  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:489)
  ... {code}
 

 ",,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-43745,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 01:31:34 UTC 2023,,,,,,,,,,"0|z1i1fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/May/23 01:31;LuciferYang;Issue resolved by pull request 41282
[https://github.com/apache/spark/pull/41282];;;",,,,,,,,,,
Subquery decorrelation rewriteDomainJoins failure from ConstantFolding to isnull,SPARK-43596,13536922,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jchen5,jchen5,19/May/23 23:49,25/May/23 08:43,13/Jul/23 08:43,25/May/23 08:43,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"We can get a decorrelation error because of rewrites that run in between DecorrelateInnerQuery and rewriteDomainJoins, that modify the correlation join conditions. In particular, ConstantFolding can transform `innercol <=> null` to `isnull(innercol)` and then rewriteDomainJoins does not recognize this and throws error Unable to rewrite domain join with conditions: ArrayBuffer(isnull(innercol#280)) because the isnull is not an equality, so it isn't usable for rewriting the domain join.

Can fix by recognizing `isnull(innercol)` as `innercol <=> null` in rewriteDomainJoins.

This area is also fragile in general and other rewrites that run between the two steps of decorrelation could potentially break their assumptions, so we may want to investigate longer-term follow ups for that.",,cloud_fan,jchen5,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 08:43:21 UTC 2023,,,,,,,,,,"0|z1i0l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/May/23 08:43;cloud_fan;Issue resolved by pull request 41265
[https://github.com/apache/spark/pull/41265];;;",,,,,,,,,,
Fix `cannotBroadcastTableOverMaxTableBytesError` to use `bytesToString`,SPARK-43589,13536824,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,19/May/23 06:36,19/May/23 15:35,13/Jul/23 08:43,19/May/23 15:35,3.3.2,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.1,,,SQL,,,0,,,,dongjoon,githubbot,,,,,,,,,,,,,,,,,,,,SPARK-37321,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 09:23:45 UTC 2023,,,,,,,,,,"0|z1hzzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/May/23 09:22;githubbot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41232;;;","19/May/23 09:23;githubbot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41232;;;",,,,,,,,,
Upgrade `kubernetes-client` to 6.6.2,SPARK-43581,13536792,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,19/May/23 00:44,19/May/23 02:46,13/Jul/23 08:43,19/May/23 02:46,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Build,Kubernetes,,0,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 02:46:30 UTC 2023,,,,,,,,,,"0|z1hzsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/May/23 02:46;dongjoon;Issue resolved by pull request 41223
[https://github.com/apache/spark/pull/41223];;;",,,,,,,,,,
"Update ""Supported Pandas API"" page to point out the proper pandas docs",SPARK-43547,13536638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,18/May/23 01:36,18/May/23 03:05,13/Jul/23 08:43,18/May/23 03:05,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Documentation,Pandas API on Spark,,0,,[https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/supported_pandas_api.html#supported-pandas-api] not point out the wrong pandas version.,,gurwls223,itholic,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 18 03:05:04 UTC 2023,,,,,,,,,,"0|z1hyu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/May/23 03:05;gurwls223;Issue resolved by pull request 41208
[https://github.com/apache/spark/pull/41208];;;",,,,,,,,,,
Incorrect column resolution on FULL OUTER JOIN with USING,SPARK-43541,13536613,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,17/May/23 19:48,19/May/23 00:25,13/Jul/23 08:43,18/May/23 18:13,3.3.2,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,0,,"This was tested on Spark 3.3.2 and Spark 3.4.0.

{code}
Causes [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `aws_dbr_a`.`key` cannot be resolved. Did you mean one of the following? [`key`].; line 4, pos 7
{code}


FULL OUTER JOIN with USING and/or the WHERE seems relevant since I can get the query to work with any of these modifications. 


{code}
# -- FULL OUTER JOIN
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   FULL OUTER JOIN gcp_pro_b USING (key)
                   WHERE aws_dbr_a.key NOT LIKE 'spark.clusterUsageTags.%';
[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `aws_dbr_a`.`key` cannot be resolved. Did you mean one of the following? [`key`].; line 4 pos 7
# -- INNER JOIN
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   JOIN gcp_pro_b USING (key)
                   WHERE aws_dbr_a.key NOT LIKE 'spark.clusterUsageTags.%';
+-----+
| key |
|-----|
| a   |
+-----+
1 row in set
Time: 0.507s
# -- NO Filter
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   FULL OUTER JOIN gcp_pro_b USING (key);
+-----+
| key |
|-----|
| a   |
+-----+
1 row in set
Time: 1.021s
# -- ON instead of USING
                   WITH
                   aws_dbr_a AS (select key from values ('a') t(key)),
                   gcp_pro_b AS (select key from values ('a') t(key))
                   SELECT aws_dbr_a.key
                   FROM aws_dbr_a
                   FULL OUTER JOIN gcp_pro_b ON aws_dbr_a.key = gcp_pro_b.key
                   WHERE aws_dbr_a.key NOT LIKE 'spark.clusterUsageTags.%';
+-----+
| key |
|-----|
| a   |
+-----+
1 row in set
Time: 0.514s
{code}
",,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 00:25:37 UTC 2023,,,,,,,,,,"0|z1hyoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/May/23 18:13;maxgekk;Issue resolved by pull request 41204
[https://github.com/apache/spark/pull/41204];;;","18/May/23 22:11;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/41220;;;","19/May/23 00:25;dongjoon;This is backported to branch-3.3 via https://github.com/apache/spark/pull/41221;;;",,,,,,,,
Spark Homebrew Formulae currently depends on non-officially-supported Java 20,SPARK-43538,13536523,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yumwang,ghislain.fourny,ghislain.fourny,17/May/23 09:42,23/May/23 00:13,13/Jul/23 08:43,23/May/23 00:12,3.2.4,3.3.2,3.4.0,,,,,,,,,,,,,3.4.0,,,,Java API,,,0,,"I am not sure if homebrew-related issues can also be reported here? The Homebrew formulae for apache-spark runs on (latest) openjdk 20.

[https://formulae.brew.sh/formula/apache-spark]

However, Apache Spark is documented to work with Java 8/11/17:

[https://spark.apache.org/docs/latest/]

Is this an overlook, or is Java 20 officially supported, too?

Thanks!","Homebrew (e.g., macOS)",ghislain.fourny,gurwls223,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 00:12:44 UTC 2023,,,,,,,,,,"0|z1hy4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/May/23 12:00;yumwang;We have not tested on Java 20, because Java 20 is not LTS.;;;","17/May/23 12:03;ghislain.fourny;Thanks, Yuming Wang! Does it mean the apache-spark Homebrew Formulae should then be adapted to openjdk@17 (or 8 or 11) to avoid unpredictable behavior?;;;","17/May/23 12:09;yumwang;Yes. I think so: https://github.com/Homebrew/homebrew-core/pull/131189;;;","23/May/23 00:12;yumwang;Issue resolved by pull request [https://github.com/Homebrew/homebrew-core/pull/131189]. Please reinstall it if you have installed the Spark Homebrew formulae.
;;;",,,,,,,
Add log4j-1.2-api and log4j-slf4j2-impl to classpath if active hadoop-provided,SPARK-43534,13536503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yumwang,yumwang,yumwang,17/May/23 07:08,20/May/23 23:45,13/Jul/23 08:43,20/May/23 23:45,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Build,,,0,,"Build Spark:
{code:sh}
./dev/make-distribution.sh --name default --tgz -Phive -Phive-thriftserver -Pyarn -Phadoop-provided
tar -zxf spark-3.5.0-SNAPSHOT-bin-default.tgz {code}
Remove the following jars from spark-3.5.0-SNAPSHOT-bin-default:
{noformat}
jars/log4j-1.2-api-2.20.0.jar
jars/log4j-slf4j2-impl-2.20.0.jar
{noformat}
Add a new log4j2.properties to spark-3.5.0-SNAPSHOT-bin-default/conf:
{code:none}
rootLogger.level = info
rootLogger.appenderRef.file.ref = File
rootLogger.appenderRef.stderr.ref = console

appender.console.type = Console
appender.console.name = console
appender.console.target = SYSTEM_ERR
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss,SSS} %p [%t] %c{2}:%L : %m%n

appender.file.type = RollingFile
appender.file.name = File
appender.file.fileName = /tmp/spark/logs/spark.log
appender.file.filePattern = /tmp/spark/logs/spark.%d{yyyyMMdd-HH}.log
appender.file.append = true
appender.file.layout.type = PatternLayout
appender.file.layout.pattern = %d{yy/MM/dd HH:mm:ss,SSS} %p [%t] %c{2}:%L : %m%n
appender.file.policies.type = Policies
appender.file.policies.time.type = TimeBasedTriggeringPolicy
appender.file.policies.time.interval = 1
appender.file.policies.time.modulate = true
appender.file.policies.size.type = SizeBasedTriggeringPolicy
appender.file.policies.size.size = 256M
appender.file.strategy.type = DefaultRolloverStrategy
appender.file.strategy.max = 100
{code}

Start Spark thriftserver:
{code:java}
sbin/start-thriftserver.sh
{code}

Check the log:
{code:sh}
cat /tmp/spark/logs/spark.log
{code}

",,ignitetcbot,yumwang,,,,,,,,,,,,,,,,,,,,,,"17/May/23 07:52;yumwang;hadoop log jars.png;https://issues.apache.org/jira/secure/attachment/13058286/hadoop+log+jars.png","17/May/23 07:16;yumwang;log4j-1.2-api-2.20.0.jar;https://issues.apache.org/jira/secure/attachment/13058284/log4j-1.2-api-2.20.0.jar","17/May/23 07:16;yumwang;log4j-slf4j2-impl-2.20.0.jar;https://issues.apache.org/jira/secure/attachment/13058285/log4j-slf4j2-impl-2.20.0.jar",,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 20 23:45:35 UTC 2023,,,,,,,,,,"0|z1hy08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/May/23 07:52;yumwang;https://github.com/apache/spark/pull/41195;;;","19/May/23 18:43;ignitetcbot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/41195;;;","20/May/23 23:45;srowen;Issue resolved by pull request 41195
[https://github.com/apache/spark/pull/41195];;;",,,,,,,,
Support general expressions as OPTIONS values ,SPARK-43529,13536464,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dtenedor,dtenedor,dtenedor,16/May/23 22:22,12/Jun/23 04:31,13/Jul/23 08:43,12/Jun/23 04:31,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,dtenedor,Gengliang.Wang,gurwls223,snoot,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 04:31:16 UTC 2023,,,,,,,,,,"0|z1hxrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/May/23 04:09;snoot;User 'dtenedor' has created a pull request for this issue:
https://github.com/apache/spark/pull/41191;;;","12/Jun/23 04:31;Gengliang.Wang;Issue resolved by pull request 41191
[https://github.com/apache/spark/pull/41191];;;",,,,,,,,,
Fix catalog.listCatalogs in PySpark,SPARK-43527,13536410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,podongfeng,podongfeng,podongfeng,16/May/23 13:27,16/May/23 23:31,13/Jul/23 08:43,16/May/23 23:31,3.4.0,3.4.1,3.5.0,,,,,,,,,,,,,3.4.1,3.5.0,,,PySpark,,,0,,,,gurwls223,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 23:31:48 UTC 2023,,,,,,,,,,"0|z1hxfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/May/23 23:31;gurwls223;Issue resolved by pull request 41186
[https://github.com/apache/spark/pull/41186];;;",,,,,,,,,,
Creating struct column occurs  error 'org.apache.spark.sql.AnalysisException [DATATYPE_MISMATCH.CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING]',SPARK-43522,13536374,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,fanjia,Heedo,Heedo,16/May/23 08:00,18/May/23 06:30,13/Jul/23 08:43,18/May/23 06:30,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,0,,"When creating a struct column in Dataframe, the code that ran without problems in version 3.3.1 does not work in version 3.4.0.

 

Example
{code:java}
val testDF = Seq(""a=b,c=d,d=f"").toDF.withColumn(""key_value"", split('value, "","")).withColumn(""map_entry"", transform(col(""key_value""), x => struct(split(x, ""="").getItem(0), split(x, ""="").getItem(1) ) )){code}
 

In 3.3.1

 
{code:java}
 
testDF.show()
+-----------+---------------+--------------------+ 
|      value|      key_value|           map_entry| 
+-----------+---------------+--------------------+ 
|a=b,c=d,d=f|[a=b, c=d, d=f]|[{a, b}, {c, d}, ...| 
+-----------+---------------+--------------------+
 
testDF.printSchema()
root
 |-- value: string (nullable = true)
 |-- key_value: array (nullable = true)
 |    |-- element: string (containsNull = false)
 |-- map_entry: array (nullable = true)
 |    |-- element: struct (containsNull = false)
 |    |    |-- col1: string (nullable = true)
 |    |    |-- col2: string (nullable = true)
{code}
 

 

In 3.4.0

 
{code:java}
org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING] Cannot resolve ""struct(split(namedlambdavariable(), =, -1)[0], split(namedlambdavariable(), =, -1)[1])"" due to data type mismatch: Only foldable `STRING` expressions are allowed to appear at odd position, but they are [""0"", ""1""].;
'Project [value#41, key_value#45, transform(key_value#45, lambdafunction(struct(0, split(lambda x_3#49, =, -1)[0], 1, split(lambda x_3#49, =, -1)[1]), lambda x_3#49, false)) AS map_entry#48]
+- Project [value#41, split(value#41, ,, -1) AS key_value#45]
   +- LocalRelation [value#41]  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:269)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
....
 
{code}
 

However, if you do an alias to struct elements, you can get the same result as the previous version.

 
{code:java}
val testDF = Seq(""a=b,c=d,d=f"").toDF.withColumn(""key_value"", split('value, "","")).withColumn(""map_entry"", transform(col(""key_value""), x => struct(split(x, ""="").getItem(0).as(""col1"") , split(x, ""="").getItem(1).as(""col2"") ) )){code}
 

 ",,cloud_fan,fanjia,Heedo,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 18 06:30:19 UTC 2023,,,,,,,,,,"0|z1hx7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/May/23 15:00;fanjia;https://github.com/apache/spark/pull/41187;;;","18/May/23 06:30;cloud_fan;Issue resolved by pull request 41187
[https://github.com/apache/spark/pull/41187];;;",,,,,,,,,
Spark application hangs when YarnAllocator adds running executors after processing completed containers,SPARK-43510,13536286,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mauzhang,mauzhang,mauzhang,15/May/23 15:35,06/Jun/23 13:30,13/Jul/23 08:43,06/Jun/23 13:30,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,YARN,,,0,,"I see application hangs when containers are preempted immediately after allocation as follows.
{code:java}
23/05/14 09:11:33 INFO YarnAllocator: Launching container container_e3812_1684033797982_57865_01_000382 on host hdc42-mcc10-01-0910-4207-015-tess0028.stratus.rno.ebay.com for executor with ID 277 for ResourceProfile Id 0 
23/05/14 09:11:33 WARN YarnAllocator: Cannot find executorId for container: container_e3812_1684033797982_57865_01_000382
23/05/14 09:11:33 INFO YarnAllocator: Completed container container_e3812_1684033797982_57865_01_000382 (state: COMPLETE, exit status: -102)
23/05/14 09:11:33 INFO YarnAllocator: Container container_e3812_1684033797982_57865_01_000382 was preempted.{code}
Note the warning log where YarnAllocator cannot find executorId for the container when processing completed containers. The only plausible cause is YarnAllocator added the running executor after processing completed containers. The former happens in a separate thread after executor launch.

YarnAllocator believes there are still running executors, although they are already lost due to preemption. Hence, the application hangs without any running executors.",,mauzhang,snoot,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 04:20:15 UTC 2023,,,,,,,,,,"0|z1hwog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/May/23 04:20;snoot;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/41173;;;",,,,,,,,,,
DataFrame.drop should support empty column,SPARK-43502,13536157,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,15/May/23 04:12,16/May/23 04:38,13/Jul/23 08:43,16/May/23 04:38,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,PySpark,,0,,,,gurwls223,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 04:38:27 UTC 2023,,,,,,,,,,"0|z1hvw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/May/23 04:38;podongfeng;Issue resolved by pull request 41180
[https://github.com/apache/spark/pull/41180];;;",,,,,,,,,,
Confused errors from the DATEADD function,SPARK-43485,13535919,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,12/May/23 06:42,15/May/23 11:34,13/Jul/23 08:43,15/May/23 10:55,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"The code example portraits the issue:

{code:sql}
spark-sql (default)> select dateadd('MONTH', 1, date'2023-05-11');
[WRONG_NUM_ARGS.WITHOUT_SUGGESTION] The `dateadd` requires 2 parameters but the actual number is 3. Please, refer to 'https://spark.apache.org/docs/latest/sql-ref-functions.html' for a fix.; line 1 pos 7
{code}

The error says about number of arguments passed to DATEADD but the issue is about the type of the first argument.
",,awsthni,maxgekk,panbingkun,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 11:34:06 UTC 2023,,,,,,,,,,"0|z1hukw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/May/23 10:55;maxgekk;Issue resolved by pull request 41143
[https://github.com/apache/spark/pull/41143];;;","15/May/23 11:34;awsthni;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/41143;;;",,,,,,,,,
Kafka/Kinesis Assembly should not package hadoop-client-runtime,SPARK-43484,13535918,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,12/May/23 06:27,12/May/23 16:18,13/Jul/23 08:43,12/May/23 16:14,3.2.4,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,3.5.0,,,,Build,Structured Streaming,,0,,,,chengpan,csun,hudson,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 16:18:34 UTC 2023,,,,,,,,,,"0|z1huko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/May/23 16:14;csun;Issue resolved by pull request 41152
[https://github.com/apache/spark/pull/41152];;;","12/May/23 16:18;hudson;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/41152;;;",,,,,,,,,
Handle missing hadoopProperties and metricsProperties,SPARK-43471,13535866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,qitan,qitan,qitan,11/May/23 20:06,11/May/23 22:30,13/Jul/23 08:43,11/May/23 22:30,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,0,,,,dongjoon,qitan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 11 22:30:35 UTC 2023,,,,,,,,,,"0|z1hu94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/May/23 22:30;dongjoon;Issue resolved by pull request 41145
[https://github.com/apache/spark/pull/41145];;;",,,,,,,,,,
makeDotNode should not fail when DeterministicLevel is absent,SPARK-43441,13535700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,qitan,qitan,qitan,10/May/23 21:22,11/May/23 04:20,13/Jul/23 08:43,11/May/23 04:20,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,0,,,,dongjoon,qitan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 11 04:20:17 UTC 2023,,,,,,,,,,"0|z1ht88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/May/23 04:20;dongjoon;Issue resolved by pull request 41124
[https://github.com/apache/spark/pull/41124];;;",,,,,,,,,,
Add TimestampNTZType to ColumnarBatchRow,SPARK-43425,13535499,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fokko,fokko,fokko,09/May/23 09:55,11/May/23 00:45,13/Jul/23 08:43,10/May/23 04:31,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Spark Core,,,0,,,,dongjoon,fokko,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 10 04:31:00 UTC 2023,,,,,,,,,,"0|z1hrzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/May/23 04:31;dongjoon;Issue resolved by pull request 41103
[https://github.com/apache/spark/pull/41103];;;",,,,,,,,,,
Tags are lost on LogicalRelation when adding _metadata,SPARK-43422,13535475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,olaky,olaky,olaky,09/May/23 06:45,10/May/23 08:17,13/Jul/23 08:43,10/May/23 08:17,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Optimizer,,,0,,The  AddMetadataColumns does not copy tags for the LogicalRelation when adding metadata output in addMetadataCol,,gurwls223,olaky,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 10 08:17:22 UTC 2023,,,,,,,,,,"0|z1hru8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/May/23 08:17;gurwls223;Issue resolved by pull request 41104
[https://github.com/apache/spark/pull/41104];;;",,,,,,,,,,
IN subquery ListQuery has wrong nullability,SPARK-43413,13535421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jchen5,jchen5,jchen5,08/May/23 21:12,16/May/23 01:41,13/Jul/23 08:43,16/May/23 01:41,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"IN subquery expressions are incorrectly marked as non-nullable, even when they are actually nullable. They correctly check the nullability of the left-hand-side, but the right-hand-side of a IN subquery, the ListQuery, is currently defined with nullability = false always. This is incorrect and can lead to incorrect query transformations.

Example: (non_nullable_col IN (select nullable_col)) <=> TRUE . Here the IN expression returns NULL when the nullable_col is null, but our code marks it as non-nullable, and therefore SimplifyBinaryComparison transforms away the <=> TRUE, transforming the expression to non_nullable_col IN (select nullable_col) , which is an incorrect transformation because NULL values of nullable_col now cause the expression to yield NULL instead of FALSE.

This bug can potentially lead to wrong results, but in most cases this doesn't directly cause wrong results end-to-end, because IN subqueries are almost always transformed to semi/anti/existence joins in RewritePredicateSubquery, and this rewrite can also incorrectly discard NULLs, which is another bug. But we can observe it causing wrong behavior in unit tests, and it could easily lead to incorrect query results if there are changes to the surrounding context, so it should be fixed regardless.

This is a long-standing bug that has existed at least since 2016, as long as the ListQuery class has existed.",,cloud_fan,jchen5,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 01:41:20 UTC 2023,,,,,,,,,,"0|z1hrig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/May/23 04:57;ci-cassandra.apache.org;User 'jchen5' has created a pull request for this issue:
https://github.com/apache/spark/pull/41094;;;","16/May/23 01:41;cloud_fan;Issue resolved by pull request 41094
[https://github.com/apache/spark/pull/41094];;;",,,,,,,,,
Filter current version while reusing sst files for RocksDB state store provider while uploading to DFS to prevent id mismatch,SPARK-43404,13535324,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,anishshri-db,anishshri-db,anishshri-db,08/May/23 03:59,09/Jun/23 11:50,13/Jul/23 08:43,08/May/23 23:29,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Structured Streaming,,,0,,Filter current version while reusing sst files for RocksDB state store provider while uploading to DFS to prevent id mismatch,,anishshri-db,githubbot,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 09:13:19 UTC 2023,,,,,,,,,,"0|z1hqww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/May/23 06:10;anishshri-db;PR here: [https://github.com/apache/spark/pull/41089]

 

cc - [~kabhwan] ;;;","08/May/23 23:29;kabhwan;Issue resolved by pull request 41089
[https://github.com/apache/spark/pull/41089];;;","09/Jun/23 09:13;githubbot;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/41530;;;",,,,,,,,
Executor timeout should be max of idleTimeout rddTimeout shuffleTimeout,SPARK-43398,13535310,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,07/May/23 18:12,12/Jun/23 07:42,13/Jul/23 08:43,12/Jun/23 07:42,3.0.0,3.1.3,3.2.4,3.3.2,3.4.0,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,Spark Core,,,0,,"When dynamic allocation enabled, Executor timeout should be max of idleTimeout, rddTimeout and shuffleTimeout.",,dongjoon,warrenzhu25,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 07:42:28 UTC 2023,,,,,,,,,,"0|z1hqts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Jun/23 07:42;dongjoon;Issue resolved by pull request 41082
[https://github.com/apache/spark/pull/41082];;;",,,,,,,,,,
Improve list of suggested column/attributes in `UNRESOLVED_COLUMN.WITH_SUGGESTION` error class,SPARK-43386,13535197,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vli-databricks,vli-databricks,vli-databricks,05/May/23 17:13,12/May/23 05:39,13/Jul/23 08:43,12/May/23 05:39,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,Match the style of unresolved column/attribute when sorting list of suggested columns. If an unresolved column name is single-part identifier - use same style for suggested columns.,,githubbot,maxgekk,vli-databricks,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 05:39:03 UTC 2023,,,,,,,,,,"0|z1hq4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/May/23 09:12;githubbot;User 'vitaliili-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/41038;;;","10/May/23 09:13;githubbot;User 'vitaliili-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/41038;;;","12/May/23 05:39;maxgekk;Issue resolved by pull request 41038
[https://github.com/apache/spark/pull/41038];;;",,,,,,,,
Fix Avro data type conversion issues to avoid producing incorrect results,SPARK-43380,13535060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zeruibao,zeruibao,zeruibao,04/May/23 23:01,02/Jun/23 22:12,13/Jul/23 08:43,02/Jun/23 22:10,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,0,,"We found the following issues with open-source Avro:
 * Interval types can be read as date or timestamp types that would lead to wildly different results
 * Decimal types can be read with lower precision, that leads to data being read as {{null}} instead of suggesting that a wider decimal format should be provided",,Gengliang.Wang,snoot,zeruibao,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 22:10:27 UTC 2023,,,,,,,,,,"0|z1hpa8:",9223372036854775807,,,,,,,,,,,,,3.5.0,,,,,,,,,"05/May/23 03:32;snoot;User 'zeruibao' has created a pull request for this issue:
https://github.com/apache/spark/pull/41052;;;","02/Jun/23 22:10;Gengliang.Wang;Issue resolved by pull request 41052
[https://github.com/apache/spark/pull/41052];;;",,,,,,,,,
SerializerHelper.deserializeFromChunkedBuffer leaks deserialization streams,SPARK-43378,13534986,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,04/May/23 12:59,06/Jun/23 07:04,13/Jul/23 08:43,05/May/23 00:34,3.4.0,3.4.1,3.5.0,,,,,,,,,,,,,3.4.1,3.5.0,,,Spark Core,,,0,,The method SerializerHelper.deserializeFromChunkedBuffer leaks serializations stream. This can lead to huge performance regressions when using kryo serializer as the spark application can become bottlenecked on the driver creating expensive kryo objects that are then leaked as part of the deserialization stream,,eejbyfeldt,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 00:34:57 UTC 2023,,,,,,,,,,"0|z1hots:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/May/23 00:34;srowen;Resolved by https://github.com/apache/spark/pull/41049;;;",,,,,,,,,,
Revert [SPARK-39203][SQL] Rewrite table location to absolute URI based on database URI,SPARK-43373,13534904,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,04/May/23 02:56,04/May/23 02:57,13/Jul/23 08:43,04/May/23 02:56,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,SQL,,,0,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 04 02:57:10 UTC 2023,,,,,,,,,,"0|z1hobk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/May/23 02:57;cloud_fan;https://github.com/apache/spark/pull/40871;;;",,,,,,,,,,
DELETE from Hive table result in INTERNAL error,SPARK-43359,13534872,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,srielau,srielau,03/May/23 18:43,16/May/23 18:21,13/Jul/23 08:43,16/May/23 18:21,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,0,,"spark-sql (default)> CREATE TABLE T1(c1 INT);
spark-sql (default)> DELETE FROM T1 WHERE c1 = 1;
[INTERNAL_ERROR] Unexpected table relation: HiveTableRelation [`spark_catalog`.`default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [c1#3], Partition Cols: []]

org.apache.spark.SparkException: [INTERNAL_ERROR] Unexpected table relation: HiveTableRelation [`spark_catalog`.`default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [c1#3], Partition Cols: []]
	at org.apache.spark.SparkException$.internalError(SparkException.scala:77)
	at org.apache.spark.SparkException$.internalError(SparkException.scala:81)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:310)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)
	at org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)",,dongjoon,panbingkun,srielau,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 18:21:53 UTC 2023,,,,,,,,,,"0|z1ho4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/May/23 12:24;panbingkun;Let me fix it.;;;","16/May/23 18:21;dongjoon;Issue resolved by pull request 41172
[https://github.com/apache/spark/pull/41172];;;",,,,,,,,,
Spark AWS Glue date partition push down broken,SPARK-43357,13534836,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sdehaes,sdehaes,sdehaes,03/May/23 14:03,10/May/23 07:13,13/Jul/23 08:43,09/May/23 13:06,3.1.0,3.1.1,3.1.2,3.1.3,3.2.0,3.2.1,3.2.2,3.2.3,3.2.4,3.3.0,3.3.1,3.3.2,,,,3.5.0,,,,SQL,,,0,,"When using the following project: [https://github.com/awslabs/aws-glue-data-catalog-client-for-apache-hive-metastore]
To have glue supported as as a hive metastore for spark there is an issue when reading a date-partitioned data set. Writing is fine.
You get the following error: 


{quote}org.apache.hadoop.hive.metastore.api.InvalidObjectException: Unsupported expression '2023 - 05 - 03' (Service: AWSGlue; Status Code: 400; Error Code: InvalidInputException; Request ID: beed68c6-b228-442e-8783-52c25b9d2243; Proxy: null)
{quote}
 

A fix for this is making sure the date passed to glue is quoted",,cloud_fan,sdehaes,snoot,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 10 07:13:49 UTC 2023,,,,,,,,,,"0|z1hnwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/May/23 03:28;snoot;User 'stijndehaes' has created a pull request for this issue:
https://github.com/apache/spark/pull/41035;;;","09/May/23 13:06;cloud_fan;Issue resolved by pull request 41035
[https://github.com/apache/spark/pull/41035];;;","10/May/23 07:13;sdehaes;Any chance we could backport this to older versions? 3.1, 3.2, 3.4 all have the same issue. I don't know which versions are actively supported?
I am willing to make new PR's to these older versions if needed.;;;",,,,,,,,
Fix flaky test for `DataFrame` creation,SPARK-43349,13534765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,03/May/23 00:16,03/May/23 10:41,13/Jul/23 08:43,03/May/23 10:41,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Pandas API on Spark,,,0,,some test of `test_creation_index` is not working properly in some envs.,,gurwls223,itholic,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 03 10:41:23 UTC 2023,,,,,,,,,,"0|z1hngw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/May/23 10:41;gurwls223;Issue resolved by pull request 41025
[https://github.com/apache/spark/pull/41025];;;",,,,,,,,,,
Spark Streaming is not able to read a .txt file whose name has [] special character,SPARK-43343,13534735,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,siying,siying,siying,02/May/23 18:28,09/May/23 03:55,13/Jul/23 08:43,09/May/23 03:55,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Structured Streaming,,,0,,"* For example, If a directory contains a following file:
/path/abc[123]
and users would load spark.readStream.format(""text"").load(""/path"") as stream input. It throws an exception, saying no matching path /path/abc[123]. Spark thinks abc[123] is a regex that only matches file named abc1, abc2 and abc3.

 * Upon investigation this is due to how we [getBatch|https://github.com/databricks/runtime/blob/3af402d23620a0952e151d96c3184d2233217c87/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala#L269] in the FileStreamSource. In `FileStreamSource` we already check file pattern matching and find all match file names. However, in DataSource we check for glob characters again and try to expend it [here|https://github.com/databricks/runtime/blob/3af402d23620a0952e151d96c3184d2233217c87/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L274].",,kabhwan,siying,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 09 03:55:44 UTC 2023,,,,,,,,,,"0|z1hna8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/May/23 03:55;kabhwan;Issue resolved by pull request 41022
[https://github.com/apache/spark/pull/41022];;;",,,,,,,,,,
Revert SPARK-39006 Show a directional error message for executor PVC dynamic allocation failure,SPARK-43342,13534704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dcoliversun,ofrenkel,ofrenkel,02/May/23 15:24,07/May/23 01:27,13/Jul/23 08:43,07/May/23 01:27,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Kubernetes,,,1,,"When using static PVC with Spark 3.4, spark PI example fails with the error below. Previous versions of Spark worked well.
{code:java}
23/04/26 13:22:02 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes for ResourceProfile Id: 0, target: 5, known: 0, sharedSlotFromPendingPods: 2147483647. 23/04/26 13:22:02 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script 23/04/26 13:22:02 ERROR ExecutorPodsSnapshotsStoreImpl: Going to stop due to IllegalArgumentException java.lang.IllegalArgumentException: PVC ClaimName: a1pvc should contain OnDemand or SPARK_EXECUTOR_ID when requiring multiple executors         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.checkPVCClaimName(MountVolumesFeatureStep.scala:135)         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.$anonfun$constructVolumes$4(MountVolumesFeatureStep.scala:75)         at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)         at scala.collection.Iterator.foreach(Iterator.scala:943)         at scala.collection.Iterator.foreach$(Iterator.scala:943)         at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)         at scala.collection.IterableLike.foreach(IterableLike.scala:74)         at scala.collection.IterableLike.foreach$(IterableLike.scala:73)         at scala.collection.AbstractIterable.foreach(Iterable.scala:56)         at scala.collection.TraversableLike.map(TraversableLike.scala:286)         at scala.collection.TraversableLike.map$(TraversableLike.scala:279)         at scala.collection.AbstractTraversable.map(Traversable.scala:108)         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.constructVolumes(MountVolumesFeatureStep.scala:58)         at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.configurePod(MountVolumesFeatureStep.scala:35)         at org.apache.spark.scheduler.cluster.k8s.KubernetesExecutorBuilder.$anonfun$buildFromFeatures$5(KubernetesExecutorBuilder.scala:83)         at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)         at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)         at scala.collection.immutable.List.foldLeft(List.scala:91)         at org.apache.spark.scheduler.cluster.k8s.KubernetesExecutorBuilder.buildFromFeatures(KubernetesExecutorBuilder.scala:82)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:430)         at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:417)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:370)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:363)         at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)         at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)         at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:363)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:134)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:134)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:143)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:131)         at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl.$anonfun$addSubscriber$1(ExecutorPodsSnapshotsStoreImpl.scala:85)         at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)         at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)         at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)         at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)         at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)         at java.base/java.lang.Thread.run(Thread.java:833)   {code}
How to reproduce:
 # Create statically provisioned PV, for example nfs PV: [https://kubernetes.io/docs/concepts/storage/volumes/#nfs]
 # Create PVC that binds to PV above.
 # Run Spark PI example: $SPARK_HOME/bin/spark-submit --master k8s://kubernetes.default.svc --properties-file spark.properties $SPARK_HOME/examples/src/main/python/pi.py 10

spark.properties contents:
{code:java}
spark.executor.instances=5
spark.kubernetes.executor.volumes.persistentVolumeClaim.nfs1.mount.path=/isilon/mnts
spark.kubernetes.executor.volumes.persistentVolumeClaim.nfs1.mount.readOnly=false
spark.kubernetes.executor.volumes.persistentVolumeClaim.nfs1.options.claimName=a1pvc
spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs1.options.claimName=a1pvc
spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs1.mount.readOnly=false
spark.kubernetes.driver.volumes.persistentVolumeClaim.nfs1.mount.path=/isilon/mnts {code}",,comet,dcoliversun,dongjoon,ofrenkel,snoot,,,,,,,,,,,,,SPARK-39006,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 07 01:27:11 UTC 2023,,,,,,,,,,"0|z1hn3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/May/23 02:17;dcoliversun;[~dongjoon] [~yikunkero] It seems like a regression caused by [SPARK-39006|https://issues.apache.org/jira/browse/SPARK-39006], please assign to me;;;","04/May/23 03:41;dongjoon;Thank you for pinging me, [~dcoliversun] . BTW, Apache Spark community has a community rule to set the `Assignee` when committers merge a proper PR.

Please make a PR first.;;;","04/May/23 03:42;dongjoon;BTW, thank you for reporting, [~ofrenkel] .;;;","05/May/23 04:17;snoot;User 'dcoliversun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41057;;;","05/May/23 04:18;snoot;User 'dcoliversun' has created a pull request for this issue:
https://github.com/apache/spark/pull/41057;;;","07/May/23 01:27;dongjoon;Issue resolved by pull request 41069
[https://github.com/apache/spark/pull/41069];;;",,,,,
Handle missing stack-trace field in eventlogs,SPARK-43340,13534698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ahussein,ahussein,ahussein,02/May/23 14:52,05/May/23 23:04,13/Jul/23 08:43,05/May/23 23:04,3.4.0,3.5.0,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,0,,"Recently I was testing with some 3.0.2 eventlogs.

The SHS-3.4+ does not interpret failed jobs/ failed SQLs correctly.

Instead it will list them as ""Incomplete/Active"" whereas it should be listed as ""Failed"".

The problem is due to missing fields in eventlogs generated by previous versions. In this case the eventlog does not have ""Stack Trace"" field which causes a NPE
{code:java}
{
   ""Event"":""SparkListenerJobEnd"",
   ""Job ID"":31,
   ""Completion Time"":1616171909785,
   ""Job Result"":{
      ""Result"":""JobFailed"",
      ""Exception"":{
         ""Message"":""Job aborted""
      }
   }
} {code}
*The SHS logfile*
{code:java}
23/05/01 21:57:16 INFO FsHistoryProvider: Parsing file:/tmp/nds_q86_fail_test to re-build UI...
23/05/01 21:57:17 ERROR ReplayListenerBus: Exception parsing Spark event log: file:/tmp/nds_q86_fail_test
java.lang.NullPointerException
    at org.apache.spark.util.JsonProtocol$JsonNodeImplicits.extractElements(JsonProtocol.scala:1589)
    at org.apache.spark.util.JsonProtocol$.stackTraceFromJson(JsonProtocol.scala:1558)
    at org.apache.spark.util.JsonProtocol$.exceptionFromJson(JsonProtocol.scala:1569)
    at org.apache.spark.util.JsonProtocol$.jobResultFromJson(JsonProtocol.scala:1423)
    at org.apache.spark.util.JsonProtocol$.jobEndFromJson(JsonProtocol.scala:967)
    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:878)
    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:865)
    at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:88)
    at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:59)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3(FsHistoryProvider.scala:1140)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3$adapted(FsHistoryProvider.scala:1138)
    at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2786)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1(FsHistoryProvider.scala:1138)
    at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1$adapted(FsHistoryProvider.scala:1136)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.apache.spark.deploy.history.FsHistoryProvider.parseAppEventLogs(FsHistoryProvider.scala:1136)
    at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:1117)
    at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1355)
    at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:345)
    at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:199)
    at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
    at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:134)
    at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
    at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:55)
    at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:51)
    at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
    at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
    at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
    at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
    at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
    at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
    at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
    at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:88)
    at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:100)
    at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:256)
    at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:104)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:503)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:590)
    at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
    at org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1656)
    at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
    at org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
    at org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1626)
    at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:552)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
    at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
    at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:772)
    at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    at org.sparkproject.jetty.server.Server.handle(Server.java:516)
    at org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
    at org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
    at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:479)
    at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
    at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)
    at org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
    at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
    at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
    at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
    at java.lang.Thread.run(Thread.java:750)
23/05/01 21:57:17 ERROR ReplayListenerBus: Malformed line #24368: {""Event"":""SparkListenerJobEnd"",""Job ID"":31,""Completion Time"":1616171909785,""Job Result"":{""Result"":""JobFailed"",""Exception"":
{""Message"":""Job aborted""}
}}
23/05/01 21:57:17 INFO FsHistoryProvider: Finished parsing file:/tmp/nds_q86_fail_test
 
{code}
 ",,ahussein,dongjoon,hudson,tgraves,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 23:04:37 UTC 2023,,,,,,,,,,"0|z1hn20:",9223372036854775807,,,,,,,,,,,,,3.4.0,3.5.0,,,,,,,,"02/May/23 15:01;tgraves;Likely related to SPARK-39489;;;","05/May/23 20:10;hudson;User 'amahussein' has created a pull request for this issue:
https://github.com/apache/spark/pull/41050;;;","05/May/23 23:04;dongjoon;Issue resolved by pull request 41050
[https://github.com/apache/spark/pull/41050];;;",,,,,,,,
Asc/desc arrow icons for sorting column does not get displayed in the table column,SPARK-43337,13534646,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maytasm,maytasm,maytasm,02/May/23 05:35,05/May/23 19:08,13/Jul/23 08:43,05/May/23 19:08,3.3.0,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.1,,,Spark Core,Web UI,,0,,"The sorting icon is not displayed when the column is clicked to sort by asc/desc.

See attached image: (The index column is sort by asc order by the down arrow is not displayed)

!image (4).png!

This broke due to the upgrade from DataTables from 1.10.20 to 1.10.25. I have confirmed that version 1.13.2 in master branch does not have this problem. 
For reference, in 1.13.2, it looks like the following:
!Screen Shot 2023-05-01 at 10.39.00 PM.png!  ",,maytasm,snoot,,,,,,,,,,,,,,SPARK-42435,,SPARK-38924,,,,SPARK-31871,,"02/May/23 05:39;maytasm;Screen Shot 2023-05-01 at 10.39.00 PM.png;https://issues.apache.org/jira/secure/attachment/13057762/Screen+Shot+2023-05-01+at+10.39.00+PM.png","02/May/23 05:38;maytasm;image (4).png;https://issues.apache.org/jira/secure/attachment/13057761/image+%284%29.png",,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 19:08:36 UTC 2023,,,,,,,,,,"0|z1hmqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/May/23 03:56;snoot;User 'maytasm' has created a pull request for this issue:
https://github.com/apache/spark/pull/41011;;;","05/May/23 19:08;srowen;Issue resolved by pull request 41060
[https://github.com/apache/spark/pull/41060];;;",,,,,,,,,
Error while serializing ExecutorPeakMetricsDistributions into API JSON response,SPARK-43334,13534633,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tejdeepg,tejdeepg,tejdeepg,01/May/23 21:29,24/May/23 23:25,13/Jul/23 08:43,24/May/23 23:25,3.3.3,,,,,,,,,,,,,,,3.5.0,,,,Web UI,,,0,,"When we try to get the ExecutorPeakMetricsDistributions [through the API|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/api/v1/api.scala#L463] (/stages), there is a possibility of encountering an issue while serializing the StagesData into a JSON if the executor metrics are empty. 

 

The following error is thrown : 
{code:java}
Caused by: com.fasterxml.jackson.databind.JsonMappingException: -1 (through reference chain: scala.c
ollection.immutable.$colon$colon[0]->org.apache.spark.status.api.v1.StageData[""executorMetricsDistri
butions""]->org.apache.spark.status.api.v1.ExecutorMetricsDistributions[""peakMemoryMetrics""])
        at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.jav
a:390)
        at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.jav
a:349) {code}
This happens because the indices for the quartiles are populated incorrectly as -1 since the metrics itself are empty and this leads to this exception being thrown.",,gurwls223,hudson,tejdeepg,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 23:25:46 UTC 2023,,,,,,,,,,"0|z1hmnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/May/23 19:50;hudson;User 'thejdeep' has created a pull request for this issue:
https://github.com/apache/spark/pull/41017;;;","24/May/23 23:25;srowen;Issue resolved by pull request 41017
[https://github.com/apache/spark/pull/41017];;;",,,,,,,,,
Typo in sql-migration-guide.md,SPARK-43330,13534593,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kpark,kpark,kpark,01/May/23 13:04,02/May/23 00:29,13/Jul/23 08:43,02/May/23 00:29,3.2.0,,,,,,,,,,,,,,,3.5.0,,,,Documentation,,,0,,"There is a minor typo in [sql-migration-guide.md|#L154]

 
| - In Spark 3.2, `TRANSFORM` operator can support `ArrayType/MapType/StructType` without Hive SerDe, in this mode, we use `StructsToJosn` to convert `ArrayType/MapType/StructType` column to `STRING` and use `JsonToStructs` to parse `STRING` to `ArrayType/MapType/StructType`. In Spark 3.1, Spark just support case `ArrayType/MapType/StructType` column as `STRING` but can't support parse `STRING` to `ArrayType/MapType/StructType` output columns.|

 

StructsToJosn -> StructsToJson",,gurwls223,kpark,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 02 00:29:07 UTC 2023,,,,,,,,,,"0|z1hmf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/May/23 13:14;kpark;I have created a PR with a fix - [https://github.com/apache/spark/pull/41004|https://github.com/apache/spark/pull/41003]

(sorry for typo in commit message 😅 ({-}[https://github.com/apache/spark/pull/41003))|https://github.com/apache/spark/pull/41003]{-}

 

 ;;;","02/May/23 00:29;gurwls223;Issue resolved by pull request 41004
[https://github.com/apache/spark/pull/41004];;;",,,,,,,,,
driver and executors shared same Kubernetes PVC in Spark 3.4+,SPARK-43329,13534580,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dcoliversun,comet,comet,01/May/23 03:25,07/May/23 01:48,13/Jul/23 08:43,07/May/23 01:48,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Kubernetes,,,0,,"I able to shared same PVC for spark 3.3. but on Spark 3.4 onward. i get below error.  I would like all the executors and driver to mount the same PVC. Is this a bug ? I don't want to use SPARK_EXECUTOR_ID or OnDemand because otherwise each of the executors will use an unique and separate PVC. 
 
Error message is ""should contain OnDemand or SPARK_EXECUTOR_ID when requiring multiple executors""
 
below is how I enabled it pvc in spark 3.3 and it works, but does not work in Spark 3.4

{code:sh}
spark.kubernetes.driver.volumes.persistentVolumeClaim.rwxpvc.options.claimName=rwxpvc 
--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.rwxpvc.mount.path=/opt/spark/work-dir 
--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.rwxpvc.options.claimName=rwxpvc 
--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.rwxpvc.mount.path=/opt/spark/work-dir 
 
{code}


 
 
 ",,comet,dcoliversun,dongjoon,,,,,,,,,,,,,,,SPARK-39006,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 07 01:48:39 UTC 2023,,,,,,,,,,"0|z1hmc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/May/23 04:18;dcoliversun;[~dongjoon] this ticket is duplicated with SPARK-43342;;;","07/May/23 01:48;dongjoon;Thank you for reporting, [~comet] .

This is fixed by reverting SPARK-39006.

Here is the reverting commit.

[https://github.com/apache/spark/commit/3ba1fa3678a4fcc0aaba8abb0d4312e8fb42efba];;;",,,,,,,,,
Adding missing default values for MERGE INSERT actions,SPARK-43313,13534373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dtenedor,dtenedor,dtenedor,28/Apr/23 00:08,18/May/23 18:39,13/Jul/23 08:43,04/May/23 20:18,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,dongjoon,dtenedor,Gengliang.Wang,,,,,,,,,,,,,,,,,,,SPARK-38334,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 18 18:39:09 UTC 2023,,,,,,,,,,"0|z1hl2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/May/23 20:18;Gengliang.Wang;Issue resolved by pull request 40996
[https://github.com/apache/spark/pull/40996];;;","18/May/23 18:34;dongjoon;This is reverted from branch-3.4 via [https://github.com/apache/spark/commit/079594ae976b377459ad09d864106734ef65c32d];;;","18/May/23 18:39;dongjoon;To [~dtenedor] , SPARK-38334 was resolved with Fixed Version 3.4.0.
I converted this issue from a subtask to an independent issue. And, add a link to SPARK-38334 instead.
Please proceed new Jira issues (both bugs or improvements) independently or with a new umbrella JIRA.

 ;;;",,,,,,,,
Cascade failure in Guava cache due to fate-sharing,SPARK-43300,13534241,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liuzq12,liuzq12,liuzq12,27/Apr/23 01:05,16/May/23 01:49,13/Jul/23 08:43,16/May/23 01:49,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,0,,"Guava cache is widely used in spark, however, it suffers from fate-sharing behavior: If there are multiple requests trying to access the same key in the {{cache}} at the same time when the key is not in the cache, Guava cache will block all requests and create the object only once. If the creation fails, all requests will fail immediately without retry. So we might see task failure due to irrelevant failure in other queries due to fate sharing.

This fate sharing behavior might lead to unexpected results in some situation.

We can wrap around Guava cache with a KeyLock to synchronize all requests with the same key, so they will run individually and fail as if they come one at a time.",,gurwls223,joshrosen,liuzq12,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 01:49:18 UTC 2023,,,,,,,,,,"0|z1hk9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/May/23 01:49;joshrosen;Fixed in https://github.com/apache/spark/pull/40982;;;",,,,,,,,,,
predict_batch_udf with scalar input fails when batch size consists of a single value,SPARK-43298,13534218,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,leewyang,leewyang,leewyang,26/Apr/23 20:31,27/Apr/23 18:51,13/Jul/23 08:43,27/Apr/23 18:51,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,ML,PySpark,,0,,"This is related to SPARK-42250.  For scalar inputs, the predict_batch_udf will fail if the batch size is 1:
{code:java}
import numpy as np
from pyspark.ml.functions import predict_batch_udf
from pyspark.sql.types import DoubleType

df = spark.createDataFrame([[1.0],[2.0]], schema=[""a""])

def make_predict_fn():
    def predict(inputs):
        return inputs
    return predict

identity = predict_batch_udf(make_predict_fn, return_type=DoubleType(), batch_size=1)
preds = df.withColumn(""preds"", identity(""a"")).collect()
{code}
fails with:
{code:java}
  File ""/.../spark/python/pyspark/worker.py"", line 869, in main
    process()
  File ""/.../spark/python/pyspark/worker.py"", line 861, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/.../spark/python/pyspark/sql/pandas/serializers.py"", line 354, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File ""/.../spark/python/pyspark/sql/pandas/serializers.py"", line 86, in dump_stream
    for batch in iterator:
  File ""/.../spark/python/pyspark/sql/pandas/serializers.py"", line 347, in init_stream_yield_batches
    for series in iterator:
  File ""/.../spark/python/pyspark/worker.py"", line 555, in func
    for result_batch, result_type in result_iter:
  File ""/.../spark/python/pyspark/ml/functions.py"", line 818, in predict
    yield _validate_and_transform_prediction_result(
  File ""/.../spark/python/pyspark/ml/functions.py"", line 339, in _validate_and_transform_prediction_result
    if len(preds_array) != num_input_rows:
TypeError: len() of unsized object
{code}",,gurwls223,leewyang,snoot,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 27 18:51:23 UTC 2023,,,,,,,,,,"0|z1hk48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Apr/23 03:31;snoot;User 'leewyang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40967;;;","27/Apr/23 18:51;gurwls223;Issue resolved by pull request 40967
[https://github.com/apache/spark/pull/40967];;;",,,,,,,,,
__qualified_access_only should be ignored in normal columns,SPARK-43293,13534167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,26/Apr/23 14:25,27/Apr/23 02:55,13/Jul/23 08:43,27/Apr/23 02:55,3.3.2,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,0,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 27 02:55:54 UTC 2023,,,,,,,,,,"0|z1hjsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Apr/23 02:55;cloud_fan;Issue resolved by pull request 40961
[https://github.com/apache/spark/pull/40961];;;",,,,,,,,,,
ArtifactManagerSuite can't run using maven,SPARK-43292,13534111,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,26/Apr/23 08:00,08/May/23 15:10,13/Jul/23 08:43,08/May/23 15:10,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,0,,"run
{code:java}
build/mvn  clean install -DskipTests -Phive 
build/mvn test -pl connector/connect/server {code}
ArtifactManagerSuite failed due to 

 
{code:java}
23/04/26 16:00:07.666 ScalaTest-main-running-DiscoverySuite ERROR Executor: Could not find org.apache.spark.repl.ExecutorClassLoader on classpath! {code}",,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-26 08:00:46.0,,,,,,,,,,"0|z1hjgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplE2ESuite consistently fails with JDK 17,SPARK-43285,13534031,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,25/Apr/23 16:16,25/Apr/23 19:11,13/Jul/23 08:43,25/Apr/23 19:11,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,0,,"[[Comment|https://github.com/apache/spark/pull/40675#discussion_r1174696470] from [~gurwls223]]

This test consistently fails with JDK 17:
{code:java}
[info] ReplE2ESuite:
[info] - Simple query *** FAILED *** (10 seconds, 4 milliseconds)
[info] java.lang.RuntimeException: REPL Timed out while running command: 
[info] spark.sql(""select 1"").collect()
[info] 
[info] Console output: 
[info] Error output: Compiling (synthetic)/ammonite/predef/ArgsPredef.sc
[info] at org.apache.spark.sql.application.ReplE2ESuite.runCommandsInShell(ReplE2ESuite.scala:87)
[info] at org.apache.spark.sql.application.ReplE2ESuite.$anonfun$new$1(ReplE2ESuite.scala:102)
[info] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info] at org.scalatest.Transformer.apply(Transformer.scala:22)
[info] at org.scalatest.Transformer.apply(Transformer.scala:20)
[info] at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
[info] at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
[info] at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
[info] at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224){code}

[https://github.com/apache/spark/actions/runs/4780630672/jobs/8498505928#step:9:4647]
[https://github.com/apache/spark/actions/runs/4774942961/jobs/8488946907]
[https://github.com/apache/spark/actions/runs/4769162286/jobs/8479293802]
[https://github.com/apache/spark/actions/runs/4759278349/jobs/8458399201]
[https://github.com/apache/spark/actions/runs/4748319019/jobs/8434392414]",,vicennial,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-25 16:16:10.0,,,,,,,,,,"0|z1hiyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix concurrent writer does not update file metrics,SPARK-43281,13533987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,25/Apr/23 12:31,16/May/23 01:42,13/Jul/23 08:43,16/May/23 01:42,3.5.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,0,,"It uses temp file path to get file status after commit task. However, the temp file has already moved to new path during commit task.",,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 01:42:54 UTC 2023,,,,,,,,,,"0|z1hiow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/May/23 01:42;cloud_fan;Issue resolved by pull request 40952
[https://github.com/apache/spark/pull/40952];;;",,,,,,,,,,
Use proper error classes when exceptions are constructed with a message,SPARK-43268,13533876,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,aokolnychyi,aokolnychyi,24/Apr/23 16:57,25/Apr/23 00:25,13/Jul/23 08:43,25/Apr/23 00:25,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"As discussed [here|https://github.com/apache/spark/pull/40679/files#r1159264585].",,aokolnychyi,csun,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 25 00:25:39 UTC 2023,,,,,,,,,,"0|z1hi08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Apr/23 00:25;csun;Issue resolved by pull request 40934
[https://github.com/apache/spark/pull/40934];;;",,,,,,,,,,
df.sql() should send metrics back(),SPARK-43249,13533794,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,grundprinzip-db,grundprinzip-db,grundprinzip-db,24/Apr/23 08:30,24/Apr/23 08:34,13/Jul/23 08:43,24/Apr/23 08:34,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Connect,,,0,,df.sql() does not return the metrics to the client when executed as a command.,,grundprinzip-db,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 24 08:34:12 UTC 2023,,,,,,,,,,"0|z1hhi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Apr/23 08:34;podongfeng;Issue resolved by pull request 40899
[https://github.com/apache/spark/pull/40899];;;",,,,,,,,,,
df.describe() method may- return wrong result if the last RDD is RDD[UnsafeRow],SPARK-43240,13533667,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Jk_Self,Jk_Self,Jk_Self,23/Apr/23 02:50,26/Apr/23 09:25,13/Jul/23 08:43,26/Apr/23 09:25,3.3.2,,,,,,,,,,,,,,,3.3.3,,,,Spark Core,,,0,,"When calling the df.describe() method, the result  maybe wrong when the last RDD is RDD[UnsafeRow]. It is because the UnsafeRow will be released after the row is used. ",,cloud_fan,Jk_Self,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 26 09:25:27 UTC 2023,,,,,,,,,,"0|z1hgqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Apr/23 09:25;cloud_fan;Issue resolved by pull request 40914
[https://github.com/apache/spark/pull/40914];;;",,,,,,,,,,
Remove null_counts from info(),SPARK-43239,13533659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bjornjorgensen,bjornjorgensen,bjornjorgensen,22/Apr/23 20:13,24/Apr/23 00:14,13/Jul/23 08:43,24/Apr/23 00:14,3.4.0,3.5.0,,,,,,,,,,,,,,3.5.0,,,,Pandas API on Spark,,,0,,"df.info() is broken now. 
It prints 

TypeError                                 Traceback (most recent call last)
Cell In[12], line 1
----> 1 F05.info()

File /opt/spark/python/pyspark/pandas/frame.py:12167, in DataFrame.info(self, verbose, buf, max_cols, null_counts)
  12163     count_func = self.count
  12164     self.count = (  # type: ignore[assignment]
  12165         lambda: count_func()._to_pandas()  # type: ignore[assignment, misc, union-attr]
  12166     )
> 12167     return pd.DataFrame.info(
  12168         self,  # type: ignore[arg-type]
  12169         verbose=verbose,
  12170         buf=buf,
  12171         max_cols=max_cols,
  12172         memory_usage=False,
  12173         null_counts=null_counts,
  12174     )
  12175 finally:
  12176     del self._data

TypeError: DataFrame.info() got an unexpected keyword argument 'null_counts'
",,bjornjorgensen,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 24 00:14:37 UTC 2023,,,,,,,,,,"0|z1hgow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Apr/23 20:31;bjornjorgensen;https://github.com/apache/spark/pull/40913;;;","24/Apr/23 00:14;gurwls223;Issue resolved by pull request 40913
[https://github.com/apache/spark/pull/40913];;;",,,,,,,,,
Handle null exception message in event log,SPARK-43237,13533654,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,22/Apr/23 17:48,29/Apr/23 04:15,13/Jul/23 08:43,29/Apr/23 04:14,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,0,,,,mridulm80,snoot,warrenzhu25,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 29 04:15:32 UTC 2023,,,,,,,,,,"0|z1hgns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Apr/23 04:14;mridulm80;Issue resolved by pull request 40911
[https://github.com/apache/spark/pull/40911];;;","29/Apr/23 04:15;snoot;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/40911;;;",,,,,,,,,
IsolatedClassLoader should close barrier class InputStream after reading,SPARK-43208,13533312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,20/Apr/23 05:30,20/Apr/23 16:53,13/Jul/23 08:43,20/Apr/23 16:53,3.3.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,chengpan,csun,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 20 16:53:55 UTC 2023,,,,,,,,,,"0|z1hek0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Apr/23 16:53;csun;Issue resolved by pull request 40867
[https://github.com/apache/spark/pull/40867];;;",,,,,,,,,,
Fix DROP table behavior in session catalog,SPARK-43203,13533279,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,aokolnychyi,aokolnychyi,19/Apr/23 19:16,27/Jun/23 22:32,13/Jul/23 08:43,19/Jun/23 12:43,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"DROP table behavior is not working correctly in 3.4.0 because we always invoke V1 drop logic if the identifier looks like a V1 identifier. This is a big blocker for external data sources that provide custom session catalogs.

See [here|https://github.com/apache/spark/pull/37879/files#r1170501180] for details.",,aokolnychyi,cloud_fan,fanjia,jinhelin404,snoot,,,,,,,,,,,,,SPARK-40425,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 22:32:35 UTC 2023,,,,,,,,,,"0|z1heco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/May/23 04:45;fanjia;https://github.com/apache/spark/pull/41348;;;","06/Jun/23 03:44;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41348;;;","19/Jun/23 12:43;cloud_fan;Issue resolved by pull request 41348
[https://github.com/apache/spark/pull/41348];;;","27/Jun/23 22:32;aokolnychyi;I unfortunately created this initially as improvement. It is actually a bug and regression, which breaks DROP in custom sessions catalogs. Can we include it in 3.4.2?;;;",,,,,,,
Make InlineCTE idempotent,SPARK-43199,13533269,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,19/Apr/23 16:34,26/Apr/23 07:33,13/Jul/23 08:43,26/Apr/23 07:33,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,cloud_fan,githubbot,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 26 07:33:38 UTC 2023,,,,,,,,,,"0|z1heag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Apr/23 09:23;githubbot;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40856;;;","26/Apr/23 07:33;cloud_fan;Issue resolved by pull request 40856
[https://github.com/apache/spark/pull/40856];;;",,,,,,,,,
"Fix ""Could not initialise class ammonite..."" error when using filter",SPARK-43198,13533265,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,19/Apr/23 16:07,26/Apr/23 16:13,13/Jul/23 08:43,26/Apr/23 16:13,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,,,0,,"When
{code:java}
spark.range(10).filter(n => n % 2 == 0).collectAsList()`{code}
 is run in the ammonite REPL (Spark Connect), the following error is thrown:
{noformat}
io.grpc.StatusRuntimeException: UNKNOWN: ammonite/repl/ReplBridge$
  io.grpc.Status.asRuntimeException(Status.java:535)
  io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:62)
  org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:114)
  org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:131)
  org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2687)
  org.apache.spark.sql.Dataset.withResult(Dataset.scala:3088)
  org.apache.spark.sql.Dataset.collect(Dataset.scala:2686)
  org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:2700)
  ammonite.$sess.cmd0$.<init>(cmd0.sc:1)
  ammonite.$sess.cmd0$.<clinit>(cmd0.sc){noformat}",,gurwls223,vicennial,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-19 16:07:06.0,,,,,,,,,,"0|z1he9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark connect's user agent validations are too restrictive,SPARK-43192,13533218,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nija,nija,nija,19/Apr/23 12:43,21/Apr/23 06:23,13/Jul/23 08:43,21/Apr/23 06:23,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Connect,PySpark,,0,,"The current restriction on allowed charset and length are too restrictive

 

https://github.com/apache/spark/blob/cac6f58318bb84d532f02d245a50d3c66daa3e4b/python/pyspark/sql/connect/client.py#L274-L275",,gurwls223,nija,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 06:23:55 UTC 2023,,,,,,,,,,"0|z1hdz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Apr/23 06:23;gurwls223;Issue resolved by pull request 40853
[https://github.com/apache/spark/pull/40853];;;",,,,,,,,,,
ListQuery.childOutput should be consistent with child output,SPARK-43190,13533216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,19/Apr/23 12:42,20/Apr/23 07:14,13/Jul/23 08:43,20/Apr/23 07:14,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 20 07:14:15 UTC 2023,,,,,,,,,,"0|z1hdyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Apr/23 07:14;cloud_fan;Issue resolved by pull request 40851
[https://github.com/apache/spark/pull/40851];;;",,,,,,,,,,
Fix SparkSQLCLIDriver completer,SPARK-43174,13533018,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,18/Apr/23 09:52,22/Apr/23 00:16,13/Jul/23 08:43,22/Apr/23 00:16,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 22 00:16:24 UTC 2023,,,,,,,,,,"0|z1hcqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Apr/23 00:16;yumwang;Issue resolved by pull request 40838
[https://github.com/apache/spark/pull/40838];;;",,,,,,,,,,
Set upperbound of pandas version in binder integrations,SPARK-43158,13532809,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,17/Apr/23 00:48,17/Apr/23 04:00,13/Jul/23 08:43,17/Apr/23 01:13,3.4.0,,,,,,,,,,,,,,,3.3.3,3.4.0,3.5.0,,Documentation,,,0,,"{code}
df.toPandas
{code}

fails with

{code}

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[14], line 1
----> 1 df.toPandas()

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:251, in PandasConversionMixin.toPandas(self)
    248 should_check_timedelta = is_timedelta64_dtype(t) and len(pdf) == 0
    250 if (t is not None and not is_timedelta64_dtype(t)) or should_check_timedelta:
--> 251     series = series.astype(t, copy=False)
    253 with catch_warnings():
    254     from pandas.errors import PerformanceWarning

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pandas/core/generic.py:6324, in NDFrame.astype(self, dtype, copy, errors)
   6317     results = [
   6318         self.iloc[:, i].astype(dtype, copy=copy)
   6319         for i in range(len(self.columns))
   6320     ]
   6322 else:
   6323     # else, only a single dtype is given
-> 6324     new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)
   6325     return self._constructor(new_data).__finalize__(self, method=""astype"")
   6327 # GH 33113: handle empty frame or series

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pandas/core/internals/managers.py:451, in BaseBlockManager.astype(self, dtype, copy, errors)
    448 elif using_copy_on_write():
    449     copy = False
--> 451 return self.apply(
    452     ""astype"",
    453     dtype=dtype,
    454     copy=copy,
    455     errors=errors,
    456     using_cow=using_copy_on_write(),
    457 )

File /srv/conda/envs/notebook/lib/python3.10/site-packages/pandas/core/internals/managers.py:352, in BaseBlockManager.apply(self, f, align_keys, **kwargs)
    350         applied = b.apply(f, **kwargs)
    351     else:
--> 352         applied = getattr(b, f)(**kwargs)
    353     result_blocks = extend_blocks(applied, result_blocks)
    355 out = type(self).from_blocks(result_blocks, self.axes)
{code}",,gurwls223,snoot,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 17 04:00:58 UTC 2023,,,,,,,,,,"0|z1hbgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 01:13;gurwls223;Issue resolved by pull request 40814
[https://github.com/apache/spark/pull/40814];;;","17/Apr/23 04:00;snoot;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/40814;;;",,,,,,,,,
TreeNode tags can become corrupted and hang driver when the dataset is cached,SPARK-43157,13532807,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,robreeves,robreeves,robreeves,16/Apr/23 22:18,18/May/23 06:25,13/Jul/23 08:43,18/May/23 06:25,2.3.0,3.5.0,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,0,,"If a cached dataset is used by multiple other datasets materialized in separate threads it can corrupt the TreeNode.tags map in any of the cached plan nodes. This will hang the driver forever. This happens because TreeNode.tags is not thread-safe. How this happens:
 # Multiple datasets are materialized at the same time in different threads that reference the same cached dataset
 # AdaptiveSparkPlanExec.onUpdatePlan will call ExplainMode.fromString
 # ExplainUtils uses the TreeNode.tags map to store the operator Id for every node in the plan. This is usually okay because the plan is cloned. When there is an InMemoryScanExec the InMemoryRelation.cachedPlan is not cloned so multiple threads can set the operator Id.

Making the TreeNode.tags field thread-safe does not solve this problem because there is still a correctness issue. The threads may be overwriting each other's operator Ids, which could be different.

Example stack trace of the infinite loop:
{code:scala}
scala.collection.mutable.HashTable.resize(HashTable.scala:265)
scala.collection.mutable.HashTable.addEntry0(HashTable.scala:158)
scala.collection.mutable.HashTable.findOrAddEntry(HashTable.scala:170)
scala.collection.mutable.HashTable.findOrAddEntry$(HashTable.scala:167)
scala.collection.mutable.HashMap.findOrAddEntry(HashMap.scala:44)
scala.collection.mutable.HashMap.put(HashMap.scala:126)
scala.collection.mutable.HashMap.update(HashMap.scala:131)
org.apache.spark.sql.catalyst.trees.TreeNode.setTagValue(TreeNode.scala:108)
org.apache.spark.sql.execution.ExplainUtils$.setOpId$1(ExplainUtils.scala:134)
…
org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:175)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.onUpdatePlan(AdaptiveSparkPlanExec.scala:662){code}
Example to show the cachedPlan object is not cloned:
{code:java}
import org.apache.spark.sql.execution.SparkPlan
import org.apache.spark.sql.execution.columnar.InMemoryTableScanExec
import spark.implicits._

def findCacheOperator(plan: SparkPlan): Option[InMemoryTableScanExec] = {
  if (plan.isInstanceOf[InMemoryTableScanExec]) {
    Some(plan.asInstanceOf[InMemoryTableScanExec])
  } else if (plan.children.isEmpty && plan.subqueries.isEmpty) {
    None
  } else {
    (plan.subqueries.flatMap(p => findCacheOperator(p)) ++
      plan.children.flatMap(findCacheOperator)).headOption
  }
}

val df = spark.range(10).filter($""id"" < 100).cache()
val df1 = df.limit(1)
val df2 = df.limit(1)

// Get the cache operator (InMemoryTableScanExec) in each plan
val plan1 = findCacheOperator(df1.queryExecution.executedPlan).get
val plan2 = findCacheOperator(df2.queryExecution.executedPlan).get

// Check if InMemoryTableScanExec references point to the same object
println(plan1.eq(plan2))
// returns false// Check if InMemoryRelation references point to the same object

println(plan1.relation.eq(plan2.relation))
// returns false

// Check if the cached SparkPlan references point to the same object
println(plan1.relation.cachedPlan.eq(plan2.relation.cachedPlan))
// returns true
// This shows that the cloned plan2 still has references to the original plan1 {code}",,cloud_fan,githubbot,gurwls223,robreeves,xkrogen,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 18 06:25:50 UTC 2023,,,,,,,,,,"0|z1hbgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/May/23 09:22;githubbot;User 'robreeves' has created a pull request for this issue:
https://github.com/apache/spark/pull/40812;;;","18/May/23 06:25;cloud_fan;Issue resolved by pull request 40812
[https://github.com/apache/spark/pull/40812];;;",,,,,,,,,
DSL expressions fail on attribute with special characters,SPARK-43142,13532662,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rshkv,rshkv,rshkv,14/Apr/23 13:11,25/Apr/23 10:23,13/Jul/23 08:43,25/Apr/23 10:23,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"Expressions on implicitly converted attributes fail if the attributes have names containing special characters. They fail even if the attributes are backtick-quoted:
{code:java}
scala> import org.apache.spark.sql.catalyst.dsl.expressions._
import org.apache.spark.sql.catalyst.dsl.expressions._

scala> ""`slashed/col`"".attr
res0: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'slashed/col

scala> ""`slashed/col`"".attr.asc
org.apache.spark.sql.catalyst.parser.ParseException:
mismatched input '/' expecting {<EOF>, '.', '-'}(line 1, pos 7)

== SQL ==
slashed/col
-------^^^
{code}",,cloud_fan,githubbot,hudson,rshkv,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 25 10:23:52 UTC 2023,,,,,,,,,,"0|z1hako:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 13:17;rshkv;Here's what's happening: {{ImplicitOperators}} methods like {{asc}} rely on a call to {{expr}} [(Github)|https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L149]. 

The {{UnresolvedAttribute}} returned by {{.attr}} is implicitly converted to {{DslAttr}}. But {{DslAttr}} does not implement {{expr}} by returning the attribute it's already wrapping. Instead, it only implements how to convert the attribute it's wrapping to a string name [(Github)|https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L273-L275].

Returning an attribute for an implicitly wrapped attribute is implemented on the super class {{ImplicitAttribute}} by creating a new {{UnresolvedAttribute}} on the string name return by {{DslAttr}} (the method call {{s}}, [Github|https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L278-L280]).

The problem is that this string name returned by {{DslAttr}} no longer has the quotes and thus the new {{UnresolvedAttribute}} parses an unquoted identifier.

{code}
scala> ""`col/slash`"".attr.name
res1: String = col/slash
{code};;;","14/Apr/23 13:18;rshkv;The solution I'd propose is to have {{DslAttr.attr}} return the attribute it's wrapping instead of creating a new attribute.

I'll put up a PR.;;;","14/Apr/23 13:45;rshkv;https://github.com/apache/spark/pull/40794;;;","19/Apr/23 17:08;hudson;User 'rshkv' has created a pull request for this issue:
https://github.com/apache/spark/pull/40794;;;","21/Apr/23 12:55;cloud_fan;Issue resolved by pull request 40794
[https://github.com/apache/spark/pull/40794];;;","22/Apr/23 09:16;githubbot;User 'rshkv' has created a pull request for this issue:
https://github.com/apache/spark/pull/40902;;;","25/Apr/23 10:23;cloud_fan;Issue resolved by pull request 40902
[https://github.com/apache/spark/pull/40902];;;",,,,
Ignore generated Java files in checkstyle,SPARK-43141,13532652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,14/Apr/23 11:36,17/Apr/23 02:57,13/Jul/23 08:43,16/Apr/23 10:47,3.4.1,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Build,,,0,,Files such as {{.../spark/core/target/scala-2.12/src_managed/main/org/apache/spark/status/protobuf/StoreTypes.java}} are checked in checkstyle. We shouldn't check them in the linter.,,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 16 10:47:41 UTC 2023,,,,,,,,,,"0|z1haig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Apr/23 10:47;gurwls223;Issue resolved by pull request 40792
[https://github.com/apache/spark/pull/40792];;;",,,,,,,,,,
Override computeStats in DummyLeafNode,SPARK-43140,13532638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,14/Apr/23 09:19,17/Apr/23 03:17,13/Jul/23 08:43,17/Apr/23 03:13,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,0,,,,gurwls223,snoot,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 17 03:17:08 UTC 2023,,,,,,,,,,"0|z1hafc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 10:18;yumwang;https://github.com/apache/spark/pull/40791;;;","17/Apr/23 03:13;gurwls223;Issue resolved by pull request 40791
[https://github.com/apache/spark/pull/40791];;;","17/Apr/23 03:17;snoot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/40791;;;",,,,,,,,
ClassNotFoundException during RDD block replication/migration,SPARK-43138,13532606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,14/Apr/23 06:06,11/May/23 13:25,13/Jul/23 08:43,11/May/23 13:25,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,0,,"During RDD block migration during decommissioning we are seeing `ClassNotFoundException` on the receiving Executor. This seems to happen when the blocks contain classes that are from the user jars.
```
2023-04-08 04:15:11,791 ERROR server.TransportRequestHandler: Error while invoking RpcHandler#receive() on RPC id 6425687122551756860
java.lang.ClassNotFoundException: com.class.from.user.jar.ClassName
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
    at java.base/java.lang.Class.forName0(Native Method)
    at java.base/java.lang.Class.forName(Class.java:398)
    at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:71)
    at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2003)
    at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)
    at java.base/java.io.ObjectInputStream.readClass(ObjectInputStream.java:1833)
    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1658)
    at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)
    at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)
    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)
    at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)
    at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)
    at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)
    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)
    at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)
    at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)
    at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
    at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)
    at org.apache.spark.network.netty.NettyBlockRpcServer.deserializeMetadata(NettyBlockRpcServer.scala:180)
    at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:119)
    at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)
    at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:829)
```",,eejbyfeldt,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 11 13:25:58 UTC 2023,,,,,,,,,,"0|z1ha88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 06:47;yumwang;Did you set some config to `com.class.from.user.jar.ClassName`?;;;","16/Apr/23 13:14;eejbyfeldt;No. The class `com.class.from.user.jar.ClassName` is just a `case class` that is used inside an RDD. 

But I think I have a good idea of what is causing it. So I created this PR which I hope solves it: https://github.com/apache/spark/pull/40808;;;","11/May/23 13:25;srowen;Issue resolved by pull request 40808
[https://github.com/apache/spark/pull/40808];;;",,,,,,,,
mark two Hive UDF expressions as stateful,SPARK-43126,13532498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,13/Apr/23 13:29,14/Apr/23 00:50,13/Jul/23 08:43,14/Apr/23 00:50,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,0,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-13 13:29:11.0,,,,,,,,,,"0|z1h9k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect Server Can't Handle Exception With Null Message Normally,SPARK-43125,13532493,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,fanjia,fanjia,fanjia,13/Apr/23 13:04,14/Apr/23 03:46,13/Jul/23 08:43,14/Apr/23 03:46,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Connect,,,0,,"!image-2023-04-13-21-05-43-617.png|width=626,height=310!!image-2023-04-13-21-05-16-994.png|width=621,height=294!

When Server Throw Exception which without message Like NPE. The setMessage method will report NPE again. But can't throw to client.",,fanjia,gurwls223,,,,,,,,,,,,,,,,,,,,,,"13/Apr/23 13:05;fanjia;image-2023-04-13-21-05-16-994.png;https://issues.apache.org/jira/secure/attachment/13057253/image-2023-04-13-21-05-16-994.png","13/Apr/23 13:05;fanjia;image-2023-04-13-21-05-43-617.png;https://issues.apache.org/jira/secure/attachment/13057254/image-2023-04-13-21-05-43-617.png",,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 14 03:46:23 UTC 2023,,,,,,,,,,"0|z1h9j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 03:46;gurwls223;Issue resolved by pull request 40780
[https://github.com/apache/spark/pull/40780];;;",,,,,,,,,,
Dataset.show should not trigger job execution on CommandResults,SPARK-43124,13532488,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,13/Apr/23 12:32,21/Apr/23 00:33,13/Jul/23 08:43,21/Apr/23 00:33,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,gurwls223,hudson,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 00:33:56 UTC 2023,,,,,,,,,,"0|z1h9i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Apr/23 17:08;hudson;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40779;;;","21/Apr/23 00:33;gurwls223;Issue resolved by pull request 40779
[https://github.com/apache/spark/pull/40779];;;",,,,,,,,,
special internal field metadata should not be leaked to catalogs,SPARK-43123,13532483,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,13/Apr/23 11:46,14/Apr/23 09:09,13/Jul/23 08:43,14/Apr/23 09:09,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 14 09:09:00 UTC 2023,,,,,,,,,,"0|z1h9gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 09:09;cloud_fan;Issue resolved by pull request 40776
[https://github.com/apache/spark/pull/40776];;;",,,,,,,,,,
Support Get SQL Keywords Dynamically,SPARK-43119,13532435,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,13/Apr/23 03:57,21/Apr/23 02:24,13/Jul/23 08:43,21/Apr/23 02:24,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,Implements the JDBC standard API and an auxiliary function,,Qin Yao,snoot,yao,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 02:24:22 UTC 2023,,,,,,,,,,"0|z1h968:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Apr/23 04:17;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40768;;;","13/Apr/23 04:17;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40768;;;","21/Apr/23 02:24;Qin Yao;Issue resolved by pull request 40768
[https://github.com/apache/spark/pull/40768];;;",,,,,,,,
Codegen error when full outer join's bound condition has multiple references to the same stream-side column,SPARK-43113,13532412,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,12/Apr/23 22:33,24/Apr/23 00:59,13/Jul/23 08:43,18/Apr/23 04:11,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,0,,"Example # 1 (sort merge join):
{noformat}
create or replace temp view v1 as
select * from values
(1, 1),
(2, 2),
(3, 1)
as v1(key, value);

create or replace temp view v2 as
select * from values
(1, 22, 22),
(3, -1, -1),
(7, null, null)
as v2(a, b, c);

select *
from v1
full outer join v2
on key = a
and value > b
and value > c;
{noformat}
The join's generated code causes the following compilation error:
{noformat}
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 277, Column 9: Redefinition of local variable ""smj_isNull_7""
{noformat}
Example #2 (shuffle hash join):
{noformat}
select /*+ SHUFFLE_HASH(v2) */ *
from v1
full outer join v2
on key = a
and value > b
and value > c;
{noformat}
The shuffle hash join's generated code causes the following compilation error:
{noformat}
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 174, Column 5: Redefinition of local variable ""shj_value_1"" 
{noformat}
With default configuration, both queries end up succeeding, since Spark falls back to running each query with whole-stage codegen disabled.

The issue happens only when the join's bound condition refers to the same stream-side column more than once.",,bersprockets,gurwls223,hudson,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 20 15:49:34 UTC 2023,,,,,,,,,,"0|z1h914:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Apr/23 00:32;bersprockets;PR here: https://github.com/apache/spark/pull/40766;;;","18/Apr/23 04:11;gurwls223;Fixed in https://github.com/apache/spark/pull/40766;;;","20/Apr/23 15:49;hudson;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/40881;;;",,,,,,,,
"`Class.getCanonicalName` return null for anonymous class on JDK15+, impacting function registry",SPARK-43099,13532248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,alexjinghn,alexjinghn,alexjinghn,11/Apr/23 22:19,17/Apr/23 03:22,13/Jul/23 08:43,17/Apr/23 03:15,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,0,,"On JDK15+, lambda and method references are implemented using hidden classes ([https://openjdk.org/jeps/371)] According to the JEP, 
{quote}{{Class::getCanonicalName}} returns {{{}null{}}}, indicating the hidden class has no canonical name. (Note that the {{Class}} object for an anonymous class in the Java language has the same behavior.)
{quote}

This means [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala#L53] will always be null.

 

This can be fixed by replacing `getCanonicalName` with `getName`

 ",,alexjinghn,cloud_fan,ggintegration,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 17 03:15:16 UTC 2023,,,,,,,,,,"0|z1h80w:",9223372036854775807,,,,,gengliang,,,,,,,,,,,,,,,,,"17/Apr/23 01:50;ggintegration;User 'alexjinghn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40747;;;","17/Apr/23 03:15;cloud_fan;Issue resolved by pull request 40747
[https://github.com/apache/spark/pull/40747];;;",,,,,,,,,
Avoid Once strategy's idempotence is broken for batch: Infer Filters,SPARK-43095,13532191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,11/Apr/23 13:27,15/Apr/23 01:25,13/Jul/23 08:43,15/Apr/23 01:25,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,githubbot,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 15 01:25:22 UTC 2023,,,,,,,,,,"0|z1h7o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Apr/23 09:03;githubbot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/40742;;;","13/Apr/23 09:04;githubbot;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/40742;;;","15/Apr/23 01:25;yumwang;Issue resolved by pull request 40742
[https://github.com/apache/spark/pull/40742];;;",,,,,,,,
Array Insert Should Throw On 'Pos' Value 0,SPARK-43094,13532168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,ddavies1,ddavies1,11/Apr/23 11:11,11/Apr/23 11:32,13/Jul/23 08:43,11/Apr/23 11:32,3.3.2,,,,,,,,,,,,,,,,,,,SQL,,,0,,"Wanted to start a discussion about the following comment:

[https://github.com/apache/spark/pull/38867/files#r1157449697]

If a 'pos' of 0 is provided, the code here operates as if the user provided a 1. This could be confusing to users (two incides now overlap and lead to the same result, which is logically a little strange), but perhaps more compellingly, the other functions relying on a provided index, such as elementAt [here|https://github.com/Daniel-Davies/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L2344], will raise an exception if provided with an index of 0.",,ddavies1,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 11 11:32:30 UTC 2023,,,,,,,,,,"0|z1h7jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Apr/23 11:32;ddavies1;Fixed already, my apologies;;;",,,,,,,,,,
"Test case ""Add a directory when spark.sql.legacy.addSingleFileInAddFile set to false"" should use random directories for testing",SPARK-43093,13532126,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,11/Apr/23 05:43,12/Apr/23 02:23,13/Jul/23 08:43,12/Apr/23 02:23,3.2.3,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,0,,,,gurwls223,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 02:23:18 UTC 2023,,,,,,,,,,"0|z1h7a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Apr/23 02:23;gurwls223;Issue resolved by pull request 40737
[https://github.com/apache/spark/pull/40737];;;",,,,,,,,,,
Improve the error message of UNRECOGNIZED_SQL_TYPE,SPARK-43077,13532012,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,10/Apr/23 02:49,11/Apr/23 02:47,13/Jul/23 08:43,11/Apr/23 02:47,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"UNRECOGNIZED_SQL_TYPE prints the jdbc type id in the error message currently. This is difficult for spark users to understand the meaning of this kind of error, especially when the type id is from a vendor extension.

For example, 
{code:java}
 org.apache.spark.SparkSQLException: Unrecognized SQL type -102{code}",,githubbot,Qin Yao,yao,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 11 02:47:48 UTC 2023,,,,,,,,,,"0|z1h6ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Apr/23 09:22;githubbot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40718;;;","11/Apr/23 02:47;Qin Yao;Issue resolved by pull request 40718
[https://github.com/apache/spark/pull/40718];;;",,,,,,,,,
Use `sbt-eclipse` instead of `sbteclipse-plugin`,SPARK-43069,13531901,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,07/Apr/23 18:57,07/Apr/23 19:55,13/Jul/23 08:43,07/Apr/23 19:55,3.2.3,3.3.2,3.4.0,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,,Build,,,0,,"Since SPARK-34959, Apache Spark 3.2+ uses SBT 1.5.0.

And, SBT 1.4+ can use `set-eclipse` instead of old `sbteclipse-plugin`.
- https://github.com/sbt/sbt-eclipse/releases",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 07 19:55:04 UTC 2023,,,,,,,,,,"0|z1h5w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Apr/23 19:55;dongjoon;Issue resolved by pull request 40708
[https://github.com/apache/spark/pull/40708];;;",,,,,,,,,,
Error class resource file in Kafka connector is misplaced,SPARK-43067,13531847,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,07/Apr/23 10:17,08/Apr/23 22:19,13/Jul/23 08:43,08/Apr/23 21:39,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,Structured Streaming,,,0,,"SPARK-41387 adopted error class framework in Kafka connector. Since it's a connector, the error class file is intentionally put to the Kafka connector module instead of being added to the Spark's central error class file.

Unfortunately I've figured out somehow that the place is wrong. It should be placed to the resources directory in source, not test. The problem cannot be captured in test suite as it's available in test artifact, but I can trigger the problem via adding Kafka connector jar into classpath and initialize KafkaExceptions object.

Hopefully the blast radius of the problem is trivial as Kafka connector uses error class only for assertions which should not be triggered unless some accident happens e.g. topic is deleted and recreated while the streaming query is running with Trigger.Available.",,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 08 21:39:19 UTC 2023,,,,,,,,,,"0|z1h5k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Apr/23 10:24;kabhwan;Will submit a PR sooner. The fix is very straightforward but I need to validate the fix manually since test cases cannot verify this as described above.;;;","08/Apr/23 21:39;kabhwan;Issue resolved by pull request 40705
[https://github.com/apache/spark/pull/40705];;;",,,,,,,,,
Handle stacktrace with null file name in event log,SPARK-43052,13531757,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,06/Apr/23 15:15,27/Apr/23 04:33,13/Jul/23 08:43,27/Apr/23 04:33,3.3.2,,,,,,,,,,,,,,,3.5.0,,,,Spark Core,,,0,,,,gurwls223,mridulm80,warrenzhu25,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 27 04:33:21 UTC 2023,,,,,,,,,,"0|z1h500:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Apr/23 03:14;gurwls223;https://github.com/apache/spark/pull/40687;;;","27/Apr/23 04:33;mridulm80;Issue resolved by pull request 40687
[https://github.com/apache/spark/pull/40687];;;",,,,,,,,,
Fix construct aggregate expressions by replacing grouping functions,SPARK-43050,13531751,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,06/Apr/23 14:20,15/Apr/23 02:22,13/Jul/23 08:43,15/Apr/23 02:22,3.5.0,,,,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,0,,"
{code:sql}
CREATE TEMPORARY VIEW grouping AS SELECT * FROM VALUES
  (""1"", ""2"", ""3"", 1),
  (""4"", ""5"", ""6"", 1),
  (""7"", ""8"", ""9"", 1)
  as grouping(a, b, c, d);
{code}

{noformat}
spark-sql (default)> SELECT CASE WHEN a IS NULL THEN count(b) WHEN b IS NULL THEN count(c) END
                   > FROM grouping
                   > GROUP BY GROUPING SETS (a, b, c);
[MISSING_AGGREGATION] The non-aggregating expression ""b"" is based on columns which are not participating in the GROUP BY clause.
{noformat}",,awsthni,gurwls223,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 15 02:22:06 UTC 2023,,,,,,,,,,"0|z1h4yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Apr/23 03:15;gurwls223;https://github.com/apache/spark/pull/40685;;;","11/Apr/23 12:01;awsthni;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/40685;;;","15/Apr/23 02:22;yumwang;Issue resolved by pull request 40685
https://github.com/apache/spark/pull/40685;;;",,,,,,,,
Use CLOB instead of VARCHAR(255) for StringType for Oracle jdbc,SPARK-43049,13531696,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,06/Apr/23 05:39,07/Apr/23 03:30,13/Jul/23 08:43,07/Apr/23 03:02,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,snoot,yao,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 07 03:30:15 UTC 2023,,,,,,,,,,"0|z1h4mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Apr/23 03:02;yao;resolved by https://github.com/apache/spark/pull/40683;;;","07/Apr/23 03:30;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40683;;;",,,,,,,,,
Restore constructors of exceptions for compatibility in connector API,SPARK-43041,13531648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,aokolnychyi,XinrongM,XinrongM,05/Apr/23 19:03,06/Apr/23 05:26,13/Jul/23 08:43,06/Apr/23 05:11,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,0,,"Thanks [~aokolnychyi] for raising the issue as shown below:
{quote}
I have a question about changes to exceptions used in the public connector API, such as NoSuchTableException and TableAlreadyExistsException.

I consider those as part of the public Catalog API (TableCatalog uses them in method definitions). However, it looks like PR #37887 has changed them in an incompatible way. Old constructors accepting Identifier objects got removed. The only way to construct such exceptions is either by passing database and table strings or Scala Seq. Shall we add back old constructors to avoid breaking connectors?
{quote}
We should restore constructors of those exceptions to preserve the compatibility in connector API.",,aokolnychyi,Gengliang.Wang,XinrongM,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 06 05:26:56 UTC 2023,,,,,,,,,,"0|z1h4c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Apr/23 20:17;aokolnychyi;https://github.com/apache/spark/pull/40679;;;","06/Apr/23 05:11;Gengliang.Wang;Issue resolved by pull request 40679
[https://github.com/apache/spark/pull/40679];;;","06/Apr/23 05:26;ci-cassandra.apache.org;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40679;;;",,,,,,,,
deduplicate relations with metadata columns,SPARK-43030,13531456,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,04/Apr/23 14:24,07/Apr/23 03:47,13/Jul/23 08:43,07/Apr/23 03:47,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,,,cloud_fan,snoot,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 07 03:47:09 UTC 2023,,,,,,,,,,"0|z1h35c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Apr/23 03:22;snoot;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40662;;;","07/Apr/23 03:47;cloud_fan;Issue resolved by pull request 40662
[https://github.com/apache/spark/pull/40662];;;",,,,,,,,,
Shuffle happens when Coalesce Buckets should occur,SPARK-43021,13531368,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Zing,neshkeev,neshkeev,04/Apr/23 02:35,14/Apr/23 03:20,13/Jul/23 08:43,14/Apr/23 03:20,3.3.1,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"h1. What I did

I define the following code:

{{from pyspark.sql import SparkSession}}

{{spark = (}}
{{  SparkSession}}
{{    .builder}}
{{    .appName(""Bucketing"")}}
{{    .master(""local[4]"")}}
{{    .config(""spark.sql.bucketing.coalesceBucketsInJoin.enabled"", True)}}
{{    .config(""spark.sql.autoBroadcastJoinThreshold"", ""-1"")}}
{{    .getOrCreate()}}
{{)}}

{{df1 = spark.range(0, 100)}}
{{df2 = spark.range(0, 100, 2)}}

{{df1.write.bucketBy(4, ""id"").mode(""overwrite"").saveAsTable(""t1"")}}
{{df2.write.bucketBy(2, ""id"").mode(""overwrite"").saveAsTable(""t2"")}}

{{t1 = spark.table(""t1"")}}
{{t2 = spark.table(""t2"")}}

{{t2.join(t1, ""id"").explain()}}

h1. What happened

There is an Exchange node in the join plan

h1. What is expected

The plan should not contain any Exchange/Shuffle nodes, because {{t1}}'s number of buckets is 4 and {{t2}}'s number of buckets is 2, and their ratio is 2 which is less than 4 ({{spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio}}) and [CoalesceBucketsInJoin|https://github.com/apache/spark/blob/c9878a212958bc54be529ef99f5e5d1ddf513ec8/sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/CoalesceBucketsInJoin.scala] should be applied",,cloud_fan,neshkeev,snoot,Zing,,,,,,,,,,,,,SPARK-43087,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 14 03:20:56 UTC 2023,,,,,,,,,,"0|z1h2m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Apr/23 16:12;Zing;PR：https://github.com/apache/spark/pull/40688;;;","13/Apr/23 03:48;snoot;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/40688;;;","14/Apr/23 03:20;cloud_fan;Issue resolved by pull request 40688
[https://github.com/apache/spark/pull/40688];;;",,,,,,,,
self.deserialized == self.deserialized typo in StorageLevel __eq__(),SPARK-43006,13531147,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tienhoayu,tienhoayu,tienhoayu,03/Apr/23 03:16,03/Apr/23 13:27,13/Jul/23 08:43,03/Apr/23 13:27,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,PySpark,,,0,,"We should fix {{self.deserialized == self.deserialized}} with {{self.deserialized == other.deserialized}}

The original expression is always True, which is likely to be a typo.",,snoot,tienhoayu,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 13:27:02 UTC 2023,,,,,,,,,,"0|z1h18w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Apr/23 03:18;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40619;;;","03/Apr/23 03:19;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40619;;;","03/Apr/23 13:27;srowen;Resolved by 40619;;;",,,,,,,,
`v is v >= 0` typo in pyspark/pandas/config.py,SPARK-43005,13531146,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tienhoayu,tienhoayu,tienhoayu,03/Apr/23 03:11,03/Apr/23 13:25,13/Jul/23 08:43,03/Apr/23 13:25,3.2.0,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,3.5.0,PySpark,,,0,,"By comparing compute.isin_limit and plotting.max_rows, {{v is v}} is likely to be a typo.

We should fix {{v is v >= 0}} with {{{}v >= 0{}}}.",,snoot,tienhoayu,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 13:25:36 UTC 2023,,,,,,,,,,"0|z1h18o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Apr/23 03:19;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40620;;;","03/Apr/23 03:19;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40620;;;","03/Apr/23 13:25;srowen;Resolved by https://github.com/apache/spark/pull/40620;;;",,,,,,,,
vendor==vendor typo in ResourceRequest.equals(),SPARK-43004,13531144,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tienhoayu,tienhoayu,tienhoayu,03/Apr/23 03:03,03/Apr/23 03:38,13/Jul/23 08:43,03/Apr/23 03:38,3.0.0,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,3.5.0,Spark Core,,,0,,"vendor == vendor is always true, this is likely to be a typo.

We should fix `vendor == vendor` with `that.vendor == vendor`, and `discoveryScript == discoveryScript` with `that.discoveryScript == discoveryScript`.",,snoot,tienhoayu,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 03:38:11 UTC 2023,,,,,,,,,,"0|z1h188:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Apr/23 03:37;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40622;;;","03/Apr/23 03:37;snoot;User 'thyecust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40622;;;","03/Apr/23 03:38;srowen;Resolved by https://github.com/apache/spark/pull/40622;;;",,,,,,,,
Correct code highlights in SQL protobuf documentation,SPARK-42987,13530882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lucaspompeun,lucaspompeun,lucaspompeun,31/Mar/23 01:13,10/Apr/23 03:24,13/Jul/23 08:43,10/Apr/23 03:24,3.3.2,,,,,,,,,,,,,,,3.5.0,,,,Documentation,,,0,docs,"Correct code highlights in SQL protobuf documentation.

Some of code highlights was in different format and not in markdown pattern",,gurwls223,lucaspompeun,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 10 03:24:01 UTC 2023,,,,,,,,,,"0|z1gzm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Mar/23 01:34;lucaspompeun;The PR was created in github

https://github.com/apache/spark/pull/40614;;;","10/Apr/23 03:24;gurwls223;Fixed in https://github.com/apache/spark/pull/40614;;;",,,,,,,,,
 Derby&PG: RENAME cannot qualify a new-table-Name with a schema-Name.,SPARK-42978,13530739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,30/Mar/23 06:26,31/Mar/23 08:15,13/Jul/23 08:43,31/Mar/23 08:15,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,https://db.apache.org/derby/docs/10.2/ref/rrefnewtablename.html#rrefnewtablename,,githubbot,yao,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 31 08:15:49 UTC 2023,,,,,,,,,,"0|z1gyqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Mar/23 09:21;githubbot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40602;;;","31/Mar/23 08:15;yao;issue resolved by [https://github.com/apache/spark/pull/40602];;;",,,,,,,,,
Restore `Utils#createTempDir` use  `ShutdownHookManager.registerShutdownDeleteDir` to cleanup tempDir,SPARK-42974,13530721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,30/Mar/23 04:45,04/Apr/23 07:27,13/Jul/23 08:43,03/Apr/23 13:28,3.4.0,3.5.0,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,0,,,,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 04 07:22:31 UTC 2023,,,,,,,,,,"0|z1gymo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Apr/23 13:28;srowen;Resolved by https://github.com/apache/spark/pull/40613;;;","04/Apr/23 07:22;dongjoon;This landed at branch-3.4 via https://github.com/apache/spark/pull/40647 .;;;",,,,,,,,,
"When processing the WorkDirCleanup event, if appDirs is empty, should print workdir ",SPARK-42971,13530706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,30/Mar/23 00:48,30/Mar/23 04:53,13/Jul/23 08:43,30/Mar/23 04:53,3.4.0,3.5.0,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,0,,"          val appDirs = workDir.listFiles()
          if (appDirs == null) {
            throw new IOException(
              s""ERROR: Failed to list files in ${appDirs.mkString(""dirs("", "", "", "")"")}"")
          }

 

Otherwise, npe will be thrown here when appDirs is null

 ",,gurwls223,LuciferYang,snoot,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 04:53:37 UTC 2023,,,,,,,,,,"0|z1gyjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Mar/23 03:26;snoot;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40597;;;","30/Mar/23 04:53;gurwls223;Issue resolved by pull request 40597
[https://github.com/apache/spark/pull/40597];;;",,,,,,,,,
Fix SparkListenerTaskStart.stageAttemptId when a task is started after the stage is cancelled,SPARK-42967,13530669,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,29/Mar/23 18:26,30/Mar/23 22:49,13/Jul/23 08:43,30/Mar/23 22:49,3.0.3,3.1.3,3.2.3,3.3.2,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,Spark Core,,,0,,"When a task is started after the stage is cancelled, the stageAttemptId field in  SparkListenerTaskStart event is set to -1. This could lead to unexpected problem for subscribers of SparkListener because -1 is not a legal stageAttemptId.",,Gengliang.Wang,jiangxb1987,thejdeep,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 22:49:05 UTC 2023,,,,,,,,,,"0|z1gybc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Mar/23 22:49;Gengliang.Wang;Issue resolved by pull request 40592
[https://github.com/apache/spark/pull/40592];;;",,,,,,,,,,
`release-build.sh` should not remove SBOM artifacts,SPARK-42957,13530543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,29/Mar/23 05:30,29/Mar/23 06:30,13/Jul/23 08:43,29/Mar/23 06:30,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Project Infra,,,0,,,,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 06:30:35 UTC 2023,,,,,,,,,,"0|z1gxjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Mar/23 06:30;dongjoon;Issue resolved by pull request 40585
[https://github.com/apache/spark/pull/40585];;;",,,,,,,,,,
Join with subquery in condition can fail with wholestage codegen and adaptive execution disabled,SPARK-42937,13530284,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,27/Mar/23 19:45,28/Mar/23 12:41,13/Jul/23 08:43,28/Mar/23 12:41,3.3.2,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.1,,,SQL,,,0,,"The below left outer join gets an error:
{noformat}
create or replace temp view v1 as
select * from values
(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1),
(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),
(3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
as v1(key, value1, value2, value3, value4, value5, value6, value7, value8, value9, value10);

create or replace temp view v2 as
select * from values
(1, 2),
(3, 8),
(7, 9)
as v2(a, b);

create or replace temp view v3 as
select * from values
(3),
(8)
as v3(col1);

set spark.sql.codegen.maxFields=10; -- let's make maxFields 10 instead of 100
set spark.sql.adaptive.enabled=false;

select *
from v1
left outer join v2
on key = a
and key in (select col1 from v3);
{noformat}
The join fails during predicate codegen:
{noformat}
23/03/27 12:24:12 WARN Predicate: Expr codegen error and falling back to interpreter mode
java.lang.IllegalArgumentException: requirement failed: input[0, int, false] IN subquery#34 has not finished
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.sql.execution.InSubqueryExec.prepareResult(subquery.scala:144)
	at org.apache.spark.sql.execution.InSubqueryExec.doGenCode(subquery.scala:156)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:201)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:196)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressions$2(CodeGenerator.scala:1278)
	at scala.collection.immutable.List.map(List.scala:293)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:1278)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:41)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.generate(GeneratePredicate.scala:33)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:73)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:70)
	at org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:51)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.create(predicates.scala:86)
	at org.apache.spark.sql.execution.joins.HashJoin.boundCondition(HashJoin.scala:146)
	at org.apache.spark.sql.execution.joins.HashJoin.boundCondition$(HashJoin.scala:140)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.boundCondition$lzycompute(BroadcastHashJoinExec.scala:40)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.boundCondition(BroadcastHashJoinExec.scala:40)
{noformat}
It fails again after fallback to interpreter mode:
{noformat}
23/03/27 12:24:12 ERROR Executor: Exception in task 2.0 in stage 2.0 (TID 7)
java.lang.IllegalArgumentException: requirement failed: input[0, int, false] IN subquery#34 has not finished
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.sql.execution.InSubqueryExec.prepareResult(subquery.scala:144)
	at org.apache.spark.sql.execution.InSubqueryExec.eval(subquery.scala:151)
	at org.apache.spark.sql.catalyst.expressions.InterpretedPredicate.eval(predicates.scala:52)
	at org.apache.spark.sql.execution.joins.HashJoin.$anonfun$boundCondition$2(HashJoin.scala:146)
	at org.apache.spark.sql.execution.joins.HashJoin.$anonfun$boundCondition$2$adapted(HashJoin.scala:146)
	at org.apache.spark.sql.execution.joins.HashJoin.$anonfun$outerJoin$1(HashJoin.scala:205)
{noformat}
Both the predicate codegen and the evaluation fail for the same reason: {{PlanSubqueries}} creates {{InSubqueryExec}} with {{shouldBroadcast=false}}. The driver waits for the subquery to finish, but it's the executor that uses the results of the subquery (for predicate codegen or evaluation). Because {{shouldBroadcast}} is set to false, the result is stored in a transient field ({{InSubqueryExec#result}}), so the result of the subquery is not serialized when the {{InSubqueryExec}} instance is sent to the executor.

When wholestage codegen is enabled, the predicate codegen happens on the driver, so the subquery's result is available. When adaptive execution is enabled, {{PlanAdaptiveSubqueries}} always sets {{shouldBroadcast=true}}, so the subquery's result is available on the executor, if needed.
",,bersprockets,dongjoon,snoot,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 12:41:20 UTC 2023,,,,,,,,,,"0|z1gvy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Mar/23 22:46;bersprockets;PR at https://github.com/apache/spark/pull/40569;;;","28/Mar/23 03:56;snoot;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/40569;;;","28/Mar/23 12:41;dongjoon;Issue resolved by pull request 40569
[https://github.com/apache/spark/pull/40569];;;",,,,,,,,
Unresolved having at the end of analysis when using with LCA with the having clause that can be resolved directly by its child Aggregate,SPARK-42936,13530280,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xyyu,xyyu,xyyu,27/Mar/23 17:54,28/Mar/23 08:42,13/Jul/23 08:43,28/Mar/23 08:42,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"{code:java}
select sum(value1) as total_1, total_1
from values(1, 'name', 100, 50) AS data(id, name, value1, value2)
having total_1 > 0

SparkException: [INTERNAL_ERROR] Found the unresolved operator: 'UnresolvedHaving (total_1#353L > cast(0 as bigint)) {code}
To trigger the issue, the having condition need to be (can be resolved by) an attribute in the select.
Without the LCA {{{}total_1{}}}, the query works fine.",,cloud_fan,xyyu,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 08:42:20 UTC 2023,,,,,,,,,,"0|z1gvx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Mar/23 17:54;xyyu;Created PR fixing the issue: https://github.com/apache/spark/pull/40558;;;","28/Mar/23 08:42;cloud_fan;Issue resolved by pull request 40558
[https://github.com/apache/spark/pull/40558];;;",,,,,,,,,
Make resolvePersistentFunction synchronized,SPARK-42928,13530180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,27/Mar/23 05:46,28/Mar/23 08:43,13/Jul/23 08:43,28/Mar/23 08:43,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,Make resolvePersistentFunction synchronized,,allisonwang-db,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 08:43:49 UTC 2023,,,,,,,,,,"0|z1gvaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Mar/23 08:43;cloud_fan;Issue resolved by pull request 40557
[https://github.com/apache/spark/pull/40557];;;",,,,,,,,,,
"Use SecureRandom, instead of Random in security sensitive contexts",SPARK-42922,13530074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mridulm80,mridulm80,mridulm80,25/Mar/23 16:15,28/Mar/23 03:49,13/Jul/23 08:43,28/Mar/23 03:49,3.2.3,3.3.2,3.4.0,3.5.0,,,,,,,,,,,,3.3.3,3.4.1,3.5.0,,SQL,,,0,,"Most uses of Random in spark are either in test cases or where we need a pseudo random number which is repeatable.
The following are usages where moving from Random to SecureRandom would be useful

a) HttpAuthUtils.createCookieToken
b) ThriftHttpServlet.RAN",,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 03:49:10 UTC 2023,,,,,,,,,,"0|z1gunc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Mar/23 03:49;srowen;Resolved by https://github.com/apache/spark/pull/40568;;;",,,,,,,,,,
SQLQueryTestSuite test failed with `SPARK_ANSI_SQL_MODE=true`,SPARK-42921,13530047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,25/Mar/23 02:45,27/Mar/23 02:20,13/Jul/23 08:43,27/Mar/23 02:20,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,Tests,,0,,"Run 
{code:java}
SPARK_ANSI_SQL_MODE=true build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite"" {code}
{code:java}
[info] - timestampNTZ/datetime-special.sql_analyzer_test *** FAILED *** (11 milliseconds)
[info]   timestampNTZ/datetime-special.sql_analyzer_test
[info]   Expected ""...date(999999, 3, 18, [false) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, fals]e) AS make_date(-1, ..."", but got ""...date(999999, 3, 18, [true) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, tru]e) AS make_date(-1, ..."" Result did not match for query #1
[info]   select make_date(999999, 3, 18), make_date(-1, 1, 28) (SQLQueryTestSuite.scala:777)
[info]   org.scalatest.exceptions.TestFailedException: {code}",,gurwls223,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 27 02:20:13 UTC 2023,,,,,,,,,,"0|z1guhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Mar/23 02:20;gurwls223;Issue resolved by pull request 40552
[https://github.com/apache/spark/pull/40552];;;",,,,,,,,,,
Replace a starting digit with `x` in resource name prefix,SPARK-42906,13529709,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,23/Mar/23 07:16,27/Mar/23 22:35,13/Jul/23 08:43,27/Mar/23 22:32,3.2.3,3.3.2,3.4.0,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,,Kubernetes,,,0,,,,chengpan,dongjoon,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 27 22:32:14 UTC 2023,,,,,,,,,,"0|z1gse8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:04;githubbot;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/40533;;;","27/Mar/23 22:32;dongjoon;Issue resolved by pull request 40533
[https://github.com/apache/spark/pull/40533];;;",,,,,,,,,
DataFrame.to(schema) fails when it contains non-nullable nested field in nullable field,SPARK-42899,13529674,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,22/Mar/23 21:53,23/Mar/23 02:16,13/Jul/23 08:43,23/Mar/23 02:16,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,SQL,,,0,,"{{DataFrame.to(schema)}} fails when it contains non-nullable nested field in nullable field:
{code:scala}
scala> val df = spark.sql(""VALUES (1, STRUCT(1 as i)), (NULL, NULL) as t(a, b)"")
df: org.apache.spark.sql.DataFrame = [a: int, b: struct<i: int>]
scala> df.printSchema()
root
 |-- a: integer (nullable = true)
 |-- b: struct (nullable = true)
 |    |-- i: integer (nullable = false)

scala> df.to(df.schema)
org.apache.spark.sql.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `b`.`i` is nullable while it's required to be non-nullable.
{code}",,gurwls223,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 23 02:16:18 UTC 2023,,,,,,,,,,"0|z1gs6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Mar/23 02:16;gurwls223;Fixed in https://github.com/apache/spark/pull/40526;;;",,,,,,,,,,
Upgrade `kubernetes-client` to 6.5.1,SPARK-42885,13529477,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,21/Mar/23 16:14,21/Mar/23 20:53,13/Jul/23 08:43,21/Mar/23 20:53,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Build,Kubernetes,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 21 20:53:26 UTC 2023,,,,,,,,,,"0|z1gqyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Mar/23 16:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40509;;;","21/Mar/23 20:53;dongjoon;Issue resolved by pull request 40509
[https://github.com/apache/spark/pull/40509];;;",,,,,,,,,
Revert NamedLambdaVariable related changes from EquivalentExpressions,SPARK-42852,13529090,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,18/Mar/23 15:51,20/Mar/23 19:08,13/Jul/23 08:43,20/Mar/23 00:57,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,SQL,,,0,,See discussion https://github.com/apache/spark/pull/40473#issuecomment-1474848224,,apachespark,gurwls223,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 00:57:22 UTC 2023,,,,,,,,,,"0|z1gokw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Mar/23 16:26;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40475;;;","20/Mar/23 00:57;gurwls223;Fixed in https://github.com/apache/spark/pull/40475;;;",,,,,,,,,
EquivalentExpressions methods need to be consistently guarded by supportedExpression,SPARK-42851,13529035,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,18/Mar/23 01:04,21/Mar/23 13:28,13/Jul/23 08:43,21/Mar/23 13:28,3.3.2,3.4.0,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"SPARK-41468 tried to fix a bug but introduced a new regression. Its change to {{EquivalentExpressions}} added a {{supportedExpression()}} guard to the {{addExprTree()}} and {{getExprState()}} methods, but didn't add the same guard to the other ""add"" entry point -- {{addExpr()}}.

As such, uses that add single expressions to CSE via {{addExpr()}} may succeed, but upon retrieval via {{getExprState()}} it'd inconsistently get a {{None}} due to failing the guard.

We need to make sure the ""add"" and ""get"" methods are consistent. It could be done by one of:
1. Adding the same {{supportedExpression()}} guard to {{addExpr()}}, or
2. Removing the guard from {{getExprState()}}, relying solely on the guard on the ""add"" path to make sure only intended state is added.
(or other alternative refactorings to fuse the guard into various methods to make it more efficient)

There are pros and cons to the two directions above, because {{addExpr()}} used to allow (potentially incorrect) more expressions to get CSE'd, making it more restrictive may cause performance regressions (for the cases that happened to work).

Example:
{code:sql}
select max(transform(array(id), x -> x)), max(transform(array(id), x -> x)) from range(2)
{code}

Running this query on Spark 3.2 branch returns the correct value:
{code}
scala> spark.sql(""select max(transform(array(id), x -> x)), max(transform(array(id), x -> x)) from range(2)"").collect
res0: Array[org.apache.spark.sql.Row] = Array([WrappedArray(1),WrappedArray(1)])
{code}
Here, {{transform(array(id), x -> x)}} is an {{AggregateExpression}} that was (potentially unsafely) recognized by {{addExpr()}} as a common subexpression, and {{getExprState()}} doesn't do extra guarding, so during physical planning, in {{PhysicalAggregation}} this expression gets CSE'd in both the aggregation expression list and the result expressions list.
{code}
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[], functions=[max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=11]
      +- SortAggregate(key=[], functions=[partial_max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))])
         +- Range (0, 2, step=1, splits=16)
{code}

Running the same query on current master triggers an error when binding the result expression to the aggregate expression in the Aggregate operators (for a WSCG-enabled operator like {{HashAggregateExec}}, the same error would show up during codegen):
{code}
ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 16) (ip-10-110-16-93.us-west-2.compute.internal executor driver): java.lang.IllegalStateException: Couldn't find max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))#4 in [max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))#3]
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:532)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:456)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)
	at scala.collection.immutable.List.map(List.scala:297)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:161)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.generateResultProjection(AggregationIterator.scala:246)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.<init>(AggregationIterator.scala:296)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:49)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:79)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)
...
{code}
Note that the aggregate expressions are deduplicated in {{PhysicalAggregation}}, but the result expressions were unable to deduplicate consistently due to the bug mentioned in this ticket.
{code}
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[], functions=[max(transform(array(id#15L), lambdafunction(lambda x#16L, lambda x#16L, false)))])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=38]
      +- SortAggregate(key=[], functions=[partial_max(transform(array(id#15L), lambdafunction(lambda x#16L, lambda x#16L, false)))])
         +- Range (0, 2, step=1, splits=16)
{code}

Fixing it via method 1 is more correct than method 2 in terms of avoiding incorrect CSE:
{code:diff}
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala
index 330d66a21b..12def60042 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala
@@ -40,7 +40,11 @@ class EquivalentExpressions {
    * Returns true if there was already a matching expression.
    */
   def addExpr(expr: Expression): Boolean = {
-    updateExprInMap(expr, equivalenceMap)
+    if (supportedExpression(expr)) {
+      updateExprInMap(expr, equivalenceMap)
+    } else {
+      false
+    }
   }
 
   /**
{code}
the query runs correctly again, but this time the aggregate expression is NOT CSE'd anymore, done consistently for both aggregate expressions and result expressions:
{code}
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[], functions=[max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false))), max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=11]
      +- SortAggregate(key=[], functions=[partial_max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false))), partial_max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))])
         +- Range (0, 2, step=1, splits=16)
{code}
and for this particular case, the CSE that used to take place was actually okay, so losing CSE here means performance regression.",,apachespark,cloud_fan,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 21 13:28:44 UTC 2023,,,,,,,,,,"0|z1go8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Mar/23 01:25;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/40473;;;","20/Mar/23 12:34;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40488;;;","20/Mar/23 12:35;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40488;;;","21/Mar/23 13:28;cloud_fan;Issue resolved by pull request 40473
[https://github.com/apache/spark/pull/40473];;;",,,,,,,
PySpark type hint returns Any for methods on GroupedData,SPARK-42828,13528832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,apachespark,j03wang,j03wang,16/Mar/23 16:44,03/Jul/23 06:38,13/Jul/23 08:43,03/Jul/23 06:38,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,3.5.0,,,,PySpark,,,0,,"Since upgrading to PySpark 3.3.x, type hints for
{code:java}
df.groupBy(...).count(){code}
are now returning Any instead of DataFrame, causing type inference issues downstream. This used to be correctly typed prior to 3.3.x.",,apachespark,gurwls223,j03wang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 06:38:11 UTC 2023,,,,,,,,,,"0|z1gmzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Mar/23 19:20;apachespark;User 'j03wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40460;;;","03/Jul/23 06:38;gurwls223;Issue resolved by pull request 40460
[https://github.com/apache/spark/pull/40460];;;",,,,,,,,,
Update ORC to 1.8.3,SPARK-42820,13528715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,16/Mar/23 02:38,16/Mar/23 05:01,13/Jul/23 08:43,16/Mar/23 05:00,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Build,,,0,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 05:00:44 UTC 2023,,,,,,,,,,"0|z1gm9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Mar/23 02:43;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40453;;;","16/Mar/23 02:44;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40453;;;","16/Mar/23 05:00;dongjoon;Issue resolved by pull request 40453
[https://github.com/apache/spark/pull/40453];;;",,,,,,,,
Spark driver logs are filled with Initializing service data for shuffle service using name,SPARK-42817,13528669,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csingh,csingh,csingh,15/Mar/23 16:53,20/Mar/23 19:10,13/Jul/23 08:43,16/Mar/23 21:27,3.2.0,,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,0,,"With SPARK-34828, we added the ability to make the shuffle service name configurable and we added a log [here|https://github.com/apache/spark/blob/8860f69455e5a722626194c4797b4b42cccd4510/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala#L118] that will log the shuffle service name. However, this log is printed in the driver logs whenever there is new executor launched and pollutes the log. 
{code}
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'
{code}
We can just log this once in the driver.",,apachespark,csingh,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 21:27:49 UTC 2023,,,,,,,,,,"0|z1glzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 18:09;csingh;Created PR https://github.com/apache/spark/pull/40448;;;","15/Mar/23 18:20;apachespark;User 'otterc' has created a pull request for this issue:
https://github.com/apache/spark/pull/40448;;;","16/Mar/23 21:27;dongjoon;Issue resolved by pull request 40448
[https://github.com/apache/spark/pull/40448];;;",,,,,,,,
Print application info when waitAppCompletion is false,SPARK-42813,13528641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,15/Mar/23 14:02,21/Mar/23 16:08,13/Jul/23 08:43,21/Mar/23 16:08,3.3.2,,,,,,,,,,,,,,,3.5.0,,,,Kubernetes,,,0,,,,apachespark,chengpan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 21 16:08:51 UTC 2023,,,,,,,,,,"0|z1glt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 14:19;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/40444;;;","15/Mar/23 14:20;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/40444;;;","21/Mar/23 16:08;dongjoon;Issue resolved by pull request 40444
[https://github.com/apache/spark/pull/40444];;;",,,,,,,,
client_type is missing from AddArtifactsRequest proto message,SPARK-42812,13528635,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,15/Mar/23 13:26,20/Mar/23 19:10,13/Jul/23 08:43,20/Mar/23 19:10,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Connect,,,0,,The client_type is missing from AddArtifactsRequest proto message,,apachespark,vicennial,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 13:33:33 UTC 2023,,,,,,,,,,"0|z1glrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 13:32;apachespark;User 'vicennial' has created a pull request for this issue:
https://github.com/apache/spark/pull/40443;;;","15/Mar/23 13:33;apachespark;User 'vicennial' has created a pull request for this issue:
https://github.com/apache/spark/pull/40443;;;",,,,,,,,,
Fix Flaky ClientE2ETestSuite,SPARK-42801,13528561,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,15/Mar/23 05:50,15/Mar/23 07:43,13/Jul/23 08:43,15/Mar/23 06:28,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Connect,Tests,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 06:28:14 UTC 2023,,,,,,,,,,"0|z1glbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 06:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40434;;;","15/Mar/23 06:28;dongjoon;Issue resolved by pull request 40434
[https://github.com/apache/spark/pull/40434];;;",,,,,,,,,
Update SBT build `xercesImpl` version to match with pom.xml,SPARK-42799,13528551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,15/Mar/23 03:32,15/Mar/23 07:43,13/Jul/23 08:43,15/Mar/23 07:42,3.2.2,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,,,,,3.2.4,3.3.3,3.4.1,,Build,,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,SPARK-39183,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 07:42:32 UTC 2023,,,,,,,,,,"0|z1gl94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 03:34;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40431;;;","15/Mar/23 07:42;dongjoon;Issue resolved by pull request 40431
[https://github.com/apache/spark/pull/40431];;;",,,,,,,,,
`connect` module requires `build_profile_flags`,SPARK-42793,13528513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/Mar/23 22:04,15/Mar/23 07:43,13/Jul/23 08:43,14/Mar/23 23:56,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Connect,,,0,,,,apachespark,dongjoon,gurwls223,,,,,,,,,,,,,,,SPARK-42656,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 23:56:24 UTC 2023,,,,,,,,,,"0|z1gl0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Mar/23 22:06;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40424;;;","14/Mar/23 22:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40424;;;","14/Mar/23 23:56;gurwls223;Issue resolved by pull request 40424
[https://github.com/apache/spark/pull/40424];;;",,,,,,,,
"[K8S][Core] When spark submit without --deploy-mode, will face NPE in Kubernetes Case",SPARK-42785,13528391,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zWangSheng,zWangSheng,zWangSheng,14/Mar/23 09:49,14/Mar/23 15:51,13/Jul/23 08:43,14/Mar/23 15:50,3.2.4,3.3.3,3.4.0,,,,,,,,,,,,,3.2.4,3.3.3,3.4.1,,Kubernetes,,,0,,"According to this PR [https://github.com/apache/spark/pull/37880#issuecomment-1347777890,] when user spark submit without `--deploy-mode XXX` or `–conf spark.submit.deployMode=XXXX`, may face NPE with this code

 
args.deployMode.equals(""client"")
 
 ",,apachespark,dongjoon,zWangSheng,,,,,,,,,,,,,,,SPARK-39399,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 15:50:11 UTC 2023,,,,,,,,,,"0|z1gk9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Mar/23 11:07;apachespark;User 'zwangsheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/40414;;;","14/Mar/23 15:50;dongjoon;Issue resolved by pull request 40414
[https://github.com/apache/spark/pull/40414];;;",,,,,,,,,
Fix the problem of incomplete creation of subdirectories in push merged localDir,SPARK-42784,13528377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,StoveM,StoveM,StoveM,14/Mar/23 08:50,01/Jul/23 03:52,13/Jul/23 08:43,01/Jul/23 03:52,3.3.2,,,,,,,,,,,,,,,3.3.3,3.4.2,3.5.0,,Shuffle,Spark Core,,0,,"After we massively enabled push-based shuffle in our production environment, we found some warn messages appearing in the server-side log messages.

the warning log like:

ShuffleBlockPusher: Pushing block shufflePush_3_0_5352_935 to BlockManagerId(shuffle-push-merger, zw06-data-hdp-dn08251.mt, 7337, None) failed.
java.lang.RuntimeException: java.lang.RuntimeException: Cannot initialize merged shuffle partition for appId application_1671244879475_44020960 shuffleId 3 shuffleMergeId 0 reduceId 935.

After investigation, we identified the triggering mechanism of the bug。

The driver requested two different containers on the same physical machine. During the creation of the 'push-merged' directory in the first container (container_1), the mergeDir was created first, then the subDir were created based on the value of the ""spark.diskStore.subDirectories"" parameter. However, the resources of container_1 were preempted during the creation of the sub-directories, resulting in subDir not being created (only part of it was created ). As the mergeDir still existed, the second container (container_2) was unable to create further subDir (as it assumed that all directories had already been created).

 ",,apachespark,lyee,StoveM,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 09:26:34 UTC 2023,,,,,,,,,,"0|z1gk6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Mar/23 09:26;apachespark;User 'Stove-hust' has created a pull request for this issue:
https://github.com/apache/spark/pull/40412;;;",,,,,,,,,,
SQLImplicitsTestSuite test failed with Java 17,SPARK-42770,13528185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,13/Mar/23 07:18,14/Mar/23 15:53,13/Jul/23 08:43,14/Mar/23 15:53,3.4.0,3.5.0,,,,,,,,,,,,,,3.4.1,,,,Connect,Tests,,0,,"[https://github.com/apache/spark/actions/runs/4318647315/jobs/7537203682]
{code:java}
[info] - test implicit encoder resolution *** FAILED *** (1 second, 329 milliseconds)
4429[info]   2023-03-02T23:00:20.404434 did not equal 2023-03-02T23:00:20.404434875 (SQLImplicitsTestSuite.scala:63)
4430[info]   org.scalatest.exceptions.TestFailedException:
4431[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
4432[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
4433[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
4434[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
4435[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.testImplicit$1(SQLImplicitsTestSuite.scala:63)
4436[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.$anonfun$new$2(SQLImplicitsTestSuite.scala:133)
4437[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
4438[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
4439[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
4440[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
4441[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
4442[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
4443[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
4444[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
4445[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
4446[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
4447[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
4448[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
4449[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
4450[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
4451[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
4452[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)
4453[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
4454[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
4455[info]   at scala.collection.immutable.List.foreach(List.scala:431)
4456[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
4457[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
4458[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
4459[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
4460[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
4461[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
4462[info]   at org.scalatest.Suite.run(Suite.scala:1114)
4463[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
4464[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
4465[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
4466[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
4467[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
4468[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
4469[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.org$scalatest$BeforeAndAfterAll$$super$run(SQLImplicitsTestSuite.scala:34)
4470[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
4471[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
4472[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
4473[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.run(SQLImplicitsTestSuite.scala:34)
4474[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
4475[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
4476[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
4477[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
4478[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
4479[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
4480[info]   at java.base/java.lang.Thread.run(Thread.java:833) {code}",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 15:53:33 UTC 2023,,,,,,,,,,"0|z1gizs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Mar/23 07:19;LuciferYang;Maybe it can only be reproduced on Linux

 ;;;","13/Mar/23 10:11;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40395;;;","14/Mar/23 15:53;dongjoon;Issue resolved by pull request 40395
[https://github.com/apache/spark/pull/40395];;;",,,,,,,,
CAST(x as int) does not generate error with overflow,SPARK-42749,13528009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tjomme,tjomme,10/Mar/23 16:27,13/Mar/23 08:00,13/Jul/23 08:43,13/Mar/23 08:00,3.2.1,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,,,,SQL,,,0,,"Hi,

When performing the following code:

{{select cast(7.415246799222789E19 as int)}}

according to the documentation, an error is expected as {{7.415246799222789E19 }}is an overflow value for datatype INT.

However, the value 2147483647 is returned. 

The behaviour of the following is correct as it returns NULL:

{{select try_cast(7.415246799222789E19 as int) }}

This results in unexpected behaviour and data corruption.","It was tested on a DataBricks environment with DBR 10.4 and above, running Spark v3.2.1 and above.",tjomme,yumwang,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 16:30;tjomme;Spark-42749.PNG;https://issues.apache.org/jira/secure/attachment/13056240/Spark-42749.PNG",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 08:00:47 UTC 2023,,,,,,,,,,"0|z1ghwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 23:36;yumwang;Please enable ansi:
{code:sql}
spark-sql (default)> set spark.sql.ansi.enabled=true;
spark.sql.ansi.enabled	true
Time taken: 0.088 seconds, Fetched 1 row(s)
spark-sql (default)> select cast(7.415246799222789E19 as int);
[CAST_OVERFLOW] The value 7.415246799222789E19D of the type ""DOUBLE"" cannot be cast to ""INT"" due to an overflow. Use `try_cast` to tolerate overflow and return NULL instead. If necessary set ""spark.sql.ansi.enabled"" to ""false"" to bypass this error.
org.apache.spark.SparkArithmeticException: [CAST_OVERFLOW] The value 7.415246799222789E19D of the type ""DOUBLE"" cannot be cast to ""INT"" due to an overflow. Use `try_cast` to tolerate overflow and return NULL instead. If necessary set ""spark.sql.ansi.enabled"" to ""false"" to bypass this error.
{code};;;","13/Mar/23 07:44;tjomme;Hi,

This does indeed solve the problem. Setting the parameter makes it behave as intended.

Can this be noted in the documentation that this is a requirement?

Thanks,

Tjomme;;;","13/Mar/23 07:59;tjomme;Just checked the documentation again: the warning aparently was recently added;;;","13/Mar/23 08:00;tjomme;Additional settings required to get the intended behaviour.

Documentation is up-to-date;;;",,,,,,,
Fix incorrect internal status of LoR and AFT,SPARK-42747,13527973,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,10/Mar/23 12:58,11/Mar/23 14:47,13/Jul/23 08:43,11/Mar/23 14:47,3.1.0,3.2.0,3.3.0,3.4.0,,,,,,,,,,,,3.2.4,3.3.2,3.4.1,3.5.0,ML,PySpark,,0,,"LoR and AFT applied internal status to optimize prediction/transform, but the status is not correctly updated in some case:


{code:java}
from pyspark.sql import Row
from pyspark.ml.classification import *
from pyspark.ml.linalg import Vectors

df = spark.createDataFrame(
    [
        (1.0, 1.0, Vectors.dense(0.0, 5.0)),
        (0.0, 2.0, Vectors.dense(1.0, 2.0)),
        (1.0, 3.0, Vectors.dense(2.0, 1.0)),
        (0.0, 4.0, Vectors.dense(3.0, 3.0)),
    ],
    [""label"", ""weight"", ""features""],
)

lor = LogisticRegression(weightCol=""weight"")
model = lor.fit(df)

# status changes 1
for t in [0.0, 0.1, 0.2, 0.5, 1.0]:
    model.setThreshold(t).transform(df)

# status changes 2
[model.setThreshold(t).predict(Vectors.dense(0.0, 5.0)) for t in [0.0, 0.1, 0.2, 0.5, 1.0]]

for t in [0.0, 0.1, 0.2, 0.5, 1.0]:
    print(t)
    model.setThreshold(t).transform(df).show()                                        #  <- error results
{code}


results:

{code:java}
0.0
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

0.1
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

0.2
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

0.5
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

1.0
+-----+------+---------+--------------------+--------------------+----------+
|label|weight| features|       rawPrediction|         probability|prediction|
+-----+------+---------+--------------------+--------------------+----------+
|  1.0|   1.0|[0.0,5.0]|[0.10932013376341...|[0.52730284774069...|       0.0|
|  0.0|   2.0|[1.0,2.0]|[-0.8619624039359...|[0.29692950635762...|       0.0|
|  1.0|   3.0|[2.0,1.0]|[-0.3634508721860...|[0.41012446452385...|       0.0|
|  0.0|   4.0|[3.0,3.0]|[2.33975176373760...|[0.91211618852612...|       0.0|
+-----+------+---------+--------------------+--------------------+----------+

{code}
",,apachespark,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 11 14:47:06 UTC 2023,,,,,,,,,,"0|z1ghoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 13:30;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/40367;;;","11/Mar/23 14:47;srowen;Resolved by https://github.com/apache/spark/pull/40367;;;",,,,,,,,,
Improved AliasAwareOutputExpression works with DSv2,SPARK-42745,13527953,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,10/Mar/23 09:16,14/Mar/23 15:41,13/Jul/23 08:43,10/Mar/23 12:59,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,SQL,,,0,,"After SPARK-40086 / SPARK-42049 the following, simple subselect expression containing query:
{noformat}
select (select sum(id) from t1)
{noformat}
fails with:

{noformat}
09:48:57.645 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.NullPointerException
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.batch$lzycompute(BatchScanExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.batch(BatchScanExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.hashCode(BatchScanExec.scala:60)
	at scala.runtime.Statics.anyHash(Statics.java:122)
        ...
	at org.apache.spark.sql.catalyst.trees.TreeNode.hashCode(TreeNode.scala:249)
	at scala.runtime.Statics.anyHash(Statics.java:122)
	at scala.collection.mutable.HashTable$HashUtils.elemHashCode(HashTable.scala:416)
	at scala.collection.mutable.HashTable$HashUtils.elemHashCode$(HashTable.scala:416)
	at scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:44)
	at scala.collection.mutable.HashTable.addEntry(HashTable.scala:149)
	at scala.collection.mutable.HashTable.addEntry$(HashTable.scala:148)
	at scala.collection.mutable.HashMap.addEntry(HashMap.scala:44)
	at scala.collection.mutable.HashTable.init(HashTable.scala:110)
	at scala.collection.mutable.HashTable.init$(HashTable.scala:89)
	at scala.collection.mutable.HashMap.init(HashMap.scala:44)
	at scala.collection.mutable.HashMap.readObject(HashMap.scala:195)
        ...
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:85)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
{noformat}
when DSv2 is enabled.",,apachespark,cloud_fan,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 12:59:13 UTC 2023,,,,,,,,,,"0|z1ghk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 09:58;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/40364;;;","10/Mar/23 12:59;cloud_fan;Issue resolved by pull request 40364
[https://github.com/apache/spark/pull/40364];;;",,,,,,,,,
Do not rely on __file__,SPARK-42709,13527576,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,08/Mar/23 05:09,08/Mar/23 18:00,13/Jul/23 08:43,08/Mar/23 18:00,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,PySpark,,,0,,We have a lot of places using __file__ which is actually optional. We shouldn't reply on them,,apachespark,dongjoon,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 18:00:01 UTC 2023,,,,,,,,,,"0|z1gf8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Mar/23 05:52;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/40328;;;","08/Mar/23 18:00;dongjoon;Issue resolved by pull request 40328
[https://github.com/apache/spark/pull/40328];;;",,,,,,,,,
Add h2 as test dependency of connect-server module,SPARK-42700,13527451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,07/Mar/23 12:40,08/Mar/23 04:46,13/Jul/23 08:43,08/Mar/23 04:46,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Build,Tests,,0,,"run 
 # mvn clean install -DskipTests -pl connector/connect/server -am
 # mvn test -pl connector/connect/server

{code:java}
*** RUN ABORTED ***
  java.lang.ClassNotFoundException: org.h2.Driver
  at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)
  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
  at java.base/java.lang.Class.forName0(Native Method)
  at java.base/java.lang.Class.forName(Class.java:398)
  at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
  at org.apache.spark.sql.connect.ProtoToParsedPlanTestSuite.beforeAll(ProtoToParsedPlanTestSuite.scala:68)
  at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)
  at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
  at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
  ...
 {code}",,apachespark,gurwls223,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 04:46:50 UTC 2023,,,,,,,,,,"0|z1gego:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Mar/23 12:56;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40317;;;","08/Mar/23 04:46;gurwls223;Issue resolved by pull request 40317
[https://github.com/apache/spark/pull/40317];;;",,,,,,,,,
/api/v1/applications return 0 for duration,SPARK-42697,13527432,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,07/Mar/23 10:12,09/Mar/23 05:35,13/Jul/23 08:43,09/Mar/23 05:35,3.1.3,3.2.3,3.3.2,3.4.0,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,Web UI,,,0,,which should be total uptime,,apachespark,gurwls223,Qin Yao,yao,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 05:35:14 UTC 2023,,,,,,,,,,"0|z1gecg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"07/Mar/23 10:50;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40313;;;","09/Mar/23 05:35;Qin Yao;Issue resolved by pull request 40313
[https://github.com/apache/spark/pull/40313];;;",,,,,,,,,
"Relax ordering constraint for ALTER TABLE ADD|REPLACE column options",SPARK-42681,13527202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vli-databricks,vli-databricks,vli-databricks,06/Mar/23 08:36,08/Mar/23 04:04,13/Jul/23 08:43,08/Mar/23 04:04,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"Currently the grammar for ALTER TABLE ADD|REPLACE column is:

qualifiedColTypeWithPosition
    : name=multipartIdentifier dataType (NOT NULL)? defaultExpression? commentSpec? colPosition?
    ;

This enforces a constraint on the order of: (NOT NULL, DEFAULT value, COMMENT value FIRST|AFTER value). We can update the grammar to allow these options in any order instead, to improve usability.",,apachespark,Gengliang.Wang,vli-databricks,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 04:04:59 UTC 2023,,,,,,,,,,"0|z1gcxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Mar/23 08:43;apachespark;User 'vitaliili-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/40295;;;","08/Mar/23 04:04;Gengliang.Wang;Issue resolved by pull request 40295
[https://github.com/apache/spark/pull/40295];;;",,,,,,,,,
Fix the invalid tests for broadcast hint,SPARK-42677,13527168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,06/Mar/23 04:05,06/Mar/23 08:17,13/Jul/23 08:43,06/Mar/23 08:17,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"Currently, there are a lot of test cases for broadcast hint is invalid. Because the data size is smaller than broadcast threshold.",,apachespark,beliefer,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 08:17:42 UTC 2023,,,,,,,,,,"0|z1gcq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Mar/23 04:12;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/40293;;;","06/Mar/23 04:13;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/40293;;;","06/Mar/23 08:17;podongfeng;Issue resolved by pull request 40293
[https://github.com/apache/spark/pull/40293];;;",,,,,,,,
Make build/mvn build Spark only with the verified maven version,SPARK-42673,13527115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,05/Mar/23 08:21,06/Mar/23 19:07,13/Jul/23 08:43,06/Mar/23 19:06,3.2.4,3.3.3,3.4.0,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,Build,,,0,,GA ,,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,SPARK-42380,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 19:06:37 UTC 2023,,,,,,,,,,"0|z1gce8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Mar/23 08:24;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40283;;;","06/Mar/23 19:06;dongjoon;Issue resolved by pull request 40283
[https://github.com/apache/spark/pull/40283];;;",,,,,,,,,
Fix bug for createDataFrame from complex type schema,SPARK-42671,13527102,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,05/Mar/23 02:21,15/Mar/23 21:31,13/Jul/23 08:43,06/Mar/23 02:08,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Connect,,,0,,,,apachespark,panbingkun,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 05 02:43:19 UTC 2023,,,,,,,,,,"0|z1gcbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Mar/23 02:42;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40280;;;","05/Mar/23 02:43;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40280;;;",,,,,,,,,
`simple udf` test failed using Maven ,SPARK-42665,13527027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,03/Mar/23 17:41,07/Mar/23 07:36,13/Jul/23 08:43,07/Mar/23 07:36,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Connect,,,0,,"{code:java}
simple udf *** FAILED ***
  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.ClientE2ETestSuite
  at io.grpc.Status.asRuntimeException(Status.java:535)
  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  at org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:61)
  at org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:106)
  at org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:123)
  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2426)
  at org.apache.spark.sql.Dataset.withResult(Dataset.scala:2747)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2425)
  at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$8(ClientE2ETestSuite.scala:85)
  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) {code}",,apachespark,gurwls223,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 07:36:37 UTC 2023,,,,,,,,,,"0|z1gbuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Mar/23 16:53;apachespark;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/40304;;;","06/Mar/23 16:53;apachespark;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/40304;;;","06/Mar/23 16:54;apachespark;User 'zhenlineo' has created a pull request for this issue:
https://github.com/apache/spark/pull/40304;;;","07/Mar/23 07:36;gurwls223;Issue resolved by pull request 40304
[https://github.com/apache/spark/pull/40304];;;",,,,,,,
Incorrect ambiguous column reference error,SPARK-42655,13526898,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,unamesk15,unamesk15,unamesk15,02/Mar/23 18:43,04/Apr/23 13:17,13/Jul/23 08:43,04/Apr/23 13:16,3.2.0,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,0,,"val df1 = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(""id"",""col2"",""col3"",""col4"", ""col5"")
val op_cols_same_case = List(""id"",""col2"",""col3"",""col4"", ""col5"", ""id"")
val df2 = df1.select(op_cols_same_case.head, op_cols_same_case.tail: _*)
df2.select(""id"").show()
 
This query runs fine.
 
But when we change the casing of the op_cols to have mix of upper & lower case (""id"" & ""ID"") it throws an ambiguous col ref error:
 
val df1 = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(""id"",""col2"",""col3"",""col4"", ""col5"")
val op_cols_mixed_case = List(""id"",""col2"",""col3"",""col4"", ""col5"", ""ID"")
val df3 = df1.select(op_cols_mixed_case.head, op_cols_mixed_case.tail: _*)
df3.select(""id"").show()



org.apache.spark.sql.AnalysisException: Reference 'id' is ambiguous, could be: id, id.

  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:363)

  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:112)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$resolveExpressionByPlanChildren$1(Analyzer.scala:1857)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$resolveExpression$2(Analyzer.scala:1787)

  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:60)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.innerResolve$1(Analyzer.scala:1794)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpression(Analyzer.scala:1812)

  at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpressionByPlanChildren(Analyzer.scala:1863)

  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$17.$anonfun$applyOrElse$94(Analyzer.scala:1577)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)

  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:209)

  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)

  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

  at scala.collection.TraversableLike.map(TraversableLike.scala:286)

  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)

  at scala.collection.AbstractTraversable.map(Traversable.scala:108)

  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:209)

 


Since, Spark is case insensitive, it should work for second case also when we have upper and lower case column names in the column list.

It also works fine in Spark 2.3.
 ",,apachespark,cloud_fan,unamesk15,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 04 13:16:42 UTC 2023,,,,,,,,,,"0|z1gb28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Mar/23 20:19;apachespark;User 'shrprasa' has created a pull request for this issue:
https://github.com/apache/spark/pull/40258;;;","04/Apr/23 13:16;cloud_fan;Issue resolved by pull request 40258
[https://github.com/apache/spark/pull/40258];;;",,,,,,,,,
Remove the standard Apache License header from the top of third-party source files,SPARK-42649,13526815,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,02/Mar/23 08:12,02/Mar/23 09:05,13/Jul/23 08:43,02/Mar/23 09:05,1.1.1,1.2.2,1.3.1,1.4.1,1.5.2,1.6.3,2.1.3,2.2.3,2.3.4,2.4.8,3.0.3,3.1.3,3.2.3,3.3.2,3.4.0,3.2.4,3.3.3,3.4.1,,Spark Core,,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 09:05:59 UTC 2023,,,,,,,,,,"0|z1gajs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Mar/23 08:33;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40249;;;","02/Mar/23 09:05;dongjoon;Issue resolved by pull request 40249
[https://github.com/apache/spark/pull/40249];;;",,,,,,,,,
Add `hive` dependency to `connect` module,SPARK-42644,13526797,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,02/Mar/23 04:49,02/Mar/23 06:46,13/Jul/23 08:43,02/Mar/23 06:46,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Project Infra,,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 06:46:45 UTC 2023,,,,,,,,,,"0|z1gafs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"02/Mar/23 05:31;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40246;;;","02/Mar/23 06:46;dongjoon;Issue resolved by pull request 40246
[https://github.com/apache/spark/pull/40246];;;",,,,,,,,,
Several counter-intuitive behaviours in the TimestampAdd expression,SPARK-42635,13526728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mashplant,mashplant,mashplant,01/Mar/23 18:16,04/Mar/23 19:16,13/Jul/23 08:43,03/Mar/23 06:39,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,3.3.3,3.4.1,,,SQL,,,0,,"# When the time is close to daylight saving time transition, the result may be discontinuous and not monotonic.

We currently have:
{code:scala}
scala> spark.conf.set(""spark.sql.session.timeZone"", ""America/Los_Angeles"")
scala> spark.sql(""select timestampadd(second, 24 * 3600 - 1, timestamp'2011-03-12 03:00:00')"").show
+------------------------------------------------------------------------+
|timestampadd(second, ((24 * 3600) - 1), TIMESTAMP '2011-03-12 03:00:00')|
+------------------------------------------------------------------------+
|                                                     2011-03-13 03:59:59|
+------------------------------------------------------------------------+
scala> spark.sql(""select timestampadd(second, 24 * 3600, timestamp'2011-03-12 03:00:00')"").show
+------------------------------------------------------------------+
|timestampadd(second, (24 * 3600), TIMESTAMP '2011-03-12 03:00:00')|
+------------------------------------------------------------------+
|                                               2011-03-13 03:00:00|
+------------------------------------------------------------------+ {code}
 

In the second query, adding one more second will set the time back one hour instead. Plus, there are only {{23 * 3600}} seconds from {{2011-03-12 03:00:00}} to {{2011-03-13 03:00:00}}, instead of {{24 * 3600}} seconds, due to the daylight saving time transition.

The root cause of the problem is the Spark code at [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L790] wrongly assumes every day has {{MICROS_PER_DAY}} seconds, and does the day and time-in-day split before looking at the timezone.

2. Adding month, quarter, and year silently ignores Int overflow during unit conversion.

The root cause is [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L1246]. {{quantity}} is multiplied by {{3}} or {{MONTHS_PER_YEAR}} without checking overflow. Note that we do have overflow checking in adding the amount to the timestamp, so the behavior is inconsistent.

This can cause counter-intuitive results like this:

{code:scala}
scala> spark.sql(""select timestampadd(quarter, 1431655764, timestamp'1970-01-01')"").show
+------------------------------------------------------------------+
|timestampadd(quarter, 1431655764, TIMESTAMP '1970-01-01 00:00:00')|
+------------------------------------------------------------------+
|                                               1969-09-01 00:00:00|
+------------------------------------------------------------------+{code}

3. Adding sub-month units (week, day, hour, minute, second, millisecond, microsecond)silently ignores Long overflow during unit conversion.

This is similar to the previous problem:

{code:scala}
 scala> spark.sql(""select timestampadd(day, 106751992, timestamp'1970-01-01')"").show(false)
+-------------------------------------------------------------+
|timestampadd(day, 106751992, TIMESTAMP '1970-01-01 00:00:00')|
+-------------------------------------------------------------+
|-290308-12-22 15:58:10.448384                                |
+-------------------------------------------------------------+{code}

 ",,apachespark,mashplant,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 06:53:04 UTC 2023,,,,,,,,,,"0|z1ga0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Mar/23 19:30;apachespark;User 'chenhao-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/40237;;;","03/Mar/23 06:39;maxgekk;Issue resolved by pull request 40237
[https://github.com/apache/spark/pull/40237];;;","03/Mar/23 06:53;apachespark;User 'chenhao-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/40264;;;",,,,,,,,
Several counter-intuitive behaviours in the TimestampAdd expression,SPARK-42634,13526727,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,mashplant,mashplant,01/Mar/23 18:13,01/Mar/23 18:54,13/Jul/23 08:43,01/Mar/23 18:52,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,,,,,,Spark Core,SQL,,0,,,,mashplant,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 18:53:52 UTC 2023,,,,,,,,,,"0|z1ga08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Mar/23 18:52;mashplant;duplicate;;;","01/Mar/23 18:53;mashplant;This is a duplicate, created by mistake.;;;",,,,,,,,,
Upgrade zstd-jni to 1.5.4-2,SPARK-42625,13526582,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,28/Feb/23 21:54,01/Mar/23 01:17,13/Jul/23 08:43,01/Mar/23 01:17,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Build,,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 01:17:27 UTC 2023,,,,,,,,,,"0|z1g940:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 21:55;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40225;;;","01/Mar/23 01:17;dongjoon;Issue resolved by pull request 40225
[https://github.com/apache/spark/pull/40225];;;",,,,,,,,,
parameter markers not blocked in DDL,SPARK-42623,13526563,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,srielau,srielau,28/Feb/23 17:34,14/Mar/23 15:42,13/Jul/23 08:43,10/Mar/23 02:35,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Spark Core,,,0,,"The parameterized query code does not block DDL statements from referencing parameter markers.
E.g. a 

 
{code:java}
scala> spark.sql(sqlText = ""CREATE VIEW v1 AS SELECT current_timestamp() + :later as stamp, :x * :x AS square"", args = Map(""later"" -> ""INTERVAL'3' HOUR"", ""x"" -> ""15.0"")).show()
++
||
++
++
{code}
It appears we have some protection that fails us when the view is invoked:

 
{code:java}
scala> spark.sql(sqlText = ""SELECT * FROM v1"", args = Map(""later"" -> ""INTERVAL'3' HOUR"", ""x"" -> ""15.0"")).show()
org.apache.spark.sql.AnalysisException: [UNBOUND_SQL_PARAMETER] Found the unbound parameter: `later`. Please, fix `args` and provide a mapping of the parameter to a SQL literal.; line 1 pos 29
{code}

Right now I think affected are:
* DEFAULT definition
* VIEW definition

but any other future standard expression popping up is at risk, such as SQL Functions, or GENERATED COLUMN.

CREATE TABLE AS is debatable, since it it executes the query at definition only.
For simplicity I propose to block the feature from ANY DDL statement (CREATE, ALTER).

 

 ",,apachespark,cloud_fan,gurwls223,srielau,Zing,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 02:35:51 UTC 2023,,,,,,,,,,"0|z1g8zs:",9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,,"08/Mar/23 04:32;cloud_fan;I think the problem occurs when we store the original SQL text, like CREATE VIEW, or GENERATED COLUMN. A proper fix should be replacing the placeholder in the original SQL text with the actual parameters, before storing them. But this would be a bit hard to implement. Given that the main motivation of SQL parameters is protecting from SQL injection, I think applying it in SELECT query should be sufficient.;;;","08/Mar/23 09:15;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40333;;;","10/Mar/23 01:50;gurwls223;Reverted in https://github.com/apache/spark/commit/b36966f6588f92548f9edad73ffd1f5795503780 and https://github.com/apache/spark/commit/12c7e75f119f6d49ce3608da6b2a34f71c0fa0aa;;;","10/Mar/23 02:35;cloud_fan;Issue resolved by pull request 40333
[https://github.com/apache/spark/pull/40333];;;",,,,,,,
StackOverflowError reading json that does not conform to schema,SPARK-42622,13526546,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jelmer1,jelmer1,jelmer1,28/Feb/23 15:11,02/Mar/23 14:44,13/Jul/23 08:43,02/Mar/23 14:44,3.4.0,,,,,,,,,,,,,,,3.4.1,,,,Input/Output,,,0,,"Databricks runtime 12.1 uses a pre-release version of spark 3.4.x we encountered the following problem

 

!https://user-images.githubusercontent.com/133639/221866500-99f187a0-8db3-42a7-85ca-b027fdec160d.png!",,apachespark,jelmer1,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,,Thu Mar 02 14:44:10 UTC 2023,,,,,,,,,,"0|z1g8w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 15:12;jelmer1;Patch available here https://github.com/apache/spark/pull/40219;;;","28/Feb/23 15:53;apachespark;User 'jelmerk' has created a pull request for this issue:
https://github.com/apache/spark/pull/40219;;;","28/Feb/23 15:54;apachespark;User 'jelmerk' has created a pull request for this issue:
https://github.com/apache/spark/pull/40219;;;","02/Mar/23 14:44;srowen;Issue resolved by pull request 40219
[https://github.com/apache/spark/pull/40219];;;",,,,,,,
SparkSQLCLIDriver shall only close started hive sessionState,SPARK-42616,13526451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,28/Feb/23 05:31,01/Mar/23 01:48,13/Jul/23 08:43,01/Mar/23 01:48,3.3.2,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,,,apachespark,Qin Yao,yao,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 01:48:27 UTC 2023,,,,,,,,,,"0|z1g8b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 05:44;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40211;;;","01/Mar/23 01:48;Qin Yao;Issue resolved by pull request 40211
[https://github.com/apache/spark/pull/40211];;;",,,,,,,,,
PythonRunner should set OMP_NUM_THREADS to task cpus instead of executor cores by default,SPARK-42613,13526423,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jzhuge,jzhuge,jzhuge,28/Feb/23 02:03,02/Mar/23 00:18,13/Jul/23 08:43,02/Mar/23 00:18,3.3.0,,,,,,,,,,,,,,,3.5.0,,,,PySpark,YARN,,0,,"Follow up from [https://github.com/apache/spark/pull/40199#discussion_r1119453996]

If OMP_NUM_THREADS is not set explicitly, we should set it to `spark.task.cpus` instead of `spark.executor.cores` as described in [PR #38699|https://github.com/apache/spark/pull/38699].",,apachespark,gurwls223,jzhuge,,,,,,,,,,,,,,,,SPARK-41188,SPARK-42596,SPARK-28843,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 00:18:41 UTC 2023,,,,,,,,,,"0|z1g84w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 05:44;apachespark;User 'jzhuge' has created a pull request for this issue:
https://github.com/apache/spark/pull/40212;;;","02/Mar/23 00:18;gurwls223;Issue resolved by pull request 40212
[https://github.com/apache/spark/pull/40212];;;",,,,,,,,,
Insert char/varchar length checks for inner fields during resolution,SPARK-42611,13526417,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,aokolnychyi,aokolnychyi,28/Feb/23 00:34,01/Mar/23 07:51,13/Jul/23 08:43,01/Mar/23 07:51,3.3.0,3.3.1,3.3.2,3.3.3,3.4.0,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"In SPARK-36498, we added support for reordering inner fields in structs during resolution. Unfortunately, we don't add any length validation for nested char/varchar columns in that path.",,aokolnychyi,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 07:51:07 UTC 2023,,,,,,,,,,"0|z1g83k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 02:26;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40206;;;","28/Feb/23 02:26;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40206;;;","01/Mar/23 07:51;cloud_fan;Issue resolved by pull request 40206
[https://github.com/apache/spark/pull/40206];;;",,,,,,,,
Use full column names for inner fields in resolution errors,SPARK-42608,13526379,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,aokolnychyi,aokolnychyi,27/Feb/23 19:37,28/Feb/23 09:00,13/Jul/23 08:43,28/Feb/23 09:00,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"If there are multiple inner columns with the same name, resolution errors may be confusing as we only use field names, not full column names.",,aokolnychyi,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 09:00:40 UTC 2023,,,,,,,,,,"0|z1g7v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"28/Feb/23 00:41;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40202;;;","28/Feb/23 09:00;dongjoon;Issue resolved by pull request 40202
[https://github.com/apache/spark/pull/40202];;;",,,,,,,,,
currentDatabase Shall use  NamespaceHelper instead of MultipartIdentifierHelper,SPARK-42600,13526309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,27/Feb/23 09:49,28/Feb/23 01:59,13/Jul/23 08:43,28/Feb/23 01:58,3.3.2,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,,,apachespark,cloud_fan,yao,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 01:58:49 UTC 2023,,,,,,,,,,"0|z1g7fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 09:57;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40192;;;","28/Feb/23 01:58;cloud_fan;Issue resolved by pull request 40192
[https://github.com/apache/spark/pull/40192];;;",,,,,,,,,
[YARN] OMP_NUM_THREADS not set to number of executor cores by default,SPARK-42596,13526279,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jzhuge,jzhuge,jzhuge,27/Feb/23 07:28,28/Feb/23 02:20,13/Jul/23 08:43,28/Feb/23 02:20,3.3.2,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,PySpark,YARN,,0,,"Run this PySpark script with `spark.executor.cores=1`
{code:python}
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf

spark = SparkSession.builder.getOrCreate()

var_name = 'OMP_NUM_THREADS'

def get_env_var():
  return os.getenv(var_name)

udf_get_env_var = udf(get_env_var)
spark.range(1).toDF(""id"").withColumn(f""env_{var_name}"", udf_get_env_var()).show(truncate=False)
{code}
Output with release `3.3.2`:
{noformat}
+---+-----------------------+
|id |env_OMP_NUM_THREADS    |
+---+-----------------------+
|0  |null                   |
+---+-----------------------+
{noformat}
Output with release `3.3.0`:
{noformat}
+---+-----------------------+
|id |env_OMP_NUM_THREADS    |
+---+-----------------------+
|0  |1                      |
+---+-----------------------+
{noformat}",,apachespark,gurwls223,jzhuge,,,,,,,,,,,,,,,SPARK-41188,,,,SPARK-42607,SPARK-42613,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 02:20:22 UTC 2023,,,,,,,,,,"0|z1g78w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 07:52;jzhuge;Looks like a regression from SPARK-41188 where it removed the code that sets the default OMP_NUM_THREADS from PythonRunner.

Its PR assumes the code can be moved to SparkContext, unfortunately `SparkContext#executorEnvs` is only used by StandaloneSchedulerBackend for Spark's standalone cluster manager, thus the PR broke YARN as shown in the test case above, probably Mesos as well but I don't have a way to test.;;;","27/Feb/23 19:23;apachespark;User 'jzhuge' has created a pull request for this issue:
https://github.com/apache/spark/pull/40199;;;","28/Feb/23 02:20;gurwls223;Issue resolved by pull request 40199
[https://github.com/apache/spark/pull/40199];;;",,,,,,,,
Logic error for StateStore.validateStateRowFormat,SPARK-42572,13526138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WweiL,WweiL,WweiL,25/Feb/23 00:57,28/Feb/23 03:13,13/Jul/23 08:43,28/Feb/23 03:13,3.5.0,,,,,,,,,,,,,,,3.5.0,,,,Structured Streaming,,,0,,SPARK-42484 Changed the logic of whether to check state store format in StateStore.validateStateRowFormat. Revert it and add unit test to make sure this won't happen again,,apachespark,gurwls223,kabhwan,WweiL,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 03:13:34 UTC 2023,,,,,,,,,,"0|z1g6e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 01:29;gurwls223;[~WweiL] are you saying that we should revert https://github.com/apache/spark/pull/40073? It won't need a new jira for that;;;","27/Feb/23 07:46;WweiL;I'm not very sure what's the true process here..

We should still use some changes in #40073 (especially the logging part)

I've create a PR for the fix: [https://github.com/apache/spark/pull/40187]

But I could also revert it and combine the two PRs if that's the correct flow;;;","27/Feb/23 07:47;apachespark;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/40187;;;","28/Feb/23 03:13;kabhwan;Issue resolved by pull request 40187
[https://github.com/apache/spark/pull/40187];;;",,,,,,,
"NonReserved keyword ""interval"" can't be column name",SPARK-42553,13526036,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiang13021,jiang13021,jiang13021,24/Feb/23 09:42,02/Mar/23 15:25,13/Jul/23 08:43,02/Mar/23 06:38,3.2.3,3.3.0,3.3.1,3.3.2,,,,,,,,,,,,3.3.3,3.4.1,,,SQL,,,0,,"INTERVAL is a Non-Reserved keyword in spark. ""Non-Reserved keywords"" have a special meaning in particular contexts and can be used as identifiers in other contexts. So by design, interval can be used as a column name.
{code:java}
scala> spark.sql(""select interval from mytable"")
org.apache.spark.sql.catalyst.parser.ParseException:
at least one time unit should be given for interval literal(line 1, pos 7)== SQL ==
select interval from mytable
-------^^^  at org.apache.spark.sql.errors.QueryParsingErrors$.invalidIntervalLiteralError(QueryParsingErrors.scala:196)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$parseIntervalLiteral$1(AstBuilder.scala:2481)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.parseIntervalLiteral(AstBuilder.scala:2466)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitInterval$1(AstBuilder.scala:2432)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInterval(AstBuilder.scala:2431)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInterval(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntervalContext.accept(SqlBaseParser.java:17308)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitIntervalLiteral(SqlBaseBaseVisitor.java:1581)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntervalLiteralContext.accept(SqlBaseParser.java:16929)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitConstantDefault(SqlBaseBaseVisitor.java:1511)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ConstantDefaultContext.accept(SqlBaseParser.java:15905)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitValueExpressionDefault(SqlBaseBaseVisitor.java:1392)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ValueExpressionDefaultContext.accept(SqlBaseParser.java:15298)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1412)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitPredicated$1(AstBuilder.scala:1548)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:1547)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext.accept(SqlBaseParser.java:14745)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitExpression(SqlBaseBaseVisitor.java:1343)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ExpressionContext.accept(SqlBaseParser.java:14606)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1412)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedExpression$1(AstBuilder.scala:1434)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:1433)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpression(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext.accept(SqlBaseParser.java:14124)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitNamedExpressionSeq$2(AstBuilder.scala:628)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitNamedExpressionSeq(AstBuilder.scala:628)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$withSelectQuerySpecification$1(AstBuilder.scala:734)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.withSelectQuerySpecification(AstBuilder.scala:728)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitRegularQuerySpecification$1(AstBuilder.scala:620)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:608)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext.accept(SqlBaseParser.java:9679)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryPrimaryDefault(SqlBaseBaseVisitor.java:846)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryPrimaryDefaultContext.accept(SqlBaseParser.java:9184)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitQueryTermDefault(SqlBaseBaseVisitor.java:832)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryTermDefaultContext.accept(SqlBaseParser.java:8953)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.typedVisit(AstBuilder.scala:61)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:112)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$1(AstBuilder.scala:118)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:117)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:57)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept(SqlBaseParser.java:6398)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:71)
  at org.apache.spark.sql.catalyst.parser.SqlBaseBaseVisitor.visitStatementDefault(SqlBaseBaseVisitor.java:69)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$StatementDefaultContext.accept(SqlBaseParser.java:1835)
  at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitSingleStatement$1(AstBuilder.scala:78)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:78)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.$anonfun$parsePlan$1(ParseDriver.scala:78)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:110)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
  ... 47 elided
{code}
Since there must be at least one time unit after the interval, why in SqlBaseParser.g4, the definition of interval is
{code:java}
interval
    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)?
    ;  {code}
instead of
{code:java}
interval
    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)
    ; {code}
If we remove the ""?"", we ensure that there must be at least one time unit after the interval from the parsing level","Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_345)

Spark version 3.2.3-SNAPSHOT",apachespark,dongjoon,gurwls223,jiang13021,maxgekk,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 12:45:06 UTC 2023,,,,,,,,,,"0|z1g5rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Feb/23 12:43;apachespark;User 'jiang13021' has created a pull request for this issue:
https://github.com/apache/spark/pull/40195;;;","02/Mar/23 06:38;maxgekk;Issue resolved by pull request 40195
[https://github.com/apache/spark/pull/40195];;;","02/Mar/23 06:47;dongjoon;Since RC2 tag is created, I changed the Fixed Version from 3.4.0 to 3.4.1 for now. We can adjust it later according to the RC2 result.;;;","02/Mar/23 12:45;apachespark;User 'jiang13021' has created a pull request for this issue:
https://github.com/apache/spark/pull/40253;;;",,,,,,,
"Get ParseException when run sql: ""SELECT 1 UNION SELECT 1;""",SPARK-42552,13526023,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,jiang13021,jiang13021,24/Feb/23 08:46,19/Apr/23 08:38,13/Jul/23 08:43,19/Apr/23 08:37,3.2.3,,,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"When I run sql
{code:java}
scala> spark.sql(""SELECT 1 UNION SELECT 1;"") {code}
I get ParseException:
{code:java}
org.apache.spark.sql.catalyst.parser.ParseException:
mismatched input 'SELECT' expecting {<EOF>, ';'}(line 1, pos 15)== SQL ==
SELECT 1 UNION SELECT 1;
---------------^^^  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:77)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:616)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:616)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
  ... 47 elided
 {code}
If I run with parentheses , it works well 
{code:java}
scala> spark.sql(""(SELECT 1) UNION (SELECT 1);"") 
res4: org.apache.spark.sql.DataFrame = [1: int]{code}
This should be a bug

 

 ","Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_345)
Spark version 3.2.3-SNAPSHOT",cloud_fan,fanjia,gurwls223,ignitetcbot,jiang13021,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 08:37:30 UTC 2023,,,,,,,,,,"0|z1g5og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Mar/23 03:18;jiang13021;The problem may be in this location: [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala#L126]

When the `PredictionMode` is `SLL`, `AstBuilder` will throw `ParseException` instead of `ParseCancellationException`，so the parser doesn't try `LL` mode. In fact, if we use `LL` mode, we can parse the sql correctly.;;;","18/Apr/23 09:04;ignitetcbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/40823;;;","19/Apr/23 08:37;cloud_fan;Issue resolved by pull request 40835
[https://github.com/apache/spark/pull/40835];;;",,,,,,,,
Make PySpark working with Python 3.7,SPARK-42547,13525982,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,gurwls223,gurwls223,24/Feb/23 03:04,24/Feb/23 05:15,13/Jul/23 08:43,24/Feb/23 05:15,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,PySpark,,,0,,"{code}
+ ./python/run-tests --python-executables=python3
Running PySpark tests. Output is in /home/ec2-user/spark/python/unit-tests.log
Will test against the following Python executables: ['python3']
Will test the following Python modules: ['pyspark-connect', 'pyspark-core', 'pyspark-errors', 'pyspark-ml', 'pyspark-mllib', 'pyspark-pandas', 'pyspark-pandas-slow', 'pyspark-resource', 'pyspark-sql', 'pyspark-streaming']
python3 python_implementation is CPython
python3 version is: Python 3.7.16
Starting test(python3): pyspark.ml.tests.test_feature (temp output: /home/ec2-user/spark/python/target/8ca9ab1a-05cc-4845-bf89-30d9001510bc/python3__pyspark.ml.tests.test_feature__kg6sseie.log)
Starting test(python3): pyspark.ml.tests.test_base (temp output: /home/ec2-user/spark/python/target/f2264f3b-6b26-4e61-9452-8d6ddd7eb002/python3__pyspark.ml.tests.test_base__0902zf9_.log)
Starting test(python3): pyspark.ml.tests.test_algorithms (temp output: /home/ec2-user/spark/python/target/d1dc4e07-e58c-4c03-abe5-09d8fab22e6a/python3__pyspark.ml.tests.test_algorithms__lh3wb2u8.log)
Starting test(python3): pyspark.ml.tests.test_evaluation (temp output: /home/ec2-user/spark/python/target/3f42dc79-c945-4cf2-a1eb-83e72b40a9ee/python3__pyspark.ml.tests.test_evaluation__89idc7fa.log)
Finished test(python3): pyspark.ml.tests.test_base (16s)
Starting test(python3): pyspark.ml.tests.test_functions (temp output: /home/ec2-user/spark/python/target/5a3b90f0-216b-4edd-9d15-6619d3e03300/python3__pyspark.ml.tests.test_functions__g5u1290s.log)
Traceback (most recent call last):
  File ""/usr/lib64/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib64/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ec2-user/spark/python/pyspark/ml/tests/test_functions.py"", line 21, in <module>
    from pyspark.ml.functions import predict_batch_udf
  File ""/home/ec2-user/spark/python/pyspark/ml/functions.py"", line 38, in <module>
    from typing import Any, Callable, Iterator, List, Mapping, Protocol, TYPE_CHECKING, Tuple, Union
ImportError: cannot import name 'Protocol' from 'typing' (/usr/lib64/python3.7/typing.py)
Had test failures in pyspark.ml.tests.test_functions with python3; see logs.
{code}",,apachespark,dongjoon,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 05:15:50 UTC 2023,,,,,,,,,,"0|z1g5fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Feb/23 03:15;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/40153;;;","24/Feb/23 05:15;dongjoon;Issue resolved by pull request 40153
[https://github.com/apache/spark/pull/40153];;;",,,,,,,,,
"User-provided JARs can override Spark's Hive metadata client JARs when using ""builtin""",SPARK-42539,13525927,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xkrogen,xkrogen,xkrogen,23/Feb/23 17:52,19/Apr/23 00:06,13/Jul/23 08:43,18/Apr/23 23:59,3.1.3,3.2.3,3.3.2,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,,"Recently we observed that on version 3.2.0 and Java 8, it is possible for user-provided Hive JARs to break the ability for Spark, via the Hive metadata client / {{IsolatedClientLoader}}, to communicate with Hive Metastore, when using the default behavior of the ""builtin"" Hive version. After SPARK-35321, when Spark is compiled against Hive >= 2.3.9 and the ""builtin"" Hive client version is used, we will call the method {{Hive.getWithoutRegisterFns()}} (from HIVE-21563) instead of {{Hive.get()}}. If the user has included, for example, {{hive-exec-2.3.8.jar}} on their classpath, the client will break with a {{NoSuchMethodError}}. This particular failure mode was resolved in 3.2.1 by SPARK-37446, but while investigating, we found a general issue that it's possible for user JARs to override Spark's own JARs -- but only inside of the IsolatedClientLoader when using ""builtin"". This happens because even when Spark is configured to use the ""builtin"" Hive classes, it still creates a separate URLClassLoader for the HiveClientImpl used for HMS communication. To get the set of JAR URLs to use for this classloader, Spark [collects all of the JARs used by the user classloader (and its parent, and that classloader's parent, and so on)|https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L412-L438]. Thus the newly created classloader will have all of the same JARs as the user classloader, but the ordering has been reversed! User JARs get prioritized ahead of system JARs, because the classloader hierarchy is traversed from bottom-to-top. For example let's say we have user JARs ""foo.jar"" and ""hive-exec-2.3.8.jar"". The user classloader will look like this:
{code}
MutableURLClassLoader
-- foo.jar
-- hive-exec-2.3.8.jar
-- parent: URLClassLoader
----- spark-core_2.12-3.2.0.jar
----- ...
----- hive-exec-2.3.9.jar
----- ...
{code}

This setup provides the expected behavior within the user classloader; it will first check the parent, so hive-exec-2.3.9.jar takes precedence, and the MutableURLClassLoader is only checked if the class doesn't exist in the parent. But when a JAR list is constructed for the IsolatedClientLoader, it traverses the URLs from MutableURLClassLoader first, then it's parent, so the final list looks like (in order):
{code}
URLClassLoader [IsolatedClientLoader]
-- foo.jar
-- hive-exec-2.3.8.jar
-- spark-core_2.12-3.2.0.jar
-- ...
-- hive-exec-2.3.9.jar
-- ...
-- parent: boot classloader (JVM classes)
{code}
Now when a lookup happens, all of the JARs are within the same URLClassLoader, and the user JARs are in front of the Spark ones, so the user JARs get prioritized. This is the opposite of the expected behavior when using the default user/application classloader in Spark, which has parent-first behavior, prioritizing the Spark/system classes over the user classes. (Note that this behavior is correct when using the {{ChildFirstURLClassLoader}}.)

After SPARK-37446, the NoSuchMethodError is no longer an issue, but this still breaks assumptions about how user JARs should be treated vs. system JARs, and presents the ability for the client to break in other ways. For example in SPARK-37446 it describes a scenario whereby Hive 2.3.8 JARs have been included; the changes in Hive 2.3.9 were needed to improve compatibility with older HMS, so if a user were to accidentally include these older JARs, it could break the ability of Spark to communicate with HMS 1.x

I see two solutions to this:

*(A) Remove the separate classloader entirely when using ""builtin""*
Starting from 3.0.0, due to SPARK-26839, when using Java 9+, we don't even create a new classloader when using ""builtin"". This makes sense, as [called out in this comment|https://github.com/apache/spark/pull/24057#discussion_r265142878], since the point of ""builtin"" is to use the existing JARs on the classpath anyway. This proposes simply extending the changes from SPARK-26839 to all Java versions, instead of restricting to Java 9+ only.

*(B) Reverse the ordering of parent/child JARs when constructing the URL list*
The most targeted fix that can be made is to simply reverse the ordering on [this line in HiveUtils|https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L419], which prioritizes child-classloader JARs over parent-classloader JARs, reversing the expected ordering. There is already special handling for {{ChildFirstURLClassLoader}}, so all that needs to be done is to reverse this order.

I prefer (A) because I think it is a clean solution in that it both simplifies the classloader setup, and reduces divergence / special handling for different Java versions. At the time SPARK-26839 went in (2019), Java 9+ support was newer and less well-tested. Now after a few years we can see that the approach in SPARK-26839 clearly works well, so I see no reason _not_ to extend this approach to other Java versions as well.",,apachespark,csun,gurwls223,jonathak,sunchao,xkrogen,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 00:06:30 UTC 2023,,,,,,,,,,"0|z1g534:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Feb/23 18:13;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40144;;;","27/Feb/23 22:58;csun;Issue resolved by pull request 40144
[https://github.com/apache/spark/pull/40144];;;","28/Feb/23 04:06;gurwls223;Reverted in https://github.com/apache/spark/commit/5627ceeddb45f2796fb8ad08b9f1c8a163823b2b and https://github.com/apache/spark/commit/26009d47c1f80897d65445fe48d8d5f2edcf848c;;;","28/Feb/23 21:48;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40224;;;","28/Feb/23 21:49;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40224;;;","18/Apr/23 23:59;xkrogen;[~csun] it looks like this didn't get marked as closed / fix-version updated when the PR was merged. I believe this went only into 3.5.0; the original PR went into branch-3.4 but was reverted and the second PR didn't make it to branch-3.4. I've marked the fix version as 3.5.0 but please correct me if I'm wrong here:
{code:java}
> glog apache/branch-3.4 | grep SPARK-42539
* 26009d47c1f 2023-02-28 Revert ""[SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client"" [Hyukjin Kwon <gurwls223@apache.org>]
* 40a4019dfc5 2023-02-27 [SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client [Erik Krogen <xkrogen@apache.org>]


> glog apache/master | grep SPARK-42539
* 2e34427d4f3 2023-03-01 [SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client [Erik Krogen <xkrogen@apache.org>]
* 5627ceeddb4 2023-02-28 Revert ""[SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client"" [Hyukjin Kwon <gurwls223@apache.org>]
* 27ad5830f9a 2023-02-27 [SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client [Erik Krogen <xkrogen@apache.org>] {code};;;","19/Apr/23 00:06;sunchao;Oops my bad [~xkrogen] - you're right, this is not in Spark 3.5 release, sorry! I must have forgotten to mark it resolved when the second PR got merged. ;;;",,,,
Fix DB2 Limit clause,SPARK-42534,13525804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,23/Feb/23 01:40,24/Feb/23 12:44,13/Jul/23 08:43,24/Feb/23 12:44,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,,,apachespark,cloud_fan,ivan.sadikov,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 12:44:12 UTC 2023,,,,,,,,,,"0|z1g4bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Feb/23 01:46;ivan.sadikov;I am going to open a PR to fix this.;;;","23/Feb/23 01:52;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40134;;;","23/Feb/23 01:53;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40134;;;","24/Feb/23 05:14;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40155;;;","24/Feb/23 12:44;cloud_fan;Issue resolved by pull request 40155
[https://github.com/apache/spark/pull/40155];;;",,,,,,
Non-captured session time zone in view creation,SPARK-42516,13525621,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,21/Feb/23 17:05,22/Feb/23 11:04,13/Jul/23 08:43,22/Feb/23 11:04,3.3.3,3.4.0,,,,,,,,,,,,,,3.3.3,3.4.0,,,SQL,,,0,,"The session time zone config is captured only when it is set explicitly but if it is not the view is instantiated with the current settings. That's might confuse users since query results depends on explicit SQL config settings while creating a view.

The example below portraits the issue:
{code:java}
val viewName = ""v1_capture_test""
withView(viewName) {
  assert(get.sessionLocalTimeZone === ""America/Los_Angeles"")
  createView(viewName,
    """"""select hour(ts) as H from (
      |  select cast('2022-01-01T00:00:00.000 America/Los_Angeles' as timestamp) as ts
      |)"""""".stripMargin, Seq(""H""))
  withDefaultTimeZone(java.time.ZoneId.of(""UTC-09:00"")) {
    withSQLConf(SESSION_LOCAL_TIMEZONE.key -> ""UTC-10:00"") {
      sql(s""select H from $viewName"").show(false)
    }
  }
} {code}
It is expected to output:
{code:java}
+---+
|H  |
+---+
|0  |
+---+ {code}
but actual output is:
{code:java}
+---+
|H  |
+---+
|8  |
+---+ {code}",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 11:04:06 UTC 2023,,,,,,,,,,"0|z1g374:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Feb/23 17:08;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/40103;;;","22/Feb/23 11:04;maxgekk;Issue resolved by pull request 40103
[https://github.com/apache/spark/pull/40103];;;",,,,,,,,,
ClientE2ETestSuite local test failed,SPARK-42515,13525616,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,21/Feb/23 16:39,28/Feb/23 03:58,13/Jul/23 08:43,28/Feb/23 03:58,3.4.0,3.5.0,,,,,,,,,,,,,,3.4.0,,,,Connect,,,0,," 

local run `build/sbt clean ""connect-client-jvm/test""`, `ClientE2ETestSuite#write table` failed, GA not failed.

 
{code:java}
[info] - rite table *** FAILED *** (41 milliseconds)
[info]   io.grpc.StatusRuntimeException: UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport
[info]   at io.grpc.Status.asRuntimeException(Status.java:535)
[info]   at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
[info]   at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)
[info]   at scala.collection.Iterator.foreach(Iterator.scala:943)
[info]   at scala.collection.Iterator.foreach$(Iterator.scala:943)
[info]   at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[info]   at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:169)
[info]   at org.apache.spark.sql.DataFrameWriter.executeWriteOperation(DataFrameWriter.scala:255)
[info]   at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:338)
[info]   at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$12(ClientE2ETestSuite.scala:145)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.sql.ClientE2ETestSuite.org$scalatest$BeforeAndAfterAll$$super$run(ClientE2ETestSuite.scala:33)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:33)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:750) {code}
 

 

local run ",,apachespark,gurwls223,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 03:58:39 UTC 2023,,,,,,,,,,"0|z1g360:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"21/Feb/23 16:40;LuciferYang;will fix this later;;;","23/Feb/23 03:21;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40136;;;","28/Feb/23 03:58;gurwls223;Issue resolved by pull request 40136
[https://github.com/apache/spark/pull/40136];;;",,,,,,,,
Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory,SPARK-42478,13525197,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kaifeiYi,kaifeiYi,kaifeiYi,17/Feb/23 11:53,06/Mar/23 22:08,13/Jul/23 08:43,27/Feb/23 08:56,3.3.2,,,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,SQL,,,0,,"https://issues.apache.org/jira/browse/SPARK-41448 make consistent MR job IDs in FileBatchWriter and FileFormatWriter, but it breaks a serializable issue, JobId is non-serializable",,apachespark,cloud_fan,kaifeiYi,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 03:19:39 UTC 2023,,,,,,,,,,"0|z1g0l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Feb/23 12:06;apachespark;User 'Yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/40064;;;","27/Feb/23 08:56;cloud_fan;Issue resolved by pull request 40064
[https://github.com/apache/spark/pull/40064];;;","06/Mar/23 03:18;apachespark;User 'Yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/40289;;;","06/Mar/23 03:19;apachespark;User 'Yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/40290;;;","06/Mar/23 03:19;apachespark;User 'Yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/40289;;;",,,,,,
An explicit cast will be needed when INSERT OVERWRITE SELECT UNION ALL,SPARK-42473,13525148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,runyao,kevinshin,kevinshin,17/Feb/23 06:27,03/Mar/23 06:58,13/Jul/23 08:43,03/Mar/23 06:58,3.3.1,,,,,,,,,,,,,,,3.3.3,,,,Optimizer,,,0,,"*when 'union all' and one select statement use* *Literal as column value , the other* *select statement  has computed expression at the same column , then the whole statement will compile failed. A explicit cast will be needed.*

for example:

{color:#4c9aff}explain{color}

{color:#4c9aff}*INSERT* OVERWRITE *TABLE* test.spark33_decimal_orc{color}

{color:#4c9aff}*select* *null* *as* amt1, {*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2{color}

{color:#4c9aff}*union* *all*{color}

{color:#4c9aff}*select* {*}cast{*}('200.99' *as* {*}decimal{*}(20,8)){*}/{*}100 *as* amt1,{*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2;{color}

*will got error :* 

org.apache.spark.{*}sql{*}.catalyst.expressions.Literal cannot be *cast* *to* org.apache.spark.{*}sql{*}.catalyst.expressions.AnsiCast

The SQL will need to change to : 

{color:#4c9aff}explain{color}

{color:#4c9aff}*INSERT* OVERWRITE *TABLE* test.spark33_decimal_orc{color}

{color:#4c9aff}*select* *null* *as* amt1,{*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2{color}

{color:#4c9aff}*union* *all*{color}

{color:#4c9aff}*select* {color:#de350b}{*}cast{*}({color}{*}cast{*}('200.99' *as* {*}decimal{*}(20,8)){*}/{*}100 *as* {*}decimal{*}(20,8){color:#de350b}){color} *as* amt1,{*}cast{*}('256.99' *as* {*}decimal{*}(20,8)) *as* amt2;{color}

 

*but this is not need in spark3.2.1 , is this a bug for spark3.3.1 ?* ",spark 3.3.1,apachespark,kevinshin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 19:14:38 UTC 2023,,,,,,,,,,"0|z1g0a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Feb/23 03:26;yumwang;What is your test.spark33_decimal_orc column type?;;;","20/Feb/23 01:47;kevinshin;[~yumwang]  'What is your test.spark33_decimal_orc column type?'

{color:#4c9aff}*CREATE* *TABLE* *IF* *NOT* *EXISTS* test.spark33_decimal_orc({color}

{color:#4c9aff}   amt1        {*}decimal{*}(20,8),{color}

{color:#4c9aff}   amt2        {*}decimal{*}(20,8){color}

{color:#4c9aff})STORED *AS* ORC;{color};;;","20/Feb/23 05:29;yumwang;It seems we should backport https://github.com/apache/spark/pull/39855.;;;","23/Feb/23 19:14;apachespark;User 'RunyaoChen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40140;;;",,,,,,,
Prevent `docker-image-tool.sh` from publishing OCI manifests,SPARK-42462,13524945,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,16/Feb/23 04:41,16/Feb/23 05:53,13/Jul/23 08:43,16/Feb/23 05:53,3.2.4,3.3.3,3.4.0,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,Kubernetes,,,0,,https://github.com/docker/buildx/issues/1509,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 05:53:08 UTC 2023,,,,,,,,,,"0|z1fz14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Feb/23 04:48;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40051;;;","16/Feb/23 04:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40051;;;","16/Feb/23 05:53;dongjoon;Issue resolved by pull request 40051
[https://github.com/apache/spark/pull/40051];;;",,,,,,,,
spark sql shell prompts wrong database info,SPARK-42448,13524813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yao,yao,yao,15/Feb/23 10:23,23/Feb/23 08:47,13/Jul/23 08:43,23/Feb/23 08:47,2.4.8,3.0.3,3.1.3,3.2.3,3.3.1,3.4.0,,,,,,,,,,3.4.0,,,,SQL,,,0,,The current db info is from hive sessionState instead of spark sessionState,,apachespark,Qin Yao,yao,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 08:47:24 UTC 2023,,,,,,,,,,"0|z1fy7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Feb/23 10:45;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/40036;;;","23/Feb/23 08:47;Qin Yao;Issue resolved by pull request 40036
[https://github.com/apache/spark/pull/40036];;;",,,,,,,,,
Fix SparkR install.spark function,SPARK-42445,13524767,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,15/Feb/23 05:50,15/Feb/23 17:32,13/Jul/23 08:43,15/Feb/23 17:32,3.3.0,3.3.1,3.3.2,3.4.0,,,,,,,,,,,,3.3.3,3.4.0,,,R,,,0,,"{code}
$ R

R version 4.2.1 (2022-06-23) -- ""Funny-Looking Kid""
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: aarch64-apple-darwin20 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(SparkR)

Attaching package: ‘SparkR’

The following objects are masked from ‘package:stats’:

    cov, filter, lag, na.omit, predict, sd, var, window

The following objects are masked from ‘package:base’:

    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,
    rank, rbind, sample, startsWith, subset, summary, transform, union

> install.spark()
Spark not found in the cache directory. Installation will start.
MirrorUrl not provided.
Looking for preferred site from apache website...
Preferred mirror site found: https://dlcdn.apache.org/spark
Downloading spark-3.3.2 for Hadoop 2.7 from:
- https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.7.tgz
trying URL 'https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.7.tgz'
simpleWarning in download.file(remotePath, localPath): downloaded length 0 != reported length 196
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 17:32:10 UTC 2023,,,,,,,,,,"0|z1fxxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"15/Feb/23 05:53;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40031;;;","15/Feb/23 17:32;dongjoon;Issue resolved by pull request 40031
[https://github.com/apache/spark/pull/40031];;;",,,,,,,,,
DataFrame.drop should handle multi columns properly,SPARK-42444,13524752,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,podongfeng,podongfeng,podongfeng,15/Feb/23 02:46,24/Feb/23 00:03,13/Jul/23 08:43,24/Feb/23 00:03,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,PySpark,,,0,,"{code:java}
from pyspark.sql import Row
df1 = spark.createDataFrame([(14, ""Tom""), (23, ""Alice""), (16, ""Bob"")], [""age"", ""name""])
df2 = spark.createDataFrame([Row(height=80, name=""Tom""), Row(height=85, name=""Bob"")])
df1.join(df2, df1.name == df2.name, 'inner').drop('name', 'age').show()
{code}

This works in 3.3

{code:java}
+------+
|height|
+------+
|    85|
|    80|
+------+
{code}

but fails in 3.4


{code:java}
---------------------------------------------------------------------------
AnalysisException                         Traceback (most recent call last)
Cell In[1], line 4
      2 df1 = spark.createDataFrame([(14, ""Tom""), (23, ""Alice""), (16, ""Bob"")], [""age"", ""name""])
      3 df2 = spark.createDataFrame([Row(height=80, name=""Tom""), Row(height=85, name=""Bob"")])
----> 4 df1.join(df2, df1.name == df2.name, 'inner').drop('name', 'age').show()

File ~/Dev/spark/python/pyspark/sql/dataframe.py:4913, in DataFrame.drop(self, *cols)
   4911     jcols = [_to_java_column(c) for c in cols]
   4912     first_column, *remaining_columns = jcols
-> 4913     jdf = self._jdf.drop(first_column, self._jseq(remaining_columns))
   4915 return DataFrame(jdf, self.sparkSession)

File ~/Dev/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)
   1316 command = proto.CALL_COMMAND_NAME +\
   1317     self.command_header +\
   1318     args_command +\
   1319     proto.END_COMMAND_PART
   1321 answer = self.gateway_client.send_command(command)
-> 1322 return_value = get_return_value(
   1323     answer, self.gateway_client, self.target_id, self.name)
   1325 for temp_arg in temp_args:
   1326     if hasattr(temp_arg, ""_detach""):

File ~/Dev/spark/python/pyspark/errors/exceptions/captured.py:159, in capture_sql_exception.<locals>.deco(*a, **kw)
    155 converted = convert_exception(e.java_exception)
    156 if not isinstance(converted, UnknownException):
    157     # Hide where the exception came from that shows a non-Pythonic
    158     # JVM exception message.
--> 159     raise converted from None
    160 else:
    161     raise

AnalysisException: [AMBIGUOUS_REFERENCE] Reference `name` is ambiguous, could be: [`name`, `name`].

{code}

",,apachespark,neilagupta,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 00:03:57 UTC 2023,,,,,,,,,,"0|z1fxug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"23/Feb/23 02:01;podongfeng;I am going to fix this one;;;","23/Feb/23 03:07;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/40135;;;","23/Feb/23 03:07;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/40135;;;","24/Feb/23 00:03;podongfeng;Issue resolved by pull request 40135
[https://github.com/apache/spark/pull/40135];;;",,,,,,,
Dateset operations should not resolve the analyzed logical plan again,SPARK-42416,13524401,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,13/Feb/23 05:36,13/Feb/23 18:58,13/Jul/23 08:43,13/Feb/23 18:58,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"For the following query

 
{code:java}
      sql(
        """"""
          |CREATE TABLE app_open (
          |  uid STRING,
          |  st TIMESTAMP,
          |  ds INT
          |) USING parquet PARTITIONED BY (ds);
          |"""""".stripMargin)

      sql(
        """"""
          |create or replace temporary view group_by_error as WITH new_app_open AS (
          |  SELECT
          |    ao.*
          |  FROM
          |    app_open ao
          |)
          |SELECT
          |    uid,
          |    20230208 AS ds
          |  FROM
          |    new_app_open
          |  GROUP BY
          |    1,
          |    2
          |"""""".stripMargin)

      sql(
        """"""
          |select
          |  `uid`
          |from
          |  group_by_error
          |"""""".stripMargin).show(){code}
Spark will throw the following error

 

 
{code:java}
[GROUP_BY_POS_OUT_OF_RANGE] GROUP BY position 20230208 is not in select list (valid range is [1, 2]).; line 9 pos 4 {code}
 

 

This is because the logical plan is not set as analyzed and it is analyzed again. The analyzer rules about aggregation/sort ordinals are not idempotent.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,SPARK-40595,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 18:58:18 UTC 2023,,,,,,,,,,"0|z1fvo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Feb/23 05:56;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39988;;;","13/Feb/23 18:58;Gengliang.Wang;Issue resolved by pull request 39988
[https://github.com/apache/spark/pull/39988];;;",,,,,,,,,
Support Scala 2.12/2.13 tests in connect module,SPARK-42410,13524371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,12/Feb/23 22:58,01/Mar/23 16:39,13/Jul/23 08:43,13/Feb/23 01:31,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Connect,Tests,,0,,"{code}
$ build/sbt -Dscala.version=2.13.8 -Pscala-2.13 -Phadoop-3 assembly/package ""connect/test""
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-42554,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 02:37:52 UTC 2023,,,,,,,,,,"0|z1fvhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Feb/23 23:38;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39982;;;","12/Feb/23 23:39;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39982;;;","13/Feb/23 01:31;dongjoon;Issue resolved by pull request 39982
[https://github.com/apache/spark/pull/39982];;;","13/Feb/23 02:37;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39986;;;",,,,,,,
[PROTOBUF] Recursive field handling is incompatible with delta,SPARK-42406,13524334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rangadi,rangadi,rangadi,11/Feb/23 20:53,28/Feb/23 04:45,13/Jul/23 08:43,28/Feb/23 04:45,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Protobuf,,,0,,"Protobuf deserializer (`from_protobuf()` function()) optionally supports recursive fields by limiting the depth to certain level. See example below. It assigns a 'NullType' for such a field when allowed depth is reached. 

It causes a few issues. E.g. a repeated field as in the following example results in a Array field with 'NullType'. Delta does not support null type in a complex type.

Actually `Array[NullType]` is not really useful anyway.

How about this fix: Drop the recursive field when the limit reached rather than using a NullType. 

The example below makes it clear:

Consider a recursive Protobuf:

 
{code:python}
message TreeNode {
  string value = 1;
  repeated TreeNode children = 2;
}
{code}
Allow depth of 2: 

 
{code:python}
   df.select(
    'proto',
     messageName = 'TreeNode',
     options = { ... ""recursive.fields.max.depth"" : ""2"" }
  ).printSchema()
{code}
Schema looks like this:
{noformat}
root
|– from_protobuf(proto): struct (nullable = true)|
| |– value: string (nullable = true)|
| |– children: array (nullable = false)|
| | |– element: struct (containsNull = false)|
| | | |– value: string (nullable = true)|
| | | |– children: array (nullable = false)|
| | | | |– element: struct (containsNull = false)|
| | | | | |– value: string (nullable = true)|
| | | | | |– children: array (nullable = false). [ === Proposed fix: Drop this field === ]|
| | | | | | |– element: void (containsNull = false) [ === NOTICE 'void' HERE === ] 
{noformat}
When we try to write this to a delta table, we get an error:
{noformat}
AnalysisException: Found nested NullType in column from_protobuf(proto).children which is of ArrayType. Delta doesn't support writing NullType in complex types.
{noformat}
 
We could just drop the field 'element' when recursion depth is reached. It is simpler and does not need to deal with NullType. We are ignoring the value anyway. There is no use in keeping the field.

Another issue is setting for 'recursive.fields.max.depth': It is not enforced correctly. '0' does not make sense. 

 ",,apachespark,Gengliang.Wang,gurwls223,rangadi,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 04:45:01 UTC 2023,,,,,,,,,,"0|z1fv9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Feb/23 21:00;rangadi;cc: [~sanysandish@gmail.com] PTAL.;;;","13/Feb/23 17:11;rangadi;I am working on a fix.;;;","14/Feb/23 08:42;apachespark;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40011;;;","14/Feb/23 23:12;Gengliang.Wang;Issue resolved by pull request 40011
[https://github.com/apache/spark/pull/40011];;;","14/Feb/23 23:48;rangadi;Thanks for merging [https://github.com/apache/spark/pull/40011] 

Keeping this ticket open to fix the issue with 'nullType' and delta.;;;","19/Feb/23 05:16;apachespark;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40080;;;","19/Feb/23 05:16;apachespark;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40080;;;","22/Feb/23 01:35;Gengliang.Wang;Resolved in https://github.com/apache/spark/pull/40080;;;","23/Feb/23 09:04;rangadi;The main for still to be merged.;;;","23/Feb/23 09:41;apachespark;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40141;;;","28/Feb/23 04:45;Gengliang.Wang;Issue resolved by pull request 40141
[https://github.com/apache/spark/pull/40141];;;"
JsonProtocol should handle null JSON strings,SPARK-42403,13524305,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,dongjoon,dongjoon,11/Feb/23 01:12,11/Feb/23 05:55,13/Jul/23 08:43,11/Feb/23 05:54,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,0,,"*Event Log*
{code}
{""Declaring Class"":""org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1"",""Method Name"":""columnartorow_nextBatch_0$"",""File Name"":null,""Line Number"":-1}
{code}

*Apache Spark 3.4*
{code}
3/02/10 16:54:46 ERROR ReplayListenerBus: Exception parsing Spark event log: file:/Users/dongjoon/data/history/eventlog_v2_spark-1676069204164-1qq70hioosynfzib9rmi77wbavnao-driver-job/events_1_spark-1676069204164-1qq70hioosynfzib9rmi77wbavnao-driver-job.zstd
java.lang.IllegalArgumentException: requirement failed: Expected string, got NULL
        at scala.Predef$.require(Predef.scala:281)
        at org.apache.spark.util.JsonProtocol$JsonNodeImplicits.extractString(JsonProtocol.scala:1614)
        at org.apache.spark.util.JsonProtocol$.$anonfun$stackTraceFromJson$1(JsonProtocol.scala:1561)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
        at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
        at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
        at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
        at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
        at scala.collection.AbstractIterator.to(Iterator.scala:1431)
        at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
        at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)
        at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
        at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)
        at org.apache.spark.util.JsonProtocol$.stackTraceFromJson(JsonProtocol.scala:1564)
        at org.apache.spark.util.JsonProtocol$.taskEndReasonFromJson(JsonProtocol.scala:1361)
        at org.apache.spark.util.JsonProtocol$.taskEndFromJson(JsonProtocol.scala:938)
        at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:876)
        at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:865)
        at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:88)
        at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:59)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3(FsHistoryProvider.scala:1140)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$3$adapted(FsHistoryProvider.scala:1138)
        at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2777)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1(FsHistoryProvider.scala:1138)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$parseAppEventLogs$1$adapted(FsHistoryProvider.scala:1136)
        at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
        at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
        at org.apache.spark.deploy.history.FsHistoryProvider.parseAppEventLogs(FsHistoryProvider.scala:1136)
        at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:1117)
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,SPARK-39489,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 11 05:54:46 UTC 2023,,,,,,,,,,"0|z1fv34:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,"11/Feb/23 01:33;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39972;;;","11/Feb/23 01:34;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39972;;;","11/Feb/23 02:10;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/39973;;;","11/Feb/23 05:54;dongjoon;Issue resolved by pull request 39973
[https://github.com/apache/spark/pull/39973];;;",,,,,,,
Incorrect results or NPE when inserting null value into array using array_insert/array_append,SPARK-42401,13524302,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,10/Feb/23 23:18,15/Feb/23 02:45,13/Jul/23 08:43,13/Feb/23 06:50,3.4.0,3.5.0,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,correctness,"Example:
{noformat}
create or replace temp view v1 as
select * from values
(array(1, 2, 3, 4), 5, 5),
(array(1, 2, 3, 4), 5, null)
as v1(col1,col2,col3);

select array_insert(col1, col2, col3) from v1;
{noformat}
This produces an incorrect result:
{noformat}
[1,2,3,4,5]
[1,2,3,4,0] <== should be [1,2,3,4,null]
{noformat}
A more succint example:
{noformat}
select array_insert(array(1, 2, 3, 4), 5, cast(null as int));
{noformat}
This also produces an incorrect result:
{noformat}
[1,2,3,4,0] <== should be [1,2,3,4,null]
{noformat}
Another example:
{noformat}
create or replace temp view v1 as
select * from values
(array('1', '2', '3', '4'), 5, '5'),
(array('1', '2', '3', '4'), 5, null)
as v1(col1,col2,col3);

select array_insert(col1, col2, col3) from v1;
{noformat}
The above query throws a {{NullPointerException}}:
{noformat}
23/02/10 11:08:05 ERROR SparkSQLDriver: Failed in [select array_insert(col1, col2, col3) from v1]
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.LocalTableScanExec.$anonfun$unsafeRows$1(LocalTableScanExec.scala:44)
{noformat}
{{array_append}} has the same issue:
{noformat}
spark-sql> select array_append(array(1, 2, 3, 4), cast(null as int));
[1,2,3,4,0] <== should be [1,2,3,4,null]
Time taken: 3.679 seconds, Fetched 1 row(s)
spark-sql> select array_append(array('1', '2', '3', '4'), cast(null as string));
23/02/10 11:13:36 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
{noformat}
",,apachespark,bersprockets,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 02:45:56 UTC 2023,,,,,,,,,,"0|z1fv2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Feb/23 23:37;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39970;;;","10/Feb/23 23:38;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39970;;;","13/Feb/23 06:50;gurwls223;Issue resolved by pull request 39970
[https://github.com/apache/spark/pull/39970];;;","15/Feb/23 00:27;bersprockets;There is another case:
{noformat}
spark-sql> select array_insert(array('1', '2', '3', '4'), -6, '5');
23/02/14 16:10:19 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
{noformat}
{{array_insert}} might implicitly add nulls, and my fix does not cover that case. I will follow up.;;;","15/Feb/23 02:45;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/40026;;;","15/Feb/23 02:45;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/40026;;;",,,,,
Mask function's generated code does not handle null input,SPARK-42384,13523803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,08/Feb/23 16:46,16/Feb/23 00:26,13/Jul/23 08:43,16/Feb/23 00:26,3.4.0,3.5.0,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"Example:
{noformat}
create or replace temp view v1 as
select * from values
(null),
('AbCD123-@$#')
as data(col1);

cache table v1;

select mask(col1) from v1;
{noformat}
This query results in a {{NullPointerException}}:
{noformat}
23/02/07 16:36:06 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
{noformat}
The generated code calls {{UnsafeWriter.write(0, value_0)}} regardless of whether {{Mask.transformInput}} returns null or not. The {{UnsafeWriter.write}} method for {{UTF8String}} does not expect a null pointer.
{noformat}
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     UTF8String value_1 = isNull_1 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */
/* 035 */
/* 036 */
/* 037 */
/* 038 */     UTF8String value_0 = null;
/* 039 */     value_0 = org.apache.spark.sql.catalyst.expressions.Mask.transformInput(value_1, ((UTF8String) references[0] /* literal */), ((UTF8String) references[1] /* literal */), ((UTF8String) references[2] /* literal */), ((UTF8String) references[3] /* literal */));;
/* 040 */     if (false) {
/* 041 */       mutableStateArray_0[0].setNullAt(0);
/* 042 */     } else {
/* 043 */       mutableStateArray_0[0].write(0, value_0);
/* 044 */     }
/* 045 */     return (mutableStateArray_0[0].getRow());
/* 046 */   }
{noformat}

The bug is not exercised by a literal null input value, since there appears to be some optimization that simply replaces the entire function call with a null literal:
{noformat}
spark-sql> explain SELECT mask(NULL);
== Physical Plan ==
*(1) Project [null AS mask(NULL, X, x, n, NULL)#47]
+- *(1) Scan OneRowRelation[]

Time taken: 0.026 seconds, Fetched 1 row(s)
spark-sql> SELECT mask(NULL);
NULL
Time taken: 0.042 seconds, Fetched 1 row(s)
spark-sql> 
{noformat}
",,apachespark,bersprockets,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 00:26:50 UTC 2023,,,,,,,,,,"0|z1frzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Feb/23 17:23;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39945;;;","16/Feb/23 00:26;Gengliang.Wang;Issue resolved by pull request 39945
[https://github.com/apache/spark/pull/39945];;;",,,,,,,,,
Cleanup orphan sst and log files in RocksDB checkpoint directory,SPARK-42353,13523168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Chaoqin,Chaoqin,Chaoqin,06/Feb/23 07:54,09/Feb/23 23:22,13/Jul/23 08:43,09/Feb/23 23:22,3.2.3,,,,,,,,,,,,,,,3.5.0,,,,Structured Streaming,,,0,,"When RocksDB version.zip file get overwritten (e.g. concurrent task execution, task/stage/batch reattempts) or the zip file don't get uploaded successfully, the associated sst and log files don't get garbage collected.([https://github.com/databricks/runtime/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala|https://github.com/databricks/runtime/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala#L305-L309]) These files consume storage. We can clean up these SST files during periodic state store maintenance. The major concern is that sst files for ongoing version also appear to be ""orphan"" because they are uploaded before zip file, we have to be careful not to delete them.",,apachespark,Chaoqin,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 23:22:11 UTC 2023,,,,,,,,,,"0|z1fo2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Feb/23 08:15;apachespark;User 'chaoqin-li1123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39897;;;","09/Feb/23 23:22;kabhwan;Issue resolved by pull request 39897
[https://github.com/apache/spark/pull/39897];;;",,,,,,,,,
distinct(count colname) with UNION ALL causes query analyzer bug,SPARK-42346,13523064,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,RobinLinacre,RobinLinacre,04/Feb/23 08:52,08/Feb/23 19:40,13/Jul/23 08:43,06/Feb/23 13:29,3.3.0,3.4.0,3.5.0,,,,,,,,,,,,,3.3.2,3.4.0,3.5.0,,SQL,,,0,,"If you combine a UNION ALL with a count(distinct colname) you get a query analyzer bug.

 

This behaviour is introduced in 3.3.0.  The bug was not present in 3.2.1.

 

Here is a reprex in PySpark:

{{df_pd = pd.DataFrame([}}
{{    \{'surname': 'a', 'first_name': 'b'}}}
{{])}}
{{df_spark = spark.createDataFrame(df_pd)}}
{{df_spark.createOrReplaceTempView(""input_table"")}}

{{sql = """"""}}

{{SELECT }}
{{    (SELECT Count(DISTINCT first_name) FROM   input_table) }}
{{        AS distinct_value_count}}
{{FROM   input_table}}
{{UNION ALL}}
{{SELECT }}
{{    (SELECT Count(DISTINCT surname) FROM   input_table) }}
{{        AS distinct_value_count}}
{{FROM   input_table """"""}}

{{spark.sql(sql).toPandas()}}

 ",,apachespark,petertoth,ritikam,RobinLinacre,viirya,yumwang,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 19:40:59 UTC 2023,,,,,,,,,,"0|z1fnfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Feb/23 15:03;yumwang;cc [~petertoth];;;","04/Feb/23 15:58;petertoth;Thanks for pinging me [~yumwang], this might be subquery merge related. I will look into it.;;;","05/Feb/23 10:19;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/39887;;;","05/Feb/23 10:23;petertoth;[~yumwang], [~RobinLinacre], https://github.com/apache/spark/pull/39887 will fix the issue.

[~viirya], as this is a regression from 3.2 to 3.3, if possible please include this in 3.3.2.;;;","05/Feb/23 17:58;viirya;I will wait for this patch before cutting RC1.;;;","06/Feb/23 13:29;yumwang;Issue resolved by pull request 39887
[https://github.com/apache/spark/pull/39887];;;","07/Feb/23 01:35;ritikam;I have Spark 3.3.0 and I do not have 39887 fix . I am not able to reproduce this issue. Am I missing something?

 

scala> val df = Seq((""a"",""b"")).toDF(""surname"",""first_name"")

*df*: *org.apache.spark.sql.DataFrame* = [surname: string, first_name: string]

 

scala> df.createOrReplaceTempView(""input_table"")

 

scala> spark.sql(""select(Select Count(Distinct first_name) from input_table) As distinct_value_count from input_table Union all select (select count(Distinct surname) from input_table) as distinct_value_count from input_table"").show()

+--------------------+                                                          

|distinct_value_count|

+--------------------+

|                   1|

|                   1|

+--------------------+

= Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Union
   :- Project [cast(Subquery subquery#46, [id=#114] as string) AS distinct_value_count#62]
   :  :  +- Subquery subquery#46, [id=#114]
   :  :     +- AdaptiveSparkPlan isFinalPlan=false
   :  :        +- HashAggregate(keys=[], functions=[count(first_name#12)], output=[count(DISTINCT first_name)#53L])
   :  :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#112]
   :  :              +- HashAggregate(keys=[], functions=[partial_count(first_name#12)], output=[count#67L])
   :  :                 +- LocalTableScan [first_name#12]
   :  +- LocalTableScan [_1#6, _2#7]
   +- Project [cast(Subquery subquery#48, [id=#125] as string) AS distinct_value_count#64]
      :  +- Subquery subquery#48, [id=#125]
      :     +- AdaptiveSparkPlan isFinalPlan=false
      :        +- HashAggregate(keys=[], functions=[count(surname#11)], output=[count(DISTINCT surname)#55L])
      :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#123]
      :              +- HashAggregate(keys=[], functions=[partial_count(surname#11)], output=[count#68L])
      :                 +- LocalTableScan [surname#11]
      +- LocalTableScan [_1#50, _2#51|#50, _2#51]


 

This is what I have in my SparkOptimizer.scala


override def defaultBatches: Seq[Batch] = (preOptimizationBatches ++ super.defaultBatches :+
Batch(""Optimize Metadata Only Query"", Once, OptimizeMetadataOnlyQuery(catalog)) :+
Batch(""PartitionPruning"", Once,
PartitionPruning) :+
Batch(""InjectRuntimeFilter"", FixedPoint(1),
InjectRuntimeFilter,
RewritePredicateSubquery) :+
Batch(""MergeScalarSubqueries"", Once,
MergeScalarSubqueries) :+
Batch(""Pushdown Filters from PartitionPruning"", fixedPoint,
PushDownPredicates) :+
Batch;;;","07/Feb/23 07:56;petertoth;[~ritikam], please use the Pyspark repro in description or add a 2nd row to your input_table if you use Scala. That's because Spark can optimize out count distinct from one row local relations.;;;","07/Feb/23 20:58;ritikam;Hello added three rows to input_table. Still no error. I do have DPP enabled.

*********************************************************************

Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 12.0.2)

Type in expressions to have them evaluated.

Type :help for more information.

 

scala> val df = Seq((""a"",""b""),(""c"",""d""),(""e"",""f"")).toDF(""surname"",""first_name"")

*df*: *org.apache.spark.sql.DataFrame* = [surname: string, first_name: string]

 

scala> df.createOrReplaceTempView(""input_table"")

 

scala> spark.sql(""select(Select Count(Distinct first_name) from input_table) As distinct_value_count from input_table Union all select (select count(Distinct surname) from input_table) as distinct_value_count from input_table"").show()

+--------------------+                                                          

|distinct_value_count|

+--------------------+

|                   3|

|                   3|

|                   3|

|                   3|

|                   3|

|                   3|

+--------------------+

 

**************************************************************

AdaptiveSparkPlan isFinalPlan=false
+- Union
   :- Project [cast(Subquery subquery#145, [id=#571] as string) AS distinct_value_count#161]
   :  :  +- Subquery subquery#145, [id=#571]
   :  :     +- AdaptiveSparkPlan isFinalPlan=false
   :  :        +- HashAggregate(keys=[], functions=[count(distinct first_name#8)], output=[count(DISTINCT first_name)#152L])
   :  :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#569]
   :  :              +- HashAggregate(keys=[], functions=[partial_count(distinct first_name#8)], output=[count#167L])
   :  :                 +- HashAggregate(keys=[first_name#8], functions=[], output=[first_name#8])
   :  :                    +- Exchange hashpartitioning(first_name#8, 200), ENSURE_REQUIREMENTS, [id=#565]
   :  :                       +- HashAggregate(keys=[first_name#8], functions=[], output=[first_name#8])
   :  :                          +- LocalTableScan [first_name#8]
   :  +- LocalTableScan [_1#2, _2#3]
   +- Project [cast(Subquery subquery#147, [id=#590] as string) AS distinct_value_count#163]
      :  +- Subquery subquery#147, [id=#590]
      :     +- AdaptiveSparkPlan isFinalPlan=false
      :        +- HashAggregate(keys=[], functions=[count(distinct surname#7)], output=[count(DISTINCT surname)#154L])
      :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#588]
      :              +- HashAggregate(keys=[], functions=[partial_count(distinct surname#7)], output=[count#170L])
      :                 +- HashAggregate(keys=[surname#7], functions=[], output=[surname#7])
      :                    +- Exchange hashpartitioning(surname#7, 200), ENSURE_REQUIREMENTS, [id=#584]
      :                       +- HashAggregate(keys=[surname#7], functions=[], output=[surname#7])
      :                          +- LocalTableScan [surname#7]
      +- LocalTableScan [_1#149, _2#150];;;","08/Feb/23 11:16;petertoth;[~ritikam], you also need to disable the ""ConvertToLocalRelation"" rule optimization `--conf ""spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation""` to get the error from spark-shell.;;;","08/Feb/23 19:40;ritikam;Yes that caused the error to appear. Thanks;;;"
The default size of the CONFIG_MAP_MAXSIZE should not be greater than 1048576,SPARK-42344,13523054,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ninebigbig,ninebigbig,ninebigbig,04/Feb/23 05:08,05/Feb/23 11:13,13/Jul/23 08:43,05/Feb/23 11:09,3.3.1,,,,,,,,,,,,,,,3.3.2,3.4.0,,,Kubernetes,Spark Submit,,0,,"Exception in thread ""main"" io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.18.123.24:6443/api/v1/namespaces/default/configmaps. Message: ConfigMap ""spark-exec-ed9f2c861aa40b48-conf-map"" is invalid: []: Too long: must have at most 1048576 bytes. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=[], message=Too long: must have at most 1048576 bytes, reason=FieldValueTooLong, additionalProperties={})], group=null, kind=ConfigMap, name=spark-exec-ed9f2c861aa40b48-conf-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=ConfigMap ""spark-exec-ed9f2c861aa40b48-conf-map"" is invalid: []: Too long: must have at most 1048576 bytes, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties={}).
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:682)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:661)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:612)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:555)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:518)
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:305)
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644)
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83)
        at io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61)
        at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.setUpExecutorConfigMap(KubernetesClusterSchedulerBackend.scala:88)
        at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:112)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:222)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:595)
        at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2714)
        at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
        at org.apache.spark.examples.JavaSparkPi.main(JavaSparkPi.java:37)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Kubernetes: 1.22.0

ETCD: 3.5.0

Spark: 3.3.2",apachespark,dongjoon,ninebigbig,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 05 11:09:10 UTC 2023,,,,,,,,,,"0|z1fndc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"04/Feb/23 05:43;apachespark;User 'ninebigbig' has created a pull request for this issue:
https://github.com/apache/spark/pull/39884;;;","05/Feb/23 11:09;dongjoon;Issue resolved by pull request 39884
[https://github.com/apache/spark/pull/39884];;;",,,,,,,,,
Fix metadata col can not been resolved,SPARK-42331,13522905,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,03/Feb/23 06:19,13/Feb/23 04:44,13/Jul/23 08:43,13/Feb/23 04:44,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,,,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 04:44:08 UTC 2023,,,,,,,,,,"0|z1fmgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Feb/23 07:07;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/39870;;;","13/Feb/23 04:44;cloud_fan;Issue resolved by pull request 39870
[https://github.com/apache/spark/pull/39870];;;",,,,,,,,,
Spark SQL not use hive partition info,SPARK-42292,13522776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,xuanzhiang,xuanzhiang,02/Feb/23 12:08,03/Feb/23 02:11,13/Jul/23 08:43,03/Feb/23 02:11,3.2.1,,,,,,,,,,,,,,,,,,,SQL,,,0,,"I use spark3 to count partition num , like : 

table a is external parquet table, it have 3 partition columns (year ,month, day).

query sql : ""select distinct month , day from a where year = '2022' ""

i think spark can find hive metadata and use partition info, but it load all  ""year = '2022'"" partition data.

in spark2.4, it use TableLocalScanExec ,but spark3 use HiveTableRelation and scan hive parquet.
 ",,xuanzhiang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 02:11:36 UTC 2023,,,,,,,,,,"0|z1flns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Feb/23 02:11;xuanzhiang;when i set spark.sql.hive.convertMetastoreParquet=true , spark3 use inner parquet reader.;;;",,,,,,,,,,
Spark Driver hangs on OOM during Broadcast when AQE is enabled ,SPARK-42290,13522751,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,fanjia,shardulm,shardulm,02/Feb/23 09:45,11/Jun/23 11:25,13/Jul/23 08:43,08/Jun/23 20:24,3.4.0,,,,,,,,,,,,,,,3.4.1,3.5.0,,,SQL,,,0,,"Repro steps:
{code}
$ spark-shell --conf spark.driver.memory=1g

val df = spark.range(5000000).withColumn(""str"", lit(""abcdabcdabcdabcdabasgasdfsadfasdfasdfasfasfsadfasdfsadfasdf""))
val df2 = spark.range(10).join(broadcast(df), Seq(""id""), ""left_outer"")

df2.collect
{code}

This will cause the driver to hang indefinitely. Heres a thread dump of the {{main}} thread when its stuck
{code}
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:285)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2819/629294880.apply(Unknown Source)
org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:809)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236) => holding Monitor(java.lang.Object@1932537396})
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:381)
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)
org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4179)
org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3420)
org.apache.spark.sql.Dataset$$Lambda$2390/1803372144.apply(Unknown Source)
org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4169)
org.apache.spark.sql.Dataset$$Lambda$2791/1357377136.apply(Unknown Source)
org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4167)
org.apache.spark.sql.Dataset$$Lambda$2391/1172042998.apply(Unknown Source)
org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
org.apache.spark.sql.execution.SQLExecution$$$Lambda$2402/721269425.apply(Unknown Source)
org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
org.apache.spark.sql.execution.SQLExecution$$$Lambda$2392/11632488.apply(Unknown Source)
org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:809)
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
org.apache.spark.sql.Dataset.withAction(Dataset.scala:4167)
org.apache.spark.sql.Dataset.collect(Dataset.scala:3420)
{code}


When we disable AQE though we get the following exception instead of driver hang.
{code}
Caused by: org.apache.spark.SparkException: Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting spark.sql.autoBroadcastJoinThreshold to -1 or increase the spark driver memory by setting spark.driver.memory to a higher value.
  ... 7 more
Caused by: java.lang.OutOfMemoryError: Java heap space
  at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.grow(HashedRelation.scala:834)
  at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.append(HashedRelation.scala:777)
  at org.apache.spark.sql.execution.joins.LongHashedRelation$.apply(HashedRelation.scala:1086)
  at org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:157)
  at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:1163)
  at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:1151)
  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:148)
  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$Lambda$2999/145945436.apply(Unknown Source)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:217)
  at org.apache.spark.sql.execution.SQLExecution$$$Lambda$3001/1900142693.call(Unknown Source)
  ... 4 more
{code}
I expect to see the same exception even when AQE is enabled. ",,dongjoon,fanjia,gurwls223,shardulm,Wayne Guo,,,,,,,,,,,,,,,,,SPARK-40663,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 11 11:25:22 UTC 2023,,,,,,,,,,"0|z1fli8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"08/Jun/23 20:24;dongjoon;Issue resolved by pull request 41517
[https://github.com/apache/spark/pull/41517];;;","09/Jun/23 00:06;fanjia;[~dongjoon] Seem the assigning people not right.;;;","10/Jun/23 08:31;dongjoon;Oh, sorry, [~fanjia]. I fixed it now.;;;","11/Jun/23 11:25;fanjia;Thanks [~dongjoon] ;;;",,,,,,,
Fix internal error for valid CASE WHEN expression with CAST when inserting into a table,SPARK-42286,13522606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,runyao,runyao,runyao,01/Feb/23 23:00,24/Feb/23 18:58,13/Jul/23 08:43,03/Feb/23 05:12,3.3.1,3.3.2,3.4.0,,,,,,,,,,,,,3.3.3,3.4.0,,,Spark Core,,,0,,"```

spark-sql> create or replace table es570639t1 as select x FROM values (1), (2), (3) as tab(x);
spark-sql> create or replace table es570639t2 (x Decimal(9, 0));
spark-sql> insert into es570639t2 select 0 - (case when x = 1 then 1 else x end) from es570639t1 where x = 1;

```

hits the following internal error
org.apache.spark.SparkException: [INTERNAL_ERROR] Child is not Cast or ExpressionProxy of Cast
 

Stack trace:
org.apache.spark.SparkException: [INTERNAL_ERROR] Child is not Cast or ExpressionProxy of Cast at org.apache.spark.SparkException$.internalError(SparkException.scala:78) at org.apache.spark.SparkException$.internalError(SparkException.scala:82) at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.checkChild(Cast.scala:2693) at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2697) at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2683) at org.apache.spark.sql.catalyst.trees.UnaryLike.$anonfun$mapChildren$5(TreeNode.scala:1315) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106) at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1314) at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1309) at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:636) at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:570) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:570)
 

This internal error comes from `CheckOverflowInTableInsert``checkChild`, where we covered only `Cast` expr and `ExpressionProxy` expr, but not the `CaseWhen` expr.",,apachespark,Gengliang.Wang,runyao,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 06:23:45 UTC 2023,,,,,,,,,,"0|z1fkmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Feb/23 23:03;apachespark;User 'RunyaoChen' has created a pull request for this issue:
https://github.com/apache/spark/pull/39855;;;","01/Feb/23 23:03;apachespark;User 'RunyaoChen' has created a pull request for this issue:
https://github.com/apache/spark/pull/39855;;;","03/Feb/23 05:12;Gengliang.Wang;Resolved in https://github.com/apache/spark/pull/39855;;;","23/Feb/23 06:23;apachespark;User 'RunyaoChen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40140;;;",,,,,,,
Add ServicesResourceTransformer to connect server module  shade configuration,SPARK-42276,13522433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,01/Feb/23 08:38,10/Feb/23 02:58,13/Jul/23 08:43,10/Feb/23 02:58,3.4.0,3.5.0,,,,,,,,,,,,,,3.4.0,,,,Build,Connect,,0,,The contents of META-INF/services directory in the shaded connect-server jar have not been relocated.,,apachespark,gurwls223,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 02:58:41 UTC 2023,,,,,,,,,,"0|z1fjkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Feb/23 13:05;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39848;;;","01/Feb/23 13:06;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39848;;;","10/Feb/23 02:58;gurwls223;Issue resolved by pull request 39848
[https://github.com/apache/spark/pull/39848];;;",,,,,,,,
Upgrade `compress-lzf` to 1.1.2,SPARK-42274,13522421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,01/Feb/23 07:56,01/Feb/23 10:12,13/Jul/23 08:43,01/Feb/23 10:12,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Build,,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 10:12:33 UTC 2023,,,,,,,,,,"0|z1fjhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"01/Feb/23 08:01;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39841;;;","01/Feb/23 10:12;dongjoon;Issue resolved by pull request 39841
[https://github.com/apache/spark/pull/39841];;;",,,,,,,,,
ResolveGroupingAnalytics should take care of Python UDAF,SPARK-42259,13522278,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,31/Jan/23 17:49,01/Feb/23 09:42,13/Jul/23 08:43,01/Feb/23 09:42,3.2.0,3.3.0,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,SQL,,,0,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 09:42:14 UTC 2023,,,,,,,,,,"0|z1fimw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Jan/23 17:53;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39824;;;","01/Feb/23 09:42;cloud_fan;Issue resolved by pull request 39824
[https://github.com/apache/spark/pull/39824];;;",,,,,,,,,
predict_batch_udf with float fails when the batch size consists of single value,SPARK-42250,13522150,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,31/Jan/23 03:31,31/Jan/23 10:42,13/Jul/23 08:43,31/Jan/23 10:42,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,ML,PySpark,,0,,"{code}
import numpy as np
import pandas as pd
from pyspark.ml.functions import predict_batch_udf
from pyspark.sql.types import ArrayType, FloatType, StructType, StructField
from typing import Mapping

df = spark.createDataFrame([[[0.0, 1.0, 2.0, 3.0], [0.0, 1.0, 2.0]], [[4.0, 5.0, 6.0, 7.0], [4.0, 5.0, 6.0]]], schema=[""t1"", ""t2""])

def make_multi_sum_fn():
    def predict(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:
        return np.sum(x1, axis=1) + np.sum(x2, axis=1)
    return predict

multi_sum_udf = predict_batch_udf(
    make_multi_sum_fn,
    return_type=FloatType(),
    batch_size=1,
    input_tensor_shapes=[[4], [3]],
)

df.select(multi_sum_udf(""t1"", ""t2"")).collect()
{code}

fails as below:

{code}
 File ""/.../spark/python/lib/pyspark.zip/pyspark/worker.py"", line 829, in main
    process()
  File ""/.../spark/python/lib/pyspark.zip/pyspark/worker.py"", line 821, in process
    serializer.dump_stream(out_iter, outfile)
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 345, in dump_stream
    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 86, in dump_stream
    for batch in iterator:
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 339, in init_stream_yield_batches
    batch = self._create_batch(series)
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 275, in _create_batch
    arrs.append(create_array(s, t))
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 245, in create_array
    raise e
  File ""/.../spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py"", line 233, in create_array
    array = pa.Array.from_pandas(s, mask=mask, type=t, safe=self._safecheck)
  File ""pyarrow/array.pxi"", line 1044, in pyarrow.lib.Array.from_pandas
  File ""pyarrow/array.pxi"", line 316, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 83, in pyarrow.lib._ndarray_to_array
  File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Could not convert array(569.) with type numpy.ndarray: tried to convert to float32

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:554)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:507)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

{code}",,apachespark,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 10:42:21 UTC 2023,,,,,,,,,,"0|z1fhug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"31/Jan/23 03:57;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39817;;;","31/Jan/23 10:42;gurwls223;Issue resolved by pull request 39817
[https://github.com/apache/spark/pull/39817];;;",,,,,,,,,
Upgrade snappy-java to 1.1.9.1,SPARK-42242,13522132,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,30/Jan/23 23:28,01/Feb/23 06:14,13/Jul/23 08:43,01/Feb/23 06:14,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Build,,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 06:14:55 UTC 2023,,,,,,,,,,"0|z1fhqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Jan/23 23:31;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39811;;;","01/Feb/23 06:14;dongjoon;Issue resolved by pull request 39811
[https://github.com/apache/spark/pull/39811];;;",,,,,,,,,
 Correct the condition for `SparkConnectServerUtils#findSparkConnectJar` to find the correct connect server jar for maven,SPARK-42241,13522105,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,30/Jan/23 17:52,31/Jan/23 00:12,13/Jul/23 08:43,31/Jan/23 00:12,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Connect,Tests,,0,,,,apachespark,gurwls223,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 00:12:09 UTC 2023,,,,,,,,,,"0|z1fhkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"30/Jan/23 17:57;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39810;;;","30/Jan/23 17:58;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39810;;;","31/Jan/23 00:12;gurwls223;Issue resolved by pull request 39810
[https://github.com/apache/spark/pull/39810];;;",,,,,,,,
connect-client-jvm module should shaded+relocation grpc,SPARK-42228,13521912,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,LuciferYang,LuciferYang,LuciferYang,29/Jan/23 08:37,01/Feb/23 18:11,13/Jul/23 08:43,01/Feb/23 18:11,3.4.0,3.5.0,,,,,,,,,,,,,,3.4.0,,,,Connect,,,0,,,,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 29 08:51:32 UTC 2023,,,,,,,,,,"0|z1fge8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Jan/23 08:51;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39789;;;",,,,,,,,,,
Branch-3.4 daily test failed,SPARK-42214,13521765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,LuciferYang,LuciferYang,27/Jan/23 14:27,30/Jan/23 12:26,13/Jul/23 08:43,30/Jan/23 12:26,3.5.0,,,,,,,,,,,,,,,,,,,Project Infra,,,0,,https://github.com/apache/spark/actions/runs/4023012095/jobs/6913400923,,apachespark,gurwls223,LuciferYang,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 12:26:32 UTC 2023,,,,,,,,,,"0|z1ffig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"27/Jan/23 14:36;LuciferYang;It seems that the environment of the daily test is different from that of the submitted test. For example, the Pandas version of the daily test is only 1.3.5(should be 1.5.3).

 ;;;","27/Jan/23 15:07;LuciferYang;{code:java}
WARNING: You are using pip version 22.0.3; however, version 22.3.1 is available.
84You should consider upgrading via the '/usr/bin/python3.9 -m pip install --upgrade pip' command.
85WARNING: The directory '/github/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H  {code}
There is such message in the failed tasks

 

and pip list result as follows:

 
{code:java}
Package                   Version
------------------------- --------------------
alembic                   1.7.6
certifi                   2019.11.28
chardet                   3.0.4
click                     8.0.3
cloudpickle               2.0.0
coverage                  6.3.1
cycler                    0.11.0
databricks-cli            0.16.4
dbus-python               1.2.16
docker                    5.0.3
entrypoints               0.4
Flask                     2.0.2
fonttools                 4.29.1
gitdb                     4.0.9
GitPython                 3.1.26
greenlet                  1.1.2
gunicorn                  20.1.0
idna                      2.8
importlib-metadata        4.10.1
itsdangerous              2.0.1
Jinja2                    3.0.3
joblib                    1.1.0
kiwisolver                1.3.2
Mako                      1.1.6
MarkupSafe                2.0.1
matplotlib                3.5.1
mlflow                    1.23.1
numpy                     1.22.2
packaging                 21.3
pandas                    1.3.5
Pillow                    9.0.1
pip                       22.0.3
plotly                    5.5.0
prometheus-client         0.13.1
prometheus-flask-exporter 0.18.7
protobuf                  3.19.4
pyarrow                   7.0.0
PyGObject                 3.36.0
pyparsing                 3.0.7
python-apt                2.0.0+ubuntu0.20.4.6
python-dateutil           2.8.2
pytz                      2021.3
PyYAML                    6.0
querystring-parser        1.2.4
requests                  2.22.0
requests-unixsocket       0.2.0
scikit-learn              1.0.2
scipy                     1.8.0
setuptools                45.2.0
six                       1.14.0
sklearn                   0.0
smmap                     5.0.0
SQLAlchemy                1.4.31
sqlparse                  0.4.2
tabulate                  0.8.9
tenacity                  8.0.1
threadpoolctl             3.1.0
urllib3                   1.25.8
websocket-client          1.2.3
Werkzeug                  2.0.3
wheel                     0.34.2
xmlrunner                 1.7.7
zipp                      3.7.0 {code}
{code:java}
Package             Version
------------------- --------------------
certifi             2019.11.28
cffi                1.14.6
chardet             3.0.4
coverage            6.3.1
cycler              0.11.0
dbus-python         1.2.16
fonttools           4.29.1
greenlet            0.4.13
hpy                 0.0.3
idna                2.8
kiwisolver          1.3.2
matplotlib          3.5.1
numpy               1.21.5
packaging           21.3
pandas              1.3.5
Pillow              9.0.1
pip                 22.0.3
PyGObject           3.36.0
pyparsing           3.0.7
python-apt          2.0.0+ubuntu0.20.4.6
python-dateutil     2.8.2
pytz                2021.3
readline            6.2.4.1
requests            2.22.0
requests-unixsocket 0.2.0
scipy               1.7.3
setuptools          45.2.0
six                 1.14.0
urllib3             1.25.8
wheel               0.34.2 {code}
Some packages are missing here, and the existing package versions are also lower than those used in the master;;;","28/Jan/23 05:58;LuciferYang;[~gurwls223] [~yikun]  branch-3.4 daily build has some failed tasks...
Because I am not familiar with python, I lack a solution to this error. I want to know whether the failed GA task in branch-3.4 daily build needs `infra-image` task.

 

[https://github.com/apache/spark/actions/runs/4023012095/jobs/6913400923];;;","28/Jan/23 06:05;gurwls223;cc [~itholic]and [~XinrongM] FYI;;;","28/Jan/23 06:30;yikunkero;https://github.com/apache/spark/blob/c8b1e526b4a980550c4eeab541f7cd3d5aa6e0f2/.github/workflows/build_and_test.yml#L61
https://github.com/apache/spark/blob/c8b1e526b4a980550c4eeab541f7cd3d5aa6e0f2/.github/workflows/build_and_test.yml#L276

We should also add the `inputs.branch == 'branch-3.4'` in these two lines.

The branch-* sheduled job is based on master branch workflow.;;;","28/Jan/23 06:56;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39778;;;","28/Jan/23 06:57;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39778;;;","30/Jan/23 12:26;yikunkero;Branch-3.4 scheduled job recovered:

https://github.com/apache/spark/pull/39778#issuecomment-1408528171;;;",,,
`build/sbt` should allow SBT_OPTS to override JVM memory setting,SPARK-42201,13521661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,26/Jan/23 20:16,27/Jan/23 00:39,13/Jul/23 08:43,27/Jan/23 00:39,2.4.8,3.0.3,3.1.3,3.2.3,3.3.1,3.4.0,,,,,,,,,,3.2.4,3.3.2,3.4.0,,Build,,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 27 00:39:57 UTC 2023,,,,,,,,,,"0|z1fevk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Jan/23 20:21;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39758;;;","27/Jan/23 00:39;dongjoon;Issue resolved by pull request 39758
[https://github.com/apache/spark/pull/39758];;;",,,,,,,,,
Typo in StreamingQuery.scala,SPARK-42196,13521585,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ganeshchand,ganeshchand,ganeshchand,26/Jan/23 05:40,30/Jan/23 05:43,13/Jul/23 08:43,30/Jan/23 05:43,3.2.3,,,,,,,,,,,,,,,3.4.0,,,,Structured Streaming,,,0,,"{color:#172b4d}/**{color}
{color:#172b4d} * Returns the unique id of this run of the query. That is, every start/restart of a query *will*{color}
{color:#172b4d} ** *generated* a unique runId. Therefore, every time a query is restarted from{color}
{color:#172b4d} * checkpoint, it will have the same [[id]] but different [[runId]]s.{color}
{color:#172b4d} */{color}
{color:#172b4d}def runId: UUID{color}",,apachespark,ganeshchand,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,Mon Jan 30 05:43:43 UTC 2023,,,,,,,,,,"0|z1feeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"26/Jan/23 05:55;ganeshchand;I have created a PR with a fix - https://github.com/apache/spark/pull/39750;;;","26/Jan/23 05:56;apachespark;User 'ganeshchand' has created a pull request for this issue:
https://github.com/apache/spark/pull/39750;;;","26/Jan/23 05:57;apachespark;User 'ganeshchand' has created a pull request for this issue:
https://github.com/apache/spark/pull/39750;;;","30/Jan/23 05:43;gurwls223;Issue resolved by pull request 39750
[https://github.com/apache/spark/pull/39750];;;",,,,,,,
Force SBT protobuf version to match Maven on branch 3.2 and 3.3,SPARK-42188,13521497,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,svaughan,svaughan,svaughan,25/Jan/23 14:41,26/Jan/23 02:31,13/Jul/23 08:43,26/Jan/23 02:31,3.2.3,3.3.1,,,,,,,,,,,,,,3.2.4,3.3.2,,,Build,,,0,,"Update SparkBuild.scala to force SBT use of protobuf-java to match the Maven version.  The Maven dependencyManagement section forces protobuf-java to use 2.5.0, but SBT is using 3.14.0.

Snippet from Maven dependency tree

 
{noformat}
[INFO] +- com.google.crypto.tink:tink:jar:1.6.0:compile
[INFO] |  +- com.google.protobuf:protobuf-java:jar:2.5.0:compile    <--- 2.x
[INFO] |  \- com.google.code.gson:gson:jar:2.8.6:compile{noformat}
  Snippet from SBT dependency tree
{noformat}
[info]   +-com.google.crypto.tink:tink:1.6.0
[info]   | +-com.google.code.gson:gson:2.8.6
[info]   | +-com.google.protobuf:protobuf-java:3.14.0               <--- 3.x{noformat}
The fix is updating SparkBuild.scala just like SPARK-11538 did with guava.  In addition we should comment on the need to keep the top-level pom.xml and SparkBuild.scala in sync as was done in SPARK-41247

 ",,apachespark,svaughan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 17:41:04 UTC 2023,,,,,,,,,,"0|z1fdvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 17:14;apachespark;User 'snmvaughan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39745;;;","25/Jan/23 17:14;apachespark;User 'snmvaughan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39745;;;","25/Jan/23 17:40;apachespark;User 'snmvaughan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39746;;;","25/Jan/23 17:41;apachespark;User 'snmvaughan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39746;;;",,,,,,,
Make SparkR able to stop properly when the connection is timed-out,SPARK-42186,13521442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,25/Jan/23 12:25,25/Jan/23 17:13,13/Jul/23 08:43,25/Jan/23 17:13,3.5.0,,,,,,,,,,,,,,,3.4.0,,,,SparkR,,,0,,"{code}
./bin/sparkR --conf spark.r.backendConnectionTimeout=10
{code}

wait 10 secs.

{code}

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.4.0-SNAPSHOT
      /_/


SparkSession Web UI available at http://192.168.35.219:4040
SparkSession available as 'spark'(master = local[*], app id = local-1674649482367).
> 23/01/25 21:24:53 WARN RBackendHandler: Ignoring read timeout in RBackendHandler
spark.session()
Error in spark.session() : could not find function ""spark.session""
> spark.session.stop()
Error in spark.session.stop() :
  could not find function ""spark.session.stop""
{code}",,apachespark,dongjoon,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 17:13:34 UTC 2023,,,,,,,,,,"0|z1fdj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 12:42;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39742;;;","25/Jan/23 17:13;dongjoon;Issue resolved by pull request 39742
[https://github.com/apache/spark/pull/39742];;;",,,,,,,,,
Upgrade ORC to 1.7.8,SPARK-42179,13521224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,25/Jan/23 07:31,25/Jan/23 11:24,13/Jul/23 08:43,25/Jan/23 11:24,3.3.2,,,,,,,,,,,,,,,3.3.2,,,,Build,SQL,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 11:24:47 UTC 2023,,,,,,,,,,"0|z1fc6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 07:38;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39735;;;","25/Jan/23 11:24;dongjoon;Issue resolved by pull request 39735
[https://github.com/apache/spark/pull/39735];;;",,,,,,,,,
Change master to brach-3.4 in GitHub Actions,SPARK-42177,13521201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,25/Jan/23 03:21,25/Jan/23 03:38,13/Jul/23 08:43,25/Jan/23 03:36,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Project Infra,,,0,,See https://github.com/apache/spark/actions/runs/4002380215/jobs/6869886029,,apachespark,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 03:38:52 UTC 2023,,,,,,,,,,"0|z1fc1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 03:36;gurwls223;Issue resolved by pull request 39731
[https://github.com/apache/spark/pull/39731];;;","25/Jan/23 03:37;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39730;;;","25/Jan/23 03:38;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39731;;;","25/Jan/23 03:38;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39731;;;",,,,,,,
Cast boolean to timestamp fails with ClassCastException,SPARK-42176,13521193,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,25/Jan/23 00:49,25/Jan/23 03:34,13/Jul/23 08:43,25/Jan/23 03:33,3.3.1,3.4.0,3.5.0,,,,,,,,,,,,,3.3.2,3.4.0,3.5.0,,SQL,,,0,,"When casting a boolean value to timestamp, the following error is thrown:
{code:java}
[info]   java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
[info]   at scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:107)
[info]   at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getWriter$5(InternalRow.scala:178)
[info]   at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getWriter$5$adapted(InternalRow.scala:178) {code}",,apachespark,gurwls223,ivan.sadikov,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 03:33:47 UTC 2023,,,,,,,,,,"0|z1fbzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 02:05;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39729;;;","25/Jan/23 03:33;gurwls223;Issue resolved by pull request 39729
[https://github.com/apache/spark/pull/39729];;;",,,,,,,,,
Use scikit-learn instead of sklearn,SPARK-42174,13521185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,24/Jan/23 22:49,25/Jan/23 00:29,13/Jul/23 08:43,25/Jan/23 00:29,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Project Infra,PySpark,,0,,,,apachespark,dongjoon,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 00:29:36 UTC 2023,,,,,,,,,,"0|z1fby0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 22:51;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39727;;;","25/Jan/23 00:29;gurwls223;Issue resolved by pull request 39727
[https://github.com/apache/spark/pull/39727];;;",,,,,,,,,
Fix `pyspark-errors` module and enable it in GitHub Action,SPARK-42171,13521153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,24/Jan/23 18:15,24/Jan/23 21:08,13/Jul/23 08:43,24/Jan/23 21:08,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,PySpark,Tests,,0,3.4.0,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 21:08:24 UTC 2023,,,,,,,,,,"0|z1fbqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 18:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39721;;;","24/Jan/23 21:08;dongjoon;Issue resolved by pull request 39721
[https://github.com/apache/spark/pull/39721];;;",,,,,,,,,
CoGroup with window function returns incorrect result when partition keys differ in order,SPARK-42168,13521114,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EnricoMi,EnricoMi,EnricoMi,24/Jan/23 12:12,30/Jan/23 10:55,13/Jul/23 08:43,26/Jan/23 01:43,3.0.3,3.1.3,3.2.3,,,,,,,,,,,,,3.2.4,,,,PySpark,SQL,,0,correctness,"The following example returns an incorrect result:
{code:java}
import pandas as pd

from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, lit, sum

spark = SparkSession \
    .builder \
    .getOrCreate()

ids = 1000
days = 1000
parts = 10

id_df = spark.range(ids)
day_df = spark.range(days).withColumnRenamed(""id"", ""day"")
id_day_df = id_df.join(day_df)
left_df = id_day_df.select(col(""id"").alias(""id""), col(""day"").alias(""day""), lit(""left"").alias(""side"")).repartition(parts).cache()
right_df = id_day_df.select(col(""id"").alias(""id""), col(""day"").alias(""day""), lit(""right"").alias(""side"")).repartition(parts).cache()  #.withColumnRenamed(""id"", ""id2"")

# note the column order is different to the groupBy(""id"", ""day"") column order below
window = Window.partitionBy(""day"", ""id"")

left_grouped_df = left_df.groupBy(""id"", ""day"")
right_grouped_df = right_df.withColumn(""day_sum"", sum(col(""day"")).over(window)).groupBy(""id"", ""day"")

def cogroup(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:
    return pd.DataFrame([{
        ""id"": left[""id""][0] if not left.empty else (right[""id""][0] if not right.empty else None),
        ""day"": left[""day""][0] if not left.empty else (right[""day""][0] if not right.empty else None),
        ""lefts"": len(left.index),
        ""rights"": len(right.index)
    }])

df = left_grouped_df.cogroup(right_grouped_df) \
         .applyInPandas(cogroup, schema=""id long, day long, lefts integer, rights integer"")

df.explain()
df.show(5)
{code}
Output is
{code}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- FlatMapCoGroupsInPandas [id#8L, day#9L], [id#29L, day#30L], cogroup(id#8L, day#9L, side#10, id#29L, day#30L, side#31, day_sum#54L), [id#64L, day#65L, lefts#66, rights#67]
   :- Sort [id#8L ASC NULLS FIRST, day#9L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(id#8L, day#9L, 200), ENSURE_REQUIREMENTS, [plan_id=117]
   :     +- ...
   +- Sort [id#29L ASC NULLS FIRST, day#30L ASC NULLS FIRST], false, 0
      +- Project [id#29L, day#30L, id#29L, day#30L, side#31, day_sum#54L]
         +- Window [sum(day#30L) windowspecdefinition(day#30L, id#29L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS day_sum#54L], [day#30L, id#29L]
            +- Sort [day#30L ASC NULLS FIRST, id#29L ASC NULLS FIRST], false, 0
               +- Exchange hashpartitioning(day#30L, id#29L, 200), ENSURE_REQUIREMENTS, [plan_id=112]
                  +- ...


+---+---+-----+------+
| id|day|lefts|rights|
+---+---+-----+------+
|  0|  3|    0|     1|
|  0|  4|    0|     1|
|  0| 13|    1|     0|
|  0| 27|    0|     1|
|  0| 31|    0|     1|
+---+---+-----+------+
only showing top 5 rows
{code}
The first child is hash-partitioned by {{id}} and {{{}day{}}}, while the second child is hash-partitioned by {{day}} and {{id}} (required by the window function). Therefore, rows end up in different partitions.

This has been fixed in Spark 3.3 by [#32875|https://github.com/apache/spark/pull/32875/files#diff-e938569a4ca4eba8f7e10fe473d4f9c306ea253df151405bcaba880a601f075fR75-R76]:
{code}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- FlatMapCoGroupsInPandas [id#8L, day#9L], [id#29L, day#30L], cogroup(id#8L, day#9L, side#10, id#29L, day#30L, side#31, day_sum#54L)#63, [id#64L, day#65L, lefts#66, rights#67]
   :- Sort [id#8L ASC NULLS FIRST, day#9L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(id#8L, day#9L, 200), ENSURE_REQUIREMENTS, [plan_id=117]
   :     +- ...
   +- Sort [id#29L ASC NULLS FIRST, day#30L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(id#29L, day#30L, 200), ENSURE_REQUIREMENTS, [plan_id=118]
         +- Project [id#29L, day#30L, id#29L, day#30L, side#31, day_sum#54L]
            +- Window [sum(day#30L) windowspecdefinition(day#30L, id#29L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS day_sum#54L], [day#30L, id#29L]
               +- Sort [day#30L ASC NULLS FIRST, id#29L ASC NULLS FIRST], false, 0
                  +- Exchange hashpartitioning(day#30L, id#29L, 200), ENSURE_REQUIREMENTS, [plan_id=112]
                     +- ...

+---+---+-----+------+
| id|day|lefts|rights|
+---+---+-----+------+
|  0| 13|    1|     1|
|  0| 63|    1|     1|
|  0| 89|    1|     1|
|  0| 95|    1|     1|
|  0| 96|    1|     1|
+---+---+-----+------+
only showing top 5 rows
{code}

Only PySpark is to be affected ({{FlatMapCoGroupsInPandas }}), as Scala API uses {{CoGroup}}. {{FlatMapCoGroupsInPandas}} reports required child distribution {{ClusteredDistribution}}, while {{CoGroup}} reports {{HashClusteredDistribution}}. The {{EnsureRequirements}} rule correctly recognizes a {{HashClusteredDistribution(id, day)}} as not compatible with {{hashpartitioning(day, id)}}, while {{ClusteredDistribution(id, day)}} is compatible with {{hashpartitioning(day, id)}}.",,apachespark,EnricoMi,gurwls223,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 10:55:11 UTC 2023,,,,,,,,,,"0|z1fbig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 14:06;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39717;;;","26/Jan/23 01:43;gurwls223;Issue resolved by pull request 39717
[https://github.com/apache/spark/pull/39717];;;","26/Jan/23 07:24;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39752;;;","28/Jan/23 10:49;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39781;;;","30/Jan/23 10:55;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39803;;;",,,,,,
Schema pruning fails on non-foldable array index or map key,SPARK-42163,13521048,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,David Cashman,David Cashman,David Cashman,23/Jan/23 23:08,31/Jan/23 02:17,13/Jul/23 08:43,31/Jan/23 02:17,3.2.3,,,,,,,,,,,,,,,3.4.0,,,,Optimizer,,,0,,"Schema pruning tries to extract selected fields from struct extractors. It looks through GetArrayItem/GetMapItem, but when doing so, it ignores the index/key, which may itself be a struct field. If it is a struct field that is not otherwise selected, and some other field of the same attribute is selected, then pruning will drop the field, resulting in an optimizer error.",,apachespark,cloud_fan,David Cashman,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 02:17:02 UTC 2023,,,,,,,,,,"0|z1fb40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 15:05;apachespark;User 'cashmand' has created a pull request for this issue:
https://github.com/apache/spark/pull/39718;;;","31/Jan/23 02:17;cloud_fan;Issue resolved by pull request 39718
[https://github.com/apache/spark/pull/39718];;;",,,,,,,,,
Memory usage on executors increased drastically for a complex query with large number of addition operations,SPARK-42162,13521035,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scnakandala,scnakandala,scnakandala,23/Jan/23 20:43,10/Feb/23 15:57,13/Jul/23 08:43,10/Feb/23 15:57,3.3.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"With the [recent changes|https://github.com/apache/spark/pull/37851]  in the expression canonicalization, a complex query with a large number of Add operations ends up consuming 10x more memory on the executors.

The reason for this issue is that with the new changes the canonicalization process ends up generating lot of intermediate objects, especially for complex queries with a large number of commutative operators. In this specific case, a heap histogram analysis shows that a large number of Add objects use the extra memory.
This issue does not happen before PR [#37851.|https://github.com/apache/spark/pull/37851]

The high memory usage causes the executors to lose heartbeat signals and results in task failures.",,apachespark,blackpig,cloud_fan,mkazia,scnakandala,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 15:57:16 UTC 2023,,,,,,,,,,"0|z1fb1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"24/Jan/23 18:49;apachespark;User 'db-scnakandala' has created a pull request for this issue:
https://github.com/apache/spark/pull/39722;;;","24/Jan/23 18:50;apachespark;User 'db-scnakandala' has created a pull request for this issue:
https://github.com/apache/spark/pull/39722;;;","10/Feb/23 15:57;cloud_fan;Issue resolved by pull request 39722
[https://github.com/apache/spark/pull/39722];;;",,,,,,,,
`spark.scheduler.mode=FAIR` should provide FAIR scheduler,SPARK-42157,13520943,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,22/Jan/23 22:39,24/Jan/23 07:48,13/Jul/23 08:43,24/Jan/23 07:48,2.2.3,2.3.4,2.4.8,3.0.3,3.1.3,3.2.3,3.3.1,3.4.0,,,,,,,,3.2.4,3.3.2,3.4.0,,Spark Core,,,0,, !Screenshot 2023-01-22 at 2.39.34 PM.png! ,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,"22/Jan/23 22:39;dongjoon;Screenshot 2023-01-22 at 2.39.34 PM.png;https://issues.apache.org/jira/secure/attachment/13054765/Screenshot+2023-01-22+at+2.39.34+PM.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 07:48:52 UTC 2023,,,,,,,,,,"0|z1fagw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Jan/23 22:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39703;;;","24/Jan/23 07:48;dongjoon;Issue resolved by pull request 39703
[https://github.com/apache/spark/pull/39703];;;",,,,,,,,,
Support client-side retries in Spark Connect Python client,SPARK-42156,13520940,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,grundprinzip-db,grundprinzip-db,grundprinzip-db,22/Jan/23 17:57,31/Jan/23 07:05,13/Jul/23 08:43,31/Jan/23 07:05,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Connect,,,0,,,,apachespark,grundprinzip-db,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-39375,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 07:05:43 UTC 2023,,,,,,,,,,"0|z1fag8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"22/Jan/23 18:29;apachespark;User 'grundprinzip' has created a pull request for this issue:
https://github.com/apache/spark/pull/39695;;;","31/Jan/23 07:05;gurwls223;Issue resolved by pull request 39695
[https://github.com/apache/spark/pull/39695];;;",,,,,,,,,
Fix getPartitionFiltersAndDataFilters() to handle filters without referenced attributes,SPARK-42134,13520766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,20/Jan/23 13:46,21/Jan/23 02:38,13/Jul/23 08:43,21/Jan/23 02:38,3.4.0,,,,,,,,,,,,,,,3.3.2,3.4.0,,,SQL,,,0,,,,apachespark,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 20 14:00:42 UTC 2023,,,,,,,,,,"0|z1f9e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"20/Jan/23 14:00;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/39676;;;",,,,,,,,,,
Push down limit through Python UDFs,SPARK-42115,13520190,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,19/Jan/23 11:52,02/Feb/23 01:01,13/Jul/23 08:43,02/Feb/23 01:01,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,PySpark,SQL,,0,,"{code}
from pyspark.sql.functions import udf

spark.range(10).write.mode(""overwrite"").parquet(""/tmp/abc"")

@udf(returnType='string')
def my_udf(arg):
    return arg


df = spark.read.parquet(""/tmp/abc"")
df.limit(10).withColumn(""prediction"", my_udf(df[""id""])).explain()
{code}

As an example. since Python UDFs are executed asynchronously, so pushing limits benefit the performance.

{code}
== Physical Plan ==
CollectLimit 10
+- *(2) Project [id#3L, pythonUDF0#10 AS prediction#6]
   +- BatchEvalPython [my_udf(id#3L)#5], [pythonUDF0#10]
      +- *(1) ColumnarToRow
         +- FileScan parquet [id#3L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/abc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>
{code}

This is a regression from Spark 3.3.1:

{code}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [id#3L, pythonUDF0#10 AS prediction#6]
   +- BatchEvalPython [my_udf(id#3L)#5], [pythonUDF0#10]
      +- GlobalLimit 10
         +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=30]
            +- LocalLimit 10
               +- FileScan parquet [id#3L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/abc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>
{code}",,apachespark,cloud_fan,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 01:01:34 UTC 2023,,,,,,,,,,"0|z1f5u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Jan/23 13:17;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/39653;;;","01/Feb/23 08:17;apachespark;User 'kelvinjian-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/39842;;;","02/Feb/23 01:01;cloud_fan;Issue resolved by pull request 39842
[https://github.com/apache/spark/pull/39842];;;",,,,,,,,
Upgrade pandas to 1.5.3,SPARK-42113,13520157,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,19/Jan/23 09:21,19/Jan/23 16:54,13/Jul/23 08:43,19/Jan/23 16:54,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Pandas API on Spark,,,0,,To support latest pandas.,,apachespark,dongjoon,itholic,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 16:54:58 UTC 2023,,,,,,,,,,"0|z1f5mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Jan/23 09:26;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/39651;;;","19/Jan/23 16:54;dongjoon;Issue resolved by pull request 39651
[https://github.com/apache/spark/pull/39651];;;",,,,,,,,,
Add null check before `ContinuousWriteRDD#compute` method close dataWriter,SPARK-42112,13520153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,19/Jan/23 08:17,20/Jan/23 03:24,13/Jul/23 08:43,20/Jan/23 03:24,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"Run 

 

mvn clean test -pl sql/core -Dtest=none -DwildcardSuites=org.apache.spark.sql.streaming.continuous.ContinuousSuite -am

 

NPE exists in the test log

 
{code:java}
- repeatedly restart
...
16:07:39.891 ERROR org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD: Writer for partition 1 is aborting.
16:07:39.891 ERROR org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD: Writer for partition 1 aborted.
16:07:39.892 WARN org.apache.spark.util.Utils: Suppressing exception in finally: null
java.lang.NullPointerException
    at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.$anonfun$compute$7(ContinuousWriteRDD.scala:91)
    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1558)
    at org.apache.spark.sql.execution.streaming.continuous.ContinuousWriteRDD.compute(ContinuousWriteRDD.scala:91)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
    at org.apache.spark.scheduler.Task.run(Task.scala:139)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1502)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750) {code}
 

 

The test did not fail because Utils.tryWithSafeFinallyAndFailureCallbacks function suppressed Exception in finally block

 ",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 20 03:24:40 UTC 2023,,,,,,,,,,"0|z1f5m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Jan/23 08:30;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39650;;;","20/Jan/23 03:24;dongjoon;This is resolved via https://github.com/apache/spark/pull/39650;;;",,,,,,,,,
Upgrade Kafka to 3.3.2,SPARK-42109,13520107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,18/Jan/23 20:06,18/Jan/23 22:38,13/Jul/23 08:43,18/Jan/23 22:38,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Structured Streaming,,,0,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 22:38:08 UTC 2023,,,,,,,,,,"0|z1f5cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"18/Jan/23 20:15;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39646;;;","18/Jan/23 20:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39646;;;","18/Jan/23 22:38;dongjoon;Issue resolved by pull request 39646
[https://github.com/apache/spark/pull/39646];;;",,,,,,,,
Support `fill_value` for `ps.Series.add`,SPARK-42094,13519830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,17/Jan/23 01:43,08/Feb/23 00:17,13/Jul/23 08:43,08/Feb/23 00:17,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,Pandas API on Spark,,,0,,For pandas function parity: https://pandas.pydata.org/docs/reference/api/pandas.Series.add.html,,apachespark,gurwls223,itholic,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 00:17:08 UTC 2023,,,,,,,,,,"0|z1f3nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"29/Jan/23 08:50;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/39790;;;","08/Feb/23 00:17;gurwls223;Issue resolved by pull request 39790
[https://github.com/apache/spark/pull/39790];;;",,,,,,,,,
Introduce sasl retry count in RetryingBlockTransferor,SPARK-42090,13519791,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuzhihong@gmail.com,yuzhihong@gmail.com,yuzhihong@gmail.com,16/Jan/23 14:00,24/Jan/23 18:24,13/Jul/23 08:43,17/Jan/23 06:49,3.4.0,,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,Spark Core,,,0,,"Previously a boolean variable, saslTimeoutSeen, was used in RetryingBlockTransferor. However, the boolean variable wouldn't cover the following scenario:

1. SaslTimeoutException
2. IOException
3. SaslTimeoutException
4. IOException

Even though IOException at #2 is retried (resulting in increment of retryCount), the retryCount would be cleared at step #4.
Since the intention of saslTimeoutSeen is to undo the increment due to retrying SaslTimeoutException, we should keep a counter for SaslTimeoutException retries and subtract the value of this counter from retryCount.",,apachespark,mridulm80,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 23 18:59:51 UTC 2023,,,,,,,,,,"0|z1f3ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Jan/23 14:05;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/39611;;;","17/Jan/23 06:49;mridulm80;Issue resolved by pull request 39611
[https://github.com/apache/spark/pull/39611];;;","18/Jan/23 00:13;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39634;;;","18/Jan/23 00:13;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39632;;;","18/Jan/23 00:14;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39634;;;","23/Jan/23 18:55;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39709;;;","23/Jan/23 18:56;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39709;;;","23/Jan/23 18:59;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/39710;;;",,,
Running python3 setup.py sdist on windows reports a permission error,SPARK-42088,13519711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zheju_he,zheju_he,zheju_he,16/Jan/23 08:02,17/Jan/23 00:38,13/Jul/23 08:43,17/Jan/23 00:38,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Build,,,0,,"My system version is windows 10, and I can run setup.py with administrator permissions, so there will be no error. However, it may be troublesome for us to upgrade permissions with Windows Server, so we need to modify the code of setup.py to ensure no error. To avoid the hassle of compiling for the user, I suggest modifying the following code to enable the out-of-the-box effect
{code:python}
def _supports_symlinks():
    """"""Check if the system supports symlinks (e.g. *nix) or not.""""""
    return getattr(os, ""symlink"", None) is not None and ctypes.windll.shell32.IsUserAnAdmin() != 0 if sys.platform == ""win32"" else True
{code}",,apachespark,gurwls223,zheju_he,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 00:38:58 UTC 2023,,,,,,,,,,"0|z1f2x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Jan/23 08:09;zheju_he;This is my pr address https://github.com/apache/spark/pull/39603;;;","16/Jan/23 08:09;apachespark;User 'zekai-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/39603;;;","17/Jan/23 00:38;gurwls223;Issue resolved by pull request 39603
[https://github.com/apache/spark/pull/39603];;;",,,,,,,,
Avoid leaking the qualified-access-only restriction,SPARK-42084,13519694,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,16/Jan/23 05:38,18/Jan/23 10:59,13/Jul/23 08:43,18/Jan/23 10:44,3.3.1,,,,,,,,,,,,,,,3.3.2,3.4.0,,,SQL,,,0,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 10:44:22 UTC 2023,,,,,,,,,,"0|z1f2tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"16/Jan/23 06:00;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39596;;;","16/Jan/23 06:00;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39596;;;","18/Jan/23 10:44;cloud_fan;Issue resolved by pull request 39596
[https://github.com/apache/spark/pull/39596];;;",,,,,,,,
The DATATYPE_MISMATCH error class contains inappropriate and duplicating subclasses,SPARK-42066,13519167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,srielau,srielau,14/Jan/23 21:56,30/Jan/23 10:56,13/Jul/23 08:43,30/Jan/23 10:56,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,0,,"subclass WRONG_NUM_ARGS (with suggestions) semantically does not belong into DATATYPE_MISMATCH and there is an error class with that same name.
We should rea the subclasses for this errorclass, which seems to have become a bit of a dumping ground...",,apachespark,cloud_fan,itholic,srielau,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 10:56:11 UTC 2023,,,,,,,,,,"0|z1ezk8:",9223372036854775807,,,,,maxgekk,,,,,,,,,,,,,,,,,"17/Jan/23 05:35;itholic;Let me take a look;;;","17/Jan/23 11:16;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/39625;;;","30/Jan/23 10:56;cloud_fan;Issue resolved by pull request 39625
[https://github.com/apache/spark/pull/39625];;;",,,,,,,,
Mark Expressions that have state has stateful,SPARK-42061,13518549,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,cloud_fan,cloud_fan,14/Jan/23 03:03,18/Jan/23 01:54,13/Jul/23 08:43,18/Jan/23 01:54,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 01:54:41 UTC 2023,,,,,,,,,,"0|z1evqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"17/Jan/23 22:11;apachespark;User 'lzlfred' has created a pull request for this issue:
https://github.com/apache/spark/pull/39630;;;","18/Jan/23 01:54;cloud_fan;Issue resolved by pull request 39630
[https://github.com/apache/spark/pull/39630];;;",,,,,,,,,
Update ORC to 1.8.2,SPARK-42059,13518458,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,14/Jan/23 00:02,14/Jan/23 02:57,13/Jul/23 08:43,14/Jan/23 02:57,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Build,,,0,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 14 02:57:16 UTC 2023,,,,,,,,,,"0|z1ev6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"14/Jan/23 00:05;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39561;;;","14/Jan/23 00:06;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39561;;;","14/Jan/23 02:57;dongjoon;Issue resolved by pull request 39561
[https://github.com/apache/spark/pull/39561];;;",,,,,,,,
Avoid losing exception info in Protobuf errors,SPARK-42057,13518357,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rangadi,rangadi,rangadi,13/Jan/23 18:03,14/Jan/23 04:30,13/Jul/23 08:43,14/Jan/23 04:30,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Protobuf,,,0,,"Protobuf connector related error handlers incorrectly report the exception. This is makes it hard for users to see actual issue. E.g. if there is a {{FileNotFoundException}} these error handlers use pass {{exception.getCause()}} rather than passing {{{}exception{}}}. As result, we lose the information that it was a {{FileNotFoundException}}",,apachespark,Gengliang.Wang,rangadi,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-40653,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 14 04:30:17 UTC 2023,,,,,,,,,,"0|z1euk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Jan/23 18:08;apachespark;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39536;;;","14/Jan/23 04:30;Gengliang.Wang;Issue resolved by pull request 39536
[https://github.com/apache/spark/pull/39536];;;",,,,,,,,,
Add `connect-client-jvm` to connect module,SPARK-42046,13518038,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,13/Jan/23 06:12,27/Jan/23 03:52,13/Jul/23 08:43,13/Jan/23 11:13,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Tests,,,0,,,,apachespark,dongjoon,gurwls223,,,,,,,,,,,,,,SPARK-42200,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 13 11:13:21 UTC 2023,,,,,,,,,,"0|z1esls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"13/Jan/23 06:15;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39549;;;","13/Jan/23 11:13;gurwls223;Issue resolved by pull request 39549
[https://github.com/apache/spark/pull/39549];;;",,,,,,,,,
"QueryExecutionListener and Observation API, df.observe do not work with `foreach` action.",SPARK-42034,13517904,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Zing,hryhoriev.nick,hryhoriev.nick,12/Jan/23 17:28,13/Feb/23 05:10,13/Jul/23 08:43,13/Feb/23 05:10,3.1.3,3.2.2,3.3.1,,,,,,,,,,,,,3.5.0,,,,SQL,,,0,sql-api,"Observation API, {{observe}} dataframe transformation, and custom QueryExecutionListener.
Do not work with {{foreach}} or {{foreachPartition actions.}}
{{This is due to }}QueryExecutionListener functions do not trigger on queries whose action is {{foreach}} or {{{}foreachPartition{}}}.
But the Spark GUI SQL tab sees this query as SQL query and shows its query plans and etc.


here is the code to reproduce it:
https://gist.github.com/GrigorievNick/e7cf9ec5584b417d9719e2812722e6d3","I test it locally and on YARN in cluster mode.
Spark 3.3.1 and 3.2.2 and 3.1.1.
Yarn 2.9.2 and 3.2.1.",apachespark,gurwls223,hryhoriev.nick,Zing,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,Mon Feb 13 05:10:11 UTC 2023,,,,,,,,,,"0|z1ers0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"25/Jan/23 00:53;gurwls223;I think the action has to be triggered by using `withAction` in the codes.;;;","25/Jan/23 00:53;gurwls223;please go ahead a PR if you're interested in doing that!;;;","09/Feb/23 15:47;Zing;I am interested in this pr, if no one else is in development I will fix this issue;;;","09/Feb/23 23:57;gurwls223;please go ahead;;;","10/Feb/23 07:36;hryhoriev.nick;I can do PR, but I need someone to target me to some doc or piece of code to understand how this part of spark works.
Do there any documentation or at least a diagram to show spark-sql architecture?
I really do not understand how rdd execution is linked to SQL query listeners.;;;","11/Feb/23 17:33;apachespark;User 'zzzzming95' has created a pull request for this issue:
https://github.com/apache/spark/pull/39976;;;","13/Feb/23 05:10;gurwls223;Issue resolved by pull request 39976
[https://github.com/apache/spark/pull/39976];;;",,,,
Interpreted mode subexpression elimination can throw exception during insert,SPARK-41991,13517671,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,11/Jan/23 19:16,23/Feb/23 19:13,13/Jul/23 08:43,13/Jan/23 00:42,3.3.1,3.4.0,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"Example:
{noformat}
drop table if exists tbl1;
create table tbl1 (a int, b int) using parquet;

set spark.sql.codegen.wholeStage=false;
set spark.sql.codegen.factoryMode=NO_CODEGEN;

insert into tbl1
select id as a, id as b
from range(1, 5);
{noformat}
This results in the following exception:
{noformat}
java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.ExpressionProxy cannot be cast to org.apache.spark.sql.catalyst.expressions.Cast
	at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2514)
	at org.apache.spark.sql.catalyst.expressions.CheckOverflowInTableInsert.withNewChildInternal(Cast.scala:2512)
{noformat}
The query produces 2 bigint values, but the table's schema expects 2 int values, so Spark wraps each output field with a {{Cast}}.

Later, in {{InterpretedUnsafeProjection}}, {{prepareExpressions}} tries to wrap the two {{Cast}} expressions with an {{ExpressionProxy}}. However, the parent expression of each {{Cast}} is a {{CheckOverflowInTableInsert}} expression, which does not accept {{ExpressionProxy}} as a child.

",,apachespark,bersprockets,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 19:13:50 UTC 2023,,,,,,,,,,"0|z1eqdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 20:41;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39518;;;","11/Jan/23 20:42;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/39518;;;","13/Jan/23 00:42;gurwls223;Fixed in https://github.com/apache/spark/pull/39518;;;","23/Feb/23 19:13;apachespark;User 'RunyaoChen' has created a pull request for this issue:
https://github.com/apache/spark/pull/40140;;;",,,,,,,
Filtering by composite field name like `field name` doesn't work with pushDownPredicate = true,SPARK-41990,13517668,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,mkrasilnikova,mkrasilnikova,11/Jan/23 19:08,13/Feb/23 06:46,13/Jul/23 08:43,15/Jan/23 06:53,3.3.0,3.3.1,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"Suppose we have some table in postgresql with field `Last Name` The following code results in error

Dataset<Row> dataset = sparkSession.read()
.format(""jdbc"")
.option(""url"", myUrl)
.option(""dbtable"", ""myTable"")
.option(""user"", ""myUser"")
.option(""password"", ""muPassword"")
.load();

dataset.where(""`Last Name`='Tessel'"").show();    //error

 

 

Exception in thread ""main"" org.apache.spark.sql.catalyst.parser.ParseException: 
Syntax error at or near 'Name': extra input 'Name'(line 1, pos 5)

== SQL ==
Last Name
-----^^^

    at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)
    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)
    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseMultipartIdentifier(ParseDriver.scala:67)
    at org.apache.spark.sql.connector.expressions.LogicalExpressions$.parseReference(expressions.scala:40)
    at org.apache.spark.sql.connector.expressions.FieldReference$.apply(expressions.scala:368)
    at org.apache.spark.sql.sources.IsNotNull.toV2(filters.scala:262)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.$anonfun$unhandledFilters$1(JDBCRelation.scala:278)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.$anonfun$unhandledFilters$1$adapted(JDBCRelation.scala:278)

 

But if we set pushDownPredicate to false everything works fine.",,apachespark,dongjoon,kc.shanmugavel,mkrasilnikova,panbingkun,,,,,,,,,,,,SPARK-42193,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 06:02:03 UTC 2023,,,,,,,,,,"0|z1eqco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"12/Jan/23 03:24;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39524;;;","12/Jan/23 03:25;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39524;;;","14/Jan/23 01:22;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/39564;;;","14/Jan/23 01:23;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/39564;;;","15/Jan/23 06:53;dongjoon;Issue resolved by pull request 39564
[https://github.com/apache/spark/pull/39564];;;","16/Jan/23 06:02;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/39597;;;",,,,,
PYARROW_IGNORE_TIMEZONE warning can break application logging setup,SPARK-41989,13517649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,soxofaan,soxofaan,soxofaan,11/Jan/23 15:53,12/Jan/23 09:26,13/Jul/23 08:43,12/Jan/23 09:24,3.2.3,,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.0,,PySpark,,,0,,"in {code}python/pyspark/pandas/__init__.py{code}  there is currently a warning when {{PYARROW_IGNORE_TIMEZONE}} env var is not set (https://github.com/apache/spark/blob/187c4a9c66758e973633c5c309b551b1d9094e6e/python/pyspark/pandas/__init__.py#L44-L59):

{code:python}
    import logging

    logging.warning(
        ""'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to ""...
{code}

The {{logging.warning()}} call  will silently do a {{logging.basicConfig()}} call (at least in python 3.9, which I tried).
(FYI: Something like {{logging.getLogger(...).warning()}} would not do this silent call)


This has the following very hard to figure out side-effect:
importing `pyspark.pandas` (directly or indirectly somewhere)  might break your logging setup (if PYARROW_IGNORE_TIMEZONE is not set).

Very basic  example (assuming PYARROW_IGNORE_TIMEZONE is not set):

{code:python}
import logging
import pyspark.pandas

logging.basicConfig(level=logging.DEBUG)

logger = logging.getLogger(""test"")
logger.warning(""I warn you"")
logger.debug(""I debug you"")
{code}

Will only produce the warning, not the debug line.
By removing the {{import pyspark.pandas}}, the debug line is produced",python 3.9 env with pyspark installed,apachespark,gurwls223,soxofaan,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 09:24:40 UTC 2023,,,,,,,,,,"0|z1eq8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 15:59;soxofaan;I would propose to change 
{code:python}
    logging.warning(
        ""'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to ""...
{code}

to 
{code:python}
    logger = logging.getLogger(__name__)
    logger.warning(
        ""'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to ""...
{code}

which has the added benefit that users can trace back this warning to the module that triggered it ;;;","11/Jan/23 16:08;soxofaan;I just tried and could reproduce this problem as well with pyspark 3.3.1 on python 3.11;;;","11/Jan/23 16:28;apachespark;User 'soxofaan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39516;;;","12/Jan/23 09:24;gurwls223;Issue resolved by pull request 39516
[https://github.com/apache/spark/pull/39516];;;",,,,,,,
Centralize more column resolution rules,SPARK-41985,13517600,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Jan/23 11:31,03/Feb/23 05:07,13/Jul/23 08:43,01/Feb/23 16:25,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 05:07:26 UTC 2023,,,,,,,,,,"0|z1epxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 11:43;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39508;;;","01/Feb/23 16:25;cloud_fan;Issue resolved by pull request 39508
[https://github.com/apache/spark/pull/39508];;;","03/Feb/23 05:07;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39867;;;",,,,,,,,
"When the inserted partition type is of string type, similar `dt=01` will be converted to `dt=1`",SPARK-41982,13517574,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zhongjingxiong,zhongjingxiong,zhongjingxiong,11/Jan/23 09:12,17/Jan/23 04:47,13/Jul/23 08:43,17/Jan/23 04:47,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,0,,"At present, during the process of upgrading Spark2.4 to Spark3.2, we carefully read the migration documentwe and found a kind of situation not involved:
{code:java}
create table if not exists test_90(a string, b string) partitioned by (dt string);
desc formatted test_90;
// case1
insert into table test_90 partition (dt=05) values(""1"",""2"");
// case2
insert into table test_90 partition (dt='05') values(""1"",""2"");
drop table test_90;{code}
in spark2.4.3, it will generate such a path:
{code:java}
// the path
hdfs://test5/user/hive/db1/test_90/dt=05 

//result
spark-sql> select * from test_90;
1       2       05
1       2       05
Time taken: 1.316 seconds, Fetched 2 row(s)

spark-sql> show partitions test_90; 
dt=05 
Time taken: 0.201 seconds, Fetched 1 row(s)

spark-sql> select * from test_90 where dt='05';
1       2       05
1       2       05
Time taken: 0.212 seconds, Fetched 2 row(s)

spark-sql> explain insert into table test_90 partition (dt=05) values(""1"",""2"");
== Physical Plan ==
Execute InsertIntoHiveTable InsertIntoHiveTable `db1`.`test_90`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, Map(dt -> Some(05)), false, false, [a, b]
+- LocalTableScan [a#116, b#117]
Time taken: 1.145 seconds, Fetched 1 row(s){code}
in spark3.2.0, it will generate two path:
{code:java}
// the path
hdfs://test5/user/hive/db1/test_90/dt=05 
hdfs://test5/user/hive/db1/test_90/dt=5 

// result
spark-sql> select * from test_90;
1       2       05
1       2       5
Time taken: 2.119 seconds, Fetched 2 row(s)

spark-sql> show partitions test_90;
dt=05
dt=5
Time taken: 0.161 seconds, Fetched 2 row(s)

spark-sql> select * from test_90 where dt='05';
1       2       05
Time taken: 0.252 seconds, Fetched 1 row(s)

spark-sql> explain insert into table test_90 partition (dt=05) values(""1"",""2"");
plan
== Physical Plan ==
Execute InsertIntoHiveTable `db1`.`test_90`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, [dt=Some(5)], false, false, [a, b]
+- LocalTableScan [a#109, b#110]{code}
This will cause problems in reading data after the user switches to spark3. The root cause is that in the process of partition field resolution, Spark3 has a process of strongly converting this string type, which will cause partition `05` to lose the previous `0`

So I think we have two solutions:

one is to record the risk clearly in the migration document, and the other is to repair this case, because we internally keep the partition of string type as string type, regardless of whether single or double quotation marks are added.

 

 ",,apachespark,cloud_fan,zhongjingxiong,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 04:47:16 UTC 2023,,,,,,,,,,"0|z1eprs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 11:16;zhongjingxiong;cc [~cloud_fan] [~gurwls223] I want to know your opinion.;;;","13/Jan/23 19:11;apachespark;User 'smallzhongfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/39558;;;","13/Jan/23 19:12;apachespark;User 'smallzhongfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/39558;;;","17/Jan/23 04:47;cloud_fan;Issue resolved by pull request 39558
[https://github.com/apache/spark/pull/39558];;;",,,,,,,
`toPandas` should support duplicate filed names when arrow-optimization is on,SPARK-41971,13517524,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,podongfeng,podongfeng,11/Jan/23 03:12,04/May/23 18:07,13/Jul/23 08:43,04/May/23 18:04,3.4.0,,,,,,,,,,,,,,,3.5.0,,,,PySpark,,,0,,"toPandas support duplicate columns name, but for a struct column, it doesnot support duplicate field names.

{code:java}
In [27]: spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", False)

In [28]: spark.sql(""select 1 v, 1 v"").toPandas()
Out[28]: 
   v  v
0  1  1

In [29]: spark.sql(""select struct(1 v, 1 v)"").toPandas()
Out[29]: 
  struct(1 AS v, 1 AS v)
0                 (1, 1)

In [30]: spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", True)

In [31]: spark.sql(""select 1 v, 1 v"").toPandas()
Out[31]: 
   v  v
0  1  1

In [32]: spark.sql(""select struct(1 v, 1 v)"").toPandas()
/Users/ruifeng.zheng/Dev/spark/python/pyspark/sql/pandas/conversion.py:204: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.
  Ran out of field metadata, likely malformed
  warn(msg)
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
Cell In[32], line 1
----> 1 spark.sql(""select struct(1 v, 1 v)"").toPandas()

File ~/Dev/spark/python/pyspark/sql/pandas/conversion.py:143, in PandasConversionMixin.toPandas(self)
    141 tmp_column_names = [""col_{}"".format(i) for i in range(len(self.columns))]
    142 self_destruct = jconf.arrowPySparkSelfDestructEnabled()
--> 143 batches = self.toDF(*tmp_column_names)._collect_as_arrow(
    144     split_batches=self_destruct
    145 )
    146 if len(batches) > 0:
    147     table = pyarrow.Table.from_batches(batches)

File ~/Dev/spark/python/pyspark/sql/pandas/conversion.py:358, in PandasConversionMixin._collect_as_arrow(self, split_batches)
    356             results.append(batch_or_indices)
    357     else:
--> 358         results = list(batch_stream)
    359 finally:
    360     # Join serving thread and raise any exceptions from collectAsArrowToPython
    361     jsocket_auth_server.getResult()

File ~/Dev/spark/python/pyspark/sql/pandas/serializers.py:55, in ArrowCollectSerializer.load_stream(self, stream)
     50 """"""
     51 Load a stream of un-ordered Arrow RecordBatches, where the last iteration yields
     52 a list of indices that can be used to put the RecordBatches in the correct order.
     53 """"""
     54 # load the batches
---> 55 for batch in self.serializer.load_stream(stream):
     56     yield batch
     58 # load the batch order indices or propagate any error that occurred in the JVM

File ~/Dev/spark/python/pyspark/sql/pandas/serializers.py:98, in ArrowStreamSerializer.load_stream(self, stream)
     95 import pyarrow as pa
     97 reader = pa.ipc.open_stream(stream)
---> 98 for batch in reader:
     99     yield batch

File ~/.dev/miniconda3/envs/spark_dev/lib/python3.9/site-packages/pyarrow/ipc.pxi:638, in __iter__()

File ~/.dev/miniconda3/envs/spark_dev/lib/python3.9/site-packages/pyarrow/ipc.pxi:674, in pyarrow.lib.RecordBatchReader.read_next_batch()

File ~/.dev/miniconda3/envs/spark_dev/lib/python3.9/site-packages/pyarrow/error.pxi:100, in pyarrow.lib.check_status()

ArrowInvalid: Ran out of field metadata, likely malformed

{code}
",,nikj,podongfeng,XinrongM,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 04 18:07:43 UTC 2023,,,,,,,,,,"0|z1epgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 03:14;podongfeng;I think that is due to something is wrong in `ArrowConverter`.

In Spark, a schema is just a StructType, but in arrow that is not the case, a schema is a class other than datatype. This difference maybe the cause.;;;","20/Mar/23 18:57;nikj;Can I work on this issue?;;;","04/May/23 18:04;XinrongM;Issue resolved by pull request 40988
[https://github.com/apache/spark/pull/40988];;;","04/May/23 18:07;XinrongM;Hi [~nikj] , the issue has been resolved. Feel free to pick other issues that you are interested in. Normally we comment on the ticket and file the pull request afterward directly.;;;",,,,,,,
Disallow arbitrary custom classpath with proxy user in cluster mode,SPARK-41958,13517367,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,10/Jan/23 03:20,06/Jun/23 02:31,13/Jul/23 08:43,10/Jan/23 09:06,2.4.8,3.0.3,3.1.3,3.2.3,3.3.1,,,,,,,,,,,3.3.3,3.4.0,,,Spark Core,,,0,,To avoid arbitrary classpath in spark cluster.,,apachespark,degant7,dongjoon,gurwls223,Ngone51,xkrogen,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 00:15:49 UTC 2023,,,,,,,,,,"0|z1eok0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"10/Jan/23 03:26;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/39474;;;","10/Jan/23 03:27;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/39474;;;","10/Jan/23 09:06;gurwls223;Issue resolved by pull request 39474
[https://github.com/apache/spark/pull/39474];;;","01/Jun/23 18:14;degant7;[~gurwls223] [~Ngone51] is it possible to backport this fix to Spark 3.3? Since it is Severity 9.9?;;;","06/Jun/23 00:15;dongjoon;This is backported to branch-3.3 via https://github.com/apache/spark/pull/41428;;;",,,,,,
Upgrade Parquet to fix off-heap memory leaks in Zstd codec,SPARK-41952,13517298,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,chengpan,alexey.kudinkin,alexey.kudinkin,09/Jan/23 23:48,20/Feb/23 17:45,13/Jul/23 08:43,20/Feb/23 17:45,3.1.3,3.2.3,3.3.1,,,,,,,,,,,,,3.2.4,3.3.3,3.4.0,,Input/Output,,,0,,"Recently, native memory leak have been discovered in Parquet in conjunction of it using Zstd decompressor from luben/zstd-jni library (PARQUET-2160).

This is very problematic to a point where we can't use Parquet w/ Zstd due to pervasive OOMs taking down our executors and disrupting our jobs.

Luckily fix addressing this had already landed in Parquet:
[https://github.com/apache/parquet-mr/pull/982]

 

Now, we just need to
 # Updated version of Parquet is released in a timely manner
 # Spark is upgraded onto this new version in the upcoming release

 ",,alexey.kudinkin,apachespark,chengpan,dongjoon,FateRin,LuciferYang,panbingkun,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 06:00:37 UTC 2023,,,,,,,,,,"0|z1eo4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"19/Feb/23 08:53;LuciferYang;For the old Spark versions, is it possible to introduce other costs by upgrading parquet?  Should we directly introduce parquet.hadoop.CodecFactory to old Spark version and fix them accordingly? 

After that, we can also revert the changes of the Spark version(for example, master and Spark 3.4) that can be solved by upgrading parquet;;;","20/Feb/23 05:38;chengpan;Fix on Spark side is feasible, I'm working on this.;;;","20/Feb/23 05:51;dongjoon;May I ask why you put me `Shepherd` field, [~alexey.kudinkin] ? Let me first remove me from there.;;;","20/Feb/23 06:00;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/40091;;;",,,,,,,
Fix NPE for error classes: CANNOT_PARSE_JSON_FIELD,SPARK-41948,13517202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,09/Jan/23 08:34,23/Jan/23 12:16,13/Jul/23 08:43,23/Jan/23 12:16,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,,,apachespark,maxgekk,panbingkun,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 23 12:16:16 UTC 2023,,,,,,,,,,"0|z1enjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Jan/23 08:42;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39466;;;","09/Jan/23 08:43;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39466;;;","23/Jan/23 12:16;maxgekk;Issue resolved by pull request 39466
[https://github.com/apache/spark/pull/39466];;;",,,,,,,,
Update the contents of error class guidelines,SPARK-41947,13517191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,09/Jan/23 07:23,09/Jan/23 20:24,13/Jul/23 08:43,09/Jan/23 20:24,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,"The error class guidelines for `core/src/main/resources/error/README.md` is out of date, we should update the guidelines to match the current behavior.",,apachespark,itholic,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 20:24:21 UTC 2023,,,,,,,,,,"0|z1enhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"09/Jan/23 07:29;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/39464;;;","09/Jan/23 07:30;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/39464;;;","09/Jan/23 20:24;maxgekk;Issue resolved by pull request 39464
[https://github.com/apache/spark/pull/39464];;;",,,,,,,,
SparkR datetime column compare with Sys.time() throws error in R (>= 4.2.0),SPARK-41937,13517141,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,atalvivek,atalvivek,atalvivek,08/Jan/23 02:37,09/Jan/23 00:41,13/Jul/23 08:43,09/Jan/23 00:41,3.3.0,,,,,,,,,,,,,,,3.4.0,,,,R,SparkR,,0,newbie,"Base R 4.2.0 introduced a change ([[Rd] R 4.2.0 is released|https://stat.ethz.ch/pipermail/r-announce/2022/000683.html]), ""{{{}Calling if() or while() with a condition of length greater than one gives an error rather than a warning.{}}}""

The below code is a reproducible example of the issue. If it is executed in R >=4.2.0 then it will generate an error, or else just a warning message. `{{{}Sys.time()`{}}} is a multi-class object in R, and throughout the Spark R repository '{{{}if{}}}' statement is used as: `{{{}if(class(x) == ""Column""){}}}` - this causes error in the latest R version >= 4.2.0. Note that R allows an object to have multiple '{{{}class{}}}' names as a character vector ([R: Object Classes|https://stat.ethz.ch/R-manual/R-devel/library/base/html/class.html]); hence this type of check itself was not a good idea in the first place.

The below chunks are executed on R version 4.1.3.
{code:java}
{
 SparkR::sparkR.session()
 t <- Sys.time()
 sdf <- SparkR::createDataFrame(data.frame(x = t + c(-1, 1, -1, 1, -1)))
 SparkR::collect(SparkR::filter(sdf, SparkR::column('x') > t))
}
#> Warning in if (class(e2) == 'Column') {: the condition has length > 1 
#> and only the first element will be used
#> x
#> 1 2023-01-07 20:40:20
#> 2 2023-01-07 20:40:20 

{code}
 

 
{code:java}
{
 Sys.setenv(`_R_CHECK_LENGTH_1_CONDITION_` = ""true"")
 SparkR::sparkR.session()
 t <- Sys.time()
 sdf <- SparkR::createDataFrame(data.frame(x = t + c(-1, 1, -1, 1, -1)))
 SparkR::collect(SparkR::filter(sdf, SparkR::column('x') > t))
}
#> Error in h(simpleError(msg, call)): error in evaluating the argument 'x' 
#> in selecting a method for function 'collect': error in evaluating the 
#> argument 'condition' in selecting a method for function 'filter': the
#> condition has length > 1 {code}
 

Similar issue is noted for these SparkR functions where {{Sys.time()}} type of multi-class data might be used: {{lit, fillna, when, otherwise, contains, ifelse }}

The suggested change is to add the `{{{}all{}}}` function (or `{{{}any{}}}`, as appropriate) while doing the check of whether `{{{}class(.){}}}` is `{{{}Column{}}}` or not: `{{{}if(all(class(.) == ""Column"")){}}}`. Or, better to use `{{{}base::inherits{}}}` for this check as `{{{}if(inherits(., ""Column"")){}}}`.",,apachespark,atalvivek,gurwls223,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,R,Mon Jan 09 00:41:17 UTC 2023,,,,,,,,,,"0|z1en68:",9223372036854775807,,,,,atalvivek,,,,,,,,,,,,,,,,,"08/Jan/23 03:39;apachespark;User 'atalv' has created a pull request for this issue:
https://github.com/apache/spark/pull/39454;;;","08/Jan/23 03:40;apachespark;User 'atalv' has created a pull request for this issue:
https://github.com/apache/spark/pull/39454;;;","09/Jan/23 00:41;gurwls223;Issue resolved by pull request 39454
[https://github.com/apache/spark/pull/39454];;;",,,,,,,,
Sorting issue with partitioned-writing and planned write optimization disabled,SPARK-41914,13516724,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EnricoMi,EnricoMi,EnricoMi,05/Jan/23 21:05,13/Jan/23 07:07,13/Jul/23 08:43,10/Jan/23 05:10,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,correctness,"Spark 3.4.0 introduced option {{{}spark.sql.optimizer.plannedWrite.enabled{}}}, which is enabled by default. When disabled, partitioned writing loses in-partition order when spilling occurs.

This is related to SPARK-40885 where setting option {{spark.sql.optimizer.plannedWrite.enabled}} to {{true}} will remove the existing sort (for {{day}} and {{{}id{}}}) entirely.

Run this with 512m memory and one executor, e.g.:
{code}
spark-shell --driver-memory 512m --master ""local[1]""
{code}
{code:scala}
import org.apache.spark.sql.SaveMode

spark.conf.set(""spark.sql.optimizer.plannedWrite.enabled"", false)

val ids = 2000000
val days = 2
val parts = 2

val ds = spark.range(0, days, 1, parts).withColumnRenamed(""id"", ""day"").join(spark.range(0, ids, 1, parts))

ds.repartition($""day"")
  .sortWithinPartitions($""day"", $""id"")
  .write
  .partitionBy(""day"")
  .mode(SaveMode.Overwrite)
  .csv(""interleaved.csv"")
{code}
Check the written files are sorted (states OK when file is sorted):
{code:bash}
for file in interleaved.csv/day\=*/part-*
do
  echo ""$(sort -n ""$file"" | md5sum | cut -d "" "" -f 1)  $file""
done | md5sum -c
{code}
Files should look like this
{code}
0
1
2
...
1048576
1048577
1048578
...
{code}
But they look like
{code}
0
1048576
1
1048577
2
1048578
...
{code}
The cause issue is the same as in SPARK-40588. A sort (for {{{}day{}}}) is added on top of the existing sort (for {{day}} and {{{}id{}}}). Spilling interleaves the sorted spill files.

{code}
Sort [input[0, bigint, false] ASC NULLS FIRST], false, 0
+- AdaptiveSparkPlan isFinalPlan=false
   +- Sort [day#2L ASC NULLS FIRST, id#4L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(day#2L, 200), REPARTITION_BY_COL, [plan_id=30]
         +- BroadcastNestedLoopJoin BuildLeft, Inner
            :- BroadcastExchange IdentityBroadcastMode, [plan_id=28]
            :  +- Project [id#0L AS day#2L]
            :     +- Range (0, 2, step=1, splits=2)
            +- Range (0, 2000000, step=1, splits=2)
{code}
",,apachespark,cloud_fan,EnricoMi,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 05:10:26 UTC 2023,,,,,,,,,,"0|z1ekls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"06/Jan/23 12:33;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/39431;;;","10/Jan/23 05:10;cloud_fan;Issue resolved by pull request 39431
[https://github.com/apache/spark/pull/39431];;;",,,,,,,,,
Subquery should not validate CTE,SPARK-41912,13516714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,amaliujia,amaliujia,amaliujia,05/Jan/23 19:04,06/Jan/23 03:31,13/Jul/23 08:43,06/Jan/23 03:31,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,,,amaliujia,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 03:31:02 UTC 2023,,,,,,,,,,"0|z1ekjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Jan/23 19:08;apachespark;User 'amaliujia' has created a pull request for this issue:
https://github.com/apache/spark/pull/39414;;;","06/Jan/23 03:31;cloud_fan;Issue resolved by pull request 39414
[https://github.com/apache/spark/pull/39414];;;",,,,,,,,,
Filtering by row_index always returns empty results,SPARK-41896,13516619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,olaky,olaky,olaky,05/Jan/23 09:17,16/Jan/23 12:18,13/Jul/23 08:43,14/Jan/23 04:29,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,0,,"Queries that include a filter with row_index currently always return an empty result. This is because we consider all metadata attributes constant per file [here|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala#L76] and the filter then always evaluates to false.

This should be fixed as a follow up to SPARK-41791",,apachespark,cloud_fan,olaky,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 12:18:38 UTC 2023,,,,,,,,,,"0|z1ejyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Jan/23 11:51;apachespark;User 'olaky' has created a pull request for this issue:
https://github.com/apache/spark/pull/39408;;;","14/Jan/23 04:29;cloud_fan;Issue resolved by pull request 39408
[https://github.com/apache/spark/pull/39408];;;","16/Jan/23 12:18;apachespark;User 'olaky' has created a pull request for this issue:
https://github.com/apache/spark/pull/39608;;;",,,,,,,,
sql/core module mvn clean failed,SPARK-41894,13516596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,05/Jan/23 07:28,09/Jan/23 02:22,13/Jul/23 08:43,09/Jan/23 02:22,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Structured Streaming,Tests,,0,,"run the following commands:
 # mvn clean install -pl sql/core -am -DskipTests
 # mvn test -pl sql/core 
 # mvn clean

 

then following error:

 
{code:java}
[INFO] Spark Project Parent POM ........................... SUCCESS [  0.133 s]
[INFO] Spark Project Tags ................................. SUCCESS [  0.008 s]
[INFO] Spark Project Sketch ............................... SUCCESS [  0.007 s]
[INFO] Spark Project Local DB ............................. SUCCESS [  0.008 s]
[INFO] Spark Project Networking ........................... SUCCESS [  0.015 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  0.020 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [  0.007 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  0.008 s]
[INFO] Spark Project Core ................................. SUCCESS [  0.279 s]
[INFO] Spark Project ML Local Library ..................... SUCCESS [  0.010 s]
[INFO] Spark Project GraphX ............................... SUCCESS [  0.016 s]
[INFO] Spark Project Streaming ............................ SUCCESS [  0.039 s]
[INFO] Spark Project Catalyst ............................. SUCCESS [  0.262 s]
[INFO] Spark Project SQL .................................. FAILURE [  1.305 s]
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] Spark Project YARN ................................. SKIPPED
[INFO] Spark Project Mesos ................................ SKIPPED
[INFO] Spark Project Kubernetes ........................... SKIPPED
[INFO] Spark Project Hive Thrift Server ................... SKIPPED
[INFO] Spark Ganglia Integration .......................... SKIPPED
[INFO] Spark Project Hadoop Cloud Integration ............. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED
[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED
[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED
[INFO] Spark Kinesis Integration .......................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED
[INFO] Spark Avro ......................................... SKIPPED
[INFO] Spark Project Connect Common ....................... SKIPPED
[INFO] Spark Project Connect Server ....................... SKIPPED
[INFO] Spark Project Connect Client ....................... SKIPPED
[INFO] Spark Protobuf ..................................... SKIPPED
[INFO] Spark Project Kinesis Assembly ..................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  2.896 s
[INFO] Finished at: 2023-01-05T15:15:57+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:3.1.0:clean (default-clean) on project spark-sql_2.13: Failed to clean project: Failed to delete /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc -> [Help 1]
 {code}
 

 

run :
 * ll /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc

 

 
{code:java}
-rw-r--r-- 1 work work 12 Dec 28 16:06 /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc{code}
 

 

and current user(work) can't rm this file:
 * rm  /${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc

 
{code:java}
rm: cannot remove `/${basedir}/sql/core/target/tmp/streaming.metadata-1b8b16d8-c9ba-4c38-9ac0-94a39f583082/commits/.0.crc': Permission denied {code}
need to use root to clean this file

 ",,apachespark,kabhwan,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 02:22:39 UTC 2023,,,,,,,,,,"0|z1ejtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"05/Jan/23 07:30;LuciferYang;The running environment is linux,  I haven't found the specific case that generated this file now, need more investigation

 

 

 ;;;","05/Jan/23 08:31;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39406;;;","05/Jan/23 08:32;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/39406;;;","09/Jan/23 02:22;kabhwan;Issue resolved by pull request 39406
[https://github.com/apache/spark/pull/39406];;;",,,,,,,
CreateHiveTableAsSelectCommand should set the overwrite flag correctly,SPARK-41859,13516322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,03/Jan/23 12:33,04/Jan/23 05:09,13/Jul/23 08:43,04/Jan/23 05:09,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 05:09:45 UTC 2023,,,,,,,,,,"0|z1ei4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Jan/23 13:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39365;;;","03/Jan/23 13:03;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/39365;;;","04/Jan/23 05:09;cloud_fan;Issue resolved by pull request 39365
[https://github.com/apache/spark/pull/39365];;;",,,,,,,,
Fix ORC reader perf regression due to DEFAULT value feature,SPARK-41858,13516291,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,03/Jan/23 09:48,03/Jan/23 18:45,13/Jul/23 08:43,03/Jan/23 18:40,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,0,,A huge ORC reader perf regression is detected by SPARK-41782. The root cause was SPARK-39862.,,apachespark,dongjoon,,,,,,,,,,,,,,,,SPARK-39862,SPARK-41862,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 18:40:57 UTC 2023,,,,,,,,,,"0|z1ehxk:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,"03/Jan/23 10:43;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39362;;;","03/Jan/23 18:40;dongjoon;Issue resolved by pull request 39362
[https://github.com/apache/spark/pull/39362];;;",,,,,,,,,
Tasks are over-scheduled with TaskResourceProfile,SPARK-41848,13516236,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ivoson,Ngone51,Ngone51,03/Jan/23 02:44,09/Jan/23 20:03,13/Jul/23 08:43,09/Jan/23 20:03,3.4.0,,,,,,,,,,,,,,,3.4.0,,,,Spark Core,,,0,,"{code:java}
test(""SPARK-XXX"") {
  val conf = new SparkConf().setAppName(""test"").setMaster(""local-cluster[1,4,1024]"")
  sc = new SparkContext(conf)
  val req = new TaskResourceRequests().cpus(3)
  val rp = new ResourceProfileBuilder().require(req).build()

  val res = sc.parallelize(Seq(0, 1), 2).withResources(rp).map { x =>
    Thread.sleep(5000)
    x * 2
  }.collect()
  assert(res === Array(0, 2))
} {code}
In this test, tasks are supposed to be scheduled in order since each task requires 3 cores but the executor only has 4 cores. However, we noticed 2 tasks are launched concurrently from the logs.

It turns out that we used the TaskResourceProfile (taskCpus=3) of the taskset for task scheduling:
{code:java}
val rpId = taskSet.taskSet.resourceProfileId
val taskSetProf = sc.resourceProfileManager.resourceProfileFromId(rpId)
val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(taskSetProf, conf) {code}
but the ResourceProfile (taskCpus=1) of the executor for updating the free cores in ExecutorData:
{code:java}
val rpId = executorData.resourceProfileId
val prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)
val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(prof, conf)
executorData.freeCores -= taskCpus {code}
which results in the inconsistency of the available cores.",,apachespark,dongjoon,Ngone51,,,,,,,,,,,,,,,SPARK-39853,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 20:03:37 UTC 2023,,,,,,,,,,"0|z1ehlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"03/Jan/23 02:51;Ngone51;cc [~ivoson] ;;;","05/Jan/23 14:31;apachespark;User 'ivoson' has created a pull request for this issue:
https://github.com/apache/spark/pull/39410;;;","09/Jan/23 20:03;dongjoon;Issue resolved by pull request 39410
[https://github.com/apache/spark/pull/39410];;;",,,,,,,,
