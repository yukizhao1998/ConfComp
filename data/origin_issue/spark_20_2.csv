Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Outward issue link (Completes),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (dependent),Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Handles a random seed consisting of an expr tree,SPARK-33945,13348471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,maropu,maropu,30/Dec/20 12:24,04/Jan/21 05:37,13/Jul/23 08:50,04/Jan/21 05:37,2.4.8,3.0.2,3.1.0,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"This ticket aims at fixing the minor bug that throws an analysis exception when a seed param in `rand`/`randn` having a expr tree (e.g., `rand(1 + 1)`) with constant folding (`ConstantFolding` and `ReorderAssociativeOperator`) disabled. A query to reproduce this issue is as follows;
{code}
// v3.1.0, v3.0.2, and v2.4.8
$./bin/spark-shell 
scala> sql(""select rand(1 + 2)"").show()
+-------------------+
|      rand((1 + 2))|
+-------------------+
|0.25738143505962285|
+-------------------+

$./bin/spark-shell --conf spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.ConstantFolding,org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperator
scala> sql(""select rand(1 + 2)"").show()
org.apache.spark.sql.AnalysisException: Input argument to rand must be an integer, long or null literal.;
  at org.apache.spark.sql.catalyst.expressions.RDG.seed$lzycompute(randomExpressions.scala:49)
  at org.apache.spark.sql.catalyst.expressions.RDG.seed(randomExpressions.scala:46)
  at org.apache.spark.sql.catalyst.expressions.Rand.doGenCode(randomExpressions.scala:98)
  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)
  at scala.Option.getOrElse(Option.scala:189)
  ...
{code}

A root cause is that the match-case code below cannot handle the case described above:
https://github.com/apache/spark/blob/42f5e62403469cec6da680b9fbedd0aa508dcbe5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/randomExpressions.scala#L46-L51
",,apachespark,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 04 05:37:41 UTC 2021,,,,,,,,,,"0|z0lxmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/20 12:28;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30977;;;","04/Jan/21 05:37;dongjoon;This is resolved via https://github.com/apache/spark/pull/30977 .;;;",,,,,,,,,,,,,,,,,,,,,,,,
Wrong metrics information in Spark Monitoring Documentation,SPARK-33942,13348456,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,coderbond007,coderbond007,coderbond007,30/Dec/20 10:46,31/Dec/20 01:28,13/Jul/23 08:50,31/Dec/20 01:28,3.0.0,3.0.1,,,,,,,,,,3.0.2,3.1.0,,,Documentation,,,,0,bug,documentation,starter,"I recently integrated the spark executor metrics documented in [Spark Monitoring|https://spark.apache.org/docs/3.0.0/monitoring.html#component-instance--executor] into our Graphite based monitoring system. I see

[Executor|https://spark.apache.org/docs/3.0.0/monitoring.html#component-instance--executor] and latest one too

+namespace=CodeGenerator+
 * *note:*: these metrics are conditional to a configuration parameter: {{spark.metrics.staticSources.enabled}} (default is true)
 * compilationTime (histogram)
 * generatedClassSize (histogram)
 * generatedMethodSize (histogram)
 * *hiveClientCalls.count - irrelevant and metric not present in metric registery too*
 * sourceCodeSize (histogram)

It is just a minor bug on documentation side. I am raising a PR for this.",,apachespark,coderbond007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 30 11:24:06 UTC 2020,,,,,,,,,,"0|z0lxj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/20 11:23;apachespark;User 'coderbond007' has created a pull request for this issue:
https://github.com/apache/spark/pull/30976;;;","30/Dec/20 11:24;apachespark;User 'coderbond007' has created a pull request for this issue:
https://github.com/apache/spark/pull/30976;;;",,,,,,,,,,,,,,,,,,,,,,,,
allow configuring the max column name length in csv writer,SPARK-33940,13348430,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,codingcat,codingcat,30/Dec/20 07:48,12/Dec/22 18:11,13/Jul/23 08:50,20/Jan/21 02:41,3.1.0,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,,"csv writer actually has an implicit limit on column name length due to univocity-parser, 

 

when we initialize a writer [https://github.com/uniVocity/univocity-parsers/blob/e09114c6879fa6c2c15e7365abc02cda3e193ff7/src/main/java/com/univocity/parsers/common/AbstractWriter.java#L211,] it calls toIdentifierGroupArray which calls valueOf in NormalizedString.java eventually ([https://github.com/uniVocity/univocity-parsers/blob/e09114c6879fa6c2c15e7365abc02cda3e193ff7/src/main/java/com/univocity/parsers/common/NormalizedString.java#L205-L209)]

 

in that stringCache.get, it has a maxStringLength cap [https://github.com/uniVocity/univocity-parsers/blob/e09114c6879fa6c2c15e7365abc02cda3e193ff7/src/main/java/com/univocity/parsers/common/StringCache.java#L104] which is 1024 by default

 

we do not expose this as configurable option, leading to NPE when we have a column name larger than 1024, 

 

```

[info]   Cause: java.lang.NullPointerException:

[info]   at com.univocity.parsers.common.AbstractWriter.submitRow(AbstractWriter.java:349)

[info]   at com.univocity.parsers.common.AbstractWriter.writeHeaders(AbstractWriter.java:444)

[info]   at com.univocity.parsers.common.AbstractWriter.writeHeaders(AbstractWriter.java:410)

[info]   at org.apache.spark.sql.catalyst.csv.UnivocityGenerator.writeHeaders(UnivocityGenerator.scala:87)

[info]   at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter$.writeHeaders(CsvOutputWriter.scala:58)

[info]   at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:44)

[info]   at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:86)

[info]   at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)

[info]   at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)

[info]   at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)

[info]   at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)

```

 

it could be reproduced by a simple unit test

 

```

val row1 = Row(""a"")
val superLongHeader = (0 until 1025).map(_ => ""c"").mkString("""")
val df = Seq(s""${row1.getString(0)}"").toDF(superLongHeader)
df.repartition(1)
 .write
 .option(""header"", ""true"")
 .option(""maxColumnNameLength"", 1025)
 .csv(dataPath)

```

 ",,apachespark,codingcat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 20 02:41:38 UTC 2021,,,,,,,,,,"0|z0lxdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/20 07:51;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/30972;;;","19/Jan/21 18:09;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/31246;;;","20/Jan/21 02:41;gurwls223;Issue resolved by pull request 31246
[https://github.com/apache/spark/pull/31246];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix CBOs cost function ,SPARK-33935,13348333,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tanelk,tanelk,tanelk,29/Dec/20 17:58,01/Apr/21 08:37,13/Jul/23 08:50,05/Jan/21 07:05,2.4.7,3.0.1,3.1.0,3.2.0,,,,,,,,3.0.2,3.1.0,3.2.0,,SQL,,,,0,,,,"The parameter spark.sql.cbo.joinReorder.card.weight is decumented as:
{code:title=spark.sql.cbo.joinReorder.card.weight}
The weight of cardinality (number of rows) for plan cost comparison in join reorder: rows * weight + size * (1 - weight).
{code}

But in the implementation the formula is a bit different:
{code:title=Current implementation}
    def betterThan(other: JoinPlan, conf: SQLConf): Boolean = {
      if (other.planCost.card == 0 || other.planCost.size == 0) {
        false
      } else {
        val relativeRows = BigDecimal(this.planCost.card) / BigDecimal(other.planCost.card)
        val relativeSize = BigDecimal(this.planCost.size) / BigDecimal(other.planCost.size)
        relativeRows * conf.joinReorderCardWeight +
          relativeSize * (1 - conf.joinReorderCardWeight) < 1
      }
    }
{code}

This change has an unfortunate consequence: 
given two plans A and B, both A betterThan B and B betterThan A might give the same results. This happes when one has many rows with small sizes and other has few rows with large sizes.

A example values, that have this fenomen with the default weight value (0.7):
A.card = 500, B.card = 300
A.size = 30, B.size = 80
Both A betterThan B and B betterThan A would have score above 1 and would return false.

A new implementation is proposed, that matches the documentation:
{code:title=Proposed implementation}
    def betterThan(other: JoinPlan, conf: SQLConf): Boolean = {
      val oldCost = BigDecimal(this.planCost.card) * conf.joinReorderCardWeight +
        BigDecimal(this.planCost.size) * (1 - conf.joinReorderCardWeight)
      val newCost = BigDecimal(other.planCost.card) * conf.joinReorderCardWeight +
        BigDecimal(other.planCost.size) * (1 - conf.joinReorderCardWeight)
      newCost < oldCost
    }
{code}",,apachespark,maropu,tanelk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 06:33:48 UTC 2021,,,,,,,,,,"0|z0lwrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/20 18:30;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30965;;;","29/Dec/20 18:31;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30965;;;","05/Jan/21 07:05;maropu;Resolved by https://github.com/apache/spark/pull/30965;;;","05/Jan/21 10:50;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31042;;;","05/Jan/21 11:08;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31043;;;","01/Apr/21 06:33;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32020;;;",,,,,,,,,,,,,,,,,,,,
Recover GitHub Action,SPARK-33931,13348231,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,29/Dec/20 05:56,01/Jan/21 10:01,13/Jul/23 08:50,01/Jan/21 10:01,2.4.8,3.0.1,3.1.0,3.2.0,,,,,,,,3.0.2,3.1.0,3.2.0,,Project Infra,,,,0,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 31 22:21:54 UTC 2020,,,,,,,,,,"0|z0lw54:",9223372036854775807,,,,,,,,,,,,,3.1.0,3.2.0,,,,,,,,,"29/Dec/20 05:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30959;;;","31/Dec/20 22:21;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30986;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix Spark Release image,SPARK-33927,13348206,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,dongjoon,dongjoon,29/Dec/20 02:32,12/Dec/22 18:10,13/Jul/23 08:50,30/Dec/20 07:38,3.1.0,,,,,,,,,,,3.1.0,,,,Project Infra,,,,1,,,,"The release script seems to be broken. This is a blocker for Apache Spark 3.1.0 release.
{code}
$ cd dev/create-release/spark-rm
$ docker build -t spark-rm .
...
exit code: 1
{code}",,apachespark,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 30 07:38:50 UTC 2020,,,,,,,,,,"0|z0lvzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/20 02:33;dongjoon;cc [~hyukjin.kwon];;;","29/Dec/20 12:47;gurwls223;Thanks for letting me know [~dongjoon]. I will likely have to take a look for this one this week :-).;;;","30/Dec/20 06:58;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30971;;;","30/Dec/20 07:38;gurwls223;Issue resolved by pull request 30971
[https://github.com/apache/spark/pull/30971];;;",,,,,,,,,,,,,,,,,,,,,,
SPARK UI Executors page stuck when ExecutorSummary.peakMemoryMetrics is unset,SPARK-33906,13347788,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Baohe Zhang,Baohe Zhang,Baohe Zhang,24/Dec/20 20:15,31/Dec/20 21:35,13/Jul/23 08:50,31/Dec/20 21:35,3.2.0,,,,,,,,,,,3.1.0,,,,Web UI,,,,0,,,,"How to reproduce it?

In mac OS standalone mode, open a spark-shell and run

$SPARK_HOME/bin/spark-shell --master spark://localhost:7077
{code:scala}
val x = sc.makeRDD(1 to 100000, 5)
x.count()
{code}
Then open the app UI in the browser, and click the Executors page, will get stuck at this page: 

 !executor-page.png! 

Also the return JSON of REST API endpoint http://localhost:4040/api/v1/applications/app-20201224134418-0003/executors miss ""peakMemoryMetrics"" for executors.
{noformat}
[ {
  ""id"" : ""driver"",
  ""hostPort"" : ""192.168.1.241:50042"",
  ""isActive"" : true,
  ""rddBlocks"" : 0,
  ""memoryUsed"" : 0,
  ""diskUsed"" : 0,
  ""totalCores"" : 0,
  ""maxTasks"" : 0,
  ""activeTasks"" : 0,
  ""failedTasks"" : 0,
  ""completedTasks"" : 0,
  ""totalTasks"" : 0,
  ""totalDuration"" : 0,
  ""totalGCTime"" : 0,
  ""totalInputBytes"" : 0,
  ""totalShuffleRead"" : 0,
  ""totalShuffleWrite"" : 0,
  ""isBlacklisted"" : false,
  ""maxMemory"" : 455501414,
  ""addTime"" : ""2020-12-24T19:44:18.033GMT"",
  ""executorLogs"" : { },
  ""memoryMetrics"" : {
    ""usedOnHeapStorageMemory"" : 0,
    ""usedOffHeapStorageMemory"" : 0,
    ""totalOnHeapStorageMemory"" : 455501414,
    ""totalOffHeapStorageMemory"" : 0
  },
  ""blacklistedInStages"" : [ ],
  ""peakMemoryMetrics"" : {
    ""JVMHeapMemory"" : 135021152,
    ""JVMOffHeapMemory"" : 149558576,
    ""OnHeapExecutionMemory"" : 0,
    ""OffHeapExecutionMemory"" : 0,
    ""OnHeapStorageMemory"" : 3301,
    ""OffHeapStorageMemory"" : 0,
    ""OnHeapUnifiedMemory"" : 3301,
    ""OffHeapUnifiedMemory"" : 0,
    ""DirectPoolMemory"" : 67963178,
    ""MappedPoolMemory"" : 0,
    ""ProcessTreeJVMVMemory"" : 0,
    ""ProcessTreeJVMRSSMemory"" : 0,
    ""ProcessTreePythonVMemory"" : 0,
    ""ProcessTreePythonRSSMemory"" : 0,
    ""ProcessTreeOtherVMemory"" : 0,
    ""ProcessTreeOtherRSSMemory"" : 0,
    ""MinorGCCount"" : 15,
    ""MinorGCTime"" : 101,
    ""MajorGCCount"" : 0,
    ""MajorGCTime"" : 0
  },
  ""attributes"" : { },
  ""resources"" : { },
  ""resourceProfileId"" : 0,
  ""isExcluded"" : false,
  ""excludedInStages"" : [ ]
}, {
  ""id"" : ""0"",
  ""hostPort"" : ""192.168.1.241:50054"",
  ""isActive"" : true,
  ""rddBlocks"" : 0,
  ""memoryUsed"" : 0,
  ""diskUsed"" : 0,
  ""totalCores"" : 12,
  ""maxTasks"" : 12,
  ""activeTasks"" : 0,
  ""failedTasks"" : 0,
  ""completedTasks"" : 5,
  ""totalTasks"" : 5,
  ""totalDuration"" : 2107,
  ""totalGCTime"" : 25,
  ""totalInputBytes"" : 0,
  ""totalShuffleRead"" : 0,
  ""totalShuffleWrite"" : 0,
  ""isBlacklisted"" : false,
  ""maxMemory"" : 455501414,
  ""addTime"" : ""2020-12-24T19:44:20.335GMT"",
  ""executorLogs"" : {
    ""stdout"" : ""http://192.168.1.241:8081/logPage/?appId=app-20201224134418-0003&executorId=0&logType=stdout"",
    ""stderr"" : ""http://192.168.1.241:8081/logPage/?appId=app-20201224134418-0003&executorId=0&logType=stderr""
  },
  ""memoryMetrics"" : {
    ""usedOnHeapStorageMemory"" : 0,
    ""usedOffHeapStorageMemory"" : 0,
    ""totalOnHeapStorageMemory"" : 455501414,
    ""totalOffHeapStorageMemory"" : 0
  },
  ""blacklistedInStages"" : [ ],
  ""attributes"" : { },
  ""resources"" : { },
  ""resourceProfileId"" : 0,
  ""isExcluded"" : false,
  ""excludedInStages"" : [ ]
} ]
{noformat}

I debugged it and observed that ExecutorMetricsPoller
.getExecutorUpdates returns an empty map, which causes peakExecutorMetrics to None in https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/LiveEntity.scala#L345. The possible reason for returning the empty map is that the stage completion time is shorter than the heartbeat interval, so the stage entry in stageTCMP has already been removed before the reportHeartbeat is called.

How to fix it?

Check if the peakMemoryMetrics is undefined in executorspage.js.",,apachespark,Baohe Zhang,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23432,,,,,,,,,"24/Dec/20 20:16;Baohe Zhang;executor-page.png;https://issues.apache.org/jira/secure/attachment/13017628/executor-page.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 31 21:35:30 UTC 2020,,,,,,,,,,"0|z0ltew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/20 20:16;Baohe Zhang;I will put a PR soon.;;;","24/Dec/20 20:36;apachespark;User 'baohe-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30920;;;","24/Dec/20 21:47;dongjoon;Thank you for reporting, [~Baohe Zhang]. Is this caused by SPARK-23432?;;;","24/Dec/20 21:58;Baohe Zhang;[~dongjoon] Yes.;;;","24/Dec/20 22:03;Baohe Zhang;The more underlay reason seems to be that the stage complete within a heartbeat period, so the heartbeat doesn't piggyback executor peak memory metrics.;;;","31/Dec/20 21:35;dongjoon;Issue resolved by pull request 30920
[https://github.com/apache/spark/pull/30920];;;",,,,,,,,,,,,,,,,,,,,
Char and Varchar display error after DDLs,SPARK-33901,13347752,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,24/Dec/20 12:44,09/Feb/21 01:50,13/Jul/23 08:50,28/Dec/20 06:48,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,CTAS / CREATE TABLE LIKE/ CVAS/ alter table add columns,,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 21 08:32:53 UTC 2021,,,,,,,,,,"0|z0lt6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/20 13:39;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30918;;;","28/Dec/20 06:48;cloud_fan;Issue resolved by pull request 30918
[https://github.com/apache/spark/pull/30918];;;","21/Jan/21 08:32;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/31277;;;",,,,,,,,,,,,,,,,,,,,,,,
Show shuffle read size / records correctly when only remotebytesread is available,SPARK-33900,13347735,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dzcxzl,dzcxzl,dzcxzl,24/Dec/20 09:29,24/Dec/20 15:59,13/Jul/23 08:50,24/Dec/20 15:59,3.0.1,,,,,,,,,,,3.0.2,3.1.0,3.2.0,,Web UI,,,,0,,,,"At present, the stage page only displays the data of Shuffle Read Size / Records when localBytesRead>0.

Sometimes the data of shuffle read metrics is remoteBytesRead>0 localBytesRead=0.",,apachespark,dzcxzl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 24 09:38:08 UTC 2020,,,,,,,,,,"0|z0lt34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/20 09:37;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/30916;;;","24/Dec/20 09:38;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/30916;;;",,,,,,,,,,,,,,,,,,,,,,,,
Can't set option 'cross' in join method.,SPARK-33897,13347692,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kozakana,kozakana,kozakana,24/Dec/20 05:27,12/Dec/22 18:10,13/Jul/23 08:50,26/Dec/20 07:34,3.0.1,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"[The PySpark documentation|https://spark.apache.org/docs/3.0.1/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join] says ""Must be one of: inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti.""
However, I get the following error when I set the cross option.

 
{code:java}
scala> val df1 = spark.createDataFrame(Seq((1,""a""),(2,""b"")))
df1: org.apache.spark.sql.DataFrame = [_1: int, _2: string]
scala> val df2 = spark.createDataFrame(Seq((1,""A""),(2,""B""), (3, ""C"")))
df2: org.apache.spark.sql.DataFrame = [_1: int, _2: string]
scala> df1.join(right = df2, usingColumns = Seq(""_1""), joinType = ""cross"").show()
java.lang.IllegalArgumentException: requirement failed: Unsupported using join type Cross
 at scala.Predef$.require(Predef.scala:281)
 at org.apache.spark.sql.catalyst.plans.UsingJoin.<init>(joinTypes.scala:106)
 at org.apache.spark.sql.Dataset.join(Dataset.scala:1025)
 ... 53 elided

{code}
 ",,apachespark,kozakana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 26 07:34:37 UTC 2020,,,,,,,,,,"0|z0lstk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/20 05:29;apachespark;User 'kozakana' has created a pull request for this issue:
https://github.com/apache/spark/pull/30803;;;","26/Dec/20 07:34;gurwls223;Issue resolved by pull request 30803
[https://github.com/apache/spark/pull/30803];;;",,,,,,,,,,,,,,,,,,,,,,,,
Char and Varchar display in show table/column definition command,SPARK-33892,13347575,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,23/Dec/20 10:57,25/Jan/21 03:26,13/Jul/23 08:50,24/Dec/20 08:56,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,shows char/varchar raw type in desc/show table/column command,,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,SPARK-34177,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 24 08:56:30 UTC 2020,,,,,,,,,,"0|z0ls3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/20 11:03;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30908;;;","24/Dec/20 08:56;cloud_fan;Issue resolved by pull request 30908
[https://github.com/apache/spark/pull/30908];;;",,,,,,,,,,,,,,,,,,,,,,,,
The position of unresolved identifier for DDL commands should be respected..,SPARK-33885,13347493,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,23/Dec/20 06:21,13/Sep/21 22:26,13/Jul/23 08:50,13/Sep/21 22:26,3.2.0,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,"Currently, there are many DDL commands where the position of the unresolved identifiers are incorrect:

{code:java}
scala> sql(""DESCRIBE TABLE abc"")
org.apache.spark.sql.AnalysisException: Table or view not found: abc; line 1 pos 0;
{code}

Note that the pos should be 15 in this case.",,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-23 06:21:59.0,,,,,,,,,,"0|z0lrlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Char Varchar values fails w/ match error as partition columns,SPARK-33879,13347313,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Qin Yao,Qin Yao,Qin Yao,22/Dec/20 09:13,12/Dec/22 18:10,13/Jul/23 08:50,23/Dec/20 07:15,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"
{code:java}
spark-sql> select * from t10 where c0='abcd';
20/12/22 15:43:38 ERROR SparkSQLDriver: Failed in [select * from t10 where c0='abcd']
scala.MatchError: CharType(10) (of class org.apache.spark.sql.types.CharType)
	at org.apache.spark.sql.catalyst.expressions.CastBase.cast(Cast.scala:815)
	at org.apache.spark.sql.catalyst.expressions.CastBase.cast$lzycompute(Cast.scala:842)
	at org.apache.spark.sql.catalyst.expressions.CastBase.cast(Cast.scala:842)
	at org.apache.spark.sql.catalyst.expressions.CastBase.nullSafeEval(Cast.scala:844)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:476)
	at org.apache.spark.sql.catalyst.catalog.CatalogTablePartition.$anonfun$toRow$2(interface.scala:164)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:102)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at org.apache.spark.sql.types.StructType.map(StructType.scala:102)
	at org.apache.spark.sql.catalyst.catalog.CatalogTablePartition.toRow(interface.scala:158)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogUtils$.$anonfun$prunePartitionsByFilter$3(ExternalCatalogUtils.scala:157)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogUtils$.$anonfun$prunePartitionsByFilter$3$adapted(ExternalCatalogUtils.scala:156)
{code}


c0 is a partition column, it fails in partition pruning rule
",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 23 07:15:39 UTC 2020,,,,,,,,,,"0|z0lqhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/20 10:03;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30887;;;","22/Dec/20 10:04;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30887;;;","23/Dec/20 07:15;gurwls223;Issue resolved by pull request 30887
[https://github.com/apache/spark/pull/30887];;;",,,,,,,,,,,,,,,,,,,,,,,
java.time.Instant and java.time.LocalDate not handled in org.apache.spark.sql.jdbc.JdbcDialect#compileValue,SPARK-33867,13347140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Chircu,Chircu,Chircu,21/Dec/20 12:45,12/Dec/22 18:10,13/Jul/23 08:50,28/Jan/21 03:08,3.0.1,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,SQL,,,,0,,,,"When using the new java time API (spark.sql.datetime.java8API.enabled=true) LocalDate and Instant aren't handled in org.apache.spark.sql.jdbc.JdbcDialect#compileValue so exceptions are thrown when they are used in filters since a filter condition would be translated to something like this: ""valid_from"" > 2020-12-21T11:40:24.413681Z.

To reproduce you can write a simple filter like where dataset is backed by a DB table (in my case PostgreSQL): 

dataset.filter(current_timestamp().gt(col(VALID_FROM)))

The error and stacktrace:

Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""T11""Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""T11""  Position: 285 at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2103) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1836) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:257) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:512) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:388) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeQuery(AbstractJdbc2Statement.java:273) at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349) at org.apache.spark.rdd.RDD.iterator(RDD.scala:313) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349) at org.apache.spark.rdd.RDD.iterator(RDD.scala:313) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349) at org.apache.spark.rdd.RDD.iterator(RDD.scala:313) at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) at org.apache.spark.scheduler.Task.run(Task.scala:127) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:834)",,apachespark,Chircu,LiaoHanwen,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 12:15:24 UTC 2021,,,,,,,,,,"0|z0lpew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/20 07:33;gurwls223;cc [~maxgekk] FYI;;;","12/Jan/21 09:57;apachespark;User 'cristichircu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31148;;;","28/Jan/21 03:08;maropu;Resolved by https://github.com/apache/spark/pull/31148;;;","28/Jan/21 08:49;apachespark;User 'cristichircu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31381;;;","20/May/21 10:55;LiaoHanwen;Is this fixed on branch-3.1?;;;","20/May/21 11:07;Chircu;looks like it: https://github.com/apache/spark/blob/branch-3.1/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala;;;","20/May/21 12:15;maropu;Please see the ""Fix Version/s"" in this jira and that includes 3.1.x, too.;;;",,,,,,,,,,,,,,,,,,,
EXPLAIN CODEGEN and BenchmarkQueryTest don't show subquery code,SPARK-33853,13346959,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,19/Dec/20 21:35,21/Dec/20 11:30,13/Jul/23 08:50,21/Dec/20 11:30,3.0.0,3.0.1,3.1.0,3.2.0,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"{{EXPLAIN CODEGEN}} and {{BenchmarkQueryTest}} don't show the corresponding code for subqueries.

The following example is about {{EXPLAIN CODEGEN}}.
{code:java}
spark.conf.set(""spark.sql.adaptive.enabled"", ""false"")
val df = spark.range(1, 100)
df.createTempView(""df"")
spark.sql(""SELECT (SELECT min(id) AS v FROM df)"").explain(""CODEGEN"")

scala> spark.sql(""SELECT (SELECT min(id) AS v FROM df)"").explain(""CODEGEN"")
Found 1 WholeStageCodegen subtrees.
== Subtree 1 / 1 (maxMethodCodeSize:55; maxConstantPoolSize:97(0.15% used); numInnerClasses:0) ==
*(1) Project [Subquery scalar-subquery#3, [id=#24] AS scalarsubquery()#5L]
:  +- Subquery scalar-subquery#3, [id=#24]
:     +- *(2) HashAggregate(keys=[], functions=[min(id#0L)], output=[v#2L])
:        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#20]
:           +- *(1) HashAggregate(keys=[], functions=[partial_min(id#0L)], output=[min#8L])
:              +- *(1) Range (1, 100, step=1, splits=12)
+- *(1) Scan OneRowRelation[]

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator rdd_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     rdd_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   private void project_doConsume_0() throws java.io.IOException {
/* 025 */     // common sub-expressions
/* 026 */
/* 027 */     project_mutableStateArray_0[0].reset();
/* 028 */
/* 029 */     if (false) {
/* 030 */       project_mutableStateArray_0[0].setNullAt(0);
/* 031 */     } else {
/* 032 */       project_mutableStateArray_0[0].write(0, 1L);
/* 033 */     }
/* 034 */     append((project_mutableStateArray_0[0].getRow()));
/* 035 */
/* 036 */   }
/* 037 */
/* 038 */   protected void processNext() throws java.io.IOException {
/* 039 */     while ( rdd_input_0.hasNext()) {
/* 040 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();
/* 041 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 042 */       project_doConsume_0();
/* 043 */       if (shouldStop()) return;
/* 044 */     }
/* 045 */   }
/* 046 */
/* 047 */ }
{code}",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 21 11:30:34 UTC 2020,,,,,,,,,,"0|z0loao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/20 21:43;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30859;;;","21/Dec/20 11:30;dongjoon;Issue resolved by pull request 30859
[https://github.com/apache/spark/pull/30859];;;",,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to Zstd 1.4.8,SPARK-33843,13346881,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,19/Dec/20 00:36,18/Feb/21 17:18,13/Jul/23 08:50,19/Dec/20 15:00,3.1.0,,,,,,,,,,,3.1.0,,,,Build,,,,0,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 19 15:00:22 UTC 2020,,,,,,,,,,"0|z0lntc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/20 00:43;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30848;;;","19/Dec/20 15:00;dongjoon;Issue resolved by pull request 30848
[https://github.com/apache/spark/pull/30848];;;",,,,,,,,,,,,,,,,,,,,,,,,
Jobs disappear intermittently from the SHS under high load,SPARK-33841,13346790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vladglinskiy,vladglinskiy,vladglinskiy,18/Dec/20 13:01,18/Dec/20 23:20,13/Jul/23 08:50,18/Dec/20 21:27,3.0.0,3.0.1,3.1.0,3.2.0,,,,,,,,3.0.2,3.1.0,3.2.0,,Spark Core,,,,0,,,,"Ran into an issue when a particular job was displayed in the SHS and disappeared after some time, but then, in several minutes showed up again.

The issue is caused by SPARK-29043, which is designated to improve the concurrent performance of the History Server. The [change|https://github.com/apache/spark/pull/25797/files#] breaks the [""app deletion"" logic|https://github.com/apache/spark/pull/25797/files#diff-128a6af0d78f4a6180774faedb335d6168dfc4defff58f5aa3021fc1bd767bc0R563] because of missing proper synchronization for {{processing}} event log entries. Since SHS now [filters out|https://github.com/apache/spark/pull/25797/files#diff-128a6af0d78f4a6180774faedb335d6168dfc4defff58f5aa3021fc1bd767bc0R462] all {{processing}} event log entries, such entries do not have a chance to be [updated with the new {{lastProcessed}}|https://github.com/apache/spark/pull/25797/files#diff-128a6af0d78f4a6180774faedb335d6168dfc4defff58f5aa3021fc1bd767bc0R472] time and thus any entity that completes processing right after [filtering|https://github.com/apache/spark/pull/25797/files#diff-128a6af0d78f4a6180774faedb335d6168dfc4defff58f5aa3021fc1bd767bc0R462] and before [the check for stale entities|https://github.com/apache/spark/pull/25797/files#diff-128a6af0d78f4a6180774faedb335d6168dfc4defff58f5aa3021fc1bd767bc0R560] will be identified as stale and will be deleted from the UI until the next {{checkForLogs}} run. This is because [updated {{lastProcessed}} time is used as criteria|https://github.com/apache/spark/pull/25797/files#diff-128a6af0d78f4a6180774faedb335d6168dfc4defff58f5aa3021fc1bd767bc0R557], and event log entries that missed to be updated with a new time, will match that criteria.

The issue can be reproduced by generating a big number of event logs and uploading them to the SHS event log directory on S3. Essentially, around 800(82.6 MB) copies of an event log file were created using [shs-monitor|https://github.com/vladhlinsky/shs-monitor] script. Strange behavior of SHS counting the total number of applications was noticed - at first, the number was increasing as expected, but with the next page refresh, the total number of applications decreased. No errors were logged by SHS.","SHS is running locally on Ubuntu 19.04

 ",apachespark,dongjoon,ekoifman,vladglinskiy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 18 21:27:20 UTC 2020,,,,,,,,,,"0|z0ln94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/20 13:07;apachespark;User 'vladhlinsky' has created a pull request for this issue:
https://github.com/apache/spark/pull/30842;;;","18/Dec/20 13:07;vladglinskiy;cc [~kabhwan];;;","18/Dec/20 18:31;dongjoon;Thank you for filing a JIRA, but I removed the target version here, [~vladglinskiy].;;;","18/Dec/20 18:51;apachespark;User 'vladhlinsky' has created a pull request for this issue:
https://github.com/apache/spark/pull/30845;;;","18/Dec/20 19:42;vladglinskiy;Ok, thank you, [~dongjoon]!;;;","18/Dec/20 19:42;apachespark;User 'vladhlinsky' has created a pull request for this issue:
https://github.com/apache/spark/pull/30847;;;","18/Dec/20 21:27;dongjoon;Issue resolved by pull request 30845
[https://github.com/apache/spark/pull/30845];;;",,,,,,,,,,,,,,,,,,,
Update Jetty to 9.4.34,SPARK-33831,13346670,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,17/Dec/20 21:59,18/Dec/20 03:14,13/Jul/23 08:50,18/Dec/20 03:14,3.0.1,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,Spark Core,Web UI,,,0,,,,"We should update Jetty to 9.4.34, from 9.4.28, to pick up fixes, plus a possible CVE fix.

https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.33.v20201020
https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.34.v20201102
",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 18 03:14:34 UTC 2020,,,,,,,,,,"0|z0lmig:",9223372036854775807,,,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,,,,,,,"17/Dec/20 22:03;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/30828;;;","18/Dec/20 03:14;dongjoon;Issue resolved by pull request 30828
[https://github.com/apache/spark/pull/30828];;;",,,,,,,,,,,,,,,,,,,,,,,,
SingleFileEventLogFileReader/RollingEventLogFilesFileReader should be `package private`,SPARK-33819,13346488,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,17/Dec/20 04:00,12/Dec/22 18:11,13/Jul/23 08:50,17/Dec/20 06:54,3.0.2,3.1.0,3.2.0,,,,,,,,,3.0.2,3.1.1,3.2.0,,Spark Core,,,,0,releasenotes,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 18 03:21:04 UTC 2021,,,,,,,,,,"0|z0lle0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/20 04:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30814;;;","17/Dec/20 04:06;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30814;;;","17/Dec/20 06:54;gurwls223;Issue resolved by pull request 30814
[https://github.com/apache/spark/pull/30814];;;","17/Dec/20 07:10;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30819;;;","17/Dec/20 07:37;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30820;;;","18/Jan/21 03:17;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/31224;;;","18/Jan/21 03:18;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/31224;;;","18/Jan/21 03:21;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/31225;;;",,,,,,,,,,,,,,,,,,
JDBC datasource fails when reading spatial datatypes with the MS SQL driver,SPARK-33813,13346400,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,mswit,mswit,16/Dec/20 17:40,08/Feb/21 13:54,13/Jul/23 08:50,22/Jan/21 04:28,3.0.0,3.1.0,,,,,,,,,,3.0.2,3.1.1,3.2.0,,SQL,,,,0,,,,"The MS SQL JDBC driver introduced support for spatial types since version 7.0. The JDBC data source lacks mappings for these types which results in an exception below. It seems that a mapping in MsSqlServerDialect.getCatalystType that maps -157 and -158 typecode to VARBINARY should address the issue.

 
{noformat}
java.sql.SQLException: Unrecognized SQL type -157
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getCatalystType(JdbcUtils.scala:251)
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getSchema$1(JdbcUtils.scala:321)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:321)
 at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:63)
 at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)
 at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:364)
 at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:366)
 at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:355)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:355)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:240)
 at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:381){noformat}
 ",,apachespark,cloud_fan,mswit,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 22 07:42:35 UTC 2021,,,,,,,,,,"0|z0lkug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/21 08:40;cloud_fan;[~sarutak] do you have time to look into this? thanks!;;;","21/Jan/21 12:41;sarutak;[~cloud_fan] O.K, I'll try it.;;;","21/Jan/21 16:03;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31283;;;","21/Jan/21 16:04;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31283;;;","22/Jan/21 04:28;cloud_fan;Issue resolved by pull request 31283
[https://github.com/apache/spark/pull/31283];;;","22/Jan/21 05:49;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31288;;;","22/Jan/21 07:38;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31289;;;","22/Jan/21 07:39;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31289;;;","22/Jan/21 07:41;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31290;;;","22/Jan/21 07:42;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31290;;;",,,,,,,,,,,,,,,,
Recover PySpark coverage in spark-master-test-sbt-hadoop-3.2 Jenkins job,SPARK-33802,13346275,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,16/Dec/20 07:08,12/Dec/22 18:11,13/Jul/23 08:50,16/Dec/20 08:22,3.2.0,,,,,,,,,,,3.2.0,,,,Project Infra,,,,0,,,,"https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-3.2/1726/console is being failed as below:

{code}
------------------------------------------------------------------------------------
TOTAL                                              31290   4915   8002    769    82%
Generating HTML files for PySpark coverage under /home/jenkins/workspace/spark-master-test-sbt-hadoop-3.2/python/test_coverage/htmlcov
/home/jenkins/workspace/spark-master-test-sbt-hadoop-3.2
Cloning into 'pyspark-coverage-site'...

*** Please tell me who you are.

Run

  git config --global user.email ""you@example.com""
  git config --global user.name ""Your Name""

to set your account's default identity.
Omit --global to set the identity only in this repository.
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 16 14:28:59 UTC 2020,,,,,,,,,,"0|z0lk2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/20 07:13;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30796;;;","16/Dec/20 07:14;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30796;;;","16/Dec/20 08:22;gurwls223;Issue resolved by pull request 30796
[https://github.com/apache/spark/pull/30796];;;","16/Dec/20 14:28;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30804;;;","16/Dec/20 14:28;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30804;;;",,,,,,,,,,,,,,,,,,,,,
Refactor usage of Executor in ExecutorSuite to ensure proper cleanup,SPARK-33793,13346113,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sgoos-db,sgoos-db,sgoos-db,15/Dec/20 13:20,12/Dec/22 18:11,13/Jul/23 08:50,16/Dec/20 14:34,3.0.1,,,,,,,,,,,3.0.2,,,,Tests,,,,0,,,,"Recently an issue was discovered that leaked Executors (which are not explicitly stopped after a test) can cause other tests to fail due to the JVM being killed after 10 min.

It is therefore crucial that tests always stop the Executor. In most tests this is already the case, but to make this pattern more explicit it would be good to have a helper function that ensures stopping the executor in a finally block. ",,apachespark,sgoos-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 16 14:34:16 UTC 2020,,,,,,,,,,"0|z0lj2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/20 14:21;apachespark;User 'sander-goos' has created a pull request for this issue:
https://github.com/apache/spark/pull/30783;;;","16/Dec/20 10:38;apachespark;User 'sander-goos' has created a pull request for this issue:
https://github.com/apache/spark/pull/30801;;;","16/Dec/20 10:40;apachespark;User 'sander-goos' has created a pull request for this issue:
https://github.com/apache/spark/pull/30801;;;","16/Dec/20 14:34;gurwls223;Issue resolved by pull request 30801
[https://github.com/apache/spark/pull/30801];;;",,,,,,,,,,,,,,,,,,,,,,
Cache's storage level is not respected when a table name is altered.,SPARK-33786,13346026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,15/Dec/20 03:43,16/Dec/20 13:52,13/Jul/23 08:50,16/Dec/20 05:46,3.2.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"To repro:
{code:java}
        Seq(1 -> ""a"").toDF(""i"", ""j"").write.parquet(path.getCanonicalPath)
        sql(s""CREATE TABLE old USING parquet LOCATION '${path.toURI}'"")
        sql(""CACHE TABLE old OPTIONS('storageLevel' 'MEMORY_ONLY')"")
        val oldStorageLevel = getStorageLevel(""old"")

        sql(""ALTER TABLE old RENAME TO new"")
        val newStorageLevel = getStorageLevel(""new"")
        assert(oldStorageLevel === newStorageLevel)
{code}
The assert fails:
Expected :StorageLevel(disk, memory, deserialized, 1 replicas)
Actual   :StorageLevel(memory, deserialized, 1 replicas)
",,apachespark,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 16 06:21:53 UTC 2020,,,,,,,,,,"0|z0lijc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/20 04:32;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/30774;;;","16/Dec/20 05:46;cloud_fan;Issue resolved by pull request 30774
[https://github.com/apache/spark/pull/30774];;;","16/Dec/20 06:21;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/30793;;;",,,,,,,,,,,,,,,,,,,,,,,
"Place spark.files, spark.jars and spark.files under the current working directory on the driver in K8S cluster mode",SPARK-33782,13345939,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pralabhkumar,gurwls223,,14/Dec/20 20:01,12/Jul/23 07:01,13/Jul/23 08:50,13/Dec/22 05:19,3.2.0,,,,,,,,,,,3.4.0,,,,Kubernetes,,,,1,,,,"In Yarn cluster modes, the passed files are able to be accessed in the current working directory. Looks like this is not the case in Kubernates cluset mode.

By doing this, users can, for example, leverage PEX to manage Python dependences in Apache Spark:

{code}
pex pyspark==3.0.1 pyarrow==0.15.1 pandas==0.25.3 -o myarchive.pex
PYSPARK_PYTHON=./myarchive.pex spark-submit --files myarchive.pex
{code}

See also https://github.com/apache/spark/pull/30735/files#r540935585.",,apachespark,dexterhu,douglasawh,gloeckner.daniel@googlemail.com,holden,pralabhkumar,pratik.malani,PuerTea,tgraves,,,,,,,,,,,,,,,SPARK-31726,,,,,,,,,SPARK-43540,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 13:32:48 UTC 2023,,,,,,,,,,"0|z0li00:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,,"15/Dec/20 05:21;gurwls223;BTW, [~tgraves], I think I would likely take a look for this if no one takes a look but it will happen a bit later in the middle of Spark 3.2 dev.;;;","13/Sep/21 22:27;holden;I think this missed the window for Spark 3.2, but I'm happy to pick this up for 3.3;;;","14/Sep/21 00:10;gurwls223;Thanks [~holden]!;;;","30/Jun/22 16:31;douglasawh;It looks like 3.3.0 is out. What is the status of this?;;;","02/Aug/22 16:22;pralabhkumar;[~hyukjin.kwon] I would like to work on this . Please let me know if its ok ;;;","03/Aug/22 02:17;gurwls223;Please go for it!;;;","05/Aug/22 04:17;apachespark;User 'pralabhkumar' has created a pull request for this issue:
https://github.com/apache/spark/pull/37417;;;","05/Aug/22 04:18;apachespark;User 'pralabhkumar' has created a pull request for this issue:
https://github.com/apache/spark/pull/37417;;;","13/Sep/22 07:35;pralabhkumar;[~dongjoon] Please review the PR . ;;;","11/Oct/22 17:48;pralabhkumar;[~hyukjin.kwon] 

 

Can u please help to review the PR . It would be of great help . ;;;","31/Oct/22 06:20;pralabhkumar;[~hyukjin.kwon] [~dongjoon] 

Please let me know, if this Jira is relevant . I have already created the PR and its been already reviewed by couple of PMC . Please help to get it reviewed if the Jira is relevant otherwise i'll close the PR;;;","02/Nov/22 14:33;gloeckner.daniel@googlemail.com;Will this fix repair the {{--jars}} flag and will JARs be added automatically to the driver and executor class path when using {{spark.kubernetes.file.upload.path}} / {{file://}} URIs?

https://spark.apache.org/docs/latest/running-on-kubernetes.html#dependency-management

https://spark.apache.org/docs/3.2.0/submitting-applications.html
{quote}
When using spark-submit, the application jar along with any jars included with the --jars option will be automatically transferred to the cluster. URLs supplied after --jars must be separated by commas. That list is included in the driver and executor classpaths. 
{quote};;;","11/Jul/23 13:32;pratik.malani;Hi [~pralabhkumar] 

The latest update in the SparkSubmit.scala is causing the NoSuchFileException.
The below mentioned jar is present at the said location /opt/spark/work-dir/, but the Files.copy statement in the SparkSubmit.scala is causing the issue.
Can you please help to check what could be possible cause?
{code:java}
Files  local:///opt/spark/work-dir/sample.jar from /opt/spark/work-dir/sample.jar to /opt/spark/work-dir/./sample.jar
Exception in thread ""main"" java.nio.file.NoSuchFileException: /opt/spark/work-dir/sample.jar
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixCopyFile.copy(UnixCopyFile.java:526)
        at sun.nio.fs.UnixFileSystemProvider.copy(UnixFileSystemProvider.java:253)
        at java.nio.file.Files.copy(Files.java:1274)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$14(SparkSubmit.scala:437)
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at scala.collection.TraversableLike.map(TraversableLike.scala:286)
        at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
        at scala.collection.AbstractTraversable.map(Traversable.scala:108)
        at org.apache.spark.deploy.SparkSubmit.downloadResourcesToCurrentDirectory$1(SparkSubmit.scala:424)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$17(SparkSubmit.scala:449)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:449)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
 {code};;;",,,,,,,,,,,,,
"""Back to Master"" returns 500 error in Standalone cluster",SPARK-33774,13345794,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,14/Dec/20 08:17,17/Dec/20 14:52,13/Jul/23 08:50,17/Dec/20 14:52,2.4.0,3.0.0,3.1.0,,,,,,,,,3.0.2,3.1.0,,,Spark Core,Web UI,,,0,,,,"When launching a Standalone cluster locally, click ""Back to Master"" in Worker page always return 500 error.",,apachespark,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21642,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 17 14:52:57 UTC 2020,,,,,,,,,,"0|z0lh3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/20 08:34;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/30759;;;","14/Dec/20 08:35;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/30759;;;","17/Dec/20 14:52;srowen;Issue resolved by pull request 30759
[https://github.com/apache/spark/pull/30759];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix the R dependencies build error on GitHub Actions and AppVeyor,SPARK-33757,13345522,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,11/Dec/20 13:29,12/Dec/22 18:11,13/Jul/23 08:50,11/Dec/20 15:56,3.1.0,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,Project Infra,,,,0,,,,"R dependencies build error happens now.
The reason seems that usethis package is updated 2020/12/10.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 14 00:27:28 UTC 2020,,,,,,,,,,"0|z0lffc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/20 13:32;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30737;;;","11/Dec/20 15:56;gurwls223;Issue resolved by pull request 30737
[https://github.com/apache/spark/pull/30737];;;","14/Dec/20 00:27;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30753;;;",,,,,,,,,,,,,,,,,,,,,,,
BytesToBytesMap's iterator hasNext method should be idempotent.,SPARK-33756,13345477,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,advancedxy,advancedxy,advancedxy,11/Dec/20 09:49,20/Dec/20 14:52,13/Jul/23 08:50,20/Dec/20 14:52,3.0.0,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,Spark Core,,,,0,,,,"BytesToBytesMap's MapIterator's hasNext method is not idempotent. 
{code:java}
// 
public boolean hasNext() {
  if (numRecords == 0) {
    if (reader != null) {
      // if called multiple multiple times, it will throw NoSuchElement exception
      handleFailedDelete();
    }
  }
  return numRecords > 0;
}
{code}
Multiple calls to this `hasNext` method will call `handleFailedDelete()` multiple times, which will throw NoSuchElementException  as the spillWrites has already been empty.

 

We observed this issue for in one of our production jobs after upgrading to Spark 3.0",,advancedxy,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 20 14:52:30 UTC 2020,,,,,,,,,,"0|z0lf5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/20 09:58;apachespark;User 'advancedxy' has created a pull request for this issue:
https://github.com/apache/spark/pull/30728;;;","20/Dec/20 14:52;srowen;Issue resolved by pull request 30728
[https://github.com/apache/spark/pull/30728];;;",,,,,,,,,,,,,,,,,,,,,,,,
Avoid the getSimpleMessage of AnalysisException adds semicolon repeatedly,SPARK-33752,13345439,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,11/Dec/20 07:04,12/Dec/22 18:11,13/Jul/23 08:50,16/Dec/20 14:35,3.2.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"The current getSimpleMessage of AnalysisException may adds semicolon repeatedly. There show an example below:

{code:java}
select decode()
{code}
The output will be:

{code:java}
org.apache.spark.sql.AnalysisException
Invalid number of arguments for function decode. Expected: 2; Found: 0;; line 1 pos 7
{code}
",,apachespark,beliefer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 16 14:35:24 UTC 2020,,,,,,,,,,"0|z0lex4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/20 07:19;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/30724;;;","11/Dec/20 18:56;apachespark;User 'n-marion' has created a pull request for this issue:
https://github.com/apache/spark/pull/30740;;;","16/Dec/20 02:22;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/30791;;;","16/Dec/20 02:22;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/30791;;;","16/Dec/20 02:35;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/30792;;;","16/Dec/20 14:35;gurwls223;Issue resolved by pull request 30792
[https://github.com/apache/spark/pull/30792];;;",,,,,,,,,,,,,,,,,,,,
Exclude target directory in pycodestyle and flake8,SPARK-33749,13345418,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,11/Dec/20 02:48,12/Dec/22 18:10,13/Jul/23 08:50,11/Dec/20 05:20,3.1.0,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,Build,,,,0,,,,"Once you build and ran K8S tests, Python lint fails as below:


{code:java}
$ ./dev/lint-python
starting python compilation test...
python compilation succeeded.

downloading pycodestyle from https://raw.githubusercontent.com/PyCQA/pycodestyle/2.6.0/pycodestyle.py...
starting pycodestyle test...
pycodestyle checks failed:
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/pyspark/cloudpickle/cloudpickle.py:15:101: E501 line too long (105 > 100 characters)
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:60:101: E501 line too long (124 > 100 characters)
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:76:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:96:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:100:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:102:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:110:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:113:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:117:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:121:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:127:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:130:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:147:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:150:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:154:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:157:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:166:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:180:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:184:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:188:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:191:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:195:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:204:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:207:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:210:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:213:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:218:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:221:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:230:1: E122 continuation line missing indentation or outdented
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:231:1: E122 continuation line missing indentation or outdented
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:231:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:233:1: E122 continuation line missing indentation or outdented
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:234:1: E122 continuation line missing indentation or outdented
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:234:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:236:1: E122 continuation line missing indentation or outdented
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:237:1: E122 continuation line missing indentation or outdented
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:237:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:244:3: E121 continuation line under-indented for hanging indent
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:250:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:254:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:257:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:260:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:263:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:266:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:279:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:288:3: E121 continuation line under-indented for hanging indent
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:294:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:297:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:300:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:303:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:315:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:321:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:325:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:328:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:332:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:335:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:338:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:341:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:345:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:349:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:355:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:358:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:361:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:364:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:367:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:370:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:373:1: E265 block comment should start with '# '
./resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked/python/docs/source/conf.py:374:1: E302 expected 2 blank lines, found 1

{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 11 05:20:08 UTC 2020,,,,,,,,,,"0|z0lesg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/20 02:57;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30718;;;","11/Dec/20 02:57;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30718;;;","11/Dec/20 05:20;gurwls223;Issue resolved by pull request 30718
[https://github.com/apache/spark/pull/30718];;;",,,,,,,,,,,,,,,,,,,,,,,
hadoop configs in hive-site.xml can overrides pre-existing hadoop ones,SPARK-33740,13345319,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,10/Dec/20 15:50,11/Dec/20 09:50,13/Jul/23 08:50,11/Dec/20 09:50,3.0.1,3.1.0,3.2.0,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,,,apachespark,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31170,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 11 09:50:11 UTC 2020,,,,,,,,,,"0|z0le6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/20 15:54;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30709;;;","11/Dec/20 03:47;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30720;;;","11/Dec/20 09:50;dongjoon;This is resolved via https://github.com/apache/spark/pull/30709;;;",,,,,,,,,,,,,,,,,,,,,,,
Jobs committed through the S3A Magic committer don't report the bytes written,SPARK-33739,13345311,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,10/Dec/20 15:17,18/Feb/21 14:45,13/Jul/23 08:50,18/Feb/21 14:45,3.0.1,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,"The spark statistics tracking doesn't correctly assess the size of the uploaded files as it only calls getFileStatus on the zero byte objects -not the yet-to-manifest files. Which, given they don't exist yet, isn't easy to do.

HADOOP-17414 will attach the final length as a custom header to the marker object, and implement getXAttr in the S3A FS to probe for it.

BasicWriteStatsTracker can probe for this custom Xattr if the size of the generated file is 0 bytes; if found and parseable use that as the declared length of the output.",,apachespark,stevel@apache.org,,,,,,,,,,,,,,,,,HADOOP-17414,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 10 20:35:27 UTC 2020,,,,,,,,,,"0|z0le4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/20 20:34;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/30714;;;","10/Dec/20 20:35;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/30714;;;",,,,,,,,,,,,,,,,,,,,,,,,
PullOutNondeterministic should check and collect deterministic field,SPARK-33733,13345208,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,10/Dec/20 06:40,18/Dec/20 21:59,13/Jul/23 08:50,14/Dec/20 14:36,3.1.0,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"The deterministic field is wider than `NonDerterministic`, we should keepe same range between pull out and check analysis.",,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 15 02:32:37 UTC 2020,,,,,,,,,,"0|z0ldhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/20 08:15;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/30703;;;","14/Dec/20 14:36;cloud_fan;Issue resolved by pull request 30703
[https://github.com/apache/spark/pull/30703];;;","15/Dec/20 02:30;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/30771;;;","15/Dec/20 02:31;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/30772;;;","15/Dec/20 02:32;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/30772;;;",,,,,,,,,,,,,,,,,,,,,
Duplicate field names causes wrong answers during aggregation,SPARK-33726,13345143,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yliou,yliou,yliou,09/Dec/20 20:49,03/Feb/21 02:36,13/Jul/23 08:50,25/Jan/21 06:55,2.4.4,3.0.1,,,,,,,,,,2.4.8,3.0.2,3.1.1,,SQL,,,,0,correctness,,,"We saw this bug at Workday.

Duplicate field names for different fields can cause  org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch#allocate to return a fixed batch when it should have returned a variable batch leading to wrong results.

This example produces wrong results in the spark shell:

scala> sql(""with T as (select id as a, -id as x from range(3)), U as (select id as b, cast(id as string) as x from range(3)) select T.x, U.x, min(a) as ma, min(b) as mb from T join U on a=b group by U.x, T.x"").show
 
|*x*|*x*|*ma*|*mb*|
|-2|2|0|null|
|-1|1|null|1|
|0|0|0|0|

 instead of correct output : 
|*x*|*x*|*ma*|*mb*|
|0|0|0|0|
|-2|2|2|2|
|-1|1|1|1|

The issue can be solved by iterating over the fields themselves instead of field names. ",,angerszhuuu,apachespark,xkrogen,yliou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 03 02:36:26 UTC 2021,,,,,,,,,,"0|z0ld3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/20 20:49;yliou;Will create a PR for the issue, which is at https://github.com/apache/spark/pull/30788;;;","15/Dec/20 18:33;apachespark;User 'yliou' has created a pull request for this issue:
https://github.com/apache/spark/pull/30788;;;","25/Jan/21 18:36;apachespark;User 'yliou' has created a pull request for this issue:
https://github.com/apache/spark/pull/31327;;;","25/Jan/21 18:37;apachespark;User 'yliou' has created a pull request for this issue:
https://github.com/apache/spark/pull/31327;;;","03/Feb/21 02:35;apachespark;User 'yliou' has created a pull request for this issue:
https://github.com/apache/spark/pull/31447;;;","03/Feb/21 02:36;apachespark;User 'yliou' has created a pull request for this issue:
https://github.com/apache/spark/pull/31447;;;",,,,,,,,,,,,,,,,,,,,
Upgrade snappy-java to 1.1.8.2,SPARK-33725,13345138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,09/Dec/20 20:10,13/Dec/20 18:32,13/Jul/23 08:50,09/Dec/20 22:28,2.4.7,3.0.1,3.1.0,3.2.0,,,,,,,,2.4.8,3.0.2,3.1.0,,Build,,,,0,,,,"Minor version upgrade that includes:
 * Fixed an initialization issue when using a recent Mac OS X version #265
 * Support Apple Silicon (M1, Mac-aarch64)
 * Fixed the pure-java Snappy fallback logic when no native library for your platform is found.",,apachespark,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 10 00:49:43 UTC 2020,,,,,,,,,,"0|z0ld28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/20 20:19;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30690;;;","09/Dec/20 20:20;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30690;;;","09/Dec/20 22:28;dongjoon;This is resolved via https://github.com/apache/spark/pull/30690;;;","09/Dec/20 23:26;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30695;;;","09/Dec/20 23:27;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30695;;;","10/Dec/20 00:44;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30697;;;","10/Dec/20 00:48;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30698;;;","10/Dec/20 00:49;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30698;;;",,,,,,,,,,,,,,,,,,
Handle DELETE in ReplaceNullWithFalseInPredicate,SPARK-33722,13345105,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,aokolnychyi,aokolnychyi,09/Dec/20 15:46,09/Dec/20 19:43,13/Jul/23 08:50,09/Dec/20 19:43,3.2.0,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,We should handle delete statements in {{ReplaceNullWithFalseInPredicate}}.,,aokolnychyi,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 19:43:21 UTC 2020,,,,,,,,,,"0|z0lcv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/20 15:46;aokolnychyi;I'll submit a PR soon.;;;","09/Dec/20 17:51;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/30688;;;","09/Dec/20 17:52;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/30688;;;","09/Dec/20 19:43;dongjoon;Issue resolved by pull request 30688
[https://github.com/apache/spark/pull/30688];;;",,,,,,,,,,,,,,,,,,,,,,
Three cases always fail in hive-thriftserver module,SPARK-33705,13344729,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Qin Yao,LuciferYang,LuciferYang,08/Dec/20 06:51,14/Dec/20 05:16,13/Jul/23 08:50,14/Dec/20 05:15,3.2.0,,,,,,,,,,,3.1.0,,,,Tests,,,,0,,,,"Seems the following tests always failed both in Jenkins and Github Action:
 * org.apache.spark.sql.hive.thriftserver.HiveThriftHttpServerSuite.JDBC query execution
 * org.apache.spark.sql.hive.thriftserver.HiveThriftHttpServerSuite.Checks Hive version
 * org.apache.spark.sql.hive.thriftserver.HiveThriftHttpServerSuite.SPARK-24829 Checks cast as float

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/132189/testReport/]

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/132403/testReport/]

 

 ",,apachespark,cloud_fan,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 14 05:15:43 UTC 2020,,,,,,,,,,"0|z0lajs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/20 06:51;LuciferYang;cc [~dongjoon];;;","08/Dec/20 08:14;LuciferYang;local test is successful.....;;;","09/Dec/20 00:38;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30643;;;","09/Dec/20 00:38;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30643;;;","14/Dec/20 05:15;cloud_fan;Issue resolved by pull request 30643
[https://github.com/apache/spark/pull/30643];;;",,,,,,,,,,,,,,,,,,,,,
UnionExec should require column ordering in RemoveRedundantProjects,SPARK-33697,13344685,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,08/Dec/20 00:53,09/Feb/21 01:50,13/Jul/23 08:50,17/Dec/20 05:48,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"UnionExec requires its children's columns to have the same order in order to merge the columns. Currently, the physical rule `RemoveRedundantProjects` can pass through the ordering requirements from its parent and incorrectly remove the necessary project nodes below a union operation.",,allisonwang-db,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 17 05:48:07 UTC 2020,,,,,,,,,,"0|z0laa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/20 01:42;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30659;;;","17/Dec/20 05:48;cloud_fan;Issue resolved by pull request 30659
[https://github.com/apache/spark/pull/30659];;;",,,,,,,,,,,,,,,,,,,,,,,,
view shouldn't use current catalog and namespace to lookup function,SPARK-33692,13344599,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linhongliu-db,linhongliu-db,linhongliu-db,07/Dec/20 15:00,10/Dec/20 09:15,13/Jul/23 08:50,10/Dec/20 09:14,3.0.1,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"Reproduce steps:
spark.sql(""CREATE FUNCTION udf_plus AS 'udf.UdfPlus10' USING JAR '/home/linhong.liu/spark-udf_2.12-0.1.0-SNAPSHOT.jar'"")

spark.sql(""create view v1 as select udf_plus(1)"")
spark.sql(""select * from v1"").show() // output 11

spark.sql(""CREATE TEMPORARY FUNCTION udf_plus AS 'udf.UdfPlus20' USING JAR '/home/linhong.liu/spark-udf_2.12-0.1.0-SNAPSHOT.jar'"")

spark.sql(""select * from v1"").show() // throw exception

org.apache.spark.sql.AnalysisException: Attribute with name 'default.udf_plus(1)' is not found in '(udf_plus(1))';;
Project [default.udf_plus(1)#60]
+- SubqueryAlias spark_catalog.default.v1
   +- View (`default`.`v1`, [default.udf_plus(1)#60])
      +- Project [HiveSimpleUDF#udf.UdfPlus20(1) AS udf_plus(1)#61]
         +- OneRowRelation",,apachespark,cloud_fan,linhongliu-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 10 09:14:42 UTC 2020,,,,,,,,,,"0|z0l9r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/20 03:51;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30662;;;","08/Dec/20 03:52;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30662;;;","10/Dec/20 09:14;cloud_fan;Issue resolved by pull request 30662
[https://github.com/apache/spark/pull/30662];;;",,,,,,,,,,,,,,,,,,,,,,,
Remove -Djava.version=11 from Scala 2.13 build in GitHub Actions,SPARK-33683,13344465,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,07/Dec/20 01:50,07/Dec/20 01:58,13/Jul/23 08:50,07/Dec/20 01:58,3.1.0,,,,,,,,,,,3.1.0,,,,Build,,,,0,,,,"In the GitHub Actions' job, the build command for Scala 2.13 is defined as follows.
{code}
./build/sbt -Pyarn -Pmesos -Pkubernetes -Phive -Phive-thriftserver -Phadoop-cloud -Pkinesis-asl -Djava.version=11 -Pscala-2.13 compile test:compile
{code}

Though, Scala 2.13 build uses Java 8 rather than 11 so let's remove -Djava.version=11.",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 01:58:11 UTC 2020,,,,,,,,,,"0|z0l8xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/20 01:54;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30633;;;","07/Dec/20 01:58;dongjoon;Issue resolved by pull request 30633
[https://github.com/apache/spark/pull/30633];;;",,,,,,,,,,,,,,,,,,,,,,,,
Increase K8s IT timeout to 3 minutes,SPARK-33681,13344459,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,07/Dec/20 00:33,12/Dec/22 18:10,13/Jul/23 08:50,07/Dec/20 01:14,2.4.7,3.0.1,,,,,,,,,,2.4.8,3.0.2,,,Kubernetes,Tests,,,0,,,,"We are using 3 minutes in master/branch-3.1. This issue only happens at branch-3.0/branch-2.4

- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/36905/console
{code}
- Run PySpark with memory customization *** FAILED ***
  The code passed to eventually never returned normally. Attempted 70 times over 2.018373577433333 minutes. Last failure message: ""++ id -u
  + myuid=0
  ++ id -g
  + mygid=0
  + set +e
  ++ getent passwd 0
  + uidentry=root:x:0:0:root:/root:/bin/bash
  + set -e
  + '[' -z root:x:0:0:root:/root:/bin/bash ']'
  + SPARK_K8S_CMD=driver-py
  + case ""$SPARK_K8S_CMD"" in
  + shift 1
  + SPARK_CLASSPATH=':/opt/spark/jars/*'
  + env
  + sort -t_ -k4 -n
  + sed 's/[^=]*=\(.*\)/\1/g'
  + grep SPARK_JAVA_OPT_
  + readarray -t SPARK_EXECUTOR_JAVA_OPTS
  + '[' -n '' ']'
  + '[' -n /opt/spark/tests/py_container_checks.py ']'
  + PYTHONPATH='/opt/spark/python/lib/pyspark.zip:/opt/spark/python/lib/py4j-*.zip:/opt/spark/tests/py_container_checks.py'
  + PYSPARK_ARGS=
  + '[' -n 209715200 ']'
  + PYSPARK_ARGS=209715200
  + R_ARGS=
  + '[' -n '' ']'
  + '[' 3 == 2 ']'
  + '[' 3 == 3 ']'
  ++ python3 -V
  + pyv3='Python 3.7.3'
  + export PYTHON_VERSION=3.7.3
  + PYTHON_VERSION=3.7.3
  + export PYSPARK_PYTHON=python3
  + PYSPARK_PYTHON=python3
  + export PYSPARK_DRIVER_PYTHON=python3
  + PYSPARK_DRIVER_PYTHON=python3
  + '[' -n '' ']'
  + '[' -z ']'
  + case ""$SPARK_K8S_CMD"" in
  + CMD=(""$SPARK_HOME/bin/spark-submit"" --conf ""spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS"" --deploy-mode client ""$@"" $PYSPARK_PRIMARY $PYSPARK_ARGS)
  + exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=172.17.0.4 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner /opt/spark/tests/worker_memory_check.py 209715200
  20/12/07 00:09:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
  20/12/07 00:09:33 INFO SparkContext: Running Spark version 2.4.8-SNAPSHOT
  20/12/07 00:09:33 INFO SparkContext: Submitted application: PyMemoryTest
  20/12/07 00:09:33 INFO SecurityManager: Changing view acls to: root
  20/12/07 00:09:33 INFO SecurityManager: Changing modify acls to: root
  20/12/07 00:09:33 INFO SecurityManager: Changing view acls groups to: 
  20/12/07 00:09:33 INFO SecurityManager: Changing modify acls groups to: 
  20/12/07 00:09:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
  20/12/07 00:09:34 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
  20/12/07 00:09:34 INFO SparkEnv: Registering MapOutputTracker
  20/12/07 00:09:34 INFO SparkEnv: Registering BlockManagerMaster
  20/12/07 00:09:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  20/12/07 00:09:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
  20/12/07 00:09:34 INFO DiskBlockManager: Created local directory at /var/data/spark-9950d0f1-8753-441f-97cf-1aa6defd1d0e/blockmgr-9f6bcf4d-ff41-4b27-8312-0fb23bf4ed1b
  20/12/07 00:09:34 INFO MemoryStore: MemoryStore started with capacity 546.3 MB
  20/12/07 00:09:34 INFO SparkEnv: Registering OutputCommitCoordinator
  20/12/07 00:09:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
  20/12/07 00:09:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://spark-test-app-1607299769587-driver-svc.7c85102d1c1d4c8fb5a453963ab5535a.svc:4040
  20/12/07 00:09:34 INFO SparkContext: Added file file:///opt/spark/tests/worker_memory_check.py at spark://spark-test-app-1607299769587-driver-svc.7c85102d1c1d4c8fb5a453963ab5535a.svc:7078/files/worker_memory_check.py with timestamp 1607299774831
  20/12/07 00:09:34 INFO Utils: Copying /opt/spark/tests/worker_memory_check.py to /var/data/spark-9950d0f1-8753-441f-97cf-1aa6defd1d0e/spark-8ae1ff4f-2989-43d3-adfe-f26e8ff71ed2/userFiles-cfe3880e-6803-4809-9c01-6f1f582e4481/worker_memory_check.py
  20/12/07 00:09:34 INFO SparkContext: Added file file:///opt/spark/tests/py_container_checks.py at spark://spark-test-app-1607299769587-driver-svc.7c85102d1c1d4c8fb5a453963ab5535a.svc:7078/files/py_container_checks.py with timestamp 1607299774847
  20/12/07 00:09:34 INFO Utils: Copying /opt/spark/tests/py_container_checks.py to /var/data/spark-9950d0f1-8753-441f-97cf-1aa6defd1d0e/spark-8ae1ff4f-2989-43d3-adfe-f26e8ff71ed2/userFiles-cfe3880e-6803-4809-9c01-6f1f582e4481/py_container_checks.py
  20/12/07 00:09:36 INFO ExecutorPodsAllocator: Going to request 1 executors from Kubernetes.
  20/12/07 00:09:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
  20/12/07 00:09:36 INFO NettyBlockTransferService: Server created on spark-test-app-1607299769587-driver-svc.7c85102d1c1d4c8fb5a453963ab5535a.svc:7079
  20/12/07 00:09:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  20/12/07 00:09:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-test-app-1607299769587-driver-svc.7c85102d1c1d4c8fb5a453963ab5535a.svc, 7079, None)
  20/12/07 00:09:36 INFO BlockManagerMasterEndpoint: Registering block manager spark-test-app-1607299769587-driver-svc.7c85102d1c1d4c8fb5a453963ab5535a.svc:7079 with 546.3 MB RAM, BlockManagerId(driver, spark-test-app-1607299769587-driver-svc.7c85102d1c1d4c8fb5a453963ab5535a.svc, 7079, None)
  20/12/07 00:09:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-test-app-1607299769587-driver-svc.7c85102d1c1d4c8fb5a453963ab5535a.svc, 7079, None)
  20/12/07 00:09:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-test-app-1607299769587-driver-svc.7c85102d1c1d4c8fb5a453963ab5535a.svc, 7079, None)
  20/12/07 00:10:06 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
  20/12/07 00:10:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/work-dir/spark-warehouse').
  20/12/07 00:10:06 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
  20/12/07 00:10:07 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
  20/12/07 00:10:07 INFO SparkContext: Starting job: collect at /opt/spark/tests/worker_memory_check.py:43
  20/12/07 00:10:07 INFO DAGScheduler: Got job 0 (collect at /opt/spark/tests/worker_memory_check.py:43) with 2 output partitions
  20/12/07 00:10:07 INFO DAGScheduler: Final stage: ResultStage 0 (collect at /opt/spark/tests/worker_memory_check.py:43)
  20/12/07 00:10:07 INFO DAGScheduler: Parents of final stage: List()
  20/12/07 00:10:07 INFO DAGScheduler: Missing parents: List()
  20/12/07 00:10:07 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at collect at /opt/spark/tests/worker_memory_check.py:43), which has no missing parents
  20/12/07 00:10:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.5 KB, free 546.3 MB)
  20/12/07 00:10:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.1 KB, free 546.3 MB)
  20/12/07 00:10:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-test-app-1607299769587-driver-svc.7c85102d1c1d4c8fb5a453963ab5535a.svc:7079 (size: 3.1 KB, free: 546.3 MB)
  20/12/07 00:10:07 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1184
  20/12/07 00:10:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at collect at /opt/spark/tests/worker_memory_check.py:43) (first 15 tasks are for partitions Vector(0, 1))
  20/12/07 00:10:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
  20/12/07 00:10:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
  20/12/07 00:10:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
  20/12/07 00:10:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
  20/12/07 00:11:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
  20/12/07 00:11:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
  20/12/07 00:11:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
  "" did not contain ""PySpark Worker Memory Check is: True"" The application did not complete.. (KubernetesSuite.scala:249)
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 01:14:49 UTC 2020,,,,,,,,,,"0|z0l8w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/20 00:40;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30632;;;","07/Dec/20 00:41;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30632;;;","07/Dec/20 01:14;gurwls223;Issue resolved by pull request 30632
[https://github.com/apache/spark/pull/30632];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix PrunePartitionSuiteBase/BucketedReadWithHiveSupportSuite not to depend on the default conf,SPARK-33680,13344458,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,07/Dec/20 00:20,28/Dec/20 03:39,13/Jul/23 08:50,07/Dec/20 03:41,3.1.0,3.2.0,,,,,,,,,,3.1.0,,,,SQL,Tests,,,0,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 28 03:39:08 UTC 2020,,,,,,,,,,"0|z0l8vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/20 00:22;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30631;;;","07/Dec/20 03:41;dongjoon;Issue resolved by pull request 30631
[https://github.com/apache/spark/pull/30631];;;","07/Dec/20 21:47;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30655;;;","07/Dec/20 21:47;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30655;;;","28/Dec/20 03:38;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/30941;;;","28/Dec/20 03:39;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/30941;;;",,,,,,,,,,,,,,,,,,,,
LikeSimplification should be skipped if pattern contains any escapeChar,SPARK-33677,13344423,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,luluorta,luluorta,luluorta,06/Dec/20 12:13,08/Dec/20 11:47,13/Jul/23 08:50,08/Dec/20 11:47,3.1.0,,,,,,,,,,,3.0.2,3.1.0,3.2.0,,SQL,,,,0,,,,"LikeSimplification rule does not work correctly for many cases that have patterns containing escape characters:
{code:sql}
SELECT s LIKE 'm%aca' ESCAPE '%' from t;
SELECT s LIKE 'maacaa' ESCAPE 'a' FROM t;
{code}",,apachespark,luluorta,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 08 11:47:19 UTC 2020,,,,,,,,,,"0|z0l8o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/20 12:58;apachespark;User 'luluorta' has created a pull request for this issue:
https://github.com/apache/spark/pull/30625;;;","08/Dec/20 11:47;maropu;Resolved by https://github.com/apache/spark/pull/30625;;;",,,,,,,,,,,,,,,,,,,,,,,,
Wrong error message from YARN application state monitor when sc.stop in yarn client mode,SPARK-33669,13344341,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sqlwindspeaker,sqlwindspeaker,sqlwindspeaker,05/Dec/20 09:37,09/Dec/20 07:27,13/Jul/23 08:50,09/Dec/20 07:25,2.4.3,3.0.1,,,,,,,,,,3.1.0,,,,YARN,,,,0,,,,"For YarnClient mode, when stopping YarnClientSchedulerBackend, it first tries to interrupt Yarn application monitor thread. In MonitorThread.run() it catches InterruptedException to gracefully response to stopping request.

But client.monitorApplication method also throws InterruptedIOException when the hadoop rpc call is calling. In this case, MonitorThread will not know it is interrupted, a Yarn App failed is returned with ""Failed to contact YARN for application xxxxx;  YARN application has exited unexpectedly with state xxxxx"" is logged with error level. which confuse user a lot.

We Should take considerate InterruptedIOException here to make it the same behavior with InterruptedException.
{code:java}
private class MonitorThread extends Thread {
  private var allowInterrupt = true

  override def run() {
    try {
      val YarnAppReport(_, state, diags) =
        client.monitorApplication(appId.get, logApplicationReport = false)
      logError(s""YARN application has exited unexpectedly with state $state! "" +
        ""Check the YARN application logs for more details."")
      diags.foreach { err =>
        logError(s""Diagnostics message: $err"")
      }
      allowInterrupt = false
      sc.stop()
    } catch {
      case e: InterruptedException => logInfo(""Interrupting monitor thread"")
    }
  }

  
{code}
{code:java}
// wrong error message
2020-12-05 03:06:58,000 ERROR [YARN application state monitor]: org.apache.spark.deploy.yarn.Client(91) - Failed to contact YARN for application application_1605868815011_1154961. 
java.io.InterruptedIOException: Call interrupted
        at org.apache.hadoop.ipc.Client.call(Client.java:1466)
        at org.apache.hadoop.ipc.Client.call(Client.java:1409)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy38.getApplicationReport(Unknown Source)
        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplicationReport(ApplicationClientProtocolPBClientImpl.java:187)
        at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
        at com.sun.proxy.$Proxy39.getApplicationReport(Unknown Source)
        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplicationReport(YarnClientImpl.java:408)
        at org.apache.spark.deploy.yarn.Client.getApplicationReport(Client.scala:327)
        at org.apache.spark.deploy.yarn.Client.monitorApplication(Client.scala:1039)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:116)
2020-12-05 03:06:58,000 ERROR [YARN application state monitor]: org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend(70) - YARN application has exited unexpectedly with state FAILED! Check the YARN application logs for more details. 

2020-12-05 03:06:58,001 ERROR [YARN application state monitor]: org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend(70) - Diagnostics message: Failed to contact YARN for application application_1605868815011_1154961.

{code}
 
{code:java}
// hadoop ipc code
public Writable call(RPC.RpcKind rpcKind, Writable rpcRequest,
    ConnectionId remoteId, int serviceClass,
    AtomicBoolean fallbackToSimpleAuth) throws IOException {
  final Call call = createCall(rpcKind, rpcRequest);
  Connection connection = getConnection(remoteId, call, serviceClass,
    fallbackToSimpleAuth);
  try {
    connection.sendRpcRequest(call);                 // send the rpc request
  } catch (RejectedExecutionException e) {
    throw new IOException(""connection has been closed"", e);
  } catch (InterruptedException e) {
    Thread.currentThread().interrupt();
    LOG.warn(""interrupted waiting to send rpc request to server"", e);
    throw new IOException(e);
  }

  synchronized (call) {
    while (!call.done) {
      try {
        call.wait();                           // wait for the result
      } catch (InterruptedException ie) {
        Thread.currentThread().interrupt();
        throw new InterruptedIOException(""Call interrupted"");
      }
    }
{code}
 ",,apachespark,chengbing.liu,mridulm80,sqlwindspeaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 07:25:45 UTC 2020,,,,,,,,,,"0|z0l85s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/20 10:10;apachespark;User 'sqlwindspeaker' has created a pull request for this issue:
https://github.com/apache/spark/pull/30617;;;","05/Dec/20 10:11;apachespark;User 'sqlwindspeaker' has created a pull request for this issue:
https://github.com/apache/spark/pull/30617;;;","09/Dec/20 07:25;mridulm80;Issue resolved by pull request 30617
[https://github.com/apache/spark/pull/30617];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix misleading message for uncaching when createOrReplaceTempView is called,SPARK-33663,13344280,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,imback82,imback82,imback82,04/Dec/20 18:40,12/Dec/22 18:10,13/Jul/23 08:50,07/Dec/20 00:49,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"To repro:

{code:java}
scala> sql(""CREATE TABLE table USING parquet AS SELECT 2"")
res0: org.apache.spark.sql.DataFrame = []                                       

scala> val df = spark.table(""table"")
df: org.apache.spark.sql.DataFrame = [2: int]

scala> df.createOrReplaceTempView(""t2"")
20/12/04 10:16:24 WARN CommandUtils: Exception when attempting to uncache $name
org.apache.spark.sql.AnalysisException: Table or view not found: t2;;
'UnresolvedRelation [t2], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:113)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:152)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:172)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:214)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:169)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:138)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:768)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:768)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:889)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:589)
	at org.apache.spark.sql.internal.CatalogImpl.uncacheTable(CatalogImpl.scala:476)
	at org.apache.spark.sql.execution.command.CommandUtils$.uncacheTableOrView(CommandUtils.scala:392)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:124)

{code}

It shouldn't log because `t2` does not exist yet.",,apachespark,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 00:49:00 UTC 2020,,,,,,,,,,"0|z0l7s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/20 18:50;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/30608;;;","07/Dec/20 00:49;gurwls223;Issue resolved by pull request 30608
[https://github.com/apache/spark/pull/30608];;;",,,,,,,,,,,,,,,,,,,,,,,,
cache table not working for persisted view,SPARK-33647,13343968,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linhongliu-db,linhongliu-db,linhongliu-db,03/Dec/20 09:54,14/Jan/21 03:14,13/Jul/23 08:50,04/Dec/20 06:49,3.0.1,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"In `CacheManager`, tables (including views) are cached by its logical plan, and
use `QueryPlan.sameResult` to lookup the cache. But the PersistedView wraps
the child plan with a `View` which always lead false for `sameResult` check.",,apachespark,cloud_fan,csun,linhongliu-db,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34108,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 08 07:45:49 UTC 2020,,,,,,,,,,"0|z0l5uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/20 09:56;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30589;;;","04/Dec/20 06:43;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30567;;;","04/Dec/20 06:49;cloud_fan;Issue resolved by pull request 30567
[https://github.com/apache/spark/pull/30567];;;","08/Dec/20 07:45;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30666;;;",,,,,,,,,,,,,,,,,,,,,,
Invalidate new char-like type in public APIs that result incorrect results,SPARK-33641,13343930,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Qin Yao,Qin Yao,Qin Yao,03/Dec/20 07:35,07/Dec/20 18:37,13/Jul/23 08:50,07/Dec/20 13:40,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"1. udf
{code:java}
scala> spark.udf.register(""abcd"", () => ""12345"", org.apache.spark.sql.types.VarcharType(2))

scala> spark.sql(""select abcd()"").show
scala.MatchError: CharType(2) (of class org.apache.spark.sql.types.VarcharType)
  at org.apache.spark.sql.catalyst.encoders.RowEncoder$.externalDataTypeFor(RowEncoder.scala:215)
  at org.apache.spark.sql.catalyst.encoders.RowEncoder$.externalDataTypeForInput(RowEncoder.scala:212)
  at org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType.<init>(objects.scala:1741)
  at org.apache.spark.sql.catalyst.encoders.RowEncoder$.$anonfun$serializerFor$3(RowEncoder.scala:175)
  at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
  at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
  at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
  at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)
  at org.apache.spark.sql.catalyst.encoders.RowEncoder$.serializerFor(RowEncoder.scala:171)
  at org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(RowEncoder.scala:66)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:768)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:611)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:768)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:606)
  ... 47 elided
{code}

2. spark.createDataframe


{code:java}
scala> spark.createDataFrame(spark.read.text(""README.md"").rdd, new org.apache.spark.sql.types.StructType().add(""c"", ""char(1)"")).show
+--------------------+
|                   c|
+--------------------+
|      # Apache Spark|
|                    |
|Spark is a unifie...|
|high-level APIs i...|
|supports general ...|
|rich set of highe...|
|MLlib for machine...|
|and Structured St...|
|                    |
|<https://spark.ap...|
|                    |
|[![Jenkins Build]...|
|[![AppVeyor Build...|
|[![PySpark Covera...|
|                    |
|                    |
|## Online Documen...|
|                    |
|You can find the ...|
|guide, on the [pr...|
+--------------------+
only showing top 20 rows
{code}


3. reader.schema

```
scala> spark.read.schema(""a varchar(2)"").text(""./README.md"").show(100)
+--------------------+
|                   a|
+--------------------+
|      # Apache Spark|
|                    |
|Spark is a unifie...|
|high-level APIs i...|
|supports general ...|
```
4. etc
",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33480,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 18:37:50 UTC 2020,,,,,,,,,,"0|z0l5mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/20 07:47;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30586;;;","07/Dec/20 13:40;cloud_fan;Issue resolved by pull request 30586
[https://github.com/apache/spark/pull/30586];;;","07/Dec/20 18:37;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30654;;;",,,,,,,,,,,,,,,,,,,,,,,
Extend connection timeout to DB server for DB2IntegrationSuite and its variants,SPARK-33640,13343914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,03/Dec/20 06:00,04/Dec/20 08:12,13/Jul/23 08:50,04/Dec/20 08:12,3.1.0,,,,,,,,,,,3.1.0,,,,Tests,,,,0,,,,"The container image ibmcom/db2 creates a database when it starts up.
The database creation can take over 2 minutes.

DB2IntegrationSuite and its variants use the container image but the connection timeout is set to 2 minutes so these suites almost always fail.",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 04 08:12:26 UTC 2020,,,,,,,,,,"0|z0l5iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/20 06:06;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30583;;;","03/Dec/20 06:06;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30583;;;","04/Dec/20 08:12;dongjoon;Issue resolved by pull request 30583
[https://github.com/apache/spark/pull/30583];;;",,,,,,,,,,,,,,,,,,,,,,,
Performance regression in Kafka read,SPARK-33635,13343823,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kabhwan,david.wyles,david.wyles,02/Dec/20 16:54,07/Jan/21 03:13,13/Jul/23 08:50,06/Jan/21 06:00,3.0.0,3.0.1,,,,,,,,,,3.0.2,3.1.1,,,SQL,,,,0,,,,"I have observed a slowdown in the reading of data from kafka on all of our systems when migrating from spark 2.4.5 to Spark 3.0.0 (and Spark 3.0.1)

I have created a sample project to isolate the problem as much as possible, with just a read all data from a kafka topic (see [https://github.com/codegorillauk/spark-kafka-read] ).

With 2.4.5, across multiple runs, 
 I get a stable read rate of 1,120,000 (1.12 mill) rows per second

With 3.0.0 or 3.0.1, across multiple runs,
 I get a stable read rate of 632,000 (0.632 mil) rows per second

The represents a *44% loss in performance*. Which is, a lot.

I have been working though the spark-sql-kafka-0-10 code base, but change for spark 3 have been ongoing for over a year and its difficult to pin point an exact change or reason for the degradation.

I am happy to help fix this problem, but will need some assitance as I am unfamiliar with the spark-sql-kafka-0-10 project.

 

A sample of the data my test reads (note: its not parsing csv - this is just test data)
 1606921800000,001e0610e532,lightsense,tsl250rd,intensity,21853,53.262,acceleration_z,651,ep,290,commit,913,pressure,138,pm1,799,uv_intensity,823,idletime,-372,count,-72,ir_intensity,185,concentration,-61,flags,-532,tx,694.36,ep_heatsink,-556.92,acceleration_x,-221.40,fw,910.53,sample_flow_rate,-959.60,uptime,-515.15,pm10,-768.03,powersupply,214.72,magnetic_field_y,-616.04,alphasense,606.73,AoT_Chicago,053,Racine Ave & 18th St Chicago IL,41.857959,-87.65642700000002,AoT Chicago (S) [C],2017/12/15 00:00:00,","A simple 5 node system. A simple data row of csv data in kafka, evenly distributed between the partitions.

Open JDK 1.8.0.252

Spark in stand alone - 5 nodes, 10 workers (2 worker per node, each locked to a distinct NUMA group)
kafka (v 2.3.1) cluster - 5 nodes (1 broker per node).
Centos 7.7.1908

1 topic, 10 partiions, 1 hour queue life

(this is just one of clusters we have, I have tested on all of them and theyall exhibit the same performance degredation)",apachespark,david.wyles,dongjoon,gsomogyi,kabhwan,LuciferYang,yukihito,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 06 15:23:15 UTC 2021,,,,,,,,,,"0|z0l4yo:",9223372036854775807,,,,,,,,,,,,,3.0.2,3.1.0,,,,,,,,,"07/Dec/20 15:32;srowen;I don't think this is actionable by others unless you can help narrow down why this changed, and why it's attributable to Spark.;;;","07/Dec/20 15:52;david.wyles;Fair point, the library I suspect is spark-sql-kafka-0-10, is that covered on these issues.;;;","07/Dec/20 16:00;david.wyles;Is anyone even able to confirm my results are not just unique to me?;;;","07/Dec/20 16:30;david.wyles;Apart from all the cached consumer changes and other things on kafka-010-sql there was the kafka client change, it went from 2.0.0 to 2.4.1.

This I can test, using just kafka libraries. I'll give it a go and see what the outcome is.;;;","09/Dec/20 10:33;david.wyles;Having performed my tests I can conclude that the kafka client versions behave reasonable identically.

I ran my test on a local partition, a single thread reading just one partition of the same data.

But the curious thing was that It would max out around 660,000 rows per second which is much more in line with the row rate provided by Spark 3.0.0/1 (of 632,000 per second)

So that leads to believe that 2.4.5 was not single threaded (per partition), as the time and numbers I measured before were from the driver - so they are correct.

Not once on any of the reads directly from kafka on this test system did I get anywhere near the 1.1 mil rows/second.

I no longer believe this is a true regression in performance, I now think that 2.4.5 was ""cheating"".

Should you wish to close this as not a bug, then I'm happy for that to happen - but I would like your thoughts on how 2.4.5 was cheating.;;;","09/Dec/20 15:58;gsomogyi;[~david.wyles] try to turn off Kafka consumer caching. Apart from that there were no super significant changes which could cause this.

I've taken a look at your application and it does groupby and stuff like that. This is not related to Kafka read performance since Spark SQL engine contains huge amount of changes.
I suggest to create an application which just moves simple data from one topic into another and please use the exact same broker version.
If it's still slow we can measure further things.
;;;","09/Dec/20 16:00;gsomogyi;BTW, I'm sure you know but using collect gathers all the data on the driver side which is not really suggested under any circumstances.;;;","09/Dec/20 16:03;gsomogyi;Since you're measuring speed I've ported the Kafka source from DSv1 to DSv2. DSv1 is the default but the DSv2 can be tried out by setting ""spark.sql.sources.useV1SourceList"" properly. If you can try it out I would appreciate it.;;;","09/Dec/20 16:11;gsomogyi;I've changed to SQL because you're not executing a Structured Streaming query but an SQL batch.;;;","09/Dec/20 16:16;gsomogyi;{quote}I no longer believe this is a true regression in performance, I now think that 2.4.5 was ""cheating"".
{quote}
If you mean by cheating Spark uses one consumer from multiple threads then the answer is no. Kafka consumer is strictly forbidden to use from multiple threads.
 If such thing happens then Kafka realizes it and exception will be thrown which will stop the query immediately.;;;","11/Dec/20 12:10;david.wyles;I'll give all those a go and get back to you.

 

The collect in this test case is only 13 items of data after the group by - so I know thats not going to impact it.

But I can modify it to just read and write to kafka.;;;","11/Dec/20 12:13;david.wyles;""Since you're measuring speed I've ported the Kafka source from DSv1 to DSv2. DSv1 is the default but the DSv2 can be tried out by setting ""spark.sql.sources.useV1SourceList"" properly. If you can try it out I would appreciate it.""

Is that availble already in the 3.0.1 build, or do I need to pull and build it myself?;;;","11/Dec/20 12:14;david.wyles;[~gsomogyi] 
""try to turn off Kafka consumer caching. Apart from that there were no super significant changes which could cause this.""

Whats the option for that?

 ;;;","13/Dec/20 10:08;gsomogyi;Mixed up with DStreams, in Strutured Streaming and SQL there is no turn off flag.;;;","13/Dec/20 10:13;gsomogyi;{quote}The collect in this test case is only 13 items of data after the group by - so I know thats not going to impact it.
 But I can modify it to just read and write to kafka.
{quote}
Yeah, we need to reduce the use-case to the most minimal app to measure only what we need. Aggregations and all those stuff don't belong to Kafka read and write performance.;;;","18/Dec/20 17:34;david.wyles;[~gsomogyi] Just so you know, I'm still doing this.

I've simplied the test case to kafka to kafka, also tried with v2 sources (by removing kafka from the v1 sources list).

On 3.0.1 My data rate is still on the order of 650 - 700k rows (10 partitions, 700 million rows, 1 core per executors - of which there are 10), there is no noticable change when using v1 or v2 sources (v2 may have been every so slightly faster, but I'd need a lot more test runs to make that a concrete fact, faster by maybe 4%).

I'll get back to you next week when I've run this on 2.4.5

Remember, based on all my testing, and raw kafka reads on my system - the 3.0.1 spark is performing in line with expectations. The odd one here is 2.4.5, and maybe when I run it with these test setup we will get numbers more in line with kafka consumer behaviour.;;;","19/Dec/20 10:20;gsomogyi;{quote}Remember, based on all my testing, and raw kafka reads on my system - the 3.0.1 spark is performing in line with expectations.{quote}
Good to hear. You don't have to hurry since I'm on vacation this year unless a breaking issue appears in the upcoming Spark release.
;;;","29/Dec/20 16:26;david.wyles;[~gsomogyi] I now have my results.
 I was so unhappy about these results I ran all the tests again, the only thing that changed between them is the version of spark running on the cluster, everything else was static - the data input from kafka was an unchanging static set of data.

Input-> *672733262* rows

+*Spark 2.4.5*:+

*440* seconds - *1,528,939* rows per second.

+*Spark 3.0.1*:+

*990* seconds - *679,528* rows per seconds.

These are multiple runs (I even took the best from spark 3.0.1)

I also captured the event logs between these two versions of spark - should anyone find them useful.

[event logs|https://drive.google.com/drive/folders/1aElmzVWmJqRALQimdOYxdJu559_3EX_9?usp=sharing]

So, no matter what I do, I can only conclude that Spark 2.4.5 was a lot faster in this test case.

Is Spark SQL reading the source data twice, just as it would if there was a ""order by"" in the query?

Sample code used:

val spark =
   SparkSession.builder.appName(""Kafka Read Performance"")
     .config(""spark.executor.memory"",""16g"")
     .config(""spark.cores.max"", ""10"")
     .config(""spark.eventLog.enabled"",""true"")
     .config(""spark.eventLog.dir"",""file:///tmp/spark-events"")
     .config(""spark.eventLog.overwrite"",""true"")
    .getOrCreate()

import spark.implicits._

val *startTime* = System.nanoTime()

val df = 
   spark
     .read
     .format(""kafka"")
     .option(""kafka.bootstrap.servers"", config.brokers)
     .option(""subscribe"", config.inTopic)
     .option(""startingOffsets"", ""earliest"")
     .option(""endingOffsets"", ""latest"")
     .option(""failOnDataLoss"",""false"")
     .load()

df
   .write
   .format(""kafka"")
   .option(""kafka.bootstrap.servers"", config.brokers)
   .option(""topic"", config.outTopic)
   .mode(SaveMode.Append)
   .save()

val *endTime* = System.nanoTime()

val elapsedSecs = (endTime - startTime) / 1E9

// static input sample was used, fixed row count.

println(s""Took $elapsedSecs secs"")
 spark.stop()

 ;;;","04/Jan/21 05:09;yukihito;[~david.wyles], I tried your sample code in my local dev environment. 

Kafka: 2.7.0

both the input topic and the output topic have a single partition

 

There are fewer messages in the test topics. However, the read speed of Spark3 is still noticeably slower. In my case, Spark2.4 took around 7s while Spark3.1 took 11s. I have tested a few times, the results are quite consistent.  

 ;;;","06/Jan/21 04:17;kabhwan;I've spent some time to trace the issue, and noticed SPARK-29054 (+SPARK-30495) caused performance regression (though the patch itself is doing the right thing).

{code}
  private[kafka010] def getOrRetrieveConsumer(): InternalKafkaConsumer = {
    if (!_consumer.isDefined) {
      retrieveConsumer()
    }
    require(_consumer.isDefined, ""Consumer must be defined"")
    if (KafkaTokenUtil.needTokenUpdate(SparkEnv.get.conf, _consumer.get.kafkaParamsWithSecurity,
        _consumer.get.clusterConfig)) {
      logDebug(""Cached consumer uses an old delegation token, invalidating."")
      releaseConsumer()
      consumerPool.invalidateKey(cacheKey)
      fetchedDataPool.invalidate(cacheKey)
      retrieveConsumer()
    }
    _consumer.get
  }
{code}

{code}
  def needTokenUpdate(
      sparkConf: SparkConf,
      params: ju.Map[String, Object],
      clusterConfig: Option[KafkaTokenClusterConf]): Boolean = {
    if (HadoopDelegationTokenManager.isServiceEnabled(sparkConf, ""kafka"") &&
        clusterConfig.isDefined && params.containsKey(SaslConfigs.SASL_JAAS_CONFIG)) {
      logDebug(""Delegation token used by connector, checking if uses the latest token."")
      val connectorJaasParams = params.get(SaslConfigs.SASL_JAAS_CONFIG).asInstanceOf[String]
      getTokenJaasParams(clusterConfig.get) != connectorJaasParams
    } else {
      false
    }
  }
{code}

{code}
  def isServiceEnabled(sparkConf: SparkConf, serviceName: String): Boolean = {
    val key = providerEnabledConfig.format(serviceName)

    deprecatedProviderEnabledConfigs.foreach { pattern =>
      val deprecatedKey = pattern.format(serviceName)
      if (sparkConf.contains(deprecatedKey)) {
        logWarning(s""${deprecatedKey} is deprecated.  Please use ${key} instead."")
      }
    }

    val isEnabledDeprecated = deprecatedProviderEnabledConfigs.forall { pattern =>
      sparkConf
        .getOption(pattern.format(serviceName))
        .map(_.toBoolean)
        .getOrElse(true)
    }

    sparkConf
      .getOption(key)
      .map(_.toBoolean)
      .getOrElse(isEnabledDeprecated)
  }
{code}

With my test data and default config, Spark pulled 500 records per a poll from Kafka, which ended up ""10,280,000"" calls to get() which always calls getOrRetrieveConsumer(). A single call of KafkaTokenUtil.needTokenUpdate() wouldn't add significant overhead, but 10,000,000 calls make a significant difference. Assuming the case where delegation token is not applied, HadoopDelegationTokenManager.isServiceEnabled is the culprit on such huge overhead.

We could probably resolve the issue via short-term solution & long-term solution.

* short-term solution: change the order of check in needTokenUpdate, so that the performance hit is only affected when using delegation token. I'll raise a PR shortly.
* long-term solution(s): 1) optimize HadoopDelegationTokenManager.isServiceEnabled 2) find a way to reduce the occurrence of checking necessarily of token update.

Note that even with short-term solution, a slight performance hit is observed as it still does more things on the code path compared to Spark 2.4. Though I'd ignore it if it affects slightly, like less than 1%, or even slightly higher but the code addition is mandatory.;;;","06/Jan/21 04:40;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/31056;;;","06/Jan/21 06:00;dongjoon;Issue resolved by pull request 31056
[https://github.com/apache/spark/pull/31056];;;","06/Jan/21 06:29;kabhwan;One more point, though the root cause is actually the changes on ""Kafka"" - you'll find the huge difference according to the size of the log file between Spark 2.4 vs 3.0.

Some ""debug"" log messages in Kafka 2.0 (which Spark 2.4 uses) were re-labeled to the ""info"" log messages in later version of Kafka (at least including Kafka 2.4 which Spark 3.0 uses). I found the changes in KafkaConsumer.seek(), and there could be more.

If you feel these messages are flooding and likely affecting the performance (whereas it would be unlikely), you can change your log4j configuration to suppress it.

log4j.logger.org.apache.kafka.clients.consumer.KafkaConsumer=WARN;;;","06/Jan/21 15:23;david.wyles;Thanks a lot for getting down to the route cause and providing a fix so quickly so it will be in the next Spark releases.

Excellent.;;;",,
Clean up `spark.core.connection.ack.wait.timeout` from `configuration.md`,SPARK-33631,13343740,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,02/Dec/20 11:07,15/Nov/21 18:26,13/Jul/23 08:50,02/Dec/20 21:02,2.0.2,2.1.3,2.2.3,2.3.4,2.4.7,3.0.1,,,,,,2.4.8,3.0.2,3.1.0,,Documentation,,,,0,,,,"After SPARK-9767, Spark remove ConnectionManager and related classes, the configuration item `spark.core.connection.ack.wait.timeout` previously used by ConnectionManager is no longer used by other Spark code, but it still exists in the `configuration.md`.",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 02 21:02:43 UTC 2020,,,,,,,,,,"0|z0l4g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/20 11:29;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30569;;;","02/Dec/20 11:30;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30569;;;","02/Dec/20 21:02;dongjoon;Issue resolved by pull request 30569
[https://github.com/apache/spark/pull/30569];;;",,,,,,,,,,,,,,,,,,,,,,,
spark.buffer.size not applied in driver from pyspark,SPARK-33629,13343719,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,gsomogyi,gsomogyi,02/Dec/20 09:15,12/Dec/22 18:10,13/Jul/23 08:50,03/Dec/20 16:40,3.1.0,,,,,,,,,,,3.0.2,3.1.0,,,PySpark,,,,0,,,,"The problem has been discovered here: [https://github.com/apache/spark/pull/30389#issuecomment-729524618]

 ",,apachespark,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 16:40:45 UTC 2020,,,,,,,,,,"0|z0l4bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/20 09:16;gsomogyi;I've started to work on this and going to file a PR soon.;;;","03/Dec/20 04:09;gurwls223;Thank you [~gsomogyi].;;;","03/Dec/20 12:47;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/30592;;;","03/Dec/20 16:40;gurwls223;Issue resolved by pull request 30592
[https://github.com/apache/spark/pull/30592];;;",,,,,,,,,,,,,,,,,,,,,,
GetMapValueUtil code generation error,SPARK-33619,13343507,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,leanken,leanken,leanken,01/Dec/20 09:23,04/Jan/21 23:20,13/Jul/23 08:50,02/Dec/20 16:11,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"```

In -SPARK-33460---

there is a bug as follow.

GetMapValueUtil generated code error when ANSI mode is on

 

s""""""throw new NoSuchElementException(""Key "" + $eval2 + "" does not exist."");""""""

should be 

s""""""throw new java.util.NoSuchElementException(""Key "" + $eval2 + "" does not exist."");""""""

 

But Why are

checkExceptionInExpression[Exception](expr, errMsg)

and sql/testOnly org.apache.spark.sql.SQLQueryTestSuite – -z ansi/map.sql

can't detect this Bug

 

it's because 

1. checkExceptionInExpression is some what error, too. it should wrap with 

withSQLConf(SQLConf.CODEGEN_FACTORY_MODE.key like CheckEvalulation, AND WE SHOULD FIX this later.

2. SQLQueryTestSuite ansi/map.sql failed to detect because of the ConstantFolding rules, it is calling eval instead of code gen  in this case

 

```",,apachespark,cloud_fan,leanken,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33948,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 02 16:11:11 UTC 2020,,,,,,,,,,"0|z0l30g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/20 09:35;apachespark;User 'leanken' has created a pull request for this issue:
https://github.com/apache/spark/pull/30560;;;","02/Dec/20 16:11;cloud_fan;Issue resolved by pull request 30560
[https://github.com/apache/spark/pull/30560];;;",,,,,,,,,,,,,,,,,,,,,,,,
hadoop-aws doesn't work,SPARK-33618,13343497,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,01/Dec/20 08:33,12/Dec/22 18:10,13/Jul/23 08:50,02/Dec/20 09:24,3.1.0,,,,,,,,,,,3.1.0,,,,Spark Core,,,,0,,,,"According to [HADOOP-16080](https://issues.apache.org/jira/browse/HADOOP-16080) since Apache Hadoop 3.1.1, `hadoop-aws` doesn't work with `hadoop-client-api`. In other words, the regression is that `dev/make-distribution.sh -Phadoop-cloud ...` doesn't make a complete distribution for cloud support. It fails at write operation like the following.

{code}
$ bin/spark-shell --conf spark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID --conf spark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY
20/11/30 23:01:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context available as 'sc' (master = local[*], app id = local-1606806088715).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.0-SNAPSHOT
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_272)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.read.parquet(""s3a://dongjoon/users.parquet"").show
20/11/30 23:01:34 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
+------+--------------+----------------+
|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+
|Alyssa|          null|  [3, 9, 15, 20]|
|   Ben|           red|              []|
+------+--------------+----------------+


scala> Seq(1).toDF.write.parquet(""s3a://dongjoon/out.parquet"")
20/11/30 23:02:14 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)/ 1]
java.lang.NoSuchMethodError: org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(Lcom/google/common/util/concurrent/ListeningExecutorService;IZ)V
{code}",,apachespark,dishka_krauch,dongjoon,xkrogen,,,,,,,,,,,,,,,,,,,,,,,SPARK-33212,HADOOP-16080,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 02 09:24:46 UTC 2020,,,,,,,,,,"0|z0l2y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/20 08:37;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30508;;;","02/Dec/20 09:24;gurwls223;Issue resolved by pull request 30508
[https://github.com/apache/spark/pull/30508];;;",,,,,,,,,,,,,,,,,,,,,,,,
Decode Query parameters of the redirect URL for reverse proxy,SPARK-33611,13343332,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,30/Nov/20 14:57,01/Dec/20 17:37,13/Jul/23 08:50,01/Dec/20 17:37,3.0.0,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,Web UI,,,,0,,,,"When running Spark with reverse proxy enabled, the query parameter of the request URL can be encoded twice:  one from the browser and another one from the reverse proxy(e.g. Nginx).  

In Spark's stage page, the URL of ""/taskTable"" contains query parameter order[0][dir].  After encoding twice,  the query parameter becomes `order%255B0%255D%255Bdir%255D` and it will be decoded as `order%5B0%5D%5Bdir%5D` instead of  `order[0][dir]`.  As a result, there will be NullPointerException from https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala#L176

Other than that, the other parameter may not work as expected after encoded twice.

We should decode the query parameters and fix the problem",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 01 17:37:41 UTC 2020,,,,,,,,,,"0|z0l1xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/20 15:08;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30552;;;","30/Nov/20 15:09;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30552;;;","01/Dec/20 17:37;Gengliang.Wang;Issue resolved by pull request 30552
[https://github.com/apache/spark/pull/30552];;;",,,,,,,,,,,,,,,,,,,,,,,
Input Rate timeline/histogram aren't rendered if built with Scala 2.13,SPARK-33607,13343295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,30/Nov/20 12:06,01/Dec/20 02:45,13/Jul/23 08:50,01/Dec/20 02:45,3.1.0,,,,,,,,,,,3.1.0,,,,Structured Streaming,Web UI,,,0,,,,"If Spark is built with Scala 2.13, the timeline and histogram for input rate wouldn't be rendered in the Streaming Query Statistics page.",,apachespark,kabhwan,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 01 02:45:54 UTC 2020,,,,,,,,,,"0|z0l1pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/20 13:00;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30546;;;","30/Nov/20 13:01;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30546;;;","01/Dec/20 02:45;kabhwan;Issue resolved by pull request 30546
[https://github.com/apache/spark/pull/30546];;;",,,,,,,,,,,,,,,,,,,,,,,
Vector reader got incorrect data with binary partition value,SPARK-33593,13343207,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,30/Nov/20 03:06,21/Dec/20 11:28,13/Jul/23 08:50,18/Dec/20 08:03,2.0.2,2.1.3,2.2.3,2.3.4,2.4.7,3.0.1,3.1.0,3.2.0,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,correctness,,,"{code:java}
test(""Parquet vector reader incorrect with binary partition value"") {
  Seq(false, true).foreach(tag => {
    withSQLConf(""spark.sql.parquet.enableVectorizedReader"" -> tag.toString) {
      withTable(""t1"") {
        sql(
          """"""CREATE TABLE t1(name STRING, id BINARY, part BINARY)
            | USING PARQUET PARTITIONED BY (part)"""""".stripMargin)
        sql(s""INSERT INTO t1 PARTITION(part = 'Spark SQL') VALUES('a', X'537061726B2053514C')"")
        if (tag) {
          checkAnswer(sql(""SELECT name, cast(id as string), cast(part as string) FROM t1""),
            Row(""a"", ""Spark SQL"", """"))
        } else {
          checkAnswer(sql(""SELECT name, cast(id as string), cast(part as string) FROM t1""),
            Row(""a"", ""Spark SQL"", ""Spark SQL""))
        }
      }
    }
  })
}
{code}",,angerszhuuu,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 18 10:28:29 UTC 2020,,,,,,,,,,"0|z0l15s:",9223372036854775807,,,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,,,,,,,"30/Nov/20 03:06;angerszhuuu;raise a pr soon;;;","17/Dec/20 15:17;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30824;;;","18/Dec/20 04:36;dongjoon;Although this is not a regression, I marked this as a Blocker because this is a correctness issue.

cc [~hyukjin.kwon] and [~cloud_fan];;;","18/Dec/20 08:03;dongjoon;Issue resolved by pull request 30824
[https://github.com/apache/spark/pull/30824];;;","18/Dec/20 10:11;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30839;;;","18/Dec/20 10:12;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30839;;;","18/Dec/20 10:27;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30840;;;","18/Dec/20 10:28;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30840;;;",,,,,,,,,,,,,,,,,,
"NULL is recognized as the ""null"" string in partition specs",SPARK-33591,13343189,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,29/Nov/20 19:27,02/Feb/21 16:13,13/Jul/23 08:50,08/Jan/21 14:14,3.1.0,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,SQL,,,,0,correctness,,,"For example:
{code:sql}
spark-sql> CREATE TABLE tbl5 (col1 INT, p1 STRING) USING PARQUET PARTITIONED BY (p1);
spark-sql> INSERT INTO TABLE tbl5 PARTITION (p1 = null) SELECT 0;
spark-sql> SELECT isnull(p1) FROM tbl5;
false
{code}

The *p1 = null* is not recognized as a partition with NULL value.",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 16:13:32 UTC 2021,,,,,,,,,,"0|z0l11s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/20 20:01;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30538;;;","08/Jan/21 14:14;cloud_fan;Issue resolved by pull request 30538
[https://github.com/apache/spark/pull/30538];;;","08/Jan/21 15:20;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31094;;;","08/Jan/21 15:21;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31094;;;","08/Jan/21 15:52;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31095;;;","08/Jan/21 15:52;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31095;;;","01/Feb/21 17:30;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31421;;;","02/Feb/21 07:35;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31434;;;","02/Feb/21 07:36;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31434;;;","02/Feb/21 15:23;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31439;;;","02/Feb/21 15:23;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31439;;;","02/Feb/21 16:12;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31441;;;","02/Feb/21 16:13;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31441;;;",,,,,,,,,,,,,
Missing submenus for Performance Tuning in Spark SQL Guide ,SPARK-33590,13343172,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,kiszk,kiszk,29/Nov/20 15:05,15/Nov/21 18:26,13/Jul/23 08:50,29/Nov/20 19:25,3.0.0,3.0.1,,,,,,,,,,3.1.0,,,,Documentation,,,,0,,,,"Sub-menus for \{Coalesce Hints for SQL Queries} and {Adaptive Query Execution) are missing 

!image-2020-11-30-00-04-07-969.png!",,apachespark,dongjoon,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/20 15:05;kiszk;image-2020-11-30-00-04-07-969.png;https://issues.apache.org/jira/secure/attachment/13016170/image-2020-11-30-00-04-07-969.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 29 19:25:34 UTC 2020,,,,,,,,,,"0|z0l0y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/20 15:47;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30537;;;","29/Nov/20 15:48;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30537;;;","29/Nov/20 19:25;dongjoon;Issue resolved by pull request 30537
[https://github.com/apache/spark/pull/30537];;;",,,,,,,,,,,,,,,,,,,,,,,
Close opened session if the initialization fails,SPARK-33589,13343168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,29/Nov/20 14:11,12/Dec/22 18:10,13/Jul/23 08:50,30/Nov/20 02:21,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,,,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 12 08:11:54 UTC 2020,,,,,,,,,,"0|z0l0xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/20 14:19;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/30536;;;","29/Nov/20 14:20;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/30536;;;","30/Nov/20 02:21;gurwls223;Issue resolved by pull request 30536
[https://github.com/apache/spark/pull/30536];;;","12/Dec/20 08:11;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/30744;;;","12/Dec/20 08:11;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/30744;;;",,,,,,,,,,,,,,,,,,,,,
Partition spec in SHOW TABLE EXTENDED doesn't respect `spark.sql.caseSensitive`,SPARK-33588,13343133,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,28/Nov/20 20:27,30/Nov/20 16:40,13/Jul/23 08:50,29/Nov/20 20:12,2.4.7,3.0.1,3.1.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"For example:
{code:sql}
spark-sql> CREATE TABLE tbl1 (price int, qty int, year int, month int)
         > USING parquet
         > partitioned by (year, month);
spark-sql> INSERT INTO tbl1 PARTITION(year = 2015, month = 1) SELECT 1, 1;
spark-sql> SHOW TABLE EXTENDED LIKE 'tbl1' PARTITION(YEAR = 2015, Month = 1);
Error in query: Partition spec is invalid. The spec (YEAR, Month) must match the partition spec (year, month) defined in table '`default`.`tbl1`';
{code}
The spark.sql.caseSensitive flag is false by default, so, the partition spec is valid.",,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 30 14:16:59 UTC 2020,,,,,,,,,,"0|z0l0pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/20 20:42;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30529;;;","29/Nov/20 20:12;dongjoon;This is resolved via https://github.com/apache/spark/pull/30529;;;","30/Nov/20 13:44;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30549;;;","30/Nov/20 13:44;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30549;;;","30/Nov/20 14:16;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30551;;;",,,,,,,,,,,,,,,,,,,,,
resolveDependencyPaths should use classifier attribute of artifact,SPARK-33580,13343064,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,28/Nov/20 04:27,28/Nov/20 20:48,13/Jul/23 08:50,28/Nov/20 20:48,3.1.0,,,,,,,,,,,3.1.0,,,,Spark Core,,,,0,,,,"`resolveDependencyPaths` now takes artifact type to decide to add ""-tests"" postfix. However, the path pattern of ivy in `resolveMavenCoordinates` is ""[organization]_[artifact]-[revision](-[classifier]).[ext]"". We should use classifier instead of type to construct file path.",,apachespark,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 28 20:48:09 UTC 2020,,,,,,,,,,"0|z0l0a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/20 04:36;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30524;;;","28/Nov/20 04:37;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30524;;;","28/Nov/20 20:48;dongjoon;Issue resolved by pull request 30524
[https://github.com/apache/spark/pull/30524];;;",,,,,,,,,,,,,,,,,,,,,,,
Executors blank page behind proxy,SPARK-33579,13343026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pgillet,pgillet,pgillet,27/Nov/20 16:46,30/Nov/20 10:37,13/Jul/23 08:50,30/Nov/20 10:37,3.0.1,,,,,,,,,,,3.0.2,3.1.0,,,Spark Core,,,,0,core,ui,,"When accessing the Web UI behind a proxy (e.g. a Kubernetes ingress), executors page is blank.

In {{/core/src/main/resources/org/apache/spark/ui/static/utils.js}}, we  should avoid the use of location.origin when constructing URLs for internal API calls within the JavaScript.
 Instead, we should use {{apiRoot}} global variable.

On one hand, it would allow to build relative URLs. On the other hand, {{apiRoot}} reflects the Spark property {{spark.ui.proxyBase}} which can be set to change the root path of the Web UI.

If {{spark.ui.proxyBase}} is actually set, original URLs become incorrect, and we end up with an executors blank page.
 I encounter this bug when accessing the Web UI behind a proxy (in my case a Kubernetes Ingress).

 

See also [https://github.com/jupyterhub/jupyter-server-proxy/issues/57#issuecomment-699163115]",Spark 3.0.1 on Kubernetes,apachespark,pgillet,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 30 10:37:28 UTC 2020,,,,,,,,,,"0|z0l01s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/20 16:52;apachespark;User 'pgillet' has created a pull request for this issue:
https://github.com/apache/spark/pull/30523;;;","27/Nov/20 16:52;apachespark;User 'pgillet' has created a pull request for this issue:
https://github.com/apache/spark/pull/30523;;;","30/Nov/20 10:37;sarutak;Issue resolved by pull request 30523.
https://github.com/apache/spark/pull/30523;;;",,,,,,,,,,,,,,,,,,,,,,,
Handling of hybrid to proleptic calendar when reading and writing Parquet data not working correctly,SPARK-33571,13342812,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,simonvanderveldt,simonvanderveldt,26/Nov/20 10:54,12/Dec/22 18:10,13/Jul/23 08:50,04/Dec/20 07:28,3.0.0,3.0.1,,,,,,,,,,3.1.0,,,,PySpark,Spark Core,,,0,,,,"The handling of old dates written with older Spark versions (<2.4.6) using the hybrid calendar in Spark 3.0.0 and 3.0.1 seems to be broken/not working correctly.

From what I understand it should work like this:
 * Only relevant for `DateType` before 1582-10-15 or `TimestampType` before 1900-01-01T00:00:00Z
 * Only applies when reading or writing parquet files
 * When reading parquet files written with Spark < 2.4.6 which contain dates or timestamps before the above mentioned moments in time a `SparkUpgradeException` should be raised informing the user to choose either `LEGACY` or `CORRECTED` for the `datetimeRebaseModeInRead`
 * When reading parquet files written with Spark < 2.4.6 which contain dates or timestamps before the above mentioned moments in time and `datetimeRebaseModeInRead` is set to `LEGACY` the dates and timestamps should show the same values in Spark 3.0.1. with for example `df.show()` as they did in Spark 2.4.5
 * When reading parquet files written with Spark < 2.4.6 which contain dates or timestamps before the above mentioned moments in time and `datetimeRebaseModeInRead` is set to `CORRECTED` the dates and timestamps should show different values in Spark 3.0.1. with for example `df.show()` as they did in Spark 2.4.5
 * When writing parqet files with Spark > 3.0.0 which contain dates or timestamps before the above mentioned moment in time a `SparkUpgradeException` should be raised informing the user to choose either `LEGACY` or `CORRECTED` for the `datetimeRebaseModeInWrite`

First of all I'm not 100% sure all of this is correct. I've been unable to find any clear documentation on the expected behavior. The understanding I have was pieced together from the mailing list ([http://apache-spark-user-list.1001560.n3.nabble.com/Spark-3-0-1-new-Proleptic-Gregorian-calendar-td38914.html)] the blog post linked there and looking at the Spark code.

From our testing we're seeing several issues:
 * Reading parquet data with Spark 3.0.1 that was written with Spark 2.4.5. that contains fields of type `TimestampType` which contain timestamps before the above mentioned moments in time without `datetimeRebaseModeInRead` set doesn't raise the `SparkUpgradeException`, it succeeds without any changes to the resulting dataframe compared to that dataframe in Spark 2.4.5
 * Reading parquet data with Spark 3.0.1 that was written with Spark 2.4.5. that contains fields of type `TimestampType` or `DateType` which contain dates or timestamps before the above mentioned moments in time with `datetimeRebaseModeInRead` set to `LEGACY` results in the same values in the dataframe as when using `CORRECTED`, so it seems like no rebasing is happening.

I've made some scripts to help with testing/show the behavior, it uses pyspark 2.4.5, 2.4.6 and 3.0.1. You can find them here [https://github.com/simonvanderveldt/spark3-rebasemode-issue]. I'll post the outputs in a comment below as well.",,apachespark,maxgekk,simonvanderveldt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 14 10:51:20 UTC 2020,,,,,,,,,,"0|z0kyq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/20 11:03;simonvanderveldt;Below the output of the timestamp test script with the noise removed
*Writing:*
{code:java}
Spark version: 2.4.5
Spark conf [('spark.master', 'local[*]'), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'generate-timestamp-data'), ('spark.ui.showConsoleProgress', 'true')]
root
 |-- row: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

Writing parquet files to output/timestampspark245/
done
...
Spark version: 2.4.6
Spark conf [('spark.master', 'local[*]'), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'generate-timestamp-data'), ('spark.ui.showConsoleProgress', 'true')]
root
 |-- row: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

Writing parquet files to output/timestampspark246/
done
...
Spark version: 3.0.1
Spark conf [('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'generate-timestamp-data'), ('spark.ui.showConsoleProgress', 'true')]
root
 |-- row: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)

+---+-------------------+                                                       
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:10:10|
|  2|1880-10-01 10:10:10|
|  3|2020-10-01 10:10:10|
+---+-------------------+

Writing parquet files to output/timestampspark301/
done
{code}

Note no exception was raised when writing old timestamps to parquet in spark 3.0.1

*Reading*
{code:java}
Spark version: 3.0.1
Spark conf [('spark.app.name', 'read-data'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]
Reading parquet files from output/timestampspark245/*.parquet
root
 |-- row: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

done
...
Spark version: 3.0.1
Spark conf [('spark.app.name', 'read-data'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]
Reading parquet files from output/timestampspark246/*.parquet
root
 |-- row: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

done
...
Spark version: 3.0.1
Spark conf [('spark.app.name', 'read-data'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]
Reading parquet files from output/timestampspark301/*.parquet
root
 |-- row: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:10:10|
|  2|1880-10-01 10:10:10|
|  3|2020-10-01 10:10:10|
+---+-------------------+

done
{code}
Note no exception was raised when reading parquet files written with Spark 2.4.5 containing old timestamps

*Reading using the two different datetimeRebaseModeInRead modes*

{code:java}
Spark version: 3.0.1
Spark conf [('spark.app.name', 'read-data'), ('spark.sql.legacy.parquet.datetimeRebaseModeInRead', 'LEGACY'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]
Reading parquet files from output/timestampspark245/*.parquet
root
 |-- row: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

done
...
Spark version: 3.0.1
Spark conf [('spark.app.name', 'read-data'), ('spark.sql.legacy.parquet.datetimeRebaseModeInRead', 'CORRECTED'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]
Reading parquet files from output/timestampspark245/*.parquet
root
 |-- row: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

done
{code}
Note no difference in the timestamps shown;;;","26/Nov/20 11:16;simonvanderveldt;Below the output of the date testscript with the noise removed
Writing without additional config works as expected. Spark 3.0.1. throws a `SparkUpgradeException` when writing to parquet and the dataframe contains old dates.
Reading without additional config works as expected. Spark 3.0.1. throws a `SparkUpgradeException` when reading parquet files written with Spark 2.4.5 in Spark 3.0.1.

Reading using the two different `datetimeRebaseModeInRead` modes doesn't work though, it shows no difference

{code:java}
Spark version: 3.0.1
Spark conf [('spark.app.name', 'read-data'), ('spark.sql.legacy.parquet.datetimeRebaseModeInRead', 'LEGACY'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]
Reading parquet files from output/datespark245/*.parquet
root
 |-- row: string (nullable = true)
 |-- date: date (nullable = true)

+---+----------+
|row|      date|
+---+----------+
|  1|0220-10-01|
|  2|1880-10-01|
|  3|2020-10-01|
+---+----------+

done
...
Spark version: 3.0.1
Spark conf [('spark.app.name', 'read-data'), ('spark.sql.legacy.parquet.datetimeRebaseModeInRead', 'CORRECTED'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]
Reading parquet files from output/datespark245/*.parquet
root
 |-- row: string (nullable = true)
 |-- date: date (nullable = true)

+---+----------+
|row|      date|
+---+----------+
|  1|0220-10-01|
|  2|1880-10-01|
|  3|2020-10-01|
+---+----------+

done
{code}
Note no difference in the dates shown;;;","30/Nov/20 03:07;gurwls223;cc [~maxgekk] FYI;;;","01/Dec/20 08:27;maxgekk;[~simonvanderveldt] Thank you for the detailed description and your investigation. Let me clarify a few things:

> From our testing we're seeing several issues:
> Reading parquet data with Spark 3.0.1 that was written with Spark 2.4.5. that contains fields of type `TimestampType` which contain timestamps before the above mentioned moments in time without `datetimeRebaseModeInRead` set doesn't raise the `SparkUpgradeException`, it succeeds without any changes to the resulting dataframe compares to that dataframe in Spark 2.4.5

Spark 2.4.5 writes timestamps as parquet INT96 type. The SQL config `datetimeRebaseModeInRead` does not influence on reading such types in Spark 3.0.1, so, Spark performs rebasing always (LEGACY mode). We recently added separate configs for INT96:
* https://github.com/apache/spark/pull/30056
* https://github.com/apache/spark/pull/30121

The changes will be released with Spark 3.1.0.

> Reading parquet data with Spark 3.0.1 that was written with Spark 2.4.5. that contains fields of type `TimestampType` or `DateType` which contain dates or timestamps before the above mentioned moments in time with `datetimeRebaseModeInRead` set to `LEGACY` results in the same values in the dataframe as when using `CORRECTED`, so it seems like no rebasing is happening.

For INT96, it seems it is correct behavior. We should observe different results for TIMESTAMP_MICROS and TIMESTAMP_MILLIS types, see the SQL config spark.sql.parquet.outputTimestampType.

The DATE case is more interesting as we must see a difference in results for ancient dates. I will investigate this case. 

 ;;;","01/Dec/20 09:21;maxgekk;I have tried to reproduce the issue on the master branch by reading the file saved by Spark 2.4.5 (https://github.com/apache/spark/tree/master/sql/core/src/test/resources/test-data):
{code:scala}
  test(""SPARK-33571: read ancient dates saved by Spark 2.4.5"") {
    withSQLConf(SQLConf.LEGACY_PARQUET_REBASE_MODE_IN_READ.key -> LEGACY.toString) {
      val path = getResourceParquetFilePath(""test-data/before_1582_date_v2_4_5.snappy.parquet"")
      val df = spark.read.parquet(path)
      df.show(false)
    }
    withSQLConf(SQLConf.LEGACY_PARQUET_REBASE_MODE_IN_READ.key -> CORRECTED.toString) {
      val path = getResourceParquetFilePath(""test-data/before_1582_date_v2_4_5.snappy.parquet"")
      val df = spark.read.parquet(path)
      df.show(false)
    }
  }
{code}

The results are different in LEGACY and in CORRECTED modes:
{code}
+----------+----------+
|dict      |plain     |
+----------+----------+
|1001-01-01|1001-01-01|
|1001-01-01|1001-01-02|
|1001-01-01|1001-01-03|
|1001-01-01|1001-01-04|
|1001-01-01|1001-01-05|
|1001-01-01|1001-01-06|
|1001-01-01|1001-01-07|
|1001-01-01|1001-01-08|
+----------+----------+

+----------+----------+
|dict      |plain     |
+----------+----------+
|1001-01-07|1001-01-07|
|1001-01-07|1001-01-08|
|1001-01-07|1001-01-09|
|1001-01-07|1001-01-10|
|1001-01-07|1001-01-11|
|1001-01-07|1001-01-12|
|1001-01-07|1001-01-13|
|1001-01-07|1001-01-14|
+----------+----------+
{code};;;","01/Dec/20 09:35;maxgekk;Spark 3.0.1 shows different results as well:
{code:scala}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.1
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_275)
scala> spark.read.parquet(""/Users/maximgekk/proj/parquet-read-2_4_5_files/sql/core/src/test/resources/test-data/before_1582_date_v2_4_5.snappy.parquet"").show(false)
20/12/01 12:31:59 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.datetimeRebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.datetimeRebaseModeInRead to 'CORRECTED' to read the datetime values as it is.

scala> spark.conf.set(""spark.sql.legacy.parquet.datetimeRebaseModeInRead"", ""LEGACY"")

scala> spark.read.parquet(""/Users/maximgekk/proj/parquet-read-2_4_5_files/sql/core/src/test/resources/test-data/before_1582_date_v2_4_5.snappy.parquet"").show(false)
+----------+----------+
|dict      |plain     |
+----------+----------+
|1001-01-01|1001-01-01|
|1001-01-01|1001-01-02|
|1001-01-01|1001-01-03|
|1001-01-01|1001-01-04|
|1001-01-01|1001-01-05|
|1001-01-01|1001-01-06|
|1001-01-01|1001-01-07|
|1001-01-01|1001-01-08|
+----------+----------+


scala> spark.conf.set(""spark.sql.legacy.parquet.datetimeRebaseModeInRead"", ""CORRECTED"")

scala> spark.read.parquet(""/Users/maximgekk/proj/parquet-read-2_4_5_files/sql/core/src/test/resources/test-data/before_1582_date_v2_4_5.snappy.parquet"").show(false)
+----------+----------+
|dict      |plain     |
+----------+----------+
|1001-01-07|1001-01-07|
|1001-01-07|1001-01-08|
|1001-01-07|1001-01-09|
|1001-01-07|1001-01-10|
|1001-01-07|1001-01-11|
|1001-01-07|1001-01-12|
|1001-01-07|1001-01-13|
|1001-01-07|1001-01-14|
+----------+----------+

{code};;;","01/Dec/20 09:51;maxgekk;[~simonvanderveldt] Looking at the dates, you tested, both dates 1880-10-01 and 2020-10-01 belong to the Gregorian calendar, so, should be no diffs.

For the date 0220-10-01, please, have a look at the table which I built in the PR: https://github.com/apache/spark/pull/28067 . The table shows that there is no diffs between 2 calendars for the year.;;;","03/Dec/20 18:43;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30596;;;","03/Dec/20 18:45;maxgekk;I opened the PR [https://github.com/apache/spark/pull/30596] with some improvements for config docs. [~hyukjin.kwon] [~cloud_fan] could you review it, please.;;;","04/Dec/20 07:28;gurwls223;Fixed in https://github.com/apache/spark/pull/30596;;;","04/Dec/20 08:56;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30604;;;","08/Dec/20 16:12;simonvanderveldt;[~maxgekk] Thanks for taking the time to look into this, for the updates to the documentation and for the explanation! I think the docs with regards to dates are clear now.
The actual data I ran into this issue with used the year 220 so that's why I used it, of course that's the one century with a 0 day diff :P The table with the different diffs between the two calendars cleared it up a lot, I used some different dates and can now also see the differences between the two read modes.

If you don't mind I have two additional questions:
> Spark 2.4.5 writes timestamps as parquet INT96 type. The SQL config `datetimeRebaseModeInRead` does not influence on reading such types in Spark 3.0.1, so, Spark performs rebasing always (LEGACY mode). We recently added separate configs for INT96...


The behavior of the to be introduced in Spark 3.1 `spark.sql.legacy.parquet.int96RebaseModeIn*` is the same as for `datetimeRebaseModeIn*`? So Spark will check the parquet metadata for Spark version and the `datetimeRebaseModeInRead` metadata key and use the correct behavior. If those are not set it will raise an exception and ask the user to define the mode. Is that correct?

(P.S. You explicitly mention Spark 2.4.5 writes timestamps as INT96, but from my testing Spark 3 does the same by default, not sure if that aligns with your findings?)

> For INT96, it seems it is correct behavior. We should observe different results for TIMESTAMP_MICROS and TIMESTAMP_MILLIS types, see the SQL config spark.sql.parquet.outputTimestampType.

What is the expected behavior for TIMESTAMP_MICROS and TIMESTAMP_MILLIS with regards to this?;;;","09/Dec/20 17:48;maxgekk;> The behavior of the to be introduced in Spark 3.1 `spark.sql.legacy.parquet.int96RebaseModeIn*` is the same as for `datetimeRebaseModeIn*`?

Yes.

> So Spark will check the parquet metadata for Spark version and the `datetimeRebaseModeInRead` metadata key and use the correct behavior.

Correct, except of names of metadata keys. Spark checks , see https://github.com/MaxGekk/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L58-L68

> If those are not set it will raise an exception and ask the user to define the mode. Is that correct?

Yes. Spark should raise the exception if it is not clear which calendar the writer used.

> but from my testing Spark 3 does the same by default, not sure if that aligns with your findings?

Spark 3.0.0-SNAPSHOT saved timestamps as TIMESTAMP_MICROS in parquet till https://github.com/apache/spark/pull/28450 . I just wanted to say that the configs datetimeRebaseModeIn* you pointed out don't impact on INT96 in Spark 3.0.

> What is the expected behavior for TIMESTAMP_MICROS and TIMESTAMP_MILLIS with regards to this?

The same as for DATE type. Spark takes into account the same SQL configs and metdata keys from parquet files.

;;;","14/Dec/20 10:51;simonvanderveldt;[~maxgekk] OK, all clear. Thanks again for the clarifications!;;;",,,,,,,,,,,,
Set the proper version of gssapi plugin automatically for MariaDBKrbIntegrationSuite,SPARK-33570,13342794,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,26/Nov/20 09:18,28/Nov/20 15:42,13/Jul/23 08:50,28/Nov/20 15:40,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,Tests,,,0,,,,"For MariaDBKrbIntegrationSuite, the version of mariadb-plugin-gssapi-server is currently set to 10.5.5 in mariadb_docker_entrypoint.sh but it's no longer available in the official apt repository and MariaDBKrbIntegrationSuite doesn't pass for now.
It seems that only the most recent three versions are available and they are 10.5.6, 10.5.7 and 10.5.8 for now.
Further, the release cycle of MariaDB seems to be very rapid (1 ~ 2 months) so I don't think it's a good idea to set to an specific version for mariadb-plugin-gssapi-server.
",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 28 15:42:24 UTC 2020,,,,,,,,,,"0|z0kym8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/20 09:30;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30515;;;","28/Nov/20 15:42;sarutak;Issue resolved by pull request 30515

https://github.com/apache/spark/pull/30515;;;",,,,,,,,,,,,,,,,,,,,,,,,
install coverage for pypy3,SPARK-33568,13342752,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shaneknapp,shaneknapp,shaneknapp,26/Nov/20 04:38,26/Nov/20 19:30,13/Jul/23 08:50,26/Nov/20 19:30,3.0.0,,,,,,,,,,,,,,,Build,PySpark,,,0,,,,"from:

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-3.0-test-sbt-hadoop-2.7-hive-1.2/1002/console

 
Coverage is not installed in Python executable 'pypy3' but 'COVERAGE_PROCESS_START' environment variable is set, exiting.",,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 26 19:29:43 UTC 2020,,,,,,,,,,"0|z0kycw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/20 19:29;shaneknapp;this is now installed on the ubuntu 16 workers;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Incorrectly Parsing CSV file,SPARK-33566,13342720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,moresmores,moresmores,25/Nov/20 22:25,12/Dec/22 18:11,13/Jul/23 08:50,27/Nov/20 06:49,2.4.7,,,,,,,,,,,3.1.0,,,,Spark Core,,,,0,,,,"Here is a test case: 

[https://github.com/mores/maven-examples/blob/master/comma/src/test/java/org/test/CommaTest.java]

It shows how I believe apache commons csv and opencsv correctly parses the sample csv file.

spark is not correctly parsing the sample csv file.",,apachespark,LuciferYang,moresmores,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 27 06:49:30 UTC 2020,,,,,,,,,,"0|z0ky5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/20 03:07;gurwls223;[~moresmores] can you copy and past the output here? ;;;","26/Nov/20 03:41;moresmores;Here is the output from running mvn clean test:

Running org.test.CommaTest

{code}
2020-11-25 17:55:45,728 INFO [CommaTest:12]
OpenCsv
2020-11-25 17:55:45,758 INFO [CommaTest:19] h1 h3 h2
2020-11-25 17:55:45,758 INFO [CommaTest:19] one three two
2020-11-25 17:55:45,760 INFO [CommaTest:19] abc xyz ^@<b><i><span style=""font-family: tahoma,sans-serif;"">Referral from Joe Smith.<A0> Fred is hard working.<A0> Super smart, though you wouldn&#39;t know it at first.<A0> 6 months, and we sold this project.<A0> Phooey he said to me!<A0> What&#39;s up with you people.<A0> You&#39;ll say anything for a sale!<A0> Until he met me of course....haar haar!</span></i></b><br><A0><br><b><i><span style=""font-family: tahoma,sans-serif;"">Internet is spotty</span></i></b><br><b><i><span style=""font-family: tahoma,sans-serif;"">Working while at home so.<A0> Will be applied this weekend. <A0></span></i></b><br><A0><br><b><i><span style=""font-family: tahoma,sans-serif;"">On Bill Recovery and 20 yr warranty added.</span></i></b><br><A0><br><b><i><span style=""font-family: tahoma,sans-serif;"">Kindness made this deal happen!</span></i></b><br><A0>
2020-11-25 17:55:45,763 INFO [CommaTest:26]

spark
2020-11-25 17:55:46,464 WARN [NativeCodeLoader:62] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-25 17:55:55,299 INFO [CommaTest:36] Count: 2
2020-11-25 17:55:55,449 INFO [CommaTest:41] one three two
2020-11-25 17:55:55,449 INFO [CommaTest:41] abc sans-serif;"""">Referral from Joe Smith.<A0> Fred is hard working.<A0> Super smart ""^@<b><i><span style=""""font-family: tahoma
2020-11-25 17:55:55,450 INFO [CommaTest:48] 

commons
2020-11-25 17:55:55,465 INFO [CommaTest:57] one three two
2020-11-25 17:55:55,467 INFO [CommaTest:57] abc xyz ^@<b><i><span style=""font-family: tahoma,sans-serif;"">Referral from Joe Smith.<A0> Fred is hard working.<A0> Super smart, though you wouldn&#39;t know it at first.<A0> 6 months, and we sold this project.<A0> Phooey he said to me!<A0> What&#39;s up with you people.<A0> You&#39;ll say anything for a sale!<A0> Until he met me of course....haar haar!</span></i></b><br><A0><br><b><i><span style=""font-family: tahoma,sans-serif;"">Internet is spotty</span></i></b><br><b><i><span style=""font-family: tahoma,sans-serif;"">Working while at home so.<A0> Will be applied this weekend. <A0></span></i></b><br><A0><br><b><i><span style=""font-family: tahoma,sans-serif;"">On Bill Recovery and 20 yr warranty added.</span></i></b><br><A0><br><b><i><span style=""font-family: tahoma,sans-serif;"">Kindness made this deal happen!</span></i></b><br><A0>
{code}
;;;","26/Nov/20 13:11;LuciferYang;I think the reason for the bad case is Spark use ""STOP_AT_DELIMITER"" as default ""UnescapedQuoteHandling"" to build ""CsvParser"".  Configure ""UnescapedQuoteHandling"" to  ""STOP_AT_CLOSING_QUOTE"" seems can resolve this issue, but Spark not support configure this option now. [~hyukjin.kwon] [~moresmores];;;","26/Nov/20 14:52;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30518;;;","26/Nov/20 14:52;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30518;;;","27/Nov/20 06:49;gurwls223;Fixed in https://github.com/apache/spark/pull/30518;;;",,,,,,,,,,,,,,,,,,,,
python/run-tests.py calling python3.8,SPARK-33565,13342717,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shaneknapp,shaneknapp,shaneknapp,25/Nov/20 21:44,03/Jan/22 03:12,13/Jul/23 08:50,25/Nov/20 23:16,3.0.1,,,,,,,,,,,3.1.0,,,,Build,,,,0,,,,"this line in run-tests.py on master:
|python_execs = [x for x in [""python3.6"", ""python3.8"", ""pypy3""] if which(x)]|

 

and this line in branch-3.0:

python_execs = [x for x in [""python3.8"", ""python2.7"", ""pypy3"", ""pypy""] if which(x)]

...are currently breaking builds on the new ubuntu 20.04LTS workers.

the default  system python is /usr/bin/python3.8 and we do NOT have a working python3.8 anaconda deployment yet.  this is causing python test breakages.

PRs incoming

 ",,apachespark,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 03 03:12:04 UTC 2022,,,,,,,,,,"0|z0ky54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/20 21:50;apachespark;User 'shaneknapp' has created a pull request for this issue:
https://github.com/apache/spark/pull/30506;;;","25/Nov/20 21:50;apachespark;User 'shaneknapp' has created a pull request for this issue:
https://github.com/apache/spark/pull/30506;;;","25/Nov/20 21:59;apachespark;User 'shaneknapp' has created a pull request for this issue:
https://github.com/apache/spark/pull/30507;;;","25/Nov/20 22:00;apachespark;User 'shaneknapp' has created a pull request for this issue:
https://github.com/apache/spark/pull/30507;;;","25/Nov/20 23:16;shaneknapp;Issue resolved by pull request 30506
[https://github.com/apache/spark/pull/30506];;;","25/Nov/20 23:29;apachespark;User 'shaneknapp' has created a pull request for this issue:
https://github.com/apache/spark/pull/30509;;;","26/Nov/20 01:10;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30510;;;","26/Nov/20 01:11;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30510;;;","26/Nov/20 01:36;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30511;;;","26/Nov/20 01:37;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30511;;;","03/Jan/22 03:11;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35087;;;","03/Jan/22 03:12;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35087;;;",,,,,,,,,,,,,,
spark.storage.blockManagerSlaveTimeoutMs default value does not follow spark.network.timeout value when the latter was changed,SPARK-33557,13342588,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,ohad,ohad,25/Nov/20 11:12,12/Dec/22 18:10,13/Jul/23 08:50,02/Dec/20 09:43,3.0.0,3.0.1,,,,,,,,,,3.0.2,3.1.0,,,Spark Core,,,,0,,,,"According to the documentation ""spark.network.timeout"" is the default timeout for ""spark.storage.blockManagerSlaveTimeoutMs"" which implies that when the user sets ""spark.network.timeout""  the effective value of ""spark.storage.blockManagerSlaveTimeoutMs"" should also be changed if it was not specifically changed.

However this is not the case since the default value of ""spark.storage.blockManagerSlaveTimeoutMs"" is always the default value of ""spark.network.timeout"" (120s)

 

""spark.storage.blockManagerSlaveTimeoutMs"" is defined in the package object of ""org.apache.spark.internal.config"" as follows:
{code:java}
private[spark] val STORAGE_BLOCKMANAGER_SLAVE_TIMEOUT =
  ConfigBuilder(""spark.storage.blockManagerSlaveTimeoutMs"")
    .version(""0.7.0"")
    .timeConf(TimeUnit.MILLISECONDS)
    .createWithDefaultString(Network.NETWORK_TIMEOUT.defaultValueString)
{code}
So it seems like the its default value is indeed ""fixed"" to ""spark.network.timeout"" default value.

 

 

 

 

 ",,apachespark,LuciferYang,ohad,,,,,,,,,,,,,,,,,,,,,SPARK-32769,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 14 00:13:45 UTC 2021,,,,,,,,,,"0|z0kxcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/20 11:07;LuciferYang;It seems that changing value of ""spark.network.timeout"" doesn't really change the value of STORAGE_BLOCKMANAGER_HEARTBEAT_TIMEOUT and their relationship is maintained by code.

For example, the treatment of ""spark.shuffle.io.connectionTimeout"" is as follows:

 
{code:java}
/** Connect timeout in milliseconds. Default 120 secs. */
public int connectionTimeoutMs() {
  long defaultNetworkTimeoutS = JavaUtils.timeStringAsSec(
    conf.get(""spark.network.timeout"", ""120s""));
  long defaultTimeoutMs = JavaUtils.timeStringAsSec(
    conf.get(SPARK_NETWORK_IO_CONNECTIONTIMEOUT_KEY, defaultNetworkTimeoutS + ""s"")) * 1000;
  return (int) defaultTimeoutMs;
}
{code}
 

 

But it seems that there is no similar treatment forSTORAGE_BLOCKMANAGER_HEARTBEAT_TIMEOUT in HeartbeatReceiver and MesosCoarseGrainedSchedulerBackend

 
{code:java}
private val executorTimeoutMs = sc.conf.get(config.STORAGE_BLOCKMANAGER_HEARTBEAT_TIMEOUT)
{code}
 

 
{code:java}
mesosExternalShuffleClient.get
  .registerDriverWithShuffleService(
    agent.hostname,
    externalShufflePort,
    sc.conf.get(config.STORAGE_BLOCKMANAGER_HEARTBEAT_TIMEOUT),
    sc.conf.get(config.EXECUTOR_HEARTBEAT_INTERVAL))
{code}
 

Maybe need to be fixed by code changes.

 

 ;;;","27/Nov/20 12:00;LuciferYang; I'm not sure whether the configurations related to ""spark.network.timeout"" really meets the expected behavior. Needs to be investigated ~
 ;;;","30/Nov/20 13:13;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30547;;;","30/Nov/20 13:13;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30547;;;","02/Dec/20 09:43;gurwls223;Issue resolved by pull request 30547
[https://github.com/apache/spark/pull/30547];;;","14/Jan/21 00:13;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31175;;;","14/Jan/21 00:13;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31175;;;",,,,,,,,,,,,,,,,,,,
Do not use custom shuffle reader for repartition,SPARK-33551,13342526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,maryannxue,maryannxue,25/Nov/20 05:04,26/Nov/20 03:33,13/Jul/23 08:50,26/Nov/20 03:33,3.0.1,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"We should have a more thorough fix for all sorts of custom shuffle readers when the original query has a repartition shuffle, based on the discussions on the initial PR: [https://github.com/apache/spark/pull/29797].",,apachespark,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 05:13:43 UTC 2020,,,,,,,,,,"0|z0kwz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/20 05:13;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/30494;;;",,,,,,,,,,,,,,,,,,,,,,,,,
CREATE TABLE LIKE should resolve hive serde correctly like CREATE TABLE,SPARK-33546,13342488,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,25/Nov/20 01:25,14/Dec/20 08:27,13/Jul/23 08:50,14/Dec/20 08:27,2.4.0,3.0.0,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,"Currently there are several inconsistency:
 # CREATE TABLE LIKE does not validate the user-specified hive serde. e.g., STORED AS PARQUET can't be used with ROW FORMAT SERDE.
 # CREATE TABLE LIKE requires STORED AS and ROW FORMAT SERDE to be specified together, which is not necessary.
 # CREATE TABLE LIKE does not respect the default hive serde.",,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 14 08:27:34 UTC 2020,,,,,,,,,,"0|z0kwqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/20 10:18;apachespark;User 'leanken' has created a pull request for this issue:
https://github.com/apache/spark/pull/30705;;;","10/Dec/20 10:18;apachespark;User 'leanken' has created a pull request for this issue:
https://github.com/apache/spark/pull/30705;;;","14/Dec/20 08:27;cloud_fan;Issue resolved by pull request 30705
[https://github.com/apache/spark/pull/30705];;;",,,,,,,,,,,,,,,,,,,,,,,
explode should not filter when used with CreateArray,SPARK-33544,13342467,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,24/Nov/20 20:55,12/Dec/22 18:10,13/Jul/23 08:50,02/Dec/20 00:54,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"https://issues.apache.org/jira/browse/SPARK-32295 added in an optimization to insert a filter for not null and size > 0 when using inner explode/inline. This is fine in most cases but the extra filter is not needed if the explode is with a create array and not using Literals (it already handles LIterals).  When this happens you know that the values aren't null and it has a size.  It already handles the empty array.

for instance:

val df = someDF.selectExpr(""number"", ""explode(array(word, col3))"")

So in this case we shouldn't be inserting the extra Filter and that filter can get pushed down into like a parquet reader as well. This is just causing extra overhead.

 ",,apachespark,maropu,tgraves,viirya,,,,,,,,,,,,,,,,,,,SPARK-24913,,,,,,,,,,SPARK-32295,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 02 11:30:01 UTC 2020,,,,,,,,,,"0|z0kwm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/20 20:56;tgraves;I'm working on a patch for this.;;;","25/Nov/20 02:40;gurwls223;cc [~viirya] FYI;;;","25/Nov/20 02:54;viirya;Thanks [~hyukjin.kwon]. Will help review if [~tgraves] create a patch.;;;","25/Nov/20 19:29;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/30504;;;","02/Dec/20 00:54;gurwls223;Issue resolved by pull request 30504
[https://github.com/apache/spark/pull/30504];;;","02/Dec/20 11:30;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30570;;;",,,,,,,,,,,,,,,,,,,,
Incorrect join results when joining twice with the same DF,SPARK-33536,13342378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,24/Nov/20 14:19,20/May/21 03:27,13/Jul/23 08:50,02/Dec/20 17:51,3.0.0,3.0.1,3.1.0,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"{code:java}
val emp1 = Seq[TestData](
  TestData(1, ""sales""),
  TestData(2, ""personnel""),
  TestData(3, ""develop""),
  TestData(4, ""IT"")).toDS()
val emp2 = Seq[TestData](
  TestData(1, ""sales""),
  TestData(2, ""personnel""),
  TestData(3, ""develop"")).toDS()
val emp3 = emp1.join(emp2, emp1(""key"") === emp2(""key"")).select(emp1(""*""))
emp1.join(emp3, emp1.col(""key"") === emp3.col(""key""), ""left_outer"").select(emp1.col(""*""), emp3.col(""key"").as(""e2"")).show()

// wrong result
+---+---------+---+
|key|    value| e2|
+---+---------+---+
|  1|    sales|  1|
|  2|personnel|  2|
|  3|  develop|  3|
|  4|       IT|  4|
+---+---------+---+

{code}",,apachespark,cloud_fan,maropu,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-35454,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 08:34:12 UTC 2020,,,,,,,,,,"0|z0kw28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/20 14:34;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/30488;;;","24/Nov/20 14:35;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/30488;;;","02/Dec/20 17:51;cloud_fan;Issue resolved by pull request 30488
[https://github.com/apache/spark/pull/30488];;;","09/Dec/20 08:34;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30682;;;","09/Dec/20 08:34;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30682;;;",,,,,,,,,,,,,,,,,,,,,
BasicConnectionProvider should consider case-sensitivity for properties.,SPARK-33533,13342368,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sarutak,sarutak,sarutak,24/Nov/20 13:35,25/Nov/20 04:19,13/Jul/23 08:50,25/Nov/20 04:19,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"After SPARK-32001, BasicConnectionProvider doesn't consider case-sensitivity for properties.
Caused by this issue, OracleIntegrationSuite doesn't pass.",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32001,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 04:19:07 UTC 2020,,,,,,,,,,"0|z0kw00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/20 13:51;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30485;;;","25/Nov/20 04:19;dongjoon;Issue resolved by pull request 30485
[https://github.com/apache/spark/pull/30485];;;",,,,,,,,,,,,,,,,,,,,,,,,
Incorrect menu item display and link in PySpark Usage Guide for Pandas with Apache Arrow,SPARK-33517,13342056,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,liucht-inspur,liucht-inspur,liucht-inspur,23/Nov/20 06:33,12/Dec/22 18:10,13/Jul/23 08:50,30/Nov/20 01:03,3.0.0,3.0.1,,,,,,,,,,3.1.0,,,,Documentation,,,,0,,,,"Error setting menu item and link, change ""Apache Arrow in Spark"" to ""Apache Arrow in PySpark""

  !image-2020-11-23-18-47-01-591.png!

 

after:

!image-2020-11-27-09-43-58-141.png!

 ",,apachespark,liucht-inspur,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/20 10:47;liucht-inspur;image-2020-11-23-18-47-01-591.png;https://issues.apache.org/jira/secure/attachment/13015844/image-2020-11-23-18-47-01-591.png","27/Nov/20 01:43;liucht-inspur;image-2020-11-27-09-43-58-141.png;https://issues.apache.org/jira/secure/attachment/13016096/image-2020-11-27-09-43-58-141.png","23/Nov/20 06:35;liucht-inspur;spark-doc.jpg;https://issues.apache.org/jira/secure/attachment/13015831/spark-doc.jpg",,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 30 01:03:46 UTC 2020,,,,,,,,,,"0|z0ku3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/20 06:41;apachespark;User 'liucht-inspur' has created a pull request for this issue:
https://github.com/apache/spark/pull/30466;;;","30/Nov/20 01:03;gurwls223;Issue resolved by pull request 30466
[https://github.com/apache/spark/pull/30466];;;",,,,,,,,,,,,,,,,,,,,,,,,
The application log in the Spark history server contains sensitive attributes such as password that should be redated instead of plain text,SPARK-33504,13341811,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,echohlne,echohlne,echohlne,20/Nov/20 15:54,24/Feb/21 08:48,13/Jul/23 08:50,02/Dec/20 15:28,3.0.1,,,,,,,,,,,3.0.3,3.1.0,,,Spark Core,,,,0,,,,"We found the secure attributes in SparkListenerJobStart and SparkListenerStageSubmitted events would not been redated, resulting in sensitive attributes can be viewd directly.",Spark 3.0.1,apachespark,echohlne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/20 16:03;echohlne;SparkListenerEnvironmentUpdate log shows ok.png;https://issues.apache.org/jira/secure/attachment/13015731/SparkListenerEnvironmentUpdate+log+shows+ok.png","20/Nov/20 16:03;echohlne;SparkListenerStageSubmitted-log-wrong.png;https://issues.apache.org/jira/secure/attachment/13015732/SparkListenerStageSubmitted-log-wrong.png","20/Nov/20 16:03;echohlne;SparkListernerJobStart-wrong.png;https://issues.apache.org/jira/secure/attachment/13015733/SparkListernerJobStart-wrong.png",,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 24 04:40:54 UTC 2021,,,,,,,,,,"0|z0ksl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/20 16:13;apachespark;User 'akiyamaneko' has created a pull request for this issue:
https://github.com/apache/spark/pull/30446;;;","02/Dec/20 19:25;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/30576;;;","02/Dec/20 19:25;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/30576;;;","24/Feb/21 04:40;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31631;;;","24/Feb/21 04:40;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31631;;;",,,,,,,,,,,,,,,,,,,,,
Do not use local shuffle reader for repartition,SPARK-33494,13341657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,19/Nov/20 19:19,25/Nov/20 02:02,13/Jul/23 08:50,25/Nov/20 02:02,3.0.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 02:02:50 UTC 2020,,,,,,,,,,"0|z0krmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/20 19:27;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/30432;;;","25/Nov/20 02:02;cloud_fan;Issue resolved by pull request 30432
[https://github.com/apache/spark/pull/30432];;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix rat exclusion patterns and add a LICENSE,SPARK-33483,13341490,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,19/Nov/20 04:05,19/Nov/20 10:42,13/Jul/23 08:50,19/Nov/20 08:00,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.7,3.0.2,3.1.0,,,,2.4.8,3.0.2,3.1.0,,Project Infra,Tests,,,0,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1144,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 08:19:59 UTC 2020,,,,,,,,,,"0|z0kqls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/20 04:08;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30418;;;","19/Nov/20 08:00;dongjoon;This is resolved via https://github.com/apache/spark/pull/30418;;;","19/Nov/20 08:10;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30423;;;","19/Nov/20 08:10;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30424;;;","19/Nov/20 08:11;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30423;;;","19/Nov/20 08:19;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30425;;;",,,,,,,,,,,,,,,,,,,,
V2 Datasources that extend FileScan preclude exchange reuse,SPARK-33482,13341480,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,bersprockets,bersprockets,19/Nov/20 02:58,24/Mar/21 15:07,13/Jul/23 08:50,23/Mar/21 09:25,3.0.0,3.0.1,3.0.2,3.1.0,3.1.1,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,,"Sample query:
{noformat}
spark.read.parquet(""tbl"").createOrReplaceTempView(""tbl"")
spark.read.parquet(""lookup"").createOrReplaceTempView(""lookup"")

sql(""""""
   select tbl.col1, fk1, fk2
   from tbl, lookup l1, lookup l2
   where fk1 = l1.key
   and fk2 = l2.key
"""""").explain
{noformat}
Test files can be created as so:
{noformat}
import scala.util.Random

val rand = Random

val tbl = spark.range(1, 10000).map { x =>
  (rand.nextLong.abs % 20,
   rand.nextLong.abs % 20,
   x)
}.toDF(""fk1"", ""fk2"", ""col1"")

tbl.write.mode(""overwrite"").parquet(""tbl"")

val lookup = spark.range(0, 20).map { x =>
  (x + 1, x * 10000, (x + 1) * 10000)
}.toDF(""key"", ""col1"", ""col2"")
lookup.write.mode(""overwrite"").parquet(""lookup"")
{noformat}
Output with V1 Parquet reader:
{noformat}
 == Physical Plan ==
*(3) Project [col1#2L, fk1#0L, fk2#1L]
+- *(3) BroadcastHashJoin [fk2#1L], [key#12L], Inner, BuildRight, false
   :- *(3) Project [fk1#0L, fk2#1L, col1#2L]
   :  +- *(3) BroadcastHashJoin [fk1#0L], [key#6L], Inner, BuildRight, false
   :     :- *(3) Filter (isnotnull(fk1#0L) AND isnotnull(fk2#1L))
   :     :  +- *(3) ColumnarToRow
   :     :     +- FileScan parquet [fk1#0L,fk2#1L,col1#2L] Batched: true, DataFilters: [isnotnull(fk1#0L), isnotnull(fk2#1L)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/bruce/github/spark_upstream/tbl], PartitionFilters: [], PushedFilters: [IsNotNull(fk1), IsNotNull(fk2)], ReadSchema: struct<fk1:bigint,fk2:bigint,col1:bigint>
   :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#75]
   :        +- *(1) Filter isnotnull(key#6L)
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet [key#6L] Batched: true, DataFilters: [isnotnull(key#6L)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/bruce/github/spark_upstream/lookup], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint>
   +- ReusedExchange [key#12L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#75]
{noformat}
With V1 Parquet reader, the exchange for lookup is reused (see last line).

Output with V2 Parquet reader (spark.sql.sources.useV1SourceList=""""):
{noformat}
 == Physical Plan ==
*(3) Project [col1#2L, fk1#0L, fk2#1L]
+- *(3) BroadcastHashJoin [fk2#1L], [key#12L], Inner, BuildRight, false
   :- *(3) Project [fk1#0L, fk2#1L, col1#2L]
   :  +- *(3) BroadcastHashJoin [fk1#0L], [key#6L], Inner, BuildRight, false
   :     :- *(3) Filter (isnotnull(fk1#0L) AND isnotnull(fk2#1L))
   :     :  +- *(3) ColumnarToRow
   :     :     +- BatchScan[fk1#0L, fk2#1L, col1#2L] ParquetScan DataFilters: [isnotnull(fk1#0L), isnotnull(fk2#1L)], Format: parquet, Location: InMemoryFileIndex[file:/Users/bruce/github/spark_upstream/tbl], PartitionFilters: [], PushedFilers: [IsNotNull(fk1), IsNotNull(fk2)], ReadSchema: struct<fk1:bigint,fk2:bigint,col1:bigint>, PushedFilters: [IsNotNull(fk1), IsNotNull(fk2)]
   :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#75]
   :        +- *(1) Filter isnotnull(key#6L)
   :           +- *(1) ColumnarToRow
   :              +- BatchScan[key#6L] ParquetScan DataFilters: [isnotnull(key#6L)], Format: parquet, Location: InMemoryFileIndex[file:/Users/bruce/github/spark_upstream/lookup], PartitionFilters: [], PushedFilers: [IsNotNull(key)], ReadSchema: struct<key:bigint>, PushedFilters: [IsNotNull(key)]
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#83]
      +- *(2) Filter isnotnull(key#12L)
         +- *(2) ColumnarToRow
            +- BatchScan[key#12L] ParquetScan DataFilters: [isnotnull(key#12L)], Format: parquet, Location: InMemoryFileIndex[file:/Users/bruce/github/spark_upstream/lookup], PartitionFilters: [], PushedFilers: [IsNotNull(key)], ReadSchema: struct<key:bigint>, PushedFilters: [IsNotNull(key)]
{noformat}
With the V2 Parquet reader, the exchange for lookup is not reused (see last 4 lines).

You can see the same issue with the Orc reader (and I assume any other datasource that extends Filescan).

The issue appears to be this check in FileScan#equals:
{code:java}
ExpressionSet(partitionFilters) == ExpressionSet(f.partitionFilters) &&
ExpressionSet(dataFilters) == ExpressionSet(f.dataFilters)
{code}
partitionFilters and dataFilters are not normalized, so their exprIds don't match. Thus FileScan objects don't match, even if they are the same.

As a side note, FileScan#equals has a dangling boolean expression:
{code:java}
fileIndex == f.fileIndex && readSchema == f.readSchema
{code}
The result of that expression is not actually used anywhere. We might want to include it in the final decision, even though that's not the issue here.",,apachespark,bersprockets,cloud_fan,csun,petertoth,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 24 09:23:55 UTC 2021,,,,,,,,,,"0|z0kqjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/21 18:45;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31820;;;","18/Mar/21 17:54;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31848;;;","18/Mar/21 17:54;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31848;;;","23/Mar/21 09:25;cloud_fan;Issue resolved by pull request 31848
[https://github.com/apache/spark/pull/31848];;;","24/Mar/21 09:23;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31952;;;","24/Mar/21 09:23;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31952;;;",,,,,,,,,,,,,,,,,,,,
Incorrect value when inserting into date type partition table with date type value,SPARK-33474,13341195,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhu,yumwang,yumwang,18/Nov/20 03:21,03/Mar/21 07:50,13/Jul/23 08:50,03/Mar/21 07:50,3.1.0,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,"{code:java}
create table test_part (name STRING) partitioned by (part date) STORED AS PARQUET;
insert into test_part partition(part = date '2019-01-02') values('a');
select * from test_part;
{code}


{noformat}
spark-sql> select * from test_part;
a	NULL
{noformat}

",,apachespark,maropu,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 03 07:50:14 UTC 2021,,,,,,,,,,"0|z0kosg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/20 06:25;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30421;;;","03/Mar/21 07:50;maropu;Resolved by https://github.com/apache/spark/pull/30421;;;",,,,,,,,,,,,,,,,,,,,,,,,
IllegalArgumentException when applying RemoveRedundantSorts before EnsureRequirements,SPARK-33472,13341179,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,18/Nov/20 01:00,03/Dec/20 06:15,13/Jul/23 08:50,20/Nov/20 17:48,2.4.8,3.0.2,3.1.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"`RemoveRedundantSorts` rule uses SparkPlan's `outputPartitioning` to check whether a sort node is redundant. Currently, it is added before `EnsureRequirements`. Since `PartitioningCollection` requires left and right partitioning to have the same number of partitions, which is not necessarily true before applying `EnsureRequirements`, the rule can fail with the following exception:

{{IllegalArgumentException: requirement failed: PartitioningCollection requires all of its partitionings have the same numPartitions.}}

We should switch the order between these two rules to satisfy the requirement when instantiating `PartitioningCollection`.

 ",,allisonwang-db,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 06:15:03 UTC 2020,,,,,,,,,,"0|z0koow:",9223372036854775807,,,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,,,,,,,"18/Nov/20 01:18;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30373;;;","18/Nov/20 01:18;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30373;;;","20/Nov/20 03:30;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30437;;;","20/Nov/20 03:31;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30437;;;","20/Nov/20 03:32;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30438;;;","03/Dec/20 06:14;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30584;;;","03/Dec/20 06:15;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30584;;;",,,,,,,,,,,,,,,,,,,
Spark schedules on updating delegation token with 0 interval under some token provider implementation,SPARK-33440,13340424,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,13/Nov/20 04:15,30/Nov/20 21:58,13/Jul/23 08:50,30/Nov/20 21:58,3.0.1,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,Spark Core,,,,0,,,,"We got a report from customer that under specific circumstance Spark schedules on updating delegation token with 0 interval, ended up with flooding log message & massive requests on token handler side.

After investigation, the problem was they have two delegation token identifiers which one of token identifier (IDBS3ATokenIdentifier) has the value of ""issue date"" to be 0, whereas another token identifier (DelegationTokenIdentifier) has correct value. 

Both are providing the expire time correctly via Token.renew(), and Spark assumes issue date is ""correct"", hence calculating the token expire period as (the result of Token.renew() - ""issue date"").

{code}
20/10/13 06:34:19 INFO security.HadoopFSDelegationTokenProvider: Renewal interval is 1603175657000 for token S3ADelegationToken/IDBroker
20/10/13 06:34:19 INFO security.HadoopFSDelegationTokenProvider: Renewal interval is 86400048 for token HDFS_DELEGATION_TOKEN
{code}

It's safe at least here because Spark picks ""minimal"" value. The thing is, to calculate the next renewal timestamp, Spark tries to add the renewal interval with issue date for every token, and pick minimum value, hence ""86400048"" is picked as the next renewal timestamp.

This is ""earlier"" than now, hence interval to schedule goes to be negative (as we apply subtract with now), and Spark applies safeguard to pick the greater between 0 and interval, hence 0 is picked up, and schedule updating token infinitely. (Schedule is one-time, but the calculation will always lead to the negative, so that's effectively immediate schedule.)

We should construct the better consideration of ""safe guard"", instead of just guarding the schedule interval doesn't go to negative.",,apachespark,kabhwan,krisden,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,HADOOP-17379,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 30 21:58:14 UTC 2020,,,,,,,,,,"0|z0kk14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/20 04:20;kabhwan;I have a fix and now refining a bit. Will raise a PR soon.;;;","13/Nov/20 05:05;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/30366;;;","13/Nov/20 05:05;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/30366;;;","30/Nov/20 21:58;kabhwan;Issue resolved by pull request 30366
[https://github.com/apache/spark/pull/30366];;;",,,,,,,,,,,,,,,,,,,,,,
Use SERIAL_SBT_TESTS=1 for SQL module like Hive module,SPARK-33439,13340422,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,13/Nov/20 03:48,13/Nov/20 05:20,13/Jul/23 08:50,13/Nov/20 05:20,3.0.2,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,Project Infra,,,,0,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 05:20:44 UTC 2020,,,,,,,,,,"0|z0kk0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/20 03:53;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30365;;;","13/Nov/20 05:20;dongjoon;Issue resolved by pull request 30365
[https://github.com/apache/spark/pull/30365];;;",,,,,,,,,,,,,,,,,,,,,,,,
set -v couldn't dump all the conf entries,SPARK-33438,13340410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linhongliu-db,linhongliu-db,linhongliu-db,13/Nov/20 02:28,12/Dec/22 18:11,13/Jul/23 08:50,08/Feb/21 13:51,3.0.1,,,,,,,,,,,3.0.2,3.1.1,,,SQL,,,,0,,,,"since scala object is lazy init, it won't be load until some code touched it. For SQL conf entries, it won't be registered if the conf object is never touched. So ""set -v"" couldn't dump all the defined configs (even if it says so)",,apachespark,linhongliu-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 08 13:51:39 UTC 2021,,,,,,,,,,"0|z0kjy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/20 03:41;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30363;;;","08/Feb/21 13:51;gurwls223;Issue resolved by pull request 30363
[https://github.com/apache/spark/pull/30363];;;",,,,,,,,,,,,,,,,,,,,,,,,
SQL parser should use active SQLConf,SPARK-33432,13340199,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,luluorta,luluorta,luluorta,12/Nov/20 11:43,06/Dec/20 12:16,13/Jul/23 08:50,14/Nov/20 21:37,3.0.1,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"In ANSI mode, schema string parsing should fail if the schema uses ANSI reserved keyword as attribute name:
{code:scala}
spark.conf.set(""spark.sql.ansi.enabled"", ""true"")
spark.sql(""""""select from_json('{""time"":""26/10/2015""}', 'time Timestamp', map('timestampFormat', 'dd/MM/yyyy'));"""""").show


output:

Cannot parse the data type: 
no viable alternative at input 'time'(line 1, pos 0)

== SQL ==
time Timestamp
^^^
{code}

But this query may accidentally succeed in certain cases cause the DataType parser sticks to the configs of the first created session in the current thread:

{code:scala}
DataType.fromDDL(""time Timestamp"")
val newSpark = spark.newSession()
newSpark.conf.set(""spark.sql.ansi.enabled"", ""true"")
newSpark.sql(""""""select from_json('{""time"":""26/10/2015""}', 'time Timestamp', map('timestampFormat', 'dd/MM/yyyy'));"""""").show


output:

+--------------------------------+
|from_json({""time"":""26/10/2015""})|
+--------------------------------+
|            {2015-10-26 00:00...|
+--------------------------------+
{code}",,apachespark,dongjoon,luluorta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 14 21:37:36 UTC 2020,,,,,,,,,,"0|z0kin4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/20 17:45;apachespark;User 'luluorta' has created a pull request for this issue:
https://github.com/apache/spark/pull/30357;;;","14/Nov/20 21:37;dongjoon;Issue resolved by pull request 30357
[https://github.com/apache/spark/pull/30357];;;",,,,,,,,,,,,,,,,,,,,,,,,
Match the behavior of conv function to MySQL's,SPARK-33428,13340121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dc-heros,yumwang,yumwang,12/Nov/20 05:32,20/Jul/21 19:23,13/Jul/23 08:50,27/May/21 12:13,3.1.0,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,"# MySQL allows leading spaces but Spark does not.
 # If the input string is way too long, Spark fails with ArrayIndexOutOfBoundException",,apachespark,cloud_fan,dc-heros,dongjoon,quanghuynguyen2001,yumwang,,,,,,,,,,,,,,,,,,,,,,,SPARK-36229,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 27 12:13:02 UTC 2021,,,,,,,,,,"0|z0ki5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/20 09:08;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30350;;;","12/Nov/20 09:08;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30350;;;","14/Dec/20 14:32;cloud_fan;Issue resolved by pull request 30350
[https://github.com/apache/spark/pull/30350];;;","16/Mar/21 16:57;dongjoon;The commit is reverted.;;;","20/May/21 02:52;quanghuynguyen2001;should we consider return null value if overflow occur since bigInt cann't handle signed and unsigned base?;;;","24/May/21 18:08;cloud_fan;AFAIK this function is from MySQL and it's better to follow the MySQL behavior. MySQL returns the max unsigned long if the input string is too big, and Spark should follow it.

However, seems Spark has different behavior in two cases:
 # MySQL allows leading spaces but Spark does not.
 # If the input string is way too long, Spark fails with ArrayIndexOutOfBoundException

[~angerszhu] would you like to look into it?;;;","27/May/21 02:23;dc-heros;[~cloud_fan] I would like to work on this

 ;;;","27/May/21 03:56;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/32684;;;","27/May/21 12:13;cloud_fan;Issue resolved by pull request 32684
[https://github.com/apache/spark/pull/32684];;;",,,,,,,,,,,,,,,,,
Incomplete menu item display in documention,SPARK-33422,13339959,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,liucht-inspur,liucht-inspur,liucht-inspur,11/Nov/20 11:05,12/Dec/22 18:10,13/Jul/23 08:50,20/Nov/20 13:20,3.0.0,3.0.1,,,,,,,,,,3.0.2,3.1.0,,,Documentation,,,,0,,,,"The bottom menu item cannot be displayed when the left menu tree is long

 ",,apachespark,liucht-inspur,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/20 11:08;liucht-inspur;left-menu.jpg;https://issues.apache.org/jira/secure/attachment/13015049/left-menu.jpg",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 20 13:20:13 UTC 2020,,,,,,,,,,"0|z0kh5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/20 11:17;apachespark;User 'liucht-inspur' has created a pull request for this issue:
https://github.com/apache/spark/pull/30335;;;","11/Nov/20 11:18;apachespark;User 'liucht-inspur' has created a pull request for this issue:
https://github.com/apache/spark/pull/30335;;;","20/Nov/20 13:20;gurwls223;Issue resolved by pull request 30335
[https://github.com/apache/spark/pull/30335];;;",,,,,,,,,,,,,,,,,,,,,,,
Unexpected behavior when using SET commands before a query in SparkSession.sql,SPARK-33419,13339915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,11/Nov/20 08:27,13/Nov/20 06:58,13/Jul/23 08:50,13/Nov/20 06:58,2.4.7,3.0.2,3.1.0,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"SparkSession.sql converts a string value to a DataFrame, and the string value should be one single SQL statement ending up w/ or w/o one or more semicolons. e.g.


{code:sql}
scala> spark.sql("" select 2"").show
+---+
|  2|
+---+
|  2|
+---+


scala> spark.sql("" select 2;"").show
+---+
|  2|
+---+
|  2|
+---+

scala> spark.sql("" select 2;;;;"").show
+---+
|  2|
+---+
|  2|
+---+
{code}


If you put 2 or more statements in, it fails in the parser e.g.  


{code:java}
scala> spark.sql("" select 2; select 1;"").show
org.apache.spark.sql.catalyst.parser.ParseException:
extraneous input 'select' expecting {<EOF>, ';'}(line 1, pos 11)

== SQL ==
 select 2; select 1;
-----------^^^

  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:263)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:130)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:51)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:81)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:610)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:610)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:769)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)
  ... 47 elided

{code}

As a very generic user scenario,  they want to change some settings before they execute
the queries. They may pass a string value like `set spark.sql.abc=2; select 1;` into this API, which creates a confusing gap between the actual effect and the user's expectations.

The user may want the query to be executed with spark.sql.abc=2, but Spark actually treats the whole part of `2; select 1;` as the value of the property 'spark.sql.abc',
 e.g.

{code:java}
scala> spark.sql(""set spark.sql.abc=2; select 1;"").show
+-------------+------------+
|          key|       value|
+-------------+------------+
|spark.sql.abc|2; select 1;|
+-------------+------------+
{code}
",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 06:58:39 UTC 2020,,,,,,,,,,"0|z0kgw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/20 08:28;Qin Yao;I'd like to fix this;;;","11/Nov/20 10:32;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30332;;;","13/Nov/20 06:58;cloud_fan;Issue resolved by pull request 30332
[https://github.com/apache/spark/pull/30332];;;",,,,,,,,,,,,,,,,,,,,,,,
Correct the behaviour of query filters in TPCDSQueryBenchmark ,SPARK-33417,13339882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,maropu,maropu,11/Nov/20 03:25,11/Nov/20 06:26,13/Jul/23 08:50,11/Nov/20 06:26,2.4.8,3.0.2,3.1.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"This PR intends to fix the behaviour of query filters in TPCDSQueryBenchmark. We can use an option --query-filter for selecting TPCDS queries to run, e.g., --query-filter q6,q8,q13. But, the current master has a weird behaviour about the option. For example, if we pass --query-filter q6 so as to run the TPCDS q6 only, TPCDSQueryBenchmark runs q6 and q6-v2.7 because the filterQueries method does not respect the name suffix. So, there is no way now to run the TPCDS q6 only.",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 06:26:57 UTC 2020,,,,,,,,,,"0|z0kgoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/20 03:28;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30324;;;","11/Nov/20 06:26;maropu;Resolved by https://github.com/apache/spark/pull/30324;;;",,,,,,,,,,,,,,,,,,,,,,,,
OverwriteByExpression should resolve its delete condition based on the table relation not the input query,SPARK-33412,13339742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/Nov/20 12:42,11/Nov/20 13:51,13/Jul/23 08:50,11/Nov/20 13:51,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 08:12:15 UTC 2020,,,,,,,,,,"0|z0kftk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/20 12:53;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/30318;;;","10/Nov/20 12:54;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/30318;;;","11/Nov/20 08:11;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/30328;;;","11/Nov/20 08:12;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/30328;;;",,,,,,,,,,,,,,,,,,,,,,
Upgrade commons-compress to 1.20,SPARK-33405,13339631,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,10/Nov/20 00:02,12/Dec/22 18:10,13/Jul/23 08:50,10/Nov/20 02:10,3.0.1,3.1.0,,,,,,,,,,2.4.8,3.0.2,3.1.0,,Build,,,,0,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 02:24:20 UTC 2020,,,,,,,,,,"0|z0kf4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/20 00:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30304;;;","10/Nov/20 00:29;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30305;;;","10/Nov/20 00:30;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30305;;;","10/Nov/20 02:10;gurwls223;Issue resolved by pull request 30304
[https://github.com/apache/spark/pull/30304];;;","10/Nov/20 02:24;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30307;;;",,,,,,,,,,,,,,,,,,,,,
"""date_trunc"" expression returns incorrect results",SPARK-33404,13339626,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,utkarsh39,utkarsh39,utkarsh39,09/Nov/20 23:42,11/Nov/20 22:46,13/Jul/23 08:50,11/Nov/20 00:36,3.0.0,3.0.1,3.1.0,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,correctness,,,"`date_trunc` SQL expression returns incorrect results for {{minute}} formatting string.

Context: The {{minute}} formatting string should truncate the timestamps such that the seconds is set to ZERO.

Repro (run the following commands in spark-shell):
{quote}
spark.conf.set(""spark.sql.session.timeZone"", ""America/Los_Angeles"")
spark.sql(""SELECT date_trunc('minute', '1769-10-17 17:10:02')"").show()
{quote}

Spark currently incorrectly returns 
{quote}
1769-10-17 17:10:02
{quote}
against the expected return value of 
{quote}
1769-10-17 17:10:00
{quote}
This happens as {{truncTimestamp}} in package {{org.apache.spark.sql.catalyst.util.DateTimeUtils}} incorrectly assumes that time zone offsets can never have the granularity of a second and thus does not account for time zone adjustment when truncating the timestamp to {{minute}}. 
This assumption is currently used when truncating the timestamps to {{microsecond, millisecond, second, or minute}}. ",,apachespark,maropu,utkarsh39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 18:15:45 UTC 2020,,,,,,,,,,"0|z0kf3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/20 23:45;apachespark;User 'utkarsh39' has created a pull request for this issue:
https://github.com/apache/spark/pull/30303;;;","11/Nov/20 00:36;maropu;Resolved by https://github.com/apache/spark/pull/30303;;;","11/Nov/20 16:38;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30338;;;","11/Nov/20 18:14;apachespark;User 'utkarsh39' has created a pull request for this issue:
https://github.com/apache/spark/pull/30339;;;","11/Nov/20 18:15;apachespark;User 'utkarsh39' has created a pull request for this issue:
https://github.com/apache/spark/pull/30339;;;",,,,,,,,,,,,,,,,,,,,,
Jobs launched in same second have duplicate MapReduce JobIDs,SPARK-33402,13339595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,09/Nov/20 20:01,11/Nov/20 22:29,13/Jul/23 08:50,11/Nov/20 22:29,2.4.8,3.0.1,3.1.0,,,,,,,,,3.0.2,3.1.0,,,Spark Core,,,,0,,,,"Spark uses the current timestamp to generate a MapReduce JobID.
If > 1 job attempt is generated in the same second, these can clash

Committers which expect this to be unique can conflict with the other jobs

* S3A staging committer (cluster FS staging dir and local task output dir)
* Any committer which supports parallel jobs writing to the same destination
  directory and requires unique names for the attempts
* Code which uses the jobID as part of its algorithm to generate unique filenames

Note: {{HadoopMapReduceCommitProtocol.getFilename()}} doesn't use this JobID for
uniqueness, it uses task attempt ID and stage ID. It probably deserves its own
audit.


",,apachespark,dongjoon,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33230,HADOOP-17318,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 22:29:10 UTC 2020,,,,,,,,,,"0|z0keww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/20 20:02;stevel@apache.org;The good news, this surfaced in a test I have, now that HADOOP-17318 allows me to configure the S3A committers to fail fast if they don't get a job ID.
{code}
2020-11-09 10:58:25,827 [Executor task launch worker for task 1] INFO  rdd.HadoopRDD (Logging.scala:logInfo(57)) - Input split: s3a://stevel-ireland/cloud-integration/DELAY_LISTING_ME/S3ABasicIOSuite/FileOutput/part-00000:0+3893
- FileOutput
- NewHadoopAPI *** FAILED ***
  org.apache.hadoop.fs.s3a.commit.PathCommitException: `': Job/task context does not contain a unique ID in spark.sql.sources.writeJobUUID
  at org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter.buildJobUUID(AbstractS3ACommitter.java:1268)
  at org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter.<init>(AbstractS3ACommitter.java:176)
  at org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter.<init>(StagingCommitter.java:113)
  at org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter.<init>(DirectoryStagingCommitter.java:57)
  at org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitterFactory.createTaskCommitter(DirectoryStagingCommitterFactory.java:45)
  at org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory.createTaskCommitter(S3ACommitterFactory.java:81)
  at org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory.createOutputCommitter(AbstractS3ACommitterFactory.java:48)
  at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:338)
  at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupCommitter(HadoopMapReduceCommitProtocol.scala:100)
  at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:161)
  ...
{code};;;","10/Nov/20 16:38;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/30319;;;","10/Nov/20 16:39;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/30319;;;","11/Nov/20 22:29;dongjoon;Issue resolved by pull request 30319
[https://github.com/apache/spark/pull/30319];;;",,,,,,,,,,,,,,,,,,,,,,
AnalysisException when loading a PipelineModel with Spark 3,SPARK-33398,13339536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,LoicH,LoicH,09/Nov/20 14:25,07/Jan/21 20:19,13/Jul/23 08:50,03/Jan/21 17:53,3.0.1,,,,,,,,,,,3.0.2,3.1.0,,,MLlib,,,,0,decisiontree,pyspark,V3,"I am upgrading my Spark version from 2.4.5 to 3.0.1 and I cannot load anymore the PipelineModel objects that use a ""DecisionTreeClassifier"" stage.

In my code I load several PipelineModel, all the PipelineModel with stages [""CountVectorizer_[uid]"", ""LinearSVC_[uid]""] are loading fine whereas the models with stages 
 [""CountVectorizer_[uid]"",""DecisionTreeClassifier_[uid]""] are throwing the following exception:
{noformat}
AnalysisException: cannot resolve '`rawCount`' given input columns: [gain, id, impurity, impurityStats, leftChild, prediction, rightChild, split];{noformat}
Here is the code I am using and the full stacktrace:
{code:python}
from pyspark.ml.pipeline import PipelineModel
PipelineModel.load(""/path/to/model"")
{code}
{noformat}
AnalysisException                         Traceback (most recent call last)
<command-1278858167154148> in <module>
----> 1 RalentModel = PipelineModel.load(MODELES_ATTRIBUTS + ""RalentModel_DT"")/databricks/spark/python/pyspark/ml/util.py in load(cls, path)
    368     def load(cls, path):
    369         """"""Reads an ML instance from the input path, a shortcut of `read().load(path)`.""""""
--> 370         return cls.read().load(path)
    371 
    372 /databricks/spark/python/pyspark/ml/pipeline.py in load(self, path)
    289         metadata = DefaultParamsReader.loadMetadata(path, self.sc)
    290         if 'language' not in metadata['paramMap'] or metadata['paramMap']['language'] != 'Python':
--> 291             return JavaMLReader(self.cls).load(path)
    292         else:
    293             uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)/databricks/spark/python/pyspark/ml/util.py in load(self, path)
    318         if not isinstance(path, basestring):
    319             raise TypeError(""path should be a basestring, got type %s"" % type(path))
--> 320         java_obj = self._jread.load(path)
    321         if not hasattr(self._clazz, ""_from_java""):
    322             raise NotImplementedError(""This Java ML type cannot be loaded into Python currently: %r""/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-> 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    131                 # Hide where the exception came from that shows a non-Pythonic
    132                 # JVM exception message.
--> 133                 raise_from(converted)
    134             else:
    135                 raise/databricks/spark/python/pyspark/sql/utils.py in raise_from(e)
AnalysisException: cannot resolve '`rawCount`' given input columns: [gain, id, impurity, impurityStats, leftChild, prediction, rightChild, split];
{noformat}
These pipeline models where saved using Spark 2.4.3, I can load them fine using Spark 2.4.5.

I tried to investigate further and load each stage separately. Loading the CountVectorizerModel with
{code:python}
from pyspark.ml.feature import CountVectorizerModel
CountVectorizerModel.read().load(""/path/to/model/stages/0_CountVectorizer_efce893314a9"")
{code}
yields a CountVectorizerModel, but my code fails when trying to load the DecisionTreeClassificationModel:
{code:python}
DecisionTreeClassificationModel.read().load(""/path/to/model/stages/1_DecisionTreeClassifier_4d2a76c565b0"")
AnalysisException: cannot resolve '`rawCount`' given input columns: [gain, id, impurity, impurityStats, leftChild, prediction, rightChild, split];
{code}
And here is the content of the ""data"" of my Decision Tree Classifier:
{code:python}
spark.read.parquet(""/path/to/model/stages/1_DecisionTreeClassifier_4d2a76c565b0/data"").show()

+---+----------+--------------------+-------------+--------------------+---------+----------+----------------+
| id|prediction|            impurity|impurityStats|                gain|leftChild|rightChild|           split|
+---+----------+--------------------+-------------+--------------------+---------+----------+----------------+
|  0|       0.0|  0.3926234384295062| [90.0, 33.0]| 0.16011830963990054|        1|        16|[190, [0.5], -1]|
|  1|       0.0|  0.2672722508516028| [90.0, 17.0]| 0.11434106988303855|        2|        15|[512, [0.5], -1]|
|  2|       0.0|  0.1652892561983472|  [90.0, 9.0]| 0.06959547629404085|        3|        14|[583, [0.5], -1]|
|  3|       0.0| 0.09972299168975082|  [90.0, 5.0]|0.026984966852376356|        4|        11|[480, [0.5], -1]|
|  4|       0.0|0.043933846736523306|  [87.0, 2.0]|0.021717299239076976|        5|        10|[555, [1.5], -1]|
|  5|       0.0|0.022469008264462766|  [87.0, 1.0]|0.011105371900826402|        6|         7|[833, [0.5], -1]|
|  6|       0.0|                 0.0|  [86.0, 0.0]|                -1.0|       -1|        -1|    [-1, [], -1]|
|  7|       0.0|                 0.5|   [1.0, 1.0]|                 0.5|        8|         9|  [0, [0.5], -1]|
|  8|       0.0|                 0.0|   [1.0, 0.0]|                -1.0|       -1|        -1|    [-1, [], -1]|
|  9|       1.0|                 0.0|   [0.0, 1.0]|                -1.0|       -1|        -1|    [-1, [], -1]|
| 10|       1.0|                 0.0|   [0.0, 1.0]|                -1.0|       -1|        -1|    [-1, [], -1]|
| 11|       0.0|                 0.5|   [3.0, 3.0]|                 0.5|       12|        13| [14, [1.5], -1]|
| 12|       0.0|                 0.0|   [3.0, 0.0]|                -1.0|       -1|        -1|    [-1, [], -1]|
| 13|       1.0|                 0.0|   [0.0, 3.0]|                -1.0|       -1|        -1|    [-1, [], -1]|
| 14|       1.0|                 0.0|   [0.0, 4.0]|                -1.0|       -1|        -1|    [-1, [], -1]|
| 15|       1.0|                 0.0|   [0.0, 8.0]|                -1.0|       -1|        -1|    [-1, [], -1]|
| 16|       1.0|                 0.0|  [0.0, 16.0]|                -1.0|       -1|        -1|    [-1, [], -1]|
+---+----------+--------------------+-------------+--------------------+---------+----------+----------------+

{code}","- Databricks runtime 7.3 ML
- Spark 3.0.1
- Python 3.7",apachespark,LoicH,nmarcott,podongfeng,,,,,,,,,,,,,,,,,,,,SPARK-33661,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 03 17:53:39 UTC 2021,,,,,,,,,,"0|z0kejs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/20 12:39;nmarcott;It appears rawCount is a new field added by [this PR|https://github.com/apache/spark/pull/21632/files#diff-0fdae8a6782091746ed20ea43f77b639f9c6a5f072dd2f600fcf9a7b37db4f47] , but there doesn't seem to be logic to handle the missing column/old data.

 +[~imatiach]+ [~podongfeng] 

Is there some type of default value or missing column handling that should be added here?

I do not see a mention of this in the [ml lib migration guide.|https://spark.apache.org/docs/latest/ml-migration-guide.html#upgrading-from-mllib-24-to-30] 

I was also able to reproduce this with the following code, saving using v2.4.7 and loading using v3.0.1

 
{code:java}
from pyspark.ml import Pipeline
from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import HashingTF, Tokenizer
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit
# Prepare training documents from a list of (id, text, label) tuples.
df = spark.createDataFrame([
    (0, ""a b c d e spark"", 1.0),
    (1, ""b d"", 0.0),
    (2, ""spark f g h"", 1.0),
    (3, ""hadoop mapreduce"", 0.0)
], [""id"", ""text"", ""label""])
tokenizer = Tokenizer(inputCol=""text"", outputCol=""words"")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=""features"", numFeatures=1000)
treeClassifier = DecisionTreeClassifier()
pipeline = Pipeline(stages=[tokenizer, hashingTF])
features = pipeline.fit(df).transform(df)
model = treeClassifier.fit(features)
model.write().overwrite().save(""/tmp/dc"")

#loading code
from pyspark.ml.classification import DecisionTreeClassificationModel
DecisionTreeClassificationModel.read().load(""/tmp/dc"")

{code}
 ;;;","22/Dec/20 05:58;podongfeng;[~nmarcott]  I can reproduce this failure, and will look into it.;;;","22/Dec/20 11:47;podongfeng;[~nmarcott]  This issue also exists in RF/GBT, I have just send a [bugfix pr|[https://github.com/apache/spark/pull/30889]]  for this. ;;;","22/Dec/20 11:54;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/30889;;;","03/Jan/21 17:53;srowen;Issue resolved by pull request 30889
[https://github.com/apache/spark/pull/30889];;;",,,,,,,,,,,,,,,,,,,,,
mistakenly generate markdown to html for available-patterns-for-shs-custom-executor-log-ur,SPARK-33397,13339522,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,09/Nov/20 12:51,10/Nov/20 01:17,13/Jul/23 08:50,10/Nov/20 01:17,3.0.1,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,Documentation,,,,0,,,,"http://spark.apache.org/docs/3.0.1/running-on-yarn.html#available-patterns-for-shs-custom-executor-log-url

",,apachespark,maropu,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 01:17:44 UTC 2020,,,,,,,,,,"0|z0kego:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/20 13:04;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30298;;;","09/Nov/20 13:05;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30298;;;","10/Nov/20 01:17;maropu;Resolved by https://github.com/apache/spark/pull/30298;;;",,,,,,,,,,,,,,,,,,,,,,,
element_at with CreateArray not respect one based index,SPARK-33391,13339451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,leanken,leanken,leanken,09/Nov/20 07:38,10/Nov/20 07:24,13/Jul/23 08:50,10/Nov/20 07:24,3.1.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"var df = spark.sql(""select element_at(array(3, 2, 1), 0)"")
df.printSchema()

df = spark.sql(""select element_at(array(3, 2, 1), 1)"")
df.printSchema()

df = spark.sql(""select element_at(array(3, 2, 1), 2)"")
df.printSchema()

df = spark.sql(""select element_at(array(3, 2, 1), 3)"")
df.printSchema()

root
 |-- element_at(array(3, 2, 1), 0): integer (nullable = false)

root
 |-- element_at(array(3, 2, 1), 1): integer (nullable = false)

root
 |-- element_at(array(3, 2, 1), 2): integer (nullable = false)

root
 |-- element_at(array(3, 2, 1), 3): integer (nullable = true)

 

In this case, the nullable property in element_at with CreateArray statement is not correct.

 ",,apachespark,cloud_fan,leanken,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 07:24:23 UTC 2020,,,,,,,,,,"0|z0ke0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/20 08:24;apachespark;User 'leanken' has created a pull request for this issue:
https://github.com/apache/spark/pull/30296;;;","09/Nov/20 08:25;apachespark;User 'leanken' has created a pull request for this issue:
https://github.com/apache/spark/pull/30296;;;","10/Nov/20 07:24;cloud_fan;Issue resolved by pull request 30296
[https://github.com/apache/spark/pull/30296];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix InSet bucket pruning,SPARK-33372,13339191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,06/Nov/20 14:51,10/Nov/20 12:32,13/Jul/23 08:50,09/Nov/20 08:33,3.1.0,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"This pr fix InSet bucket pruning because of it's values should not be Literal:

https://github.com/apache/spark/blob/cbd3fdea62dab73fc4a96702de8fd1f07722da66/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala#L253-L255",,apachespark,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 05:07:03 UTC 2020,,,,,,,,,,"0|z0kcf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/20 14:59;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/30279;;;","09/Nov/20 08:33;cloud_fan;Issue resolved by pull request 30279
[https://github.com/apache/spark/pull/30279];;;","10/Nov/20 05:07;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/30308;;;",,,,,,,,,,,,,,,,,,,,,,,
skipSchemaResolution should still require query to be resolved,SPARK-33362,13339033,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,05/Nov/20 14:56,05/Nov/20 17:24,13/Jul/23 08:50,05/Nov/20 17:24,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,,,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 17:24:16 UTC 2020,,,,,,,,,,"0|z0kbg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/20 14:59;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/30265;;;","05/Nov/20 17:24;dongjoon;Issue resolved by pull request 30265
[https://github.com/apache/spark/pull/30265];;;",,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL CLI command processing loop can't exit while one comand fail,SPARK-33358,13338987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,artiship,artiship,artiship,05/Nov/20 09:57,16/Nov/20 05:50,13/Jul/23 08:50,16/Nov/20 00:59,3.0.0,3.0.1,3.1.0,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,correctness,,," 

When submit a multiple statements sql script through bin/spark-sql, if one of the prior command fail, the processing loop will not exit and continuing executing the following statements, and finally makes the whole program success. 

 
{code:java}
for (oneCmd <- commands) {
  if (StringUtils.endsWith(oneCmd, ""\\"")) {
    command += StringUtils.chop(oneCmd) + "";""
  } else {
    command += oneCmd
    if (!StringUtils.isBlank(command)) {
      val ret = processCmd(command)
      command = """"
      lastRet = ret
      val ignoreErrors = HiveConf.getBoolVar(conf, HiveConf.ConfVars.CLIIGNOREERRORS)
      if (ret != 0 && !ignoreErrors) {
        CommandProcessorFactory.clean(conf.asInstanceOf[HiveConf])
        ret // for loop will not return if one of the commands fail
      }
    }
  }

{code}
 ",,apachespark,artiship,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 16 00:59:24 UTC 2020,,,,,,,,,,"0|z0kb60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/20 10:20;apachespark;User 'artiship' has created a pull request for this issue:
https://github.com/apache/spark/pull/30263;;;","16/Nov/20 00:19;dongjoon;I verified this and updated the PR description. This is a regression at Apache Spark 3.0.0 (and 3.0.1). Not only the final return code of `Spark SQL shell` is wrong, but also the query result and DDL can be wrong due to the ignorance of the failure.;;;","16/Nov/20 00:59;dongjoon;Issue resolved by pull request 30263
[https://github.com/apache/spark/pull/30263];;;",,,,,,,,,,,,,,,,,,,,,,,
Cache dependencies for Coursier with new sbt in GitHub Actions,SPARK-33353,13338933,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,05/Nov/20 05:02,05/Nov/20 17:30,13/Jul/23 08:50,05/Nov/20 17:30,3.1.0,,,,,,,,,,,3.1.0,,,,Build,,,,0,,,,"SPARK-33226 upgraded sbt to 1.4.1.
As of 1.3.0, sbt uses Coursier as the dependency resolver / fetcher.
So let's change the dependency cache configuration for the GitHub Actions job.",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 17:30:13 UTC 2020,,,,,,,,,,"0|z0kau0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/20 05:10;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30259;;;","05/Nov/20 17:30;dongjoon;Issue resolved by pull request 30259
[https://github.com/apache/spark/pull/30259];;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix the build with sbt to copy hadoop-client-runtime.jar,SPARK-33343,13338842,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,04/Nov/20 14:39,04/Nov/20 23:05,13/Jul/23 08:50,04/Nov/20 23:05,3.1.0,,,,,,,,,,,3.1.0,,,,Build,,,,0,,,,"With the current master, spark-shell doesn't work if it's built with sbt package.
It's due to hadoop-client-runtime.jar isn't copied to assembly/target/scala-2.12/jars.
{code}
$ bin/spark-shell
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/shaded/com/ctc/wstx/io/InputBootstrapper
	at org.apache.spark.deploy.SparkHadoopUtil$.newConfiguration(SparkHadoopUtil.scala:426)
	at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$2(SparkSubmit.scala:342)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:342)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:877)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1013)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1022)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.shaded.com.ctc.wstx.io.InputBootstrapper
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 11 more
{code}",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 23:05:52 UTC 2020,,,,,,,,,,"0|z0ka9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/20 14:47;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30250;;;","04/Nov/20 14:48;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30250;;;","04/Nov/20 23:05;dongjoon;Issue resolved by pull request 30250
[https://github.com/apache/spark/pull/30250];;;",,,,,,,,,,,,,,,,,,,,,,,
"In the threadDump page, when a thread is blocked by anther thread, the blocking thread name and url were both displayed incorrect, causing the url to fail to jump.",SPARK-33342,13338819,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,echohlne,echohlne,echohlne,04/Nov/20 11:40,06/Nov/20 05:45,13/Jul/23 08:50,06/Nov/20 05:45,3.0.1,,,,,,,,,,,3.1.0,,,,Web UI,,,,0,,,,"In the threadDump page, when a thread is blocked by anther thread, the blocking thread name and url were both displayed incorrect, causing the url to fail to jump.

such as *Thread 73*  but shows `*Thread some(73)*`",spark version: spark3.0.1,apachespark,echohlne,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/20 11:43;echohlne;cannot-jump.gif;https://issues.apache.org/jira/secure/attachment/13014706/cannot-jump.gif","04/Nov/20 11:43;echohlne;display error.png;https://issues.apache.org/jira/secure/attachment/13014707/display+error.png",,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 05:45:42 UTC 2020,,,,,,,,,,"0|z0ka4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/20 11:51;apachespark;User 'akiyamaneko' has created a pull request for this issue:
https://github.com/apache/spark/pull/30249;;;","04/Nov/20 11:52;apachespark;User 'akiyamaneko' has created a pull request for this issue:
https://github.com/apache/spark/pull/30249;;;","06/Nov/20 05:45;Gengliang.Wang;Issue resolved by pull request 30249
[https://github.com/apache/spark/pull/30249];;;",,,,,,,,,,,,,,,,,,,,,,,
Pyspark application will hang due to non Exception,SPARK-33339,13338796,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lrz,lrz,lrz,04/Nov/20 09:39,12/Dec/22 18:10,13/Jul/23 08:50,10/Nov/20 10:42,2.4.5,3.0.0,3.0.1,,,,,,,,,2.4.8,3.0.2,3.1.0,,PySpark,,,,0,,,,"When a system.exit exception occurs during the process, the python worker exits abnormally, and then the executor task is still waiting for the worker for reading from socket, causing it to hang.
 The system.exit exception may be caused by the user's error code, but spark should at least throw an error to remind the user, not get stuck
 we can run a simple test to reproduce this case:
{code:python}
from pyspark.sql import SparkSession

def err(line):
  raise SystemExit

spark = SparkSession.builder.appName(""test"").getOrCreate()
spark.sparkContext.parallelize(range(1,2), 2).map(err).collect()
spark.stop()
{code}
  ",,apachespark,lrz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 01:46:36 UTC 2020,,,,,,,,,,"0|z0k9zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/20 11:30;apachespark;User 'li36909' has created a pull request for this issue:
https://github.com/apache/spark/pull/30248;;;","10/Nov/20 10:42;gurwls223;Fixed in https://github.com/apache/spark/pull/30248;;;","13/Nov/20 01:46;apachespark;User 'li36909' has created a pull request for this issue:
https://github.com/apache/spark/pull/30361;;;",,,,,,,,,,,,,,,,,,,,,,,
GROUP BY using literal map should not fail,SPARK-33338,13338792,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,04/Nov/20 09:23,25/Jun/21 17:48,13/Jul/23 08:50,04/Nov/20 16:41,2.0.2,2.1.3,2.2.3,2.3.4,2.4.7,3.0.1,3.1.0,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"Apache Spark 2.x ~ 3.0.1 raise`RuntimeException` for the following queries.
*SQL*
{code}
CREATE TABLE t USING ORC AS SELECT map('k1', 'v1') m, 'k1' k
SELECT map('k1', 'v1')[k] FROM t GROUP BY 1
SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k]
SELECT map('k1', 'v1')[k] a FROM t GROUP BY a
{code}

*ERROR*
{code}
Caused by: java.lang.RuntimeException: Couldn't find k#3 in [keys: [k1], values: [v1][k#3]#6]
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:85)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:79)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
{code}

This is a regression from Apache Spark 1.6.x.
{code}
scala> sc.version
res1: String = 1.6.3

scala> sqlContext.sql(""SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k]"").show
+---+
|_c0|
+---+
| v1|
+---+
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 25 17:48:27 UTC 2021,,,,,,,,,,"0|z0k9yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/20 09:27;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30246;;;","04/Nov/20 16:41;dongjoon;Issue resolved by pull request 30246
[https://github.com/apache/spark/pull/30246];;;","25/Jun/21 17:48;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33092;;;",,,,,,,,,,,,,,,,,,,,,,,
Upgrade Jetty to 9.4.28.v20200408,SPARK-33333,13338744,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,03/Nov/20 23:36,06/Nov/20 16:33,13/Jul/23 08:50,04/Nov/20 06:31,2.4.7,3.0.1,,,,,,,,,,2.4.8,3.0.2,,,Build,,,,0,,,,,,apachespark,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 07:23:40 UTC 2020,,,,,,,,,,"0|z0k9o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/20 23:51;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30240;;;","03/Nov/20 23:52;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30240;;;","03/Nov/20 23:53;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30241;;;","03/Nov/20 23:54;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30241;;;","04/Nov/20 06:31;viirya;Issue resolved by pull request 30240
[https://github.com/apache/spark/pull/30240];;;","06/Nov/20 07:23;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30276;;;",,,,,,,,,,,,,,,,,,,,
Avro reader drops rows,SPARK-33314,13338324,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,bersprockets,bersprockets,bersprockets,02/Nov/20 00:48,12/Dec/22 18:10,13/Jul/23 08:50,05/Nov/20 02:50,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,correctness,,,"Under certain circumstances, the V1 Avro reader drops rows. For example:
{noformat}
scala> val df = spark.range(0, 25).toDF(""index"")
df: org.apache.spark.sql.DataFrame = [index: bigint]

scala> df.write.mode(""overwrite"").format(""avro"").save(""index_avro"")

scala> val loaded = spark.read.format(""avro"").load(""index_avro"")
loaded: org.apache.spark.sql.DataFrame = [index: bigint]

scala> loaded.collect.size
res1: Int = 25

scala> loaded.orderBy(""index"").collect.size
res2: Int = 17   <== expected 25

scala> loaded.orderBy(""index"").write.mode(""overwrite"").format(""parquet"").save(""index_as_parquet"")

scala> spark.read.parquet(""index_as_parquet"").count
res4: Long = 17

scala>
{noformat}
SPARK-32346 slightly refactored the AvroFileFormat and AvroPartitionReaderFactory to use a new iterator-like trait called AvroUtils#RowReader. RowReader#hasNextRow consumes a raw input record and stores the deserialized row for the next call to RowReader#nextRow. Unfortunately, sometimes hasNextRow is called twice before nextRow is called, resulting in a lost row (see [BypassMergeSortShuffleWriter#write|https://github.com/apache/spark/blob/69c27f49acf2fe6fbc8335bde2aac4afd4188678/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java#L132], which calls records.hasNext once before calling it again [here|https://github.com/apache/spark/blob/69c27f49acf2fe6fbc8335bde2aac4afd4188678/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java#L155]).

RowReader consumes the Avro record in hasNextRow, rather than nextRow, because AvroDeserializer#deserialize potentially filters out the record.

Two possible fixes that I thought of:

1) keep state in RowReader such that multiple calls to RowReader#hasNextRow with no intervening call to RowReader#nextRow avoids consuming more than 1 Avro record. This requires no changes to any code that extends RowReader, just RowReader itself.
 2) Move record consumption to RowReader#nextRow (such that RowReader#nextRow could potentially return None) and wrap any iterator that extends RowReader with a new iterator created by flatMap. This last iterator will filter out the Nones and extract rows from the Somes. This requires changes to AvroFileFormat and AvroPartitionReaderFactory as well as RowReader.

The first one seems simplest and most straightfoward, and doesn't require changes to AvroFileFormat and AvroPartitionReaderFactory, only to AvroUtils#RowReader. So I propose this.",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32346,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 02:50:39 UTC 2020,,,,,,,,,,"0|z0k72w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/20 01:10;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/30221;;;","05/Nov/20 02:50;gurwls223;Issue resolved by pull request 30221
[https://github.com/apache/spark/pull/30221];;;",,,,,,,,,,,,,,,,,,,,,,,,
R/run-tests.sh is not compatible with testthat >= 3.0,SPARK-33313,13338318,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,01/Nov/20 21:07,12/Dec/22 18:10,13/Jul/23 08:50,01/Nov/20 23:56,3.0.0,3.1.0,,,,,,,,,,2.4.8,3.0.2,3.1.0,,R,Tests,,,0,,,,"Currently we use {{testthat:::test_package_dir}} to run full SparkR tests with {{testthat > 1.0}}. However, it has been dropped in {{testthat}} 3.0 (https://github.com/r-lib/testthat/pull/1054).

Because of that AppVeyor tests fail with

{code:r}
Spark package found in SPARK_HOME: C:\projects\spark\bin\..
Error in get(name, envir = asNamespace(pkg), inherits = FALSE) : 
  object 'test_package_dir' not found
Calls: ::: -> get
Execution halted
{code}

It seems like we can use {{testthat::test_dir}} which, since {{testthat}} 3.0, supports {{package}} parameter.",,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 02 00:07:46 UTC 2020,,,,,,,,,,"0|z0k71k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/20 22:25;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/30219;;;","01/Nov/20 22:26;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/30219;;;","01/Nov/20 23:56;gurwls223;Issue resolved by pull request 30219
[https://github.com/apache/spark/pull/30219];;;","02/Nov/20 00:06;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30220;;;","02/Nov/20 00:07;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30220;;;",,,,,,,,,,,,,,,,,,,,,
TimezoneID is needed when there cast from Date to String,SPARK-33306,13338168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EdisonWang,EdisonWang,EdisonWang,31/Oct/20 01:46,22/Feb/21 00:56,13/Jul/23 08:50,31/Oct/20 22:15,3.0.0,3.0.1,3.1.0,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"A simple way to reproduce this is 

{code}
spark-shell --conf spark.sql.legacy.typeCoercion.datetimeToString.enabled

scala> sql(""""""

select a.d1 from
 (select to_date(concat('2000-01-0', id)) as d1 from range(1, 2)) a
 join
 (select concat('2000-01-0', id) as d2 from range(1, 2)) b
 on a.d1 = b.d2

"""""").show
{code}

 

it will throw

{code}

java.util.NoSuchElementException: None.get
 at scala.None$.get(Option.scala:529)
 at scala.None$.get(Option.scala:527)
 at org.apache.spark.sql.catalyst.expressions.TimeZoneAwareExpression.zoneId(datetimeExpressions.scala:56)
 at org.apache.spark.sql.catalyst.expressions.TimeZoneAwareExpression.zoneId$(datetimeExpressions.scala:56)
 at org.apache.spark.sql.catalyst.expressions.CastBase.zoneId$lzycompute(Cast.scala:253)
 at org.apache.spark.sql.catalyst.expressions.CastBase.zoneId(Cast.scala:253)
 at org.apache.spark.sql.catalyst.expressions.CastBase.dateFormatter$lzycompute(Cast.scala:287)
 at org.apache.spark.sql.catalyst.expressions.CastBase.dateFormatter(Cast.scala:287)

{code}",,apachespark,dongjoon,EdisonWang,,,,,,,,,,,,,,,,,,,,,,,SPARK-33420,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 02 09:16:31 UTC 2020,,,,,,,,,,"0|z0k648:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/20 08:30;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30213;;;","31/Oct/20 08:31;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30213;;;","31/Oct/20 22:15;dongjoon;Issue resolved by pull request 30213
[https://github.com/apache/spark/pull/30213];;;","02/Nov/20 09:15;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30223;;;","02/Nov/20 09:16;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30223;;;",,,,,,,,,,,,,,,,,,,,,
Intermittent Compilation failure In GitHub Actions after SBT upgrade,SPARK-33297,13337999,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,30/Oct/20 03:39,12/Dec/22 18:10,13/Jul/23 08:50,30/Oct/20 09:02,3.1.0,,,,,,,,,,,3.1.0,,,,Build,,,,0,,,,"https://github.com/apache/spark/runs/1314691686

{code}
Error:  java.util.MissingResourceException: Can't find bundle for base name org.scalactic.ScalacticBundle, locale en
Error:  	at java.util.ResourceBundle.throwMissingResourceException(ResourceBundle.java:1581)
Error:  	at java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1396)
Error:  	at java.util.ResourceBundle.getBundle(ResourceBundle.java:782)
Error:  	at org.scalactic.Resources$.resourceBundle$lzycompute(Resources.scala:8)
Error:  	at org.scalactic.Resources$.resourceBundle(Resources.scala:8)
Error:  	at org.scalactic.Resources$.pleaseDefineScalacticFillFilePathnameEnvVar(Resources.scala:256)
Error:  	at org.scalactic.source.PositionMacro$PositionMacroImpl.apply(PositionMacro.scala:65)
Error:  	at org.scalactic.source.PositionMacro$.genPosition(PositionMacro.scala:85)
Error:  	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
Error:  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Error:  	at java.lang.reflect.Method.invoke(Method.java:498)
{code}
",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21708,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 30 09:02:29 UTC 2020,,,,,,,,,,"0|z0k52w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/20 03:45;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30198;;;","30/Oct/20 09:02;gurwls223;Issue resolved by pull request 30198
[https://github.com/apache/spark/pull/30198];;;",,,,,,,,,,,,,,,,,,,,,,,,
Make Literal ArrayBasedMapData string representation disambiguous,SPARK-33292,13337960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,29/Oct/20 22:57,30/Oct/20 02:55,13/Jul/23 08:50,30/Oct/20 02:11,2.0.2,2.1.3,2.2.3,2.3.4,2.4.7,3.0.1,3.1.0,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"Literal ArrayBasedMapData has inconsistent string representation from `LogicalPlan` to `Optimized Logical Plan/Physical Plan`. Also, the representation at `Optimized Logical Plan` and `Physical Plan` is ambiguous like `keys: [key1], values: [value1] AS c#0`.

{code}
scala> spark.version
res0: String = 2.4.7

scala> sql(""SELECT map('key1', 'value1') c"").explain(true)
== Parsed Logical Plan ==
'Project ['map(key1, value1) AS c#0]
+- OneRowRelation

== Analyzed Logical Plan ==
c: map<string,string>
Project [map(key1, value1) AS c#0]
+- OneRowRelation

== Optimized Logical Plan ==
Project [keys: [key1], values: [value1] AS c#0]
+- OneRowRelation

== Physical Plan ==
*(1) Project [keys: [key1], values: [value1] AS c#0]
+- Scan OneRowRelation[]
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 30 02:55:01 UTC 2020,,,,,,,,,,"0|z0k4u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/20 23:01;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30190;;;","30/Oct/20 02:11;dongjoon;Issue resolved by pull request 30190
[https://github.com/apache/spark/pull/30190];;;","30/Oct/20 02:54;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30196;;;","30/Oct/20 02:55;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30196;;;",,,,,,,,,,,,,,,,,,,,,,
"In the Storage UI page, clicking any field to sort the table will cause the header content to be lost",SPARK-33284,13337827,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,echohlne,echohlne,echohlne,29/Oct/20 08:14,03/Nov/20 14:51,13/Jul/23 08:50,03/Nov/20 14:51,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,Web UI,,,,0,,,,"Spark Version: spark3.0.1

problem：In the Storage UI page, clicking any field to sort the table will cause the header content to be lost，secreen gif  is shown in the attachment.

 ",,apachespark,echohlne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/20 08:15;echohlne;reproduce.gif;https://issues.apache.org/jira/secure/attachment/13014340/reproduce.gif",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 03 14:51:00 UTC 2020,,,,,,,,,,"0|z0k40o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/20 08:24;apachespark;User 'akiyamaneko' has created a pull request for this issue:
https://github.com/apache/spark/pull/30182;;;","29/Oct/20 08:25;apachespark;User 'akiyamaneko' has created a pull request for this issue:
https://github.com/apache/spark/pull/30182;;;","03/Nov/20 14:51;srowen;Resolved by https://github.com/apache/spark/pull/30182;;;",,,,,,,,,,,,,,,,,,,,,,,
Python/Pandas UDF right after off-heap vectorized reader could cause executor crash.,SPARK-33277,13337771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,29/Oct/20 00:11,12/Dec/22 18:10,13/Jul/23 08:50,23/Dec/20 22:51,2.4.7,3.0.1,,,,,,,,,,2.4.8,3.0.2,3.1.0,3.2.1,PySpark,SQL,,,0,,,,"Python/Pandas UDF right after off-heap vectorized reader could cause executor crash.

E.g.,:
{code:java}
spark.range(0, 100000, 1, 1).write.parquet(path)

spark.conf.set(""spark.sql.columnVector.offheap.enabled"", True)

def f(x):
    return 0

fUdf = udf(f, LongType())

spark.read.parquet(path).select(fUdf('id')).head()
{code}
This is because, the Python evaluation consumes the parent iterator in a separate thread and it consumes more data from the parent even after the task ends and the parent is closed. If an off-heap column vector exists in the parent iterator, it could cause segmentation fault which crashes the executor.",,ankurd,apachespark,dongjoon,ueshin,viirya,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-39084,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 11 18:12:14 UTC 2021,,,,,,,,,,"0|z0k3o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/20 00:24;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30177;;;","29/Oct/20 00:24;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30177;;;","01/Nov/20 11:29;gurwls223;Fixed in https://github.com/apache/spark/pull/30177;;;","01/Nov/20 20:10;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30217;;;","01/Nov/20 20:11;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30217;;;","01/Nov/20 21:04;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30218;;;","01/Nov/20 21:05;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30218;;;","04/Nov/20 00:56;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30242;;;","04/Nov/20 00:56;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30242;;;","05/Nov/20 07:17;gurwls223;Reverted in:

master: https://github.com/apache/spark/commit/d530ed0ea8bdba09fba6dcd51f8e4f7745781c2e
branch-3.0: https://github.com/apache/spark/commit/74d8eacbe9cdc0b25a177543eb48ac54bd065cbb
branch-2.4: https://github.com/apache/spark/commit/c342bcd4c4ba68506ca6b459bd3a9c688d2aecfa;;;","23/Dec/20 04:07;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30899;;;","23/Dec/20 22:51;dongjoon;This is resolved via https://github.com/apache/spark/pull/30899;;;","23/Dec/20 23:03;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/30913;;;","11/Oct/21 18:11;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/34245;;;","11/Oct/21 18:12;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/34245;;;",,,,,,,,,,,
Fix Flaky Test: ThriftServerQueryTestSuite. subquery_scalar_subquery_scalar_subquery_select_sql,SPARK-33273,13337697,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,dongjoon,dongjoon,28/Oct/20 15:56,12/Dec/22 18:11,13/Jul/23 08:50,15/Dec/20 09:31,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,Tests,,,0,correctness,,,"- https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/130369/testReport/org.apache.spark.sql.hive.thriftserver/ThriftServerQueryTestSuite/subquery_scalar_subquery_scalar_subquery_select_sql/

{code}
[info] - subquery/scalar-subquery/scalar-subquery-select.sql *** FAILED *** (3 seconds, 877 milliseconds)
[info]   Expected ""[1]0	2017-05-04 01:01:0..."", but got ""[]0	2017-05-04 01:01:0..."" Result did not match for query #3
[info]   SELECT (SELECT min(t3d) FROM t3) min_t3d,
[info]          (SELECT max(t2h) FROM t2) max_t2h
[info]   FROM   t1
[info]   WHERE  t1a = 'val1c' (ThriftServerQueryTestSuite.scala:197)
{code}",,apachespark,dongjoon,ganeshraju,gsomogyi,huangtianhua,Qin Yao,ulysses,yumwang,,,,,,,,,,,,,,,,,,,SPARK-33119,,,,,,,,,"03/Nov/20 18:28;dongjoon;failures.png;https://issues.apache.org/jira/secure/attachment/13014662/failures.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 15 09:31:22 UTC 2020,,,,,,,,,,"0|z0k37s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/20 13:15;gsomogyi;I've just faced with this too: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/130409/testReport/org.apache.spark.sql.hive.thriftserver/ThriftServerQueryTestSuite/subquery_scalar_subquery_scalar_subquery_select_sql/;;;","29/Oct/20 15:07;dongjoon;Thank you for sharing, [~gsomogyi]!;;;","29/Oct/20 23:24;dongjoon;This happens again on master branch again.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/lastCompletedBuild/testReport/org.apache.spark.sql.hive.thriftserver/ThriftServerQueryTestSuite/subquery_scalar_subquery_scalar_subquery_select_sql/;;;","29/Oct/20 23:50;dongjoon;This failure is frequent and might be a potential correctness issue in Spark Thrift Server. I'll raise the priority of this issue.;;;","02/Nov/20 01:35;gurwls223;cc [~yumwang] do you have any clue?;;;","02/Nov/20 14:50;yumwang;I have no idea. I cannot reproduce locally.;;;","03/Nov/20 18:28;dongjoon;Even in a single profile combination, this fails 3 times out of 6 runs recently.
 !failures.png! ;;;","10/Dec/20 08:27;Qin Yao;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/132529/testReport/org.apache.spark.sql/SQLQueryTestSuite/subquery_scalar_subquery_scalar_subquery_select_sql/

SQLQueryTestSuite fails too;;;","10/Dec/20 09:59;dongjoon;Thanks for sharing, [~Qin Yao].;;;","14/Dec/20 01:25;huangtianhua;Test scalar-subquery-select.sql is failed on arm64 job, see https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-arm/513/testReport/org.apache.spark.sql.hive.thriftserver/ThriftServerQueryTestSuite/subquery_scalar_subquery_scalar_subquery_select_sql/  https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-arm/516/testReport/org.apache.spark.sql.hive.thriftserver/ThriftServerQueryTestSuite/subquery_scalar_subquery_scalar_subquery_select_sql/;;;","14/Dec/20 08:27;dongjoon;[~huangtianhua]. This is a long-standing general issue. :);;;","14/Dec/20 08:28;dongjoon;BTW, [~sarutak] provided another reproduced result here.
- https://github.com/apache/spark/pull/30755

According to his result, this is really a correctness issue and a blocker for Apache Spark 3.1.0.;;;","14/Dec/20 08:41;gurwls223;cc [~maryannxue] and [~cloud_fan];;;","14/Dec/20 08:50;Qin Yao;
{code:java}
## once
build/sbt clean -Phive-2.3 -Pkinesis-asl -Pspark-ganglia-lgpl -Pyarn -Phive-thriftserver test:package streaming-kinesis-asl-assembly/assembly -Dsbt.override.build.repos=true
{code}


{code:java}
build/sbt -Phive  ""sql/testOnly *SQLQueryTestSuite -- -z scalar-subquery-select.sql""

{code}


the possibility goes up to 5-10% when I run the 2nd command repeatedly on my mac

the SQL that hits the failure is not certain, but it seems always failed for the output `min(t3d) ` w/ `0` as its wrong result.

here is one of the debug msg
{code:java}
14:22:32.702 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[10,2017-05-04 01:01:00.0]
14:22:33.481 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1a,2]
14:22:33.885 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[10,null]
14:22:33.885 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[null,2017-05-04 01:01:00.0]
14:22:34.329 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[12]
14:22:34.918 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[null,val2a,null,200.83333333333334]
14:22:34.918 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1c,val1c,0,200.83333333333334]
14:22:34.918 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1c,val1c,0,200.83333333333334]
14:22:34.918 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1e,null,0,null]
14:22:34.918 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1e,null,0,null]
14:22:34.918 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1e,null,0,null]
14:22:35.240 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[19,2017-05-04 01:01:00.0]
14:22:35.636 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[19]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1a,6]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1a,16]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1a,16]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1c,8]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1d,null]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1a,6]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1d,null]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1e,10]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1e,10]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1d,10]
14:22:36.183 ERROR org.apache.spark.sql.execution.HiveResult: debugging ==>[val1e,10]
14:22:36.206 ERROR org.apache.spark.sql.SQLQueryTestSuite: Error using configs:
[info] - subquery/scalar-subquery/scalar-subquery-select.sql *** FAILED *** (4 seconds, 522 milliseconds)
[info]   subquery/scalar-subquery/scalar-subquery-select.sql
[info]   Expected ""...3333334
[info]   val1c	val1c	[10	200.83333333333334
[info]   val1c	val1c	10	200.83333333333334
[info]   val1e	NULL	10	NULL
[info]   val1e	NULL	10	NULL
[info]   val1e	NULL	1]0	NULL"", but got ""...3333334
[info]   val1c	val1c	[0	200.83333333333334
[info]   val1c	val1c	0	200.83333333333334
[info]   val1e	NULL	0	NULL
[info]   val1e	NULL	0	NULL
[info]   val1e	NULL	]0	NULL"" Result did not match for query #8
[info]   SELECT q1.t1a, q2.t2a, q1.min_t3d, q2.avg_t3d
[info]   FROM   (SELECT t1a, (SELECT min(t3d) FROM t3) min_t3d
[info]           FROM   t1
[info]           WHERE  t1a IN ('val1e', 'val1c')) q1
[info]          FULL OUTER JOIN
[info]          (SELECT t2a, (SELECT avg(t3d) FROM t3) avg_t3d
[info]           FROM   t2
[info]           WHERE  t2a IN ('val1c', 'val2a')) q2
[info]   ON     q1.t1a = q2.t2a
[info]   AND    q1.min_t3d < q2.avg_t3d (SQLQueryTestSuite.scala:464)
[info]   org.scalatest.exceptions.TestFailedException:
{code}
;;;","14/Dec/20 08:54;Qin Yao;The msg pattern ‘debugging ==>’ is printed w/ sparkPlan.executeCollectPublic()

I also have set the compression code from lz4 to zstd, and the problem still there, that said, the compression/decompression is not related;;;","14/Dec/20 18:00;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/30765;;;","14/Dec/20 18:58;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30766;;;","14/Dec/20 18:58;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/30766;;;","15/Dec/20 09:31;gurwls223;Fixed in https://github.com/apache/spark/pull/30765;;;",,,,,,,
from_json() cannot parse schema from schema_of_json(),SPARK-33270,13337677,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,28/Oct/20 14:06,12/Dec/22 18:10,13/Jul/23 08:50,29/Oct/20 01:31,3.0.2,3.1.0,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"For example:
{code:scala}
val in = Seq(""""""{""a b"": 1}"""""").toDS()
in.select(from_json('value, schema_of_json(""""""{""a b"": 100}"""""")) as ""parsed"")
{code}
This fails with the exception:
{code:java}
org.apache.spark.sql.catalyst.parser.ParseException: 
extraneous input '<' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 6)

== SQL ==
struct<a b:bigint>
------^^^

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:263)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:130)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseTableSchema(ParseDriver.scala:76)
	at org.apache.spark.sql.types.DataType$.fromDDL(DataType.scala:131)
	at org.apache.spark.sql.catalyst.expressions.ExprUtils$.evalTypeExpr(ExprUtils.scala:33)
	at org.apache.spark.sql.catalyst.expressions.JsonToStructs.<init>(jsonExpressions.scala:537)
	at org.apache.spark.sql.functions$.from_json(functions.scala:4141)
	at org.apache.spark.sql.functions$.from_json(functions.scala:4124)
{code}",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 29 01:31:30 UTC 2020,,,,,,,,,,"0|z0k33c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/20 14:22;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30172;;;","29/Oct/20 01:31;gurwls223;Issue resolved by pull request 30172
[https://github.com/apache/spark/pull/30172];;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix bugs for casting data from/to PythonUserDefinedType,SPARK-33268,13337658,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,28/Oct/20 12:22,30/Oct/20 02:44,13/Jul/23 08:50,28/Oct/20 15:35,2.4.8,3.0.2,3.1.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,PySpark,SQL,,,0,,,,"This PR intends to fix bus for casting data from/to PythonUserDefinedType. A sequence of queries to reproduce this issue is as follows;

{code} 
>>> from pyspark.sql import Row
>>> from pyspark.sql.functions import col
>>> from pyspark.sql.types import *
>>> from pyspark.testing.sqlutils import *
>>> 
>>> row = Row(point=ExamplePoint(1.0, 2.0))
>>> df = spark.createDataFrame([row])
>>> df.select(col(""point"").cast(PythonOnlyUDT()))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/maropu/Repositories/spark/spark-master/python/pyspark/sql/dataframe.py"", line 1402, in select
    jdf = self._jdf.select(self._jcols(*cols))
  File ""/Users/maropu/Repositories/spark/spark-master/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1305, in __call__
  File ""/Users/maropu/Repositories/spark/spark-master/python/pyspark/sql/utils.py"", line 111, in deco
    return f(*a, **kw)
  File ""/Users/maropu/Repositories/spark/spark-master/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py"", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o44.select.
: java.lang.NullPointerException
	at org.apache.spark.sql.types.UserDefinedType.acceptsType(UserDefinedType.scala:84)
	at org.apache.spark.sql.catalyst.expressions.Cast$.canCast(Cast.scala:96)
	at org.apache.spark.sql.catalyst.expressions.CastBase.checkInputDataTypes(Cast.scala:267)
	at org.apache.spark.sql.catalyst.expressions.CastBase.resolved$lzycompute(Cast.scala:290)
	at org.apache.spark.sql.catalyst.expressions.CastBase.resolved(Cast.scala:290)}}
{code} 

A root cause of this issue is that, since {{PythonUserDefinedType#userClassis}} always null, {{isAssignableFrom}} in {{UserDefinedType#acceptsType}} throws a null exception. To fix it, this PR defines {{acceptsType}} in {{PythonUserDefinedType}} and filters out the null case in {{UserDefinedType#acceptsType}}.",,apachespark,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 29 23:30:54 UTC 2020,,,,,,,,,,"0|z0k2z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/20 12:24;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30169;;;","28/Oct/20 12:25;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30169;;;","28/Oct/20 15:35;dongjoon;This is resolved via https://github.com/apache/spark/pull/30169;;;","29/Oct/20 23:19;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30191;;;","29/Oct/20 23:20;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30191;;;","29/Oct/20 23:30;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/30192;;;",,,,,,,,,,,,,,,,,,,,
"Query with having null in ""in"" condition against data source V2 source table supporting push down filter fails with NPE",SPARK-33267,13337655,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kabhwan,kabhwan,kabhwan,28/Oct/20 12:14,20/Feb/22 16:25,13/Jul/23 08:50,28/Oct/20 17:05,3.0.0,3.0.1,3.1.0,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"The query with having null in ""in"" condition against data source V2 source table supporting push down filter fails with NPE.

{code}
scala> spark.sql(""SELECT * FROM catalog.default.t1 WHERE id IN (1, null)"").show()
java.lang.NullPointerException
  at org.apache.spark.sql.sources.In.$anonfun$hashCode$1(filters.scala:167)
  at org.apache.spark.sql.sources.In.$anonfun$hashCode$1$adapted(filters.scala:165)
  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
  at org.apache.spark.sql.sources.In.hashCode(filters.scala:165)
  at scala.runtime.Statics.anyHash(Statics.java:122)
  at scala.collection.mutable.HashTable$HashUtils.elemHashCode(HashTable.scala:416)
  at scala.collection.mutable.HashTable$HashUtils.elemHashCode$(HashTable.scala:416)
  at scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:44)
  at scala.collection.mutable.HashTable.findOrAddEntry(HashTable.scala:168)
  at scala.collection.mutable.HashTable.findOrAddEntry$(HashTable.scala:167)
  at scala.collection.mutable.HashMap.findOrAddEntry(HashMap.scala:44)
  at scala.collection.mutable.HashMap.put(HashMap.scala:126)
  at scala.collection.mutable.HashMap.update(HashMap.scala:131)
  at org.apache.spark.sql.execution.datasources.DataSourceStrategy$.translateFilterWithMapping(DataSourceStrategy.scala:576)
  at org.apache.spark.sql.execution.datasources.v2.PushDownUtils$.$anonfun$pushFilters$1(PushDownUtils.scala:52)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.execution.datasources.v2.PushDownUtils$.pushFilters(PushDownUtils.scala:49)
  at org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$$anonfun$apply$1.applyOrElse(V2ScanRelationPushDown.scala:44)
  at org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$$anonfun$apply$1.applyOrElse(V2ScanRelationPushDown.scala:32)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.apply(V2ScanRelationPushDown.scala:32)
  at org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.apply(V2ScanRelationPushDown.scala:29)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:149)
  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  at scala.collection.immutable.List.foldLeft(List.scala:89)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:146)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:138)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:138)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:116)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:116)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:82)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:133)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:133)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:82)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:79)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$writePlans$4(QueryExecution.scala:197)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$.append(QueryPlan.scala:381)
  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$writePlans(QueryExecution.scala:197)
  at org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:207)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:95)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:824)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:783)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:792)
{code}",,apachespark,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 17:05:47 UTC 2020,,,,,,,,,,"0|z0k2yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/20 12:16;kabhwan;Will submit a PR soon.;;;","28/Oct/20 12:29;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/30170;;;","28/Oct/20 17:05;dongjoon;Issue resolved by pull request 30170
[https://github.com/apache/spark/pull/30170];;;",,,,,,,,,,,,,,,,,,,,,,,
SortExec produces incorrect results if sortOrder is a Stream,SPARK-33260,13337398,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,ankurd,ankurd,27/Oct/20 11:00,16/Nov/20 05:50,13/Jul/23 08:50,27/Oct/20 20:20,3.0.0,3.0.1,3.1.0,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,correctness,,,"The following query produces incorrect results. The query has two essential features: (1) it contains a string aggregate, resulting in a {{SortExec}} node, and (2) it contains a duplicate grouping key, causing {{RemoveRepetitionFromGroupExpressions}} to produce a sort order stored as a Stream.

SELECT bigint_col_1, bigint_col_9, MAX(CAST(bigint_col_1 AS string))
FROM table_4
GROUP BY bigint_col_1, bigint_col_9, bigint_col_9

When the sort order is stored as a {{Stream}}, the line {{ordering.map(_.child.genCode(ctx))}} in {{GenerateOrdering#createOrderKeys()}} produces unpredictable side effects to {{ctx}}. This is because {{genCode(ctx)}} modifies {{ctx}}. When {{ordering}} is a {{Stream}}, the modifications will not happen immediately as intended, but will instead occur lazily when the returned {{Stream}} is used later.

Similar bugs have occurred at least three times in the past: https://issues.apache.org/jira/browse/SPARK-24500, https://issues.apache.org/jira/browse/SPARK-25767, https://issues.apache.org/jira/browse/SPARK-26680.

The fix is to check if {{ordering}} is a {{Stream}} and force the modifications to happen immediately if so.",,ankurd,apachespark,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24500,SPARK-26680,SPARK-25767,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 27 20:22:38 UTC 2020,,,,,,,,,,"0|z0k1dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/20 11:31;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/30160;;;","27/Oct/20 11:31;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/30160;;;","27/Oct/20 20:20;dongjoon;Issue resolved by pull request 30160
[https://github.com/apache/spark/pull/30160];;;","27/Oct/20 20:22;dongjoon;I added `correctness` label.;;;",,,,,,,,,,,,,,,,,,,,,,
Joining 3 streams results in incorrect output,SPARK-33259,13337383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,hiddenbit,hiddenbit,27/Oct/20 09:49,12/Nov/20 23:32,13/Jul/23 08:50,12/Nov/20 23:32,3.0.1,,,,,,,,,,,3.1.0,,,,Structured Streaming,,,,0,correctness,,,"I encountered an issue with Structured Streaming when doing a ((A LEFT JOIN B) INNER JOIN C) operation. Below you can see example code I [posted on Stackoverflow|https://stackoverflow.com/questions/64503539/]...

I created a minimal example of ""sessions"", that have ""start"" and ""end"" events and optionally some ""metadata"".

The script generates two outputs: {{sessionStartsWithMetadata}} result from ""start"" events that are left-joined with the ""metadata"" events, based on {{sessionId}}. A ""left join"" is used, since we like to get an output event even when no corresponding metadata exists.

Additionally a DataFrame {{endedSessionsWithMetadata}} is created by joining ""end"" events to the previously created DataFrame. Here an ""inner join"" is used, since we only want some output when a session has ended for sure.

This code can be executed in {{spark-shell}}:
{code:scala}
import java.sql.Timestamp
import org.apache.spark.sql.execution.streaming.{MemoryStream, StreamingQueryWrapper}
import org.apache.spark.sql.streaming.StreamingQuery
import org.apache.spark.sql.{DataFrame, SQLContext}
import org.apache.spark.sql.functions.{col, expr, lit}

import spark.implicits._
implicit val sqlContext: SQLContext = spark.sqlContext

// Main data processing, regardless whether batch or stream processing
def process(
    sessionStartEvents: DataFrame,
    sessionOptionalMetadataEvents: DataFrame,
    sessionEndEvents: DataFrame
): (DataFrame, DataFrame) = {
  val sessionStartsWithMetadata: DataFrame = sessionStartEvents
    .join(
      sessionOptionalMetadataEvents,
      sessionStartEvents(""sessionId"") === sessionOptionalMetadataEvents(""sessionId"") &&
        sessionStartEvents(""sessionStartTimestamp"").between(
          sessionOptionalMetadataEvents(""sessionOptionalMetadataTimestamp"").minus(expr(s""INTERVAL 1 seconds"")),
          sessionOptionalMetadataEvents(""sessionOptionalMetadataTimestamp"").plus(expr(s""INTERVAL 1 seconds""))
        ),
      ""left"" // metadata is optional
    )
    .select(
      sessionStartEvents(""sessionId""),
      sessionStartEvents(""sessionStartTimestamp""),
      sessionOptionalMetadataEvents(""sessionOptionalMetadataTimestamp"")
    )

  val endedSessionsWithMetadata = sessionStartsWithMetadata.join(
    sessionEndEvents,
    sessionStartsWithMetadata(""sessionId"") === sessionEndEvents(""sessionId"") &&
      sessionStartsWithMetadata(""sessionStartTimestamp"").between(
        sessionEndEvents(""sessionEndTimestamp"").minus(expr(s""INTERVAL 10 seconds"")),
        sessionEndEvents(""sessionEndTimestamp"")
      )
  )

  (sessionStartsWithMetadata, endedSessionsWithMetadata)
}

def streamProcessing(
    sessionStartData: Seq[(Timestamp, Int)],
    sessionOptionalMetadata: Seq[(Timestamp, Int)],
    sessionEndData: Seq[(Timestamp, Int)]
): (StreamingQuery, StreamingQuery) = {

  val sessionStartEventsStream: MemoryStream[(Timestamp, Int)] = MemoryStream[(Timestamp, Int)]
  sessionStartEventsStream.addData(sessionStartData)

  val sessionStartEvents: DataFrame = sessionStartEventsStream
    .toDS()
    .toDF(""sessionStartTimestamp"", ""sessionId"")
    .withWatermark(""sessionStartTimestamp"", ""1 second"")

  val sessionOptionalMetadataEventsStream: MemoryStream[(Timestamp, Int)] = MemoryStream[(Timestamp, Int)]
  sessionOptionalMetadataEventsStream.addData(sessionOptionalMetadata)

  val sessionOptionalMetadataEvents: DataFrame = sessionOptionalMetadataEventsStream
    .toDS()
    .toDF(""sessionOptionalMetadataTimestamp"", ""sessionId"")
    .withWatermark(""sessionOptionalMetadataTimestamp"", ""1 second"")

  val sessionEndEventsStream: MemoryStream[(Timestamp, Int)] = MemoryStream[(Timestamp, Int)]
  sessionEndEventsStream.addData(sessionEndData)

  val sessionEndEvents: DataFrame = sessionEndEventsStream
    .toDS()
    .toDF(""sessionEndTimestamp"", ""sessionId"")
    .withWatermark(""sessionEndTimestamp"", ""1 second"")

  val (sessionStartsWithMetadata, endedSessionsWithMetadata) =
    process(sessionStartEvents, sessionOptionalMetadataEvents, sessionEndEvents)

  val sessionStartsWithMetadataQuery = sessionStartsWithMetadata
    .select(lit(""sessionStartsWithMetadata""), col(""*"")) // Add label to see which query's output it is
    .writeStream
    .outputMode(""append"")
    .format(""console"")
    .option(""truncate"", ""false"")
    .option(""numRows"", ""1000"")
    .start()

  val endedSessionsWithMetadataQuery = endedSessionsWithMetadata
    .select(lit(""endedSessionsWithMetadata""), col(""*"")) // Add label to see which query's output it is
    .writeStream
    .outputMode(""append"")
    .format(""console"")
    .option(""truncate"", ""false"")
    .option(""numRows"", ""1000"")
    .start()

  (sessionStartsWithMetadataQuery, endedSessionsWithMetadataQuery)
}

def batchProcessing(
    sessionStartData: Seq[(Timestamp, Int)],
    sessionOptionalMetadata: Seq[(Timestamp, Int)],
    sessionEndData: Seq[(Timestamp, Int)]
): Unit = {

  val sessionStartEvents = spark.createDataset(sessionStartData).toDF(""sessionStartTimestamp"", ""sessionId"")
  val sessionOptionalMetadataEvents = spark.createDataset(sessionOptionalMetadata).toDF(""sessionOptionalMetadataTimestamp"", ""sessionId"")
  val sessionEndEvents = spark.createDataset(sessionEndData).toDF(""sessionEndTimestamp"", ""sessionId"")

  val (sessionStartsWithMetadata, endedSessionsWithMetadata) =
    process(sessionStartEvents, sessionOptionalMetadataEvents, sessionEndEvents)

  println(""sessionStartsWithMetadata"")
  sessionStartsWithMetadata.show(100, truncate = false)

  println(""endedSessionsWithMetadata"")
  endedSessionsWithMetadata.show(100, truncate = false)
}


// Data is represented as tuples of (eventTime, sessionId)...
val sessionStartData = Vector(
  (new Timestamp(1), 0),
  (new Timestamp(2000), 1),
  (new Timestamp(2000), 2),
  (new Timestamp(20000), 10)
)

val sessionOptionalMetadata = Vector(
  (new Timestamp(1), 0),
  // session `1` has no metadata
  (new Timestamp(2000), 2),
  (new Timestamp(20000), 10)
)

val sessionEndData = Vector(
  (new Timestamp(10000), 0),
  (new Timestamp(11000), 1),
  (new Timestamp(12000), 2),
  (new Timestamp(30000), 10)
)

batchProcessing(sessionStartData, sessionOptionalMetadata, sessionEndData)

val (sessionStartsWithMetadataQuery, endedSessionsWithMetadataQuery) =
  streamProcessing(sessionStartData, sessionOptionalMetadata, sessionEndData)
{code}
In the example session with ID {{1}} has no metadata, so the respective metadata column is {{null}}.

The main functionality of joining the data is implemented in {{def process(…)}}, which is called using both batch data and stream data.

In the batch version the output is as expected:
{noformat}
sessionStartsWithMetadata
+---------+-----------------------+--------------------------------+
|sessionId|sessionStartTimestamp  |sessionOptionalMetadataTimestamp|
+---------+-----------------------+--------------------------------+
|0        |1970-01-01 01:00:00.001|1970-01-01 01:00:00.001         |
|1        |1970-01-01 01:00:02    |null                            | ← has no metadata ✔
|2        |1970-01-01 01:00:02    |1970-01-01 01:00:02             |
|10       |1970-01-01 01:00:20    |1970-01-01 01:00:20             |
+---------+-----------------------+--------------------------------+

endedSessionsWithMetadata
+---------+-----------------------+--------------------------------+-------------------+---------+
|sessionId|sessionStartTimestamp  |sessionOptionalMetadataTimestamp|sessionEndTimestamp|sessionId|
+---------+-----------------------+--------------------------------+-------------------+---------+
|0        |1970-01-01 01:00:00.001|1970-01-01 01:00:00.001         |1970-01-01 01:00:10|0        |
|1        |1970-01-01 01:00:02    |null                            |1970-01-01 01:00:11|1        |  ← has no metadata ✔
|2        |1970-01-01 01:00:02    |1970-01-01 01:00:02             |1970-01-01 01:00:12|2        |
|10       |1970-01-01 01:00:20    |1970-01-01 01:00:20             |1970-01-01 01:00:30|10       |
+---------+-----------------------+--------------------------------+-------------------+---------+
{noformat}
But when the same processing is run as stream processing the output of {{endedSessionsWithMetadata}} does not contain the entry of session {{1}} that has no metadata:
{noformat}
-------------------------------------------
Batch: 0 (""start event"")
-------------------------------------------
+-------------------------+---------+-----------------------+--------------------------------+
|sessionStartsWithMetadata|sessionId|sessionStartTimestamp  |sessionOptionalMetadataTimestamp|
+-------------------------+---------+-----------------------+--------------------------------+
|sessionStartsWithMetadata|10       |1970-01-01 01:00:20    |1970-01-01 01:00:20             |
|sessionStartsWithMetadata|2        |1970-01-01 01:00:02    |1970-01-01 01:00:02             |
|sessionStartsWithMetadata|0        |1970-01-01 01:00:00.001|1970-01-01 01:00:00.001         |
+-------------------------+---------+-----------------------+--------------------------------+

-------------------------------------------
Batch: 0 (""end event"")
-------------------------------------------
+-------------------------+---------+-----------------------+--------------------------------+-------------------+---------+
|endedSessionsWithMetadata|sessionId|sessionStartTimestamp  |sessionOptionalMetadataTimestamp|sessionEndTimestamp|sessionId|
+-------------------------+---------+-----------------------+--------------------------------+-------------------+---------+
|endedSessionsWithMetadata|10       |1970-01-01 01:00:20    |1970-01-01 01:00:20             |1970-01-01 01:00:30|10       |
|endedSessionsWithMetadata|2        |1970-01-01 01:00:02    |1970-01-01 01:00:02             |1970-01-01 01:00:12|2        |
|endedSessionsWithMetadata|0        |1970-01-01 01:00:00.001|1970-01-01 01:00:00.001         |1970-01-01 01:00:10|0        |
+-------------------------+---------+-----------------------+--------------------------------+-------------------+---------+

-------------------------------------------
Batch: 1 (""start event"")
-------------------------------------------
+-------------------------+---------+---------------------+--------------------------------+
|sessionStartsWithMetadata|sessionId|sessionStartTimestamp|sessionOptionalMetadataTimestamp|
+-------------------------+---------+---------------------+--------------------------------+
|sessionStartsWithMetadata|1        |1970-01-01 01:00:02  |null                            | ← has no metadata ✔
+-------------------------+---------+---------------------+--------------------------------+

-------------------------------------------
Batch: 1 (""end event"")
-------------------------------------------
+-------------------------+---------+---------------------+--------------------------------+-------------------+---------+
|endedSessionsWithMetadata|sessionId|sessionStartTimestamp|sessionOptionalMetadataTimestamp|sessionEndTimestamp|sessionId|
+-------------------------+---------+---------------------+--------------------------------+-------------------+---------+
+-------------------------+---------+---------------------+--------------------------------+-------------------+---------+
  ↳ ✘ here I would have expected a line with sessionId=1, that has ""start"" and ""end"" information, but no ""metadata"" ✘
{noformat}
In a response it was suggested the issue looks related to [~kabhwan]'s [mailing list post|http://apache-spark-developers-list.1001551.n3.nabble.com/correctness-issue-on-chained-streaming-streaming-join-td27358.html], but since I couldn't find a ticket here tracking the above mentioned issue, I'm creating this one.",,apachespark,csun,dongjoon,hiddenbit,kabhwan,tgraves,viirya,,,,,,,,,,,,,,,,,,,SPARK-28094,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 12 23:32:31 UTC 2020,,,,,,,,,,"0|z0k1a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/20 13:05;kabhwan;As you already figured out, this is a known limitation, and at least for now we ended up with documenting such limitation to compensate.

https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#limitation-of-global-watermark

This requires major change on the concept of watermark, so without huge demand on this it may be unlikely to be addressed.;;;","27/Oct/20 13:21;hiddenbit;Oh I see, thanks for pointing to the section in the documentation!

For our team's use case having this working would definitely be very useful, as we do a lot of such data joins...;;;","30/Oct/20 17:08;dongjoon;Is there a way to block this at the analysis stage instead of giving an incorrect output?

cc [~viirya] and [~dbtsai];;;","30/Oct/20 21:57;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30210;;;","30/Oct/20 21:58;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30210;;;","31/Oct/20 18:05;dongjoon;According to the JIRA report, I added `correctness` label here, [~kabhwan] and [~viirya].;;;","02/Nov/20 06:47;kabhwan;I'll also link to the first JIRA issue which the problem was pointed out earlier.;;;","12/Nov/20 23:32;dongjoon;Issue resolved by pull request 30210
[https://github.com/apache/spark/pull/30210];;;",,,,,,,,,,,,,,,,,,
Add asc_nulls_* and desc_nulls_* methods to SparkR,SPARK-33258,13337372,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,27/Oct/20 08:51,12/Dec/22 18:10,13/Jul/23 08:50,28/Oct/20 00:46,3.1.0,,,,,,,,,,,3.1.0,,,,R,SQL,,,0,,,,"At the moment Spark provides only

- {{asc}}
- {{desc}}

but {{NULL}} handling variants

- {{asc_nulls_first}}
- {{asc_nulls_last}}
- {{desc_nulls_first}}
- {{desc_nulls_last}}

are missing.",,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 00:46:40 UTC 2020,,,,,,,,,,"0|z0k17k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/20 10:00;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/30159;;;","27/Oct/20 10:00;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/30159;;;","28/Oct/20 00:46;gurwls223;Issue resolved by pull request 30159
[https://github.com/apache/spark/pull/30159];;;",,,,,,,,,,,,,,,,,,,,,,,
"Support Column inputs in PySpark ordering functions (asc*, desc*)",SPARK-33257,13337371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,27/Oct/20 08:47,12/Dec/22 18:10,13/Jul/23 08:50,03/Nov/20 13:51,3.1.0,,,,,,,,,,,3.1.0,,,,PySpark,SQL,,,1,,,,"According to SPARK-26979, PySpark functions should support both {{Column}} and {{str}} arguments, when possible.

However, the following ordering support only {{str}}

- {{asc}}
- {{desc}}
- {{asc_nulls_first}}
- {{asc_nulls_last}}
- {{desc_nulls_first}}
- {{desc_nulls_last}}

support only {{str}}. This is because Scala side doesn't provide {{Column => Column}} variants.

To fix this, we do one of the following:

- Call corresponding {{Column}} methods as [suggested|https://github.com/apache/spark/pull/30143#discussion_r512366978] by  [~hyukjin.kwon]
- Add missing signatures on Scala side.",,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 03 13:51:54 UTC 2020,,,,,,,,,,"0|z0k17c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/20 13:46;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/30227;;;","02/Nov/20 13:47;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/30227;;;","03/Nov/20 13:51;gurwls223;Issue resolved by pull request 30227
[https://github.com/apache/spark/pull/30227];;;",,,,,,,,,,,,,,,,,,,,,,,
Fail fast when fails to instantiate configured v2 session catalog,SPARK-33240,13337131,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,26/Oct/20 05:23,28/Oct/20 04:56,13/Jul/23 08:50,28/Oct/20 03:31,3.0.0,3.0.1,3.1.0,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"Now Spark fails back to use ""default catalog"" when Spark fails to instantiate configured v2 session catalog.

While the error log message says nothing about why the instantiation has been failing and the error log message pollutes the log file (as it's logged every time when resolving the catalog), it should be considered as ""incorrect"" behavior as end users are intended to set the custom catalog and Spark sometimes ignores it, which is against the intention.

We should simply fail in the case so that end users indicate the failure earlier and try to fix the issue.",,apachespark,cloud_fan,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 04:56:21 UTC 2020,,,,,,,,,,"0|z0jzq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/20 05:23;kabhwan;Will work on it.;;;","26/Oct/20 06:31;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/30147;;;","26/Oct/20 06:32;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/30147;;;","28/Oct/20 03:31;cloud_fan;Issue resolved by pull request 30147
[https://github.com/apache/spark/pull/30147];;;","28/Oct/20 04:55;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/30167;;;","28/Oct/20 04:56;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/30167;;;",,,,,,,,,,,,,,,,,,,,
FileOutputWriter jobs have duplicate JobIDs if launched in same second,SPARK-33230,13336928,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,23/Oct/20 13:39,09/Nov/20 20:02,13/Jul/23 08:50,26/Oct/20 19:32,2.4.7,3.0.1,,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"The Hadoop S3A staging committer has problems with >1 spark sql query being launched simultaneously, as it uses the jobID for its path in the clusterFS to pass the commit information from tasks to job committer. 

If two queries are launched in the same second, they conflict and the output of job 1 includes that of all job2 files written so far; job 2 will fail with FNFE.

Proposed:
job conf to set {{""spark.sql.sources.writeJobUUID""}} to the value of {{WriteJobDescription.uuid}}

That was the property name which used to serve this purpose; any committers already written which use this property will pick it up without needing any changes.",,apachespark,dongjoon,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33402,,,HADOOP-17318,,SPARK-31911,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 19:47:12 UTC 2020,,,,,,,,,,"0|z0jygw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/20 14:16;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/30141;;;","23/Oct/20 14:17;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/30141;;;","26/Oct/20 19:32;dongjoon;Issue resolved by pull request 30141
[https://github.com/apache/spark/pull/30141];;;","27/Oct/20 17:10;stevel@apache.org;Thanks. Still got some changes to work through my side to make sure the are no assumptions that app attempt is unique.

For the curious see HADOOP-17318
* Staging committer is using task attemptID (jobId+ taskId + task-attempt) for a path to the local temp dir
* Magic committer uses app attemptId for the path under the dest/__magic dir. Only an issue once that committer allows >1 job to write to same dest.

;;;","28/Oct/20 05:54;dongjoon;Thank you for sharing the status, [~stevel@apache.org]!;;;","09/Nov/20 19:47;stevel@apache.org;Related issue: classic FileOutputCommitter has the same problem if the same destination is being used simultaneously for two jobs;;;",,,,,,,,,,,,,,,,,,,,
Set upper bound of Pandas and PyArrow version in GitHub Actions in branch-2.4,SPARK-33217,13336631,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,22/Oct/20 04:12,12/Dec/22 18:11,13/Jul/23 08:50,22/Oct/20 09:18,2.4.8,,,,,,,,,,,2.4.8,,,,Project Infra,,,,0,,,,,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 22 09:18:00 UTC 2020,,,,,,,,,,"0|z0jwn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/20 04:35;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30128;;;","22/Oct/20 09:18;gurwls223;Issue resolved by pull request 30128
[https://github.com/apache/spark/pull/30128];;;",,,,,,,,,,,,,,,,,,,,,,,,
HiveExternalCatalogVersionsSuite shouldn't use or delete hard-coded /tmp directory ,SPARK-33214,13336586,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xkrogen,xkrogen,xkrogen,21/Oct/20 21:47,04/Nov/20 06:52,13/Jul/23 08:50,04/Nov/20 06:52,3.0.1,,,,,,,,,,,3.1.0,,,,SQL,Tests,,,0,,,,"In SPARK-22356, the {{sparkTestingDir}} used by {{HiveExternalCatalogVersionsSuite}} became hard-coded to enable re-use of the downloaded Spark tarball between test executions:
{code}
  // For local test, you can set `sparkTestingDir` to a static value like `/tmp/test-spark`, to
  // avoid downloading Spark of different versions in each run.
  private val sparkTestingDir = new File(""/tmp/test-spark"")
{code}
However this doesn't work, since it gets deleted every time:
{code}
  override def afterAll(): Unit = {
    try {
      Utils.deleteRecursively(wareHousePath)
      Utils.deleteRecursively(tmpDataDir)
      Utils.deleteRecursively(sparkTestingDir)
    } finally {
      super.afterAll()
    }
  }
{code}

It's bad that we're hard-coding to a {{/tmp}} directory, as in some cases this is not the proper place to store temporary files. We're not currently making any good use of it.",,apachespark,cloud_fan,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 06:52:10 UTC 2020,,,,,,,,,,"0|z0jwdc:",9223372036854775807,,,,,,,,,,,,,3.1.0,,,,,,,,,,"21/Oct/20 21:55;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/30122;;;","04/Nov/20 06:52;cloud_fan;Issue resolved by pull request 30122
[https://github.com/apache/spark/pull/30122];;;",,,,,,,,,,,,,,,,,,,,,,,,
Spark Shuffle Index Cache calculates memory usage wrong,SPARK-33206,13336476,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,larsfrancke,larsfrancke,21/Oct/20 13:10,04/Mar/22 18:53,13/Jul/23 08:50,01/Mar/22 19:34,2.4.0,3.0.1,,,,,,,,,,3.0.4,3.1.3,3.2.2,3.3.0,Shuffle,,,,0,,,,"SPARK-21501 changed the spark shuffle index service to be based on memory instead of the number of files.

Unfortunately, there's a problem with the calculation which is based on size information provided by `ShuffleIndexInformation`.

It is based purely on the file size of the cached file on disk.

We're running in OOMs with very small index files (byte size ~16 bytes) but the overhead of the ShuffleIndexInformation around this is much larger (e.g. 184 bytes, see screenshot). We need to take this into account and should probably add a fixed overhead of somewhere between 152 and 180 bytes according to my tests. I'm not 100% sure what the correct number is and it'll also depend on the architecture etc. so we can't be exact anyway.

If we do that we can maybe get rid of the size field in ShuffleIndexInformation to save a few more bytes per entry.

In effect this means that for small files we use up about 70-100 times as much memory as we intend to. Our NodeManagers OOM with 4GB and more of indexShuffleCache.

 

 ",,apachespark,attilapiros,chenyechao,dongjoon,larsfrancke,rajesh.balamohan,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/20 13:10;larsfrancke;image001(1).png;https://issues.apache.org/jira/secure/attachment/13013926/image001%281%29.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 03 14:32:22 UTC 2022,,,,,,,,,,"0|z0jvow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/20 14:15;larsfrancke;I used YourKit (thank you for the free license!) and it claims that ShuffleIndexInformation uses 152 byte of retained memory when it caches a 0 byte file.;;;","17/Feb/22 08:07;attilapiros;I am working on this soon a PR will be opened.;;;","17/Feb/22 18:50;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/35559;;;","17/Feb/22 18:51;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/35559;;;","01/Mar/22 19:34;dongjoon;This is resolved via https://github.com/apache/spark/pull/35559;;;","02/Mar/22 16:29;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/35714;;;","02/Mar/22 16:29;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/35714;;;","03/Mar/22 10:31;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/35720;;;","03/Mar/22 10:32;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/35720;;;","03/Mar/22 14:32;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/35723;;;",,,,,,,,,,,,,,,,
`Event Timeline`  in Spark Job UI sometimes cannot be opened,SPARK-33204,13336459,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,echohlne,echohlne,echohlne,21/Oct/20 11:41,28/Oct/20 01:42,13/Jul/23 08:50,26/Oct/20 12:42,3.0.1,,,,,,,,,,,3.1.0,,,,Web UI,,,,0,,,,"The Event Timeline area  cannot be expanded when a spark application has some failed jobs.

show as the attachment.",,apachespark,echohlne,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/20 11:44;echohlne;reproduce.gif;https://issues.apache.org/jira/secure/attachment/13013918/reproduce.gif",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 26 12:42:51 UTC 2020,,,,,,,,,,"0|z0jvl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/20 11:46;apachespark;User 'akiyamaneko' has created a pull request for this issue:
https://github.com/apache/spark/pull/30119;;;","26/Oct/20 12:42;Gengliang.Wang;Issue resolved by pull request 30119
[https://github.com/apache/spark/pull/30119];;;",,,,,,,,,,,,,,,,,,,,,,,,
Changes to spark.sql.analyzer.maxIterations do not take effect at runtime,SPARK-33197,13336352,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuningzh,yuningzh,yuningzh,20/Oct/20 18:23,26/Oct/20 17:50,13/Jul/23 08:50,26/Oct/20 07:21,3.0.2,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"`spark.sql.analyzer.maxIterations` is not a static conf. However, changes to it do not take effect at runtime.",,apachespark,maropu,yuningzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 26 07:21:13 UTC 2020,,,,,,,,,,"0|z0juxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/20 18:31;apachespark;User 'yuningzh-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30108;;;","20/Oct/20 18:31;apachespark;User 'yuningzh-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30108;;;","26/Oct/20 07:21;maropu;Resolved by https://github.com/apache/spark/pull/30108;;;",,,,,,,,,,,,,,,,,,,,,,,
Bug in optimizer rule EliminateSorts,SPARK-33183,13336120,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,19/Oct/20 17:54,19/Nov/20 05:07,13/Jul/23 08:50,28/Oct/20 05:52,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,2.4.6,2.4.7,2.4.8,3.0.2,3.1.0,2.4.8,3.0.2,3.1.0,,SQL,,,,0,correctness,,,"Currently, the rule {{EliminateSorts}} removes a global sort node if its child plan already satisfies the required sort order without checking if the child plan's ordering is local or global. For example, in the following scenario, the first sort shouldn't be removed because it has a stronger guarantee than the second sort even if the sort orders are the same for both sorts. 
{code:java}
Sort(orders, global = True, ...)
  Sort(orders, global = False, ...){code}
 ",,allisonwang-db,apachespark,cloud_fan,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,SPARK-23973,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 05:07:26 UTC 2020,,,,,,,,,,"0|z0jti0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/20 18:32;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30093;;;","28/Oct/20 05:52;cloud_fan;Issue resolved by pull request 30093
[https://github.com/apache/spark/pull/30093];;;","28/Oct/20 06:01;dongjoon;[~cloud_fan]. Could you label this as `correctness`?;;;","30/Oct/20 02:44;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30194;;;","30/Oct/20 02:54;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30195;;;","14/Nov/20 00:00;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30373;;;","14/Nov/20 00:01;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30373;;;","16/Nov/20 06:22;dongjoon;Hi, [~allisonwang-db]. Does Apache Spark 2.3 also has this bug?;;;","16/Nov/20 20:22;dongjoon;I added SPARK-23973 as ""is caused by"". If then, Apache Spark 2.3 seems to be okay. Please let me know if this affects older Sparks.;;;","19/Nov/20 05:07;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30420;;;",,,,,,,,,,,,,,,,
"Fix Flaky Test ""SPARK-33088: executor failed tasks trigger plugin calls""",SPARK-33173,13335838,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,17/Oct/20 00:36,17/Dec/20 13:40,13/Jul/23 08:50,17/Oct/20 04:23,3.1.0,,,,,,,,,,,3.1.0,,,,Spark Core,Tests,,,0,,,,"- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-3.2-hive-2.3-jdk-11/lastCompletedBuild/testReport/org.apache.spark.internal.plugin/PluginContainerSuite/SPARK_33088__executor_failed_tasks_trigger_plugin_calls/

{code}
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: 1 did not equal 2
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
	at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
	at org.apache.spark.internal.plugin.PluginContainerSuite.$anonfun$new$8(PluginContainerSuite.scala:161)
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33088,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 17 13:40:10 UTC 2020,,,,,,,,,,"0|z0jrrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/20 00:39;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30072;;;","17/Oct/20 04:23;dongjoon;Issue resolved by pull request 30072
[https://github.com/apache/spark/pull/30072];;;","17/Dec/20 13:39;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/30823;;;","17/Dec/20 13:40;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/30823;;;",,,,,,,,,,,,,,,,,,,,,,
Jenkins PRB is not responding,SPARK-33151,13335473,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shaneknapp,holden,holden,14/Oct/20 18:37,14/Oct/20 23:32,13/Jul/23 08:50,14/Oct/20 23:32,3.1.0,,,,,,,,,,,,,,,Project Infra,Tests,,,0,,,,I noticed there has not been a successful build recently and all the workers seem empty. When I ask jenkins to test a PR I'm not getting the normal response (  [https://github.com/apache/spark/pull/29924#issuecomment-707990107] ).,,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-14 18:37:35.0,,,,,,,,,,"0|z0jpig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encountering an invalid rolling event log folder prevents loading other applications in SHS,SPARK-33146,13335392,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,14/Oct/20 11:15,19/Oct/20 20:10,13/Jul/23 08:50,15/Oct/20 03:00,3.0.1,,,,,,,,,,,3.0.2,3.1.0,,,Spark Core,,,,0,,,,"A follow-on issue from https://issues.apache.org/jira/browse/SPARK-33133

If an invalid rolling event log folder is encountered by the Spark History Server upon startup, it crashes the whole loading process and prevents any valid applications from loading. We should simply catch the error, log it, and continue loading other applications.",,apachespark,dongjoon,kabhwan,kimahriman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 19 20:10:29 UTC 2020,,,,,,,,,,"0|z0jp0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/20 11:54;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/30037;;;","15/Oct/20 03:00;kabhwan;Issue resolved by pull request 30037
[https://github.com/apache/spark/pull/30037];;;","15/Oct/20 07:13;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/30051;;;","19/Oct/20 20:10;dongjoon;Hi, [~kabhwan]

I don't think the revert of revert is a correct way in this JIRA. At least, if you really want to `revert of revert`, you should make a PR and pass the UT in the community instead of silent reverting of reverting.

- https://github.com/apache/spark/commit/02f80cf293739f4d2881316897dbdcea74daa0bc
- Revert ""Revert ""[SPARK-33146][CORE] Check for non-fatal errors when loading new applications in SHS""""

In general, `Revert of revert` is used when the first revert decision was wrong. Here, although you didn't intend it, it looks like you use it as a claim that your first commit decision was right. 

The following is the way I see this:
1. The original commit was wrong because it's committed without testing.
2. Hence, the first revert is legitimate to recover `branch-3.0`.
3. After you tweaked `branch-3.0` yesterday, you had better land SPARK-33146 as a normal backporting PR with `[3.0]` tag instead of depending your test results under `WIP` PR.;;;",,,,,,,,,,,,,,,,,,,,,,
Handling nullability for complex types is broken during resolution of V2 write command,SPARK-33136,13335291,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,13/Oct/20 22:04,21/Jun/22 15:25,13/Jul/23 08:50,14/Oct/20 15:33,3.0.0,3.0.1,3.1.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"I figured out Spark 3.x cannot write to complex type with nullable if matching column type in DataFrame is non-nullable.

For example, 
{code:java}
case class StructData(a: String, b: Int)

case class Data(col_b: Boolean, col_i: Int, col_l: Long, col_f: Float, col_d: Double, col_s: String, col_fi: Array[Byte], col_bi: Array[Byte], col_de: Double, col_st: StructData, col_li: Seq[String], col_ma: Map[Int, String]){code}
`col_st.b` would be non-nullable in DataFrame, which should not matter when we insert from DataFrame to the table which has `col_st.b` as nullable. (non-nullable to nullable should be possible)

This looks to be broken in V2 write command.

 ",,apachespark,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-39484,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 02:03:03 UTC 2020,,,,,,,,,,"0|z0joe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/20 22:05;kabhwan;will submit a PR soon.;;;","13/Oct/20 22:23;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/30033;;;","14/Oct/20 05:14;kabhwan;Note that AppendData in branch-2.4 is also broken as same, but the usage of AppendData is reverted in [{{b6e4aca}}|https://github.com/apache/spark/commit/b6e4aca0be7f3b863c326063a3c02aa8a1c266a3] for branch-2.4 and shipped to Spark 2.4.0. (That said, no Spark 2.x version is affected.)

So while the code in AppendData for branch-2.4 is broken as well, it's a dead code.;;;","14/Oct/20 15:33;dongjoon;This is resolved via https://github.com/apache/spark/pull/30033;;;","15/Oct/20 02:03;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/30043;;;",,,,,,,,,,,,,,,,,,,,,
Incorrect nested complex JSON fields raise an exception,SPARK-33134,13335229,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,13/Oct/20 13:07,12/Dec/22 18:10,13/Jul/23 08:50,14/Oct/20 03:14,3.0.2,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"The code below:
{code:scala}
val pokerhand_raw = Seq(""""""[{""cards"": [19], ""playerId"": 123456}]"""""").toDF(""events"")
    val event = new StructType()
      .add(""playerId"", LongType)
      .add(""cards"", ArrayType(
        new StructType()
          .add(""id"", LongType)
          .add(""rank"", StringType)))
    val pokerhand_events = pokerhand_raw
      .select(explode(from_json($""events"", ArrayType(event))).as(""event""))
    pokerhand_events.show
{code}
throw the exception in the PERMISSIVE mode (default):
{code:java}
Caused by: java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.catalyst.util.ArrayData
  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray(rows.scala:48)
  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray$(rows.scala:48)
  at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getArray(rows.scala:195)
  at org.apache.spark.sql.catalyst.expressions.JsonToStructs.$anonfun$converter$2(jsonExpressions.scala:560)
  at org.apache.spark.sql.catalyst.expressions.JsonToStructs.nullSafeEval(jsonExpressions.scala:597)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:461)
  at org.apache.spark.sql.catalyst.expressions.ExplodeBase.eval(generators.scala:313)
  at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$8(GenerateExec.scala:108)
{code}
The same works in Spark 2.4:
{code:scala}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.6
      /_/

Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_265)
...
scala> pokerhand_events.show()
+-----+
|event|
+-----+
+-----+
{code}",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-40646,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 14 03:14:57 UTC 2020,,,,,,,,,,"0|z0jo0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/20 13:21;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30031;;;","13/Oct/20 13:21;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30031;;;","13/Oct/20 16:50;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/30032;;;","14/Oct/20 03:14;gurwls223;Issue resolved by pull request 30032
[https://github.com/apache/spark/pull/30032];;;",,,,,,,,,,,,,,,,,,,,,,
Fix grouping sets with having clause can not resolve qualified col name,SPARK-33131,13335214,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,13/Oct/20 11:18,18/Oct/20 07:24,13/Jul/23 08:50,16/Oct/20 11:26,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,2.4.6,2.4.7,3.1.0,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"Grouping sets construct new aggregate lost the qualified name of grouping expression. Here is a example:
{code:java}
-- Works resolved by ResolveReferences
select c1 from values (1) as t1(c1) group by grouping sets(t1.c1) having c1 = 1

-- Works because of the extra expression c1
select c1 as c2 from values (1) as t1(c1) group by grouping sets(t1.c1) having t1.c1 = 1

-- Failed
select c1 from values (1) as t1(c1) group by grouping sets(t1.c1) having t1.c1 = 1{code}",,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 17 02:22:30 UTC 2020,,,,,,,,,,"0|z0jnx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/20 11:22;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/30029;;;","16/Oct/20 11:26;cloud_fan;Issue resolved by pull request 30029
[https://github.com/apache/spark/pull/30029];;;","17/Oct/20 01:41;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/30075;;;","17/Oct/20 02:22;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/30077;;;",,,,,,,,,,,,,,,,,,,,,,
"Since the sbt version is now upgraded, old `test-only` needs to be replaced with `testOnly`",SPARK-33129,13335196,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,prashant,prashant,13/Oct/20 10:17,15/Nov/21 18:26,13/Jul/23 08:50,13/Oct/20 16:21,3.1.0,,,,,,,,,,,3.1.0,,,,Build,Documentation,,,0,,,,"Follow up to SPARK-21708, updating the references to test-only with testOnly. As the older syntax no longer works.",,apachespark,dongjoon,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 16:21:30 UTC 2020,,,,,,,,,,"0|z0jnt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/20 10:30;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/30028;;;","13/Oct/20 16:21;dongjoon;Issue resolved by pull request 30028
[https://github.com/apache/spark/pull/30028];;;",,,,,,,,,,,,,,,,,,,,,,,,
CREATE TEMPORARY TABLE fails with location,SPARK-33118,13334960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,planga82,planga82,planga82,12/Oct/20 08:50,12/Oct/20 21:20,13/Jul/23 08:50,12/Oct/20 21:20,3.0.0,3.0.1,3.1.0,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"The problem is produced when you use CREATE TEMPORARY TABLE with LOCATION

 
{code:java}
spark.range(3).write.parquet(""/data/tmp/testspark1"")

spark.sql(""CREATE TEMPORARY TABLE t USING parquet OPTIONS (path '/data/tmp/testspark1')"")
spark.sql(""CREATE TEMPORARY TABLE t USING parquet LOCATION '/data/tmp/testspark1'"")
{code}
The error message in both cases is 
{code:java}
org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.;
  at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$12(DataSource.scala:200)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:200)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
  at org.apache.spark.sql.execution.datasources.CreateTempViewUsing.run(ddl.scala:94)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)
{code}
 ",,apachespark,dongjoon,planga82,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30507,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 12 21:20:16 UTC 2020,,,,,,,,,,"0|z0jmco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/20 08:51;planga82;I'm working on it;;;","12/Oct/20 09:15;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/30014;;;","12/Oct/20 21:02;dongjoon;Thank you for your contribution, [~planga82].;;;","12/Oct/20 21:20;dongjoon;Issue resolved by pull request 30014
[https://github.com/apache/spark/pull/30014];;;",,,,,,,,,,,,,,,,,,,,,,
`kvstore` and `unsafe` doc tasks fail,SPARK-33115,13334901,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gemelen,gemelen,gemelen,11/Oct/20 22:51,12/Dec/22 18:10,13/Jul/23 08:50,13/Oct/20 12:39,3.1.0,,,,,,,,,,,3.0.2,3.1.0,,,Build,Documentation,,,0,,,,"`build/sbt publishLocal` task fails in two modules:
{code:java}
[error] stack trace is suppressed; run last kvstore / Compile / doc for the full output
[error] stack trace is suppressed; run last unsafe / Compile / doc for the full output
{code}
{code:java}
 sbt:spark-parent> kvstore/Compile/doc 
[info] Main Java API documentation to /home/gemelen/work/src/spark/common/kvstore/target/scala-2.12/api... 
[error] /home/gemelen/work/src/spark/common/kvstore/src/main/java/org/apache/spark/util/kvstore/InMemoryStore.java:167:1:  error: malformed HTML 
[error]    * An alias class for the type ""ConcurrentHashMap<Comparable<Object>, Boolean>"", which is used 
[error]                                                    ^ 
[error] /home/gemelen/work/src/spark/common/kvstore/src/main/java/org/apache/spark/util/kvstore/InMemoryStore.java:167:1:  error: unknown tag: Object 
[error]    * An alias class for the type ""ConcurrentHashMap<Comparable<Object>, Boolean>"", which is used 
[error]                                                               ^ 
[error] /home/gemelen/work/src/spark/common/kvstore/src/main/java/org/apache/spark/util/kvstore/InMemoryStore.java:167:1:  error: bad use of '>' 
[error]    * An alias class for the type ""ConcurrentHashMap<Comparable<Object>, Boolean>"", which is used
[error]                                                                                ^
{code}
{code:java}
 sbt:spark-parent> unsafe/Compile/doc 
[info] Main Java API documentation to /home/gemelen/work/src/spark/common/unsafe/target/scala-2.12/api... 
[error] /home/gemelen/work/src/spark/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java:566:1:  error: malformed HTML 
[error]    * Trims whitespaces (<= ASCII 32) from both ends of this string. [error]
{code}",,apachespark,gemelen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 12:39:00 UTC 2020,,,,,,,,,,"0|z0jlzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/20 01:14;apachespark;User 'gemelen' has created a pull request for this issue:
https://github.com/apache/spark/pull/30007;;;","12/Oct/20 01:15;apachespark;User 'gemelen' has created a pull request for this issue:
https://github.com/apache/spark/pull/30007;;;","13/Oct/20 12:39;gurwls223;Fixed in https://github.com/apache/spark/pull/30007;;;",,,,,,,,,,,,,,,,,,,,,,,
Remove sbt-dependency-graph SBT plugin,SPARK-33108,13334760,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,10/Oct/20 03:56,10/Oct/20 05:35,13/Jul/23 08:50,10/Oct/20 05:35,3.1.0,,,,,,,,,,,3.1.0,,,,Build,,,,0,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33109,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 10 05:35:41 UTC 2020,,,,,,,,,,"0|z0jl48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/20 04:08;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29997;;;","10/Oct/20 05:35;dongjoon;Issue resolved by pull request 29997
[https://github.com/apache/spark/pull/29997];;;",,,,,,,,,,,,,,,,,,,,,,,,
Broken installation of source packages on AppVeyor,SPARK-33105,13334732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,09/Oct/20 22:30,12/Dec/22 18:10,13/Jul/23 08:50,10/Oct/20 04:50,3.1.0,,,,,,,,,,,3.1.0,,,,Project Infra,R,,,0,,,,"It looks like AppVeyor configuration is broken, which leads to failure of installation of  source packages (become a problem when {{rlang}} has been updated from 0.4.7 and 0.4.8, with latter available only as a source package).

{code}

[00:01:48] trying URL
'https://cloud.r-project.org/src/contrib/rlang_0.4.8.tar.gz'
[00:01:48] Content type 'application/x-gzip' length 847517 bytes (827 KB)
[00:01:48] ==================================================
[00:01:48] downloaded 827 KB
[00:01:48] 
[00:01:48] Warning in strptime(xx, f, tz = tz) :
[00:01:48]   unable to identify current timezone 'C':
[00:01:48] please set environment variable 'TZ'
[00:01:49] * installing *source* package 'rlang' ...
[00:01:49] ** package 'rlang' successfully unpacked and MD5 sums checked
[00:01:49] ** using staged installation
[00:01:49] ** libs
[00:01:49] 
[00:01:49] *** arch - i386
[00:01:49] C:/Rtools40/mingw64/bin/gcc  -I""C:/R/include"" -DNDEBUG
-I./lib/         -O2 -Wall  -std=gnu99 -mfpmath=sse -msse2
-mstackrealign -c capture.c -o capture.o
[00:01:49] C:/Rtools40/mingw64/bin/gcc  -I""C:/R/include"" -DNDEBUG
-I./lib/         -O2 -Wall  -std=gnu99 -mfpmath=sse -msse2
-mstackrealign -c export.c -o export.o
[00:01:49] C:/Rtools40/mingw64/bin/gcc  -I""C:/R/include"" -DNDEBUG
-I./lib/         -O2 -Wall  -std=gnu99 -mfpmath=sse -msse2
-mstackrealign -c internal.c -o internal.o
[00:01:50] In file included from ./lib/rlang.h:74,
[00:01:50]                  from internal/arg.c:1,
[00:01:50]                  from internal.c:1:
[00:01:50] internal/eval-tidy.c: In function 'rlang_tilde_eval':
[00:01:50] ./lib/env.h:33:10: warning: 'top' may be used uninitialized
in this function [-Wmaybe-uninitialized]
[00:01:50]    return ENCLOS(env);
[00:01:50]           ^~~~~~~~~~~
[00:01:50] In file included from internal.c:8:
[00:01:50] internal/eval-tidy.c:406:9: note: 'top' was declared here
[00:01:50]    sexp* top;
[00:01:50]          ^~~
[00:01:50] C:/Rtools40/mingw64/bin/gcc  -I""C:/R/include"" -DNDEBUG
-I./lib/         -O2 -Wall  -std=gnu99 -mfpmath=sse -msse2
-mstackrealign -c lib.c -o lib.o
[00:01:51] C:/Rtools40/mingw64/bin/gcc  -I""C:/R/include"" -DNDEBUG
-I./lib/         -O2 -Wall  -std=gnu99 -mfpmath=sse -msse2
-mstackrealign -c version.c -o version.o
[00:01:52] C:/Rtools40/mingw64/bin/gcc -shared -s -static-libgcc -o
rlang.dll tmp.def capture.o export.o internal.o lib.o version.o
-LC:/R/bin/i386 -lR
[00:01:52]
c:/Rtools40/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/8.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe:
skipping incompatible C:/R/bin/i386/R.dll when searching for -lR
[00:01:52]
c:/Rtools40/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/8.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe:
skipping incompatible C:/R/bin/i386/R.dll when searching for -lR
[00:01:52]
c:/Rtools40/mingw64/bin/../lib/gcc/x86_64-w64-mingw32/8.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe:
cannot find -lR
[00:01:52] collect2.exe: error: ld returned 1 exit status
[00:01:52] no DLL was created
[00:01:52] ERROR: compilation failed for package 'rlang'
[00:01:52] * removing 'C:/RLibrary/rlang'
[00:01:52] 
[00:01:52] The downloaded source packages are in
[00:01:52]    
'C:\Users\appveyor\AppData\Local\Temp\1\Rtmp8qrryA\downloaded_packages'
[00:01:52] Warning message:
[00:01:52] In install.packages(c(""knitr"", ""rmarkdown"", ""testthat"",
""e1071"",  :
[00:01:52]   installation of package 'rlang' had non-zero exit status 
{code}

This leads to failures to install {{devtools}} and generate Rd files and, as a result, CRAN check failure.

There are some discrepancies in the {{dev/appveyor-install-dependencies.ps1}}, but the direct source of this issue seems to be {{$env:BINPREF}}, which forces usage of 64 bit mingw, even if packages are compiled for 32 bit. 

Modifying the variable to include current architecture:

{code}
$env:BINPREF=$RtoolsDrive + '/Rtools40/mingw$(WIN)/bin/'
{code}

(as proposed [here|https://stackoverflow.com/a/44035904] by R Yoda) looks like a valid fix, though we might want to clean remaining issues as well.",*strong text*,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 10 04:50:05 UTC 2020,,,,,,,,,,"0|z0jky0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/20 22:52;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/29991;;;","10/Oct/20 04:50;gurwls223;Fixed in https://github.com/apache/spark/pull/29991;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix `YarnClusterSuite.yarn-cluster should respect conf overrides in SparkHadoopUtil`,SPARK-33104,13334720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,dongjoon,dongjoon,09/Oct/20 20:57,12/Dec/22 18:11,13/Jul/23 08:50,23/Oct/20 10:19,3.1.0,,,,,,,,,,,3.1.0,,,,Tests,,,,0,,,,"- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/1377/testReport/org.apache.spark.deploy.yarn/YarnClusterSuite/yarn_cluster_should_respect_conf_overrides_in_SparkHadoopUtil__SPARK_16414__SPARK_23630_/

{code}
20/10/09 05:18:13.211 ContainersLauncher #0 WARN DefaultContainerExecutor: Exit code from container container_1602245728426_0006_02_000001 is : 15
20/10/09 05:18:13.211 ContainersLauncher #0 WARN DefaultContainerExecutor: Exception from container-launch with container ID: container_1602245728426_0006_02_000001 and exit code: 15
ExitCodeException exitCode=15: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:585)
	at org.apache.hadoop.util.Shell.run(Shell.java:482)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/10/09 05:18:13.211 ContainersLauncher #0 WARN ContainerLaunch: Container exited with a non-zero exit code 15
20/10/09 05:18:13.237 AsyncDispatcher event handler WARN NMAuditLogger: USER=jenkins	OPERATION=Container Finished - Failed	TARGET=ContainerImpl	RESULT=FAILURE	DESCRIPTION=Container failed with state: EXITED_WITH_FAILURE	APPID=application_1602245728426_0006	CONTAINERID=container_1602245728426_0006_02_000001
20/10/09 05:18:13.244 Socket Reader #1 for port 37112 INFO Server: Auth successful for appattempt_1602245728426_0006_000002 (auth:SIMPLE)
20/10/09 05:18:13.326 IPC Parameter Sending Thread #0 DEBUG Client: IPC Client (1123559518) connection to amp-jenkins-worker-04.amp/192.168.10.24:43090 from jenkins sending #37
20/10/09 05:18:13.327 IPC Client (1123559518) connection to amp-jenkins-worker-04.amp/192.168.10.24:43090 from jenkins DEBUG Client: IPC Client (1123559518) connection to amp-jenkins-worker-04.amp/192.168.10.24:43090 from jenkins got value #37
20/10/09 05:18:13.328 main DEBUG ProtobufRpcEngine: Call: getApplicationReport took 2ms
20/10/09 05:18:13.328 main INFO Client: Application report for application_1602245728426_0006 (state: FINISHED)
20/10/09 05:18:13.328 main DEBUG Client: 
	 client token: N/A
	 diagnostics: User class threw exception: org.scalatest.exceptions.TestFailedException: null was not equal to ""testvalue""
	at org.scalatest.matchers.MatchersHelper$.indicateFailure(MatchersHelper.scala:344)
	at org.scalatest.matchers.should.Matchers$ShouldMethodHelperClass.shouldMatcher(Matchers.scala:6778)
	at org.scalatest.matchers.should.Matchers$AnyShouldWrapper.should(Matchers.scala:6822)
	at org.apache.spark.deploy.yarn.YarnClusterDriverUseSparkHadoopUtilConf$.$anonfun$main$2(YarnClusterSuite.scala:383)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.deploy.yarn.YarnClusterDriverUseSparkHadoopUtilConf$.main(YarnClusterSuite.scala:382)
	at org.apache.spark.deploy.yarn.YarnClusterDriverUseSparkHadoopUtilConf.main(YarnClusterSuite.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:732)

	 ApplicationMaster host: amp-jenkins-worker-04.amp
	 ApplicationMaster RPC port: 36200
	 queue: default
	 start time: 1602245859148
	 final status: FAILED
	 tracking URL: http://amp-jenkins-worker-04.amp:39546/proxy/application_1602245728426_0006/
	 user: jenkins
20/10/09 05:18:13.331 main ERROR Client: Application diagnostics message: User class threw exception: org.scalatest.exceptions.TestFailedException: null was not equal to ""testvalue""
	at org.scalatest.matchers.MatchersHelper$.indicateFailure(MatchersHelper.scala:344)
	at org.scalatest.matchers.should.Matchers$ShouldMethodHelperClass.shouldMatcher(Matchers.scala:6778)
	at org.scalatest.matchers.should.Matchers$AnyShouldWrapper.should(Matchers.scala:6822)
	at org.apache.spark.deploy.yarn.YarnClusterDriverUseSparkHadoopUtilConf$.$anonfun$main$2(YarnClusterSuite.scala:383)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.deploy.yarn.YarnClusterDriverUseSparkHadoopUtilConf$.main(YarnClusterSuite.scala:382)
	at org.apache.spark.deploy.yarn.YarnClusterDriverUseSparkHadoopUtilConf.main(YarnClusterSuite.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:732)

20/10/09 05:18:13.332 launcher-proc-6 INFO YarnClusterDriverUseSparkHadoopUtilConf: Exception in thread ""main"" org.apache.spark.SparkException: Application application_1602245728426_0006 finished with failed status
20/10/09 05:18:13.332 launcher-proc-6 INFO YarnClusterDriverUseSparkHadoopUtilConf: 	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1199)
20/10/09 05:18:13.332 launcher-proc-6 INFO YarnClusterDriverUseSparkHadoopUtilConf: 	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1590)
20/10/09 05:18:13.332 launcher-proc-6 INFO YarnClusterDriverUseSparkHadoopUtilConf: 	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:934)
20/10/09 05:18:13.332 launcher-proc-6 INFO YarnClusterDriverUseSparkHadoopUtilConf: 	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
20/10/09 05:18:13.332 launcher-proc-6 INFO YarnClusterDriverUseSparkHadoopUtilConf: 	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
20/10/09 05:18:13.332 launcher-proc-6 INFO YarnClusterDriverUseSparkHadoopUtilConf: 	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
20/10/09 05:18:13.332 launcher-proc-6 INFO YarnClusterDriverUseSparkHadoopUtilConf: 	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1013)
20/10/09 05:18:13.332 launcher-proc-6 INFO YarnClusterDriverUseSparkHadoopUtilConf: 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1022)
20/10/09 05:18:13.332 launcher-proc-6 INFO YarnClusterDriverUseSparkHadoopUtilConf: 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
20/10/09 05:18:13.335 Thread-1 INFO ShutdownHookManager: Shutdown hook called
20/10/09 05:18:13.337 Thread-1 INFO ShutdownHookManager: Deleting directory /tmp/spark-74d7ab7c-4fd7-4980-ac22-9b73e3f8955c
20/10/09 05:18:13.343 Thread-1 INFO ShutdownHookManager: Deleting directory /tmp/spark-27d8a061-ba44-4a0e-a7d4-985443f7b4ca
20/10/09 05:18:14.176 pool-1-thread-1-ScalaTest-running-YarnClusterSuite INFO YarnClusterSuite: 

===== FINISHED o.a.s.deploy.yarn.YarnClusterSuite: 'yarn-cluster should respect conf overrides in SparkHadoopUtil (SPARK-16414, SPARK-23630)' =====
{code}",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21708,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 26 00:57:02 UTC 2020,,,,,,,,,,"0|z0jkvc:",9223372036854775807,,,,,,,,,,,,,3.1.0,,,,,,,,,,"10/Oct/20 05:31;LuciferYang;Is this an inevitable problem? `mvn test` can pass, may need to add some log to determine the file loading path of `core-site.xml`;;;","11/Oct/20 00:30;dongjoon;It turns out that Hadoop 2.7 / Hive 2.3 combination SBT build is broken by SPARK-21708 .

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/

*BEFORE THIS COMMIT*
{code}
$ build/sbt ""yarn/testOnly *.YarnClusterSuite -- -z SparkHadoopUtil"" -Pyarn -Phadoop-2.7 -Phive -Phive-2.3
...
[info] YarnClusterSuite:
[info] - yarn-cluster should respect conf overrides in SparkHadoopUtil (SPARK-16414, SPARK-23630) (12 seconds, 193 milliseconds)
[info] ScalaTest
[info] Run completed in 29 seconds, 774 milliseconds.
[info] Total number of tests run: 1
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 1, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 1, Failed 0, Errors 0, Passed 1
[success] Total time: 122 s, completed Oct 10, 2020 5:21:55 PM
~s:23afc930ae $ git log --oneline -n1
23afc930ae (HEAD) [SPARK-26499][SQL][FOLLOWUP] Print the loading provider exception starting from the INFO level
{code}

*AFTER THIS COMMIT*
{code}
$ build/sbt ""yarn/testOnly *.YarnClusterSuite -- -z SparkHadoopUtil"" -Pyarn -Phadoop-2.7 -Phive -Phive-2.3
...
[info] *** 1 TEST FAILED ***
[error] Failed: Total 1, Failed 1, Errors 0, Passed 0
[error] Failed tests:
[error] 	org.apache.spark.deploy.yarn.YarnClusterSuite
[error] (yarn / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 132 s (02:12), completed Oct 10, 2020 5:28:29 PM
{code};;;","11/Oct/20 00:31;dongjoon;Please see https://github.com/apache/spark/pull/29286#issuecomment-706630303;;;","11/Oct/20 00:37;dongjoon;Although this is a test case issue, I will raise this issue as a blocker because this is an evidence of SBT 1.3 regression . We need to fix this before Apache Spark 3.1.0. Otherwise, this can hide more issues.;;;","21/Oct/20 05:24;gurwls223;cc [~Ngone51] FYI since you're taking a look for this.;;;","22/Oct/20 15:06;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30133;;;","23/Oct/20 10:19;gurwls223;Issue resolved by pull request 30133
[https://github.com/apache/spark/pull/30133];;;","26/Nov/20 00:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30508;;;",,,,,,,,,,,,,,,,,,
Use stringToSeq on SQL list typed parameters,SPARK-33102,13334675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gsomogyi,gsomogyi,gsomogyi,09/Oct/20 15:59,12/Dec/22 18:10,13/Jul/23 08:50,10/Oct/20 04:53,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,,,apachespark,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 10 04:53:46 UTC 2020,,,,,,,,,,"0|z0jklc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/20 16:00;gsomogyi;Filing a PR soon...;;;","09/Oct/20 16:51;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/29989;;;","10/Oct/20 04:53;gurwls223;Issue resolved by pull request 29989
[https://github.com/apache/spark/pull/29989];;;",,,,,,,,,,,,,,,,,,,,,,,
LibSVM format does not propagate Hadoop config from DS options to underlying HDFS file system,SPARK-33101,13334581,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,09/Oct/20 07:28,09/Oct/20 11:50,13/Jul/23 08:50,09/Oct/20 09:38,3.1.0,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"When running:
{code:java}
spark.read.format(""libsvm"").options(conf).load(path)
{code}
The underlying file system will not receive the `conf` options.",,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,,SPARK-33094,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 09 10:24:38 UTC 2020,,,,,,,,,,"0|z0jk0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/20 07:50;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29984;;;","09/Oct/20 07:50;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29984;;;","09/Oct/20 09:38;dongjoon;Issue resolved by pull request 29984
[https://github.com/apache/spark/pull/29984];;;","09/Oct/20 10:24;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29986;;;","09/Oct/20 10:24;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29986;;;",,,,,,,,,,,,,,,,,,,,,
Support parse the sql statements with c-style comments,SPARK-33100,13334565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hzfeiwang,hzfeiwang,hzfeiwang,09/Oct/20 06:22,18/Jan/21 07:35,13/Jul/23 08:50,05/Jan/21 06:58,3.0.1,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,SQL,,,,0,,,,"Now the spark-sql does not support parse the sql statements with C-style comments.
For the sql statements:
{code:java}
/* SELECT 'test'; */
SELECT 'test';
{code}
Would be split to two statements:
The first: ""/* SELECT 'test'""
The second: ""*/ SELECT 'test'""

Then it would throw an exception because the first one is illegal.
",,apachespark,hzfeiwang,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 06 04:38:40 UTC 2021,,,,,,,,,,"0|z0jjww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/20 07:21;apachespark;User 'turboFei' has created a pull request for this issue:
https://github.com/apache/spark/pull/29982;;;","05/Jan/21 06:58;maropu;Resolved by https://github.com/apache/spark/pull/29982;;;","05/Jan/21 08:41;apachespark;User 'turboFei' has created a pull request for this issue:
https://github.com/apache/spark/pull/31033;;;","05/Jan/21 09:49;apachespark;User 'turboFei' has created a pull request for this issue:
https://github.com/apache/spark/pull/31040;;;","05/Jan/21 09:49;apachespark;User 'turboFei' has created a pull request for this issue:
https://github.com/apache/spark/pull/31040;;;","06/Jan/21 04:37;apachespark;User 'turboFei' has created a pull request for this issue:
https://github.com/apache/spark/pull/31054;;;","06/Jan/21 04:38;apachespark;User 'turboFei' has created a pull request for this issue:
https://github.com/apache/spark/pull/31054;;;",,,,,,,,,,,,,,,,,,,
ORC format does not propagate Hadoop config from DS options to underlying HDFS file system,SPARK-33094,13334383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,08/Oct/20 09:17,10/Oct/20 05:18,13/Jul/23 08:50,08/Oct/20 19:00,3.1.0,,,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"When running:
{code:java}
spark.read.format(""orc"").options(conf).load(path)
{code}
The underlying file system will not receive the `conf` options.",,apachespark,dongjoon,maxgekk,,,,,,,,,,,,,,,,,SPARK-33101,SPARK-33089,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 09 14:10:06 UTC 2020,,,,,,,,,,"0|z0jisw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/20 09:27;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29976;;;","08/Oct/20 09:28;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29976;;;","08/Oct/20 19:00;dongjoon;Issue resolved by pull request 29976
[https://github.com/apache/spark/pull/29976];;;","09/Oct/20 10:16;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29985;;;","09/Oct/20 10:17;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29985;;;","09/Oct/20 14:09;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29987;;;","09/Oct/20 14:10;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29987;;;",,,,,,,,,,,,,,,,,,,
avro format does not propagate Hadoop config from DS options to underlying HDFS file system,SPARK-33089,13334288,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuningzh,yuningzh,yuningzh,07/Oct/20 21:02,12/Dec/22 18:10,13/Jul/23 08:50,08/Oct/20 03:19,3.1.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"When running:
{code:java}
spark.read.format(""avro"").options(conf).load(path)
{code}
The underlying file system will not receive the `conf` options.",,apachespark,yuningzh,,,,,,,,,,,,,,,,,,SPARK-33094,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 08 03:19:34 UTC 2020,,,,,,,,,,"0|z0ji7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/20 21:21;apachespark;User 'yuningzh-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/29971;;;","08/Oct/20 03:19;gurwls223;Fixed in https://github.com/apache/spark/pull/29971;;;",,,,,,,,,,,,,,,,,,,,,,,,
Join with ambiguous column succeeding but giving wrong output,SPARK-33071,13333791,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Ngone51,gcooper120,gcooper120,05/Oct/20 20:10,09/Dec/20 08:32,13/Jul/23 08:50,02/Dec/20 17:51,2.4.4,3.0.1,3.1.0,,,,,,,,,3.1.0,,,,SQL,,,,1,correctness,,,"When joining two datasets where one column in each dataset is sourced from the same input dataset, the join successfully runs, but does not select the correct columns, leading to incorrect output.

Repro using pyspark:
{code:java}
sc.version
import pyspark.sql.functions as F
d = [{'key': 'a', 'sales': 1, 'units' : 2}, {'key': 'a', 'sales': 2, 'units' : 4}, {'key': 'b', 'sales': 5, 'units' : 10}, {'key': 'c', 'sales': 1, 'units' : 2}, {'key': 'd', 'sales': 3, 'units' : 6}]
input_df = spark.createDataFrame(d)
df1 = input_df.groupBy(""key"").agg(F.sum('sales').alias('sales'))
df2 = input_df.groupBy(""key"").agg(F.sum('units').alias('units'))
df1 = df1.filter(F.col(""key"") != F.lit(""c""))
df2 = df2.filter(F.col(""key"") != F.lit(""d""))
ret = df1.join(df2, df1.key == df2.key, ""full"").select(
df1[""key""].alias(""df1_key""),
df2[""key""].alias(""df2_key""),
df1[""sales""],
df2[""units""],
F.coalesce(df1[""key""], df2[""key""]).alias(""key""))
ret.show()
ret.explain(){code}
output for 2.4.4:
{code:java}
>>> sc.version
u'2.4.4'
>>> import pyspark.sql.functions as F
>>> d = [{'key': 'a', 'sales': 1, 'units' : 2}, {'key': 'a', 'sales': 2, 'units' : 4}, {'key': 'b', 'sales': 5, 'units' : 10}, {'key': 'c', 'sales': 1, 'units' : 2}, {'key': 'd', 'sales': 3, 'units' : 6}]
>>> input_df = spark.createDataFrame(d)
>>> df1 = input_df.groupBy(""key"").agg(F.sum('sales').alias('sales'))
>>> df2 = input_df.groupBy(""key"").agg(F.sum('units').alias('units'))
>>> df1 = df1.filter(F.col(""key"") != F.lit(""c""))
>>> df2 = df2.filter(F.col(""key"") != F.lit(""d""))
>>> ret = df1.join(df2, df1.key == df2.key, ""full"").select(
... df1[""key""].alias(""df1_key""),
... df2[""key""].alias(""df2_key""),
... df1[""sales""],
... df2[""units""],
... F.coalesce(df1[""key""], df2[""key""]).alias(""key""))
20/10/05 15:46:14 WARN Column: Constructing trivially true equals predicate, 'key#213 = key#213'. Perhaps you need to use aliases.
>>> ret.show()
+-------+-------+-----+-----+----+
|df1_key|df2_key|sales|units| key|
+-------+-------+-----+-----+----+
|      d|      d|    3| null|   d|
|   null|   null| null|    2|null|
|      b|      b|    5|   10|   b|
|      a|      a|    3|    6|   a|
+-------+-------+-----+-----+----+>>> ret.explain()
== Physical Plan ==
*(5) Project [key#213 AS df1_key#258, key#213 AS df2_key#259, sales#223L, units#230L, coalesce(key#213, key#213) AS key#260]
+- SortMergeJoin [key#213], [key#237], FullOuter
   :- *(2) Sort [key#213 ASC NULLS FIRST], false, 0
   :  +- *(2) HashAggregate(keys=[key#213], functions=[sum(sales#214L)])
   :     +- Exchange hashpartitioning(key#213, 200)
   :        +- *(1) HashAggregate(keys=[key#213], functions=[partial_sum(sales#214L)])
   :           +- *(1) Project [key#213, sales#214L]
   :              +- *(1) Filter (isnotnull(key#213) && NOT (key#213 = c))
   :                 +- Scan ExistingRDD[key#213,sales#214L,units#215L]
   +- *(4) Sort [key#237 ASC NULLS FIRST], false, 0
      +- *(4) HashAggregate(keys=[key#237], functions=[sum(units#239L)])
         +- Exchange hashpartitioning(key#237, 200)
            +- *(3) HashAggregate(keys=[key#237], functions=[partial_sum(units#239L)])
               +- *(3) Project [key#237, units#239L]
                  +- *(3) Filter (isnotnull(key#237) && NOT (key#237 = d))
                     +- Scan ExistingRDD[key#237,sales#238L,units#239L]
{code}
output for 3.0.1:


{code:java}
// code placeholder
>>> sc.version
u'3.0.1'
>>> import pyspark.sql.functions as F
>>> d = [{'key': 'a', 'sales': 1, 'units' : 2}, {'key': 'a', 'sales': 2, 'units' : 4}, {'key': 'b', 'sales': 5, 'units' : 10}, {'key': 'c', 'sales': 1, 'units' : 2}, {'key': 'd', 'sales': 3, 'units' : 6}]
>>> input_df = spark.createDataFrame(d)
/usr/local/lib/python2.7/site-packages/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead
  warnings.warn(""inferring schema from dict is deprecated,""
>>> df1 = input_df.groupBy(""key"").agg(F.sum('sales').alias('sales'))
>>> df2 = input_df.groupBy(""key"").agg(F.sum('units').alias('units'))
>>> df1 = df1.filter(F.col(""key"") != F.lit(""c""))
>>> df2 = df2.filter(F.col(""key"") != F.lit(""d""))
>>> ret = df1.join(df2, df1.key == df2.key, ""full"").select(
... df1[""key""].alias(""df1_key""),
... df2[""key""].alias(""df2_key""),
... df1[""sales""],
... df2[""units""],
... F.coalesce(df1[""key""], df2[""key""]).alias(""key""))
>>> ret.show()
+-------+-------+-----+-----+----+
|df1_key|df2_key|sales|units| key|
+-------+-------+-----+-----+----+
|      d|      d|    3| null|   d|
|   null|   null| null|    2|null|
|      b|      b|    5|   10|   b|
|      a|      a|    3|    6|   a|
+-------+-------+-----+-----+----+>>> ret.explain()
== Physical Plan ==
*(5) Project [key#0 AS df1_key#45, key#0 AS df2_key#46, sales#10L, units#17L, coalesce(key#0, key#0) AS key#47]
+- SortMergeJoin [key#0], [key#24], FullOuter
   :- *(2) Sort [key#0 ASC NULLS FIRST], false, 0
   :  +- *(2) HashAggregate(keys=[key#0], functions=[sum(sales#1L)])
   :     +- Exchange hashpartitioning(key#0, 200), true, [id=#152]
   :        +- *(1) HashAggregate(keys=[key#0], functions=[partial_sum(sales#1L)])
   :           +- *(1) Project [key#0, sales#1L]
   :              +- *(1) Filter (isnotnull(key#0) AND NOT (key#0 = c))
   :                 +- *(1) Scan ExistingRDD[key#0,sales#1L,units#2L]
   +- *(4) Sort [key#24 ASC NULLS FIRST], false, 0
      +- *(4) HashAggregate(keys=[key#24], functions=[sum(units#26L)])
         +- Exchange hashpartitioning(key#24, 200), true, [id=#158]
            +- *(3) HashAggregate(keys=[key#24], functions=[partial_sum(units#26L)])
               +- *(3) Project [key#24, units#26L]
                  +- *(3) Filter (isnotnull(key#24) AND NOT (key#24 = d))
                     +- *(3) Scan ExistingRDD[key#24,sales#25L,units#26L]{code}
key#0 is the reference used for both alias operations and both sides of the coalesce, despite the query plan projecting key#24 for the right side of the join.
Concretely, I believe the output of the join should be this:
{code:java}
+-------+-------+-----+-----+----+
|df1_key|df2_key|sales|units| key|
+-------+-------+-----+-----+----+
|      d|   null|    3| null|   d|
|   null|      c| null|    2|   c|
|      b|      b|    5|   10|   b|
|      a|      a|    3|    6|   a|
+-------+-------+-----+-----+----+
{code}
 ",,angerszhuuu,apachespark,avoutilainen,cloud_fan,dongjoon,EveLiao,gcooper120,LuciferYang,maropu,rshkv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 08:32:24 UTC 2020,,,,,,,,,,"0|z0jfl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/20 18:36;EveLiao;Need to talk with experienced contributors to see if it is a bug. The issue here is that there will be duplicate columns after join. So df1[""key""] and df2[""key""] in select statement actually refer to the same column: key from df1. A workaround should be:
{code:java}
>>> df1=df1.alias(""df1"")
>>> df2=df2.alias(""df2"")
>>> df1.join(df2, df1.key == df2.key, ""full"").select(F.col(""df1.key"").alias(""df1_key""),F.col(""df2.key"").alias(""df2_key""),""sales"",""units"").show()
{code};;;","06/Oct/20 19:08;gcooper120;[~EveLiao] Yeah that sounds correct, another workaround I found was something like
{code:java}
df1_cols = list(map(lambda x: x + ""_1"", df1.columns))
df1 = toDF(df1_cols){code}
 

In my mind the reason why I view this as a bug is that it takes a fairly deep understanding of how spark handles columns, copying and references to expect this behavior. For many developers, this just wouldn't be the behavior they expect to see.;;;","06/Oct/20 19:18;EveLiao;Yeah, I agree with you. IMHO, would prefer a error message pops up when select( df1[""key""]) after join, like "" Reference 'key' is ambiguous"" when using reference as ""key"". Right now, the mistake is actually made out of sight, which is not ideal.;;;","13/Oct/20 06:25;angerszhuuu;it is wrong, I am working on this ;;;","13/Oct/20 07:40;angerszhuuu;cc [~hyukjin.kwon] [~maropu] [~cloud_fan] [~dongjoon]  This error cause by ResolveReference `dedupRight` generate new LogicalPlan with  new ExprId, but `df2['key']` resolved as origin right side ExprId, then two `key` all resolved to `df1.key`. I believe this is not the only case.  Seems a serious data issue. Any suggestion?

 

 ;;;","13/Oct/20 16:25;cloud_fan;To confirm: is this a long-standing bug that 2.4, 3.0, and the master branch all give wrong (but same) result?;;;","13/Oct/20 18:55;EveLiao;The master branch has the same bug.;;;","25/Nov/20 03:26;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/30488;;;","02/Dec/20 17:51;cloud_fan;Issue resolved by pull request 30488
[https://github.com/apache/spark/pull/30488];;;","09/Dec/20 08:32;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/30682;;;",,,,,,,,,,,,,,,,
Expand the stack size of a thread in a test in LocalityPlacementStrategySuite for Java 11,SPARK-33065,13333631,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,04/Oct/20 21:41,04/Oct/20 23:12,13/Jul/23 08:50,04/Oct/20 23:12,3.1.0,,,,,,,,,,,3.0.2,3.1.0,,,Tests,,,,0,,,,"`LocalityPlacementStrategySuite` fails with Java 11 due to `StackOverflowError`.

{code}
[info] - handle large number of containers and tasks (SPARK-18750) *** FAILED *** (170 milliseconds)
[info]   StackOverflowError should not be thrown; however, got:
[info]   
[info]   java.lang.StackOverflowError
[info]          at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1012)
[info]          at java.base/java.util.concurrent.ConcurrentHashMap.putIfAbsent(ConcurrentHashMap.java:1541)
[info]          at java.base/java.lang.ClassLoader.getClassLoadingLock(ClassLoader.java:668)
[info]          at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:591)
[info]          at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:579)
[info]          at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
[info]          at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
{code}",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 04 23:12:29 UTC 2020,,,,,,,,,,"0|z0jelk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/20 22:57;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29943;;;","04/Oct/20 23:12;dongjoon;Issue resolved by pull request 29943
[https://github.com/apache/spark/pull/29943];;;",,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Apache ORC to 1.5.12,SPARK-33050,13330479,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,02/Oct/20 01:43,02/Oct/20 07:06,13/Jul/23 08:50,02/Oct/20 07:06,3.1.0,,,,,,,,,,,3.1.0,,,,Build,,,,0,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 02 07:06:32 UTC 2020,,,,,,,,,,"0|z0j3nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/20 01:45;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29930;;;","02/Oct/20 07:06;dongjoon;Issue resolved by pull request 29930
[https://github.com/apache/spark/pull/29930];;;",,,,,,,,,,,,,,,,,,,,,,,,
Decommission Core Integration Test is flaky.,SPARK-33049,13330478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,holden,holden,holden,02/Oct/20 01:31,03/Oct/20 22:15,13/Jul/23 08:50,03/Oct/20 22:15,3.1.0,,,,,,,,,,,3.1.0,,,,Spark Core,Tests,,,0,,,,See https://github.com/apache/spark/pull/29923#issuecomment-702344724,,apachespark,dongjoon,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 03 22:15:10 UTC 2020,,,,,,,,,,"0|z0j3nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/20 01:32;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29929;;;","03/Oct/20 22:15;dongjoon;Issue resolved by pull request 29929
[https://github.com/apache/spark/pull/29929];;;",,,,,,,,,,,,,,,,,,,,,,,,
RowMatrix is incompatible with spark.driver.maxResultSize=0,SPARK-33043,13330263,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,karenfeng,karenfeng,01/Oct/20 00:31,06/Oct/20 23:58,13/Jul/23 08:50,03/Oct/20 18:13,3.0.0,3.0.1,,,,,,,,,,3.0.2,3.1.0,,,MLlib,,,,0,,,,"RowMatrix does not work if spark.driver.maxResultSize=0, as this requirement breaks:

 
{code:java}
require(maxDriverResultSizeInBytes > aggregatedObjectSizeInBytes,      s""Cannot aggregate object of size $aggregatedObjectSizeInBytes Bytes, ""        + s""as it's bigger than maxResultSize ($maxDriverResultSizeInBytes Bytes)"")
{code}
 

[https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/RowMatrix.scala#L795.]

 

This check should likely only happen if maxDriverResultSizeInBytes > 0.",,apachespark,karenfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 23:58:52 UTC 2020,,,,,,,,,,"0|z0j2bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/20 19:12;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/29925;;;","01/Oct/20 19:13;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/29925;;;","03/Oct/20 18:13;srowen;Issue resolved by pull request 29925
[https://github.com/apache/spark/pull/29925];;;","06/Oct/20 23:37;karenfeng;When can we anticipate a 3.0.2 release with the fix?;;;","06/Oct/20 23:58;srowen;My unofficial guess is December, just based on previous maintenance release cadences.;;;",,,,,,,,,,,,,,,,,,,,,
Updates the obsoleted entries of attribute mapping in QueryPlan#transformUpWithNewOutput,SPARK-33035,13330162,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,30/Sep/20 12:20,07/Oct/20 06:12,13/Jul/23 08:50,06/Oct/20 08:34,3.0.1,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"This ticket aims at fixing corner-case bugs in the `QueryPlan#transformUpWithNewOutput` that is used to propagate updated `ExprId`s in a bottom-up way. Let's say we have a rule to simply assign new `ExprId`s in a projection list like this;
{code}
case class TestRule extends Rule[LogicalPlan] {
  override def apply(plan: LogicalPlan): LogicalPlan = plan.transformUpWithNewOutput {
    case p @ Project(projList, _) =>
      val newPlan = p.copy(projectList = projList.map { _.transform {
        // Assigns a new `ExprId` for references
        case a: AttributeReference => Alias(a, a.name)()
      }}.asInstanceOf[Seq[NamedExpression]])

      val attrMapping = p.output.zip(newPlan.output)
      newPlan -> attrMapping
  }
}
{code}
Then, this rule is applied into a plan below;
{code}
(3) Project [a#5, b#6]
+- (2) Project [a#5, b#6]
   +- (1) Project [a#5, b#6]
      +- LocalRelation <empty>, [a#5, b#6]
{code}
In the first transformation, the rule assigns new `ExprId`s in `(1) Project` (e.g., a#5 AS a#7, b#6 AS b#8). In the second transformation, the rule corrects the input references of `(2) Project`  first by using attribute mapping given from `(1) Project` (a#5->a#7 and b#6->b#8) and then assigns new `ExprId`s (e.g., a#7 AS a#9, b#8 AS b#10). But, in the third transformation, the rule fails because it tries to correct the references of `(3) Project` by using incorrect attribute mapping (a#7->a#9 and b#8->b#10) even though the correct one is a#5->a#9 and b#6->b#10. 
",,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 11:50:45 UTC 2020,,,,,,,,,,"0|z0j1pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/20 12:23;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29911;;;","30/Sep/20 12:24;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29911;;;","06/Oct/20 08:34;cloud_fan;Issue resolved by pull request 29911
[https://github.com/apache/spark/pull/29911];;;","06/Oct/20 11:50;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29953;;;","06/Oct/20 11:50;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29953;;;",,,,,,,,,,,,,,,,,,,,,
Standalone mode blacklist executors page UI marks driver as blacklisted,SPARK-33029,13330018,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Baohe Zhang,tgraves,tgraves,29/Sep/20 18:51,07/Jan/21 03:13,13/Jul/23 08:50,06/Jan/21 03:17,3.0.0,,,,,,,,,,,3.0.2,3.1.1,,,Spark Core,,,,0,,,,"I am running a spark shell on a 1 node standalone cluster.  I noticed that the executors page ui was marking the driver as blacklisted for the stage that is running.  Attached a screen shot.

Also, in my case one of the executors died and it doesn't seem like the schedule rpicked up the new one.  It doesn't show up on the stages page and just shows it as active but none of the tasks ran there.

 

You can reproduce this by starting a master and slave on a single node, then launch a shell like where you will get multiple executors (in this case I got 3)

$SPARK_HOME/bin/spark-shell --master spark://yourhost:7077 --executor-cores 4 --conf spark.blacklist.enabled=true

 

From shell run:
{code:java}
import org.apache.spark.TaskContext
val rdd = sc.makeRDD(1 to 1000, 5).mapPartitions { it =>
 val context = TaskContext.get()
 if (context.attemptNumber() < 2) {
 throw new Exception(""test attempt num"")
 }
 it
}{code}",,apachespark,Baohe Zhang,dongjoon,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/20 18:52;tgraves;Screen Shot 2020-09-29 at 1.52.09 PM.png;https://issues.apache.org/jira/secure/attachment/13012316/Screen+Shot+2020-09-29+at+1.52.09+PM.png","29/Sep/20 18:54;tgraves;Screen Shot 2020-09-29 at 1.53.37 PM.png;https://issues.apache.org/jira/secure/attachment/13012317/Screen+Shot+2020-09-29+at+1.53.37+PM.png",,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 06 04:53:30 UTC 2021,,,,,,,,,,"0|z0j0tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/20 19:56;tgraves;NOte I filed https://issues.apache.org/jira/browse/SPARK-33031 for the issue of it not using the other executor as it seems more critical ;;;","28/Dec/20 18:10;Baohe Zhang;With the blacklist feature enabled, by default, a node will be excluded when 2 executors on this node have been excluded. In this case, the node is excluded and we will mark all executors in that node as excluded. Since we are running standalone mode in a single node, the driver and all executors share the same hostname. the driver will be marked as excluded on AppStatusListener when handling ""SparkListenerNodeExcludedForStage"" event. We can fix it by filter out the driver entity when handling this event, hence the UI won't show the driver is excluded.;;;","28/Dec/20 19:38;apachespark;User 'baohe-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30954;;;","28/Dec/20 19:38;apachespark;User 'baohe-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30954;;;","06/Jan/21 03:17;dongjoon;Issue resolved by pull request 30954
[https://github.com/apache/spark/pull/30954];;;","06/Jan/21 04:53;apachespark;User 'baohe-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31057;;;",,,,,,,,,,,,,,,,,,,,
Fix CodeGen fallback issue of UDFSuite in Scala 2.13,SPARK-33024,13329945,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,29/Sep/20 13:40,01/Oct/20 13:37,13/Jul/23 08:50,01/Oct/20 13:37,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"After SPARK-32851 set `CODEGEN_FACTORY_MODE` to `CODEGEN_ONLY` in SharedSparkSessionBase of sparkConf to construction SparkSession in Test,

The test suite `SPARK-32459: UDF should not fail on WrappedArray` in s.sql.UDFSuite exposed a codegen fallback issue in Scala 2.13 as follow:
{code:java}
- SPARK-32459: UDF should not fail on WrappedArray *** FAILED ***
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 99: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 99: No applicable constructor/method found for zero actual parameters; candidates are: ""public scala.collection.mutable.Builder scala.collection.mutable.ArraySeq$.newBuilder(java.lang.Object)"", ""public scala.collection.mutable.Builder scala.collection.mutable.ArraySeq$.newBuilder(scala.reflect.ClassTag)"", ""public abstract scala.collection.mutable.Builder scala.collection.EvidenceIterableFactory.newBuilder(java.lang.Object)""

{code}",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 13:37:26 UTC 2020,,,,,,,,,,"0|z0j0d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/20 13:58;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29903;;;","29/Sep/20 13:59;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29903;;;","01/Oct/20 13:37;srowen;Issue resolved by pull request 29903
[https://github.com/apache/spark/pull/29903];;;",,,,,,,,,,,,,,,,,,,,,,,
Use spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=1 by default,SPARK-33019,13329793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,28/Sep/20 18:40,07/Oct/20 21:01,13/Jul/23 08:50,29/Sep/20 19:03,3.0.0,3.0.1,3.1.0,,,,,,,,,3.0.2,3.1.0,,,Spark Core,,,,0,correctness,,,"By default, Spark should use a safe file output committer algorithm to avoid MAPREDUCE-7282.",,apachespark,dongjoon,jakubwaller,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-7282,MAPREDUCE-7300,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 07 21:01:31 UTC 2020,,,,,,,,,,"0|z0izfc:",9223372036854775807,,,,,,,,,,,,,3.0.2,3.1.0,,,,,,,,,"28/Sep/20 18:45;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29895;;;","29/Sep/20 19:03;dongjoon;Issue resolved by pull request 29895
[https://github.com/apache/spark/pull/29895];;;","07/Oct/20 18:36;stevel@apache.org;Related to this, I'm proposing we add a method which will let the MR engine and spark driver work out if a committer can be recovered from -and choose how to react if it says ""no"" - fail or warn + commit another attempt

That way if you want full due diligence you can still use v2 committer, (or EMR committer), but get the ability to make failures during the commit phase something which triggers a failure. Most of the time, it won't.
;;;","07/Oct/20 21:01;dongjoon;[~stevel@apache.org]. The user sill use v2 committer if they already set the conf explicitly. In addition, the user still can use v2 committer if they want.
>  you can still use v2 committer

We only prevent the users blindly expect the same behavior during migration from Apache Spark 3.0 to Apache Spark 3.1.;;;",,,,,,,,,,,,,,,,,,,,,,
Fix compute statistics issue,SPARK-33018,13329772,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,28/Sep/20 15:35,29/Sep/20 16:46,13/Jul/23 08:50,29/Sep/20 16:46,3.0.0,3.0.1,3.1.0,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"The statistics may be incorrect if it contains {{0}}.
https://github.com/apache/spark/blob/98be8953c75c026c1cb432cc8f66dd312feed0c6/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/SizeInBytesOnlyStatsPlanVisitor.scala#L54-L58",,apachespark,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/20 16:04;yumwang;SPARK-33018.jpg;https://issues.apache.org/jira/secure/attachment/13012254/SPARK-33018.jpg",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 29 16:46:36 UTC 2020,,,,,,,,,,"0|z0izao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/20 15:54;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29894;;;","28/Sep/20 15:55;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29894;;;","29/Sep/20 16:46;cloud_fan;Issue resolved by pull request 29894
[https://github.com/apache/spark/pull/29894];;;",,,,,,,,,,,,,,,,,,,,,,,
Potential SQLMetrics missed which might cause WEB UI display issue while AQE is on.,SPARK-33016,13329722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,leanken,leanken,leanken,28/Sep/20 10:37,12/Oct/20 14:49,13/Jul/23 08:50,12/Oct/20 14:49,3.0.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"In current AQE execution, there might be a following scenario which might cause SQLMetrics being incorrectly override.
 # Stage A and B are created, and UI updated thru event onAdaptiveExecutionUpdate.
 # Stage A and B are running. Subquery in stage A keep updating metrics thru event onAdaptiveSQLMetricUpdate.
 # Stage B completes, while stage A's subquery is still running, updating metrics.
 # Completion of stage B triggers new stage creation and UI update thru event onAdaptiveExecutionUpdate again (just like step 1).

 

But it's very hard to re-produce this issue, since it was only happened with high concurrency. For the fix, I suggested that we might be able to keep all duplicated metrics instead of updating it every time.",,apachespark,cloud_fan,leanken,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 12 14:49:07 UTC 2020,,,,,,,,,,"0|z0iyzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/20 09:43;apachespark;User 'leanken' has created a pull request for this issue:
https://github.com/apache/spark/pull/29965;;;","07/Oct/20 09:43;apachespark;User 'leanken' has created a pull request for this issue:
https://github.com/apache/spark/pull/29965;;;","12/Oct/20 14:49;cloud_fan;Issue resolved by pull request 29965
[https://github.com/apache/spark/pull/29965];;;",,,,,,,,,,,,,,,,,,,,,,,
Compute the current date only once,SPARK-33015,13329702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,28/Sep/20 08:20,29/Sep/20 07:48,13/Jul/23 08:50,29/Sep/20 05:13,3.0.2,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"According to the doc for current_date(), it must compute the current date at the start of query evaluation: http://spark.apache.org/docs/latest/api/sql/#current_date but it can compute it multiple times: https://github.com/apache/spark/blob/0df8dd60733066076967f0525210bbdb5e12415a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala#L85",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 29 07:48:03 UTC 2020,,,,,,,,,,"0|z0iyv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/20 08:20;maxgekk;I am working on this;;;","28/Sep/20 08:35;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29889;;;","28/Sep/20 08:36;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29889;;;","29/Sep/20 05:13;cloud_fan;Issue resolved by pull request 29889
[https://github.com/apache/spark/pull/29889];;;","29/Sep/20 07:48;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29901;;;",,,,,,,,,,,,,,,,,,,,,
TreeNode.nodeName should not throw malformed class name error,SPARK-32999,13329489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,25/Sep/20 21:09,29/Sep/20 00:16,13/Jul/23 08:50,26/Sep/20 23:11,2.4.0,3.0.0,3.1.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"Similar to SPARK-32238, the use of {{java.lang.Class.getSimpleName}} in {{TreeNode.nodeName}} is problematic because Scala classes may trigger {{java.lang.InternalError: Malformed class name}}.

This happens more often when using nested classes in Scala (or declaring classes in Scala REPL which implies class nesting).

Note that on newer versions of JDK the underlying malformed class name no longer reproduces, so it's less of an issue there. But on JDK8u this problem still exists so we still have to fix it.",,apachespark,maropu,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 28 22:54:22 UTC 2020,,,,,,,,,,"0|z0ixk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/20 21:31;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/29875;;;","28/Sep/20 22:53;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/29896;;;","28/Sep/20 22:54;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/29896;;;",,,,,,,,,,,,,,,,,,,,,,,
Handle Option.empty v1.ExecutorSummary#peakMemoryMetrics,SPARK-32996,13329453,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shrutig,shrutig,shrutig,25/Sep/20 17:05,02/Oct/20 02:29,13/Jul/23 08:50,28/Sep/20 17:17,3.0.1,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,Web UI,,,,0,,,,"When {{peakMemoryMetrics}} in {{ExecutorSummary}} is {{Option.empty}}, then the {{ExecutorMetricsJsonSerializer#serialize}} method does not execute the {{jsonGenerator.writeObject}} method. This causes the json to be generated with {{peakMemoryMetrics}} key added to the serialized string, but no corresponding value.
This causes an error to be thrown when it is the next key {{attributes}} turn to be added to the json:
{{com.fasterxml.jackson.core.JsonGenerationException: Can not write a field name, expecting a value.}}

{{}}

{{}}",,apachespark,shrutig,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 19:45:07 UTC 2020,,,,,,,,,,"0|z0ixc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/20 17:14;apachespark;User 'shrutig' has created a pull request for this issue:
https://github.com/apache/spark/pull/29872;;;","25/Sep/20 17:14;apachespark;User 'shrutig' has created a pull request for this issue:
https://github.com/apache/spark/pull/29872;;;","28/Sep/20 17:18;viirya;Resolved by https://github.com/apache/spark/pull/29872.;;;","30/Sep/20 17:50;apachespark;User 'shrutig' has created a pull request for this issue:
https://github.com/apache/spark/pull/29914;;;","01/Oct/20 19:45;apachespark;User 'shrutig' has created a pull request for this issue:
https://github.com/apache/spark/pull/29926;;;",,,,,,,,,,,,,,,,,,,,,
"In OracleDialect, ""RowID"" SQL type should be converted into ""String"" Catalyst type",SPARK-32992,13329298,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maxgekk,peng,peng,24/Sep/20 22:28,01/Oct/20 05:52,13/Jul/23 08:50,01/Oct/20 05:52,2.4.7,3.1.0,,,,,,,,,,3.1.0,,,,SQL,,,,0,jdbc,jdbc_connector,,"Most JDBC drivers use long SQL type for dataset row ID:

 

(in org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils)
{code:java}
private def getCatalystType(
 sqlType: Int,
 precision: Int,
 scale: Int,
 signed: Boolean): DataType = {
 val answer = sqlType match {
 // scalastyle:off
 ...
 case java.sql.Types.ROWID => LongType
...
 case _ =>
 throw new SQLException(""Unrecognized SQL type "" + sqlType)
 // scalastyle:on
 }
if (answer == null)
{ throw new SQLException(""Unsupported type "" + JDBCType.valueOf(sqlType).getName) }
answer
{code}
 

Oracle JDBC drivers (of all versions) are rare exception, only String value can be extracted:

 

(in oracle.jdbc.driver.RowidAccessor, decompiled bytecode)
{code:java}
...
String getString(int var1) throws SQLException
{ return this.isNull(var1) ? null : this.rowData.getString(this.getOffset(var1), this.getLength(var1), this.statement.connection.conversion.getCharacterSet((short)1)); }
Object getObject(int var1) throws SQLException
{ return this.getROWID(var1); }
...
{code}
 

This caused an exception to be thrown when importing datasets from an Oracle DB, as reported in [https://stackoverflow.com/questions/52244492/spark-jdbc-dataframereader-fails-to-read-oracle-table-with-datatype-as-rowid:]
{code:java}
 
 {{18/09/08 11:38:17 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 5.0 (TID 23, gbrdsr000002985.intranet.barcapint.com, executor 21): java.sql.SQLException: Invalid column type: getLong not implemented for class oracle.jdbc.driver.T4CRowidAccessor at oracle.jdbc.driver.GeneratedAccessor.getLong(GeneratedAccessor.java:440)
 at oracle.jdbc.driver.GeneratedStatement.getLong(GeneratedStatement.java:228)
 at oracle.jdbc.driver.GeneratedScrollableResultSet.getLong(GeneratedScrollableResultSet.java:620)
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$8.apply(JdbcUtils.scala:365)
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$8.apply(JdbcUtils.scala:364)}}
 
{code}
 

Therefore, the default SQL type => Catalyst type conversion rule should be overriden in OracleDialect. Specifically, the following rule should be added:
{code:java}
case Types.ROWID => Some(StringType)
{code}
 ",,apachespark,maropu,peng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://stackoverflow.com/questions/52244492/spark-jdbc-dataframereader-fails-to-read-oracle-table-with-datatype-as-rowid,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 05:52:13 UTC 2020,,,,,,,,,,"0|z0iwdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/20 16:13;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29884;;;","01/Oct/20 05:52;maropu;Resolved by https://github.com/apache/spark/pull/29884;;;",,,,,,,,,,,,,,,,,,,,,,,,
Performance regression when selecting from str_to_map,SPARK-32989,13329274,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,ondrej,ondrej,24/Sep/20 19:36,12/Dec/22 18:11,13/Jul/23 08:50,12/Oct/20 09:53,3.0.1,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"When I create a map using str_to_map and select more than a single value, I notice a notable performance regression in 3.0.1 compared to 2.4.7. When selecting a single value, the performance is the same. Plans are identical between versions.

It seems like in 2.x the map from str_to_map is preserved for a given row, but in 3.x it's recalculated for each column. One hint that it might be the case is that when I tried forcing materialisation of said map in 3.x (by a coalesce, don't know if there's a better way), I got the performance roughly to 2.x levels.

Here's a reproducer (the csv in question gets autogenerated by the python code):
{code:java}
$ head regression.csv 
foo
foo=bar&baz=bak&bar=foo
foo=bar&baz=bak&bar=foo
foo=bar&baz=bak&bar=foo
foo=bar&baz=bak&bar=foo
foo=bar&baz=bak&bar=foo
... (10M more rows)
{code}
{code:python}
import time
import os

import pyspark  
from pyspark.sql import SparkSession

import pyspark.sql.functions as f

if __name__ == '__main__':
    print(pyspark.__version__)
    spark = SparkSession.builder.getOrCreate()

    filename = 'regression.csv'
    if not os.path.isfile(filename):
        with open(filename, 'wt') as fw:
            fw.write('foo\n')
            for _ in range(10_000_000):
                fw.write('foo=bar&baz=bak&bar=foo\n')

    df = spark.read.option('header', True).csv(filename)
    t = time.time()
    dd = (df
            .withColumn('my_map', f.expr('str_to_map(foo, ""&"", ""="")'))
            .select(
                f.col('my_map')['foo'],
            )
        )
    dd.write.mode('overwrite').csv('tmp')
    t2 = time.time()
    print('selected one', t2 - t)

    dd = (df
            .withColumn('my_map', f.expr('str_to_map(foo, ""&"", ""="")'))
            # .coalesce(100) # forcing evaluation before selection speeds it up in 3.0.1
            .select(
                f.col('my_map')['foo'],
                f.col('my_map')['bar'],
                f.col('my_map')['baz'],
            )
        )
    dd.explain(True)
    dd.write.mode('overwrite').csv('tmp')
    t3 = time.time()
    print('selected three', t3 - t2)
{code}
Results for 2.4.7 and 3.0.1, both installed from PyPI, Python 3.7, macOS (times are in seconds)
{code:java}
# 3.0.1
# selected one 6.375471830368042                                                  
# selected three 14.847578048706055

# 2.4.7
# selected one 6.679579019546509                                                  
# selected three 6.5622029304504395  
{code}",,dongjoon,LuciferYang,maropu,ondrej,Qin Yao,viirya,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33092,,,SPARK-30356,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 08 03:58:39 UTC 2020,,,,,,,,,,"0|z0iw88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/20 09:20;gurwls223;[~ondrej] would you mind showing a self-contained reproducer? (e.g., removing the I/O like reading CSV) so people can test which commit introduced this regression.;;;","27/Sep/20 09:22;gurwls223;From a cursory look, there are two JIRAs that might need to be checked SPARK-25829 and SPARK-30356. [~cloud_fan] and [~Qin Yao] FYI.;;;","27/Sep/20 10:17;ondrej;[~hyukjin.kwon] I wanted to do so originally, but removing input IO (by spark.createDataFrame) and output IO (by .count()) lead to some implicit caching that rendered the reproducer invalid, so this was the only reproducer I could come up with.

I guess I could replace the write IO by a .collect, but I can't get rid of that caching/preallocation when using createDataFrame (even if I create one for each of the two subruns).;;;","27/Sep/20 11:18;gurwls223;Can you upload the full CSV file?;;;","27/Sep/20 11:30;ondrej;[~hyukjin.kwon] I've added a file generator in the python code, so it can be run without prepping the input file;;;","30/Sep/20 13:49;LuciferYang;[~hyukjin.kwon] master also has this problem, may be related to SPARK-30356. After revert it, the performance return normal;;;","08/Oct/20 03:22;LuciferYang;[~ondrej] You're right, It will execute N times with codegen(SPARK-30356.) when selecting N columns use stringToMap expression compared to selecting One column, cc [~Qin Yao] [~cloud_fan];;;","08/Oct/20 03:58;LuciferYang;I found that if  stringToMap use codegen, the optimization of `spark.sql.subexpressionElimination.enabled` will be ignored.;;;",,,,,,,,,,,,,,,,,,
Launcher Client tests flake with minikube,SPARK-32980,13329065,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,holden,holden,23/Sep/20 19:56,23/Sep/20 22:40,13/Jul/23 08:50,23/Sep/20 22:40,3.1.0,,,,,,,,,,,3.1.0,,,,Kubernetes,Tests,,,0,,,,Launcher Client tests flake with minikube,,apachespark,dongjoon,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 22:40:05 UTC 2020,,,,,,,,,,"0|z0iuy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/20 20:27;holden;Our method of getting the service assumes the service is on the first line, but when a new version of minikube is released the first few lines are upgrade info.;;;","23/Sep/20 20:28;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29854;;;","23/Sep/20 22:40;dongjoon;Issue resolved by pull request 29854
[https://github.com/apache/spark/pull/29854];;;",,,,,,,,,,,,,,,,,,,,,,,
Incorrect number of dynamic part metric,SPARK-32978,13329026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,yumwang,yumwang,23/Sep/20 16:05,22/Oct/20 14:02,13/Jul/23 08:50,22/Oct/20 14:01,3.0.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"How to reproduce this issue:
{code:sql}
create table dynamic_partition(i bigint, part bigint) using parquet partitioned by (part);
insert overwrite table dynamic_partition partition(part) select id, id % 50 as part  from range(10000);
{code}
The number of dynamic part should be 50, but it is 800 on web UI.",,apachespark,Chen Zhang,cloud_fan,codingcat,LuciferYang,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/20 16:05;yumwang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13012020/screenshot-1.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 22 14:01:42 UTC 2020,,,,,,,,,,"0|z0iupc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/20 13:21;Chen Zhang;Hello, [~yumwang]

I used the default config Spark to run this code and failed to reproduce this issue.
{code:none}
number of written files: 50
written output: 55.4 KiB
number of output rows: 10,000
number of dynamic part: 50
{code};;;","24/Sep/20 13:43;yumwang;[~Chen Zhang] It is because your written files is also 50. Could you try to start Spark context by:

{code:sh}
bin/spark-sql --master local[4]
{code}
;;;","10/Oct/20 09:24;LuciferYang;It seems that there is no de-duplication mechanism;;;","13/Oct/20 09:34;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30026;;;","13/Oct/20 09:34;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/30026;;;","22/Oct/20 14:01;cloud_fan;Issue resolved by pull request 30026
[https://github.com/apache/spark/pull/30026];;;",,,,,,,,,,,,,,,,,,,,
[SQL] JavaDoc on Default Save mode Incorrect,SPARK-32977,13329022,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rspitzer,rspitzer,rspitzer,23/Sep/20 15:47,24/Sep/20 03:03,13/Jul/23 08:50,24/Sep/20 03:03,3.0.1,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"The JavaDoc says that the default save mode is dependent on DataSource version which is incorrect. It is always ErrorOnExists.

http://apache-spark-developers-list.1001551.n3.nabble.com/DatasourceV2-Default-Mode-for-DataFrameWriter-not-Dependent-on-DataSource-Version-td29434.html",,apachespark,dongjoon,rspitzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 24 03:03:33 UTC 2020,,,,,,,,,,"0|z0iuog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/20 16:28;rspitzer;[~brkyvz] We talked about this a while back, just submitted the PR to fix the doc. Could you please review?;;;","23/Sep/20 16:29;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/29853;;;","23/Sep/20 16:30;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/29853;;;","24/Sep/20 03:03;dongjoon;Issue resolved by pull request 29853
[https://github.com/apache/spark/pull/29853];;;",,,,,,,,,,,,,,,,,,,,,,
Add config for driver readiness timeout before executors start,SPARK-32975,13328980,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cchriswu,shensonj,shensonj,23/Sep/20 11:48,18/Feb/22 06:52,13/Jul/23 08:50,04/Jun/21 14:01,2.4.4,3.0.2,3.1.2,3.2.0,,,,,,,,3.1.3,3.2.0,,,Kubernetes,,,,0,,,,"We are using v1beta2-1.1.2-2.4.5 version of operator with spark-2.4.4

spark executors keeps getting killed with exit code 1 and we are seeing following exception in the executor which goes to error state. Once this error happens, driver doesn't restart executor. 

 

Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1713)
at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:64)
at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:188)
at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:281)
at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:201)
at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:65)
at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:64)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
... 4 more
Caused by: java.io.IOException: Failed to connect to act-pipeline-app-1600187491917-driver-svc.default.svc:7078
at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:198)
at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: act-pipeline-app-1600187491917-driver-svc.default.svc
at java.net.InetAddress.getAllByName0(InetAddress.java:1281)
at java.net.InetAddress.getAllByName(InetAddress.java:1193)
at java.net.InetAddress.getAllByName(InetAddress.java:1127)
at java.net.InetAddress.getByName(InetAddress.java:1077)
at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:146)
at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:143)
at java.security.AccessController.doPrivileged(Native Method)
at io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:143)
at io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:43)
at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:63)
at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:55)
at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:57)
at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:32)
at io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:108)
at io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:208)
at io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:49)
at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:188)
at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:174)
at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)
at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)
at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)
at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)
at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:82)
at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:978)
at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:512)
at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:423)
at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:482)
at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
... 1 more
CodeCache: size=245760Kb used=4762Kb max_used=4763Kb free=240997Kb
bounds [0x00007f49f5000000, 0x00007f49f54b0000, 0x00007f4a04000000]
total_blobs=1764 nmethods=1356 adapters=324
compilation: enabled

 

 

 

*Additional information:*

*The status of spark application shows it is RUNNING:*

kubectl describe sparkapplications.sparkoperator.k8s.io act-pipeline-app

...

...

Status:

  Application State:

    State:  RUNNING

  Driver Info:

    Pod Name:             act-pipeline-app-driver

    Web UI Address:       10.233.57.201:40550

    Web UI Port:          40550

    Web UI Service Name:  act-pipeline-app-ui-svc

  Execution Attempts:     1

  Executor State:

    act-pipeline-app-1600097064694-exec-1:  RUNNING

  Last Submission Attempt Time:             2020-09-14T15:24:26Z

  Spark Application Id:                     spark-942bb2e500c54f92ac357b818c712558

  Submission Attempts:                      1

  Submission ID:                            4ecdb6ca-d237-4524-b05e-c42cfcc73dc7

  Termination Time:                         <nil>

Events:                                     <none>

 

*The executor pod is reporting that it is Terminated:*

kubectl describe pod -l sparkoperator.k8s.io/app-name=act-pipeline-app,spark-role=executor

...

...

Containers:

  executor:

    Container ID:  docker://9aa5b585e8fb7390b87a4771f3ed1402cae41f0fe55905d0172ed6e90dde34e6

...

    Ports:         7079/TCP, 8090/TCP

    Host Ports:    0/TCP, 0/TCP

    Args:

      executor

    State:          Terminated

      Reason:       Error

      Exit Code:    1

      Started:      Mon, 14 Sep 2020 11:25:35 -0400

      Finished:     Mon, 14 Sep 2020 11:25:39 -0400

    Ready:          False

    Restart Count:  0

...

Conditions:

  Type              Status

  Initialized       True

  Ready             False

  ContainersReady   False

  PodScheduled      True

...

QoS Class:       Burstable

Node-Selectors:  <none>

Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s

                 node.kubernetes.io/unreachable:NoExecute for 300s

Events:          <none>

In early stage of the driver’s life the failed executor is not detected (it is assumed to be running) and therefore it will not be restarted.

 ",,apachespark,dongjoon,shensonj,singh-abhijeet,tfasanga,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 18 06:47:40 UTC 2022,,,,,,,,,,"0|z0iuf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/20 11:51;shensonj;[~anirudh4444] [~eje] [~liyinan926];;;","24/Sep/20 01:51;tfasanga;Note that the main problem is that the executor POD quits with error and Spark driver and Spark operator think it is still running, therefore the executor is never restarted.

This is intermittent problem. Our testing shows that this happens frequently when the following is true: 
 # the driver POD has a sidecar container, and
 # it takes longer to initialize and start the sidecar container (this delay is caused by time required to pull the image of the sidecar container)

In other words, this problem manifests itself when there is a delay between starting the driver *container* and the time the driver *POD* is fully started (the POD contains the driver container and the sidecar container).

In this case we see the following events in the description of the driver POD: (see the ""_Pulling image ""registry.nspos.nokia.local/fluent/fluent-bit:1.5.5_"" event that is present in this case) 
{code:java}
Events:
  Type     Reason       Age        From               Message
  ----     ------       ----       ----               -------
  Normal   Scheduled    <unknown>  default-scheduler  Successfully assigned default/act-pipeline-app-driver to node5
  Warning  FailedMount  20m        kubelet, node5     MountVolume.SetUp failed for volume ""spark-conf-volume"" : configmap ""act-pipeline-app-1600699152173-driver-conf-map"" not found
  Normal   Pulled       20m        kubelet, node5     Container image ""registry.nspos.nokia.local/nspos-pki-container:20.9.0-rel.1"" already present on machine
  Normal   Created      20m        kubelet, node5     Created container nspos-pki
  Normal   Started      20m        kubelet, node5     Started container nspos-pki
  Normal   Pulling      20m        kubelet, node5     Pulling image ""registry.nspos.nokia.local/analytics-rtanalytics-pipeline-app:20.9.0-rel.48""
  Normal   Pulled       19m        kubelet, node5     Successfully pulled image ""registry.nspos.nokia.local/analytics-rtanalytics-pipeline-app:20.9.0-rel.48""
  Normal   Created      19m        kubelet, node5     Created container spark-kubernetes-driver
  Normal   Started      19m        kubelet, node5     Started container spark-kubernetes-driver
  Normal   Pulling      19m        kubelet, node5     Pulling image ""registry.nspos.nokia.local/fluent/fluent-bit:1.5.5""
  Normal   Pulled       18m        kubelet, node5     Successfully pulled image ""registry.nspos.nokia.local/fluent/fluent-bit:1.5.5""
  Normal   Created      18m        kubelet, node5     Created container log-sidecar
  Normal   Started      18m        kubelet, node5     Started container log-sidecar
{code}
Note: The message ""_MountVolume.SetUp failed for volume ""spark-conf-volume"" : configmap ""act-pipeline-app-1600699152173-driver-conf-map"" not found_"" seems to be unrelated and does not seem to cause any problems.;;;","02/Jun/21 06:43;apachespark;User 'cchriswu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32739;;;","02/Jun/21 16:12;apachespark;User 'cchriswu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32752;;;","04/Jun/21 14:01;dongjoon;Issue resolved by pull request 32752
[https://github.com/apache/spark/pull/32752];;;","09/Jun/21 02:26;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/32830;;;","18/Feb/22 06:47;singh-abhijeet;Though the issue points to driver and the fix is related to a driver config, but I was getting the same error because sidecar injection was happening to executor pod and sidecar container was taking more time to initialize than the exec container.

 

I resolved it by adding a sleep/wait time in entrypoint.sh for exec, but it would be neat to have a _spark.kubernetes.allocation.executor.readinessWait_ config which allows to set wait time.;;;",,,,,,,,,,,,,,,,,,,
empty string should be consistent for schema name in SparkGetSchemasOperation,SPARK-32963,13328721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,22/Sep/20 08:48,06/Oct/20 16:02,13/Jul/23 08:50,06/Oct/20 16:02,3.0.1,3.1.0,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"When the schema name is empty string, it is considered as "".*"" and can match all databases in the catalog.
But when it can not match the global temp view as it is not converted to "".*""",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 16:02:00 UTC 2020,,,,,,,,,,"0|z0istk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/20 08:59;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29834;;;","22/Sep/20 08:59;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29834;;;","06/Oct/20 16:02;cloud_fan;Issue resolved by pull request 29834
[https://github.com/apache/spark/pull/29834];;;",,,,,,,,,,,,,,,,,,,,,,,
"Fix the ""Relation: view text"" test in DataSourceV2SQLSuite",SPARK-32959,13328639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,imback82,imback82,imback82,21/Sep/20 20:21,23/Sep/20 05:50,13/Jul/23 08:50,23/Sep/20 05:50,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"The existing code just defines a function literal and doesn't execute it:
{code:java}
test(""Relation: view text"") {
  val t1 = ""testcat.ns1.ns2.tbl""
  withTable(t1) {
    withView(""view1"") { v1: String =>
      sql(s""CREATE TABLE $t1 USING foo AS SELECT id, data FROM source"")
      sql(s""CREATE VIEW $v1 AS SELECT * from $t1"")
      checkAnswer(sql(s""TABLE $v1""), spark.table(""source""))
    }
  }
}
{code}",,apachespark,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 05:50:03 UTC 2020,,,,,,,,,,"0|z0isbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/20 20:24;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/29811;;;","21/Sep/20 20:24;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/29811;;;","23/Sep/20 05:50;cloud_fan;Issue resolved by pull request 29811
[https://github.com/apache/spark/pull/29811];;;",,,,,,,,,,,,,,,,,,,,,,,
An item in the navigation bar in the WebUI has a wrong link.,SPARK-32955,13328582,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,21/Sep/20 14:49,12/Dec/22 18:11,13/Jul/23 08:50,22/Sep/20 05:47,2.4.7,3.0.2,3.1.0,,,,,,,,,3.1.0,,,,Documentation,,,,0,,,,"The item ""More"" in the navigation bar in the WebUI links to api.html. It seems wrong.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 05:47:09 UTC 2020,,,,,,,,,,"0|z0iryo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/20 15:35;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29821;;;","21/Sep/20 15:36;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29821;;;","22/Sep/20 05:47;gurwls223;Issue resolved by pull request 29821
[https://github.com/apache/spark/pull/29821];;;",,,,,,,,,,,,,,,,,,,,,,,
DecomissionSuite in k8s integration tests is failing.,SPARK-32937,13328229,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,prashant,prashant,18/Sep/20 11:45,12/Dec/22 18:11,13/Jul/23 08:50,23/Sep/20 22:39,3.1.0,,,,,,,,,,,3.1.0,,,,Kubernetes,,,,0,,,,"
Logs from the failing test, copied from jenkins. As of now, it is always failing. 

{code}
- Test basic decommissioning *** FAILED ***
  The code passed to eventually never returned normally. Attempted 182 times over 3.00377927275 minutes. Last failure message: ""++ id -u
  + myuid=185
  ++ id -g
  + mygid=0
  + set +e
  ++ getent passwd 185
  + uidentry=
  + set -e
  + '[' -z '' ']'
  + '[' -w /etc/passwd ']'
  + echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false'
  + SPARK_CLASSPATH=':/opt/spark/jars/*'
  + env
  + grep SPARK_JAVA_OPT_
  + sort -t_ -k4 -n
  + sed 's/[^=]*=\(.*\)/\1/g'
  + readarray -t SPARK_EXECUTOR_JAVA_OPTS
  + '[' -n '' ']'
  + '[' 3 == 2 ']'
  + '[' 3 == 3 ']'
  ++ python3 -V
  + pyv3='Python 3.7.3'
  + export PYTHON_VERSION=3.7.3
  + PYTHON_VERSION=3.7.3
  + export PYSPARK_PYTHON=python3
  + PYSPARK_PYTHON=python3
  + export PYSPARK_DRIVER_PYTHON=python3
  + PYSPARK_DRIVER_PYTHON=python3
  + '[' -n '' ']'
  + '[' -z ']'
  + '[' -z x ']'
  + SPARK_CLASSPATH='/opt/spark/conf::/opt/spark/jars/*'
  + case ""$1"" in
  + shift 1
  + CMD=(""$SPARK_HOME/bin/spark-submit"" --conf ""spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS"" --deploy-mode client ""$@"")
  + exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=172.17.0.4 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner local:///opt/spark/tests/decommissioning.py
  20/09/17 11:06:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  Starting decom test
  Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
  20/09/17 11:06:56 INFO SparkContext: Running Spark version 3.1.0-SNAPSHOT
  20/09/17 11:06:57 INFO ResourceUtils: ==============================================================
  20/09/17 11:06:57 INFO ResourceUtils: No custom resources configured for spark.driver.
  20/09/17 11:06:57 INFO ResourceUtils: ==============================================================
  20/09/17 11:06:57 INFO SparkContext: Submitted application: PyMemoryTest
  20/09/17 11:06:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
  20/09/17 11:06:57 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
  20/09/17 11:06:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
  20/09/17 11:06:57 INFO SecurityManager: Changing view acls to: 185,jenkins
  20/09/17 11:06:57 INFO SecurityManager: Changing modify acls to: 185,jenkins
  20/09/17 11:06:57 INFO SecurityManager: Changing view acls groups to: 
  20/09/17 11:06:57 INFO SecurityManager: Changing modify acls groups to: 
  20/09/17 11:06:57 INFO SecurityManager: SecurityManager: authentication enabled; ui acls disabled; users  with view permissions: Set(185, jenkins); groups with view permissions: Set(); users  with modify permissions: Set(185, jenkins); groups with modify permissions: Set()
  20/09/17 11:06:57 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
  20/09/17 11:06:57 INFO SparkEnv: Registering MapOutputTracker
  20/09/17 11:06:57 INFO SparkEnv: Registering BlockManagerMaster
  20/09/17 11:06:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
  20/09/17 11:06:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
  20/09/17 11:06:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
  20/09/17 11:06:57 INFO DiskBlockManager: Created local directory at /var/data/spark-7985c075-3b02-42ec-9111-cefba535adf0/blockmgr-3bd403d0-6689-46be-997e-5bc699ecefd3
  20/09/17 11:06:57 INFO MemoryStore: MemoryStore started with capacity 593.9 MiB
  20/09/17 11:06:57 INFO SparkEnv: Registering OutputCommitCoordinator
  20/09/17 11:06:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
  20/09/17 11:06:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc:4040
  20/09/17 11:06:58 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
  20/09/17 11:06:59 INFO ExecutorPodsAllocator: Going to request 3 executors from Kubernetes.
  20/09/17 11:06:59 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : 
  20/09/17 11:07:00 INFO BasicExecutorFeatureStep: Adding decommission script to lifecycle
  20/09/17 11:07:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
  20/09/17 11:07:00 INFO NettyBlockTransferService: Server created on spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc:7079
  20/09/17 11:07:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
  20/09/17 11:07:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc, 7079, None)
  20/09/17 11:07:00 INFO BlockManagerMasterEndpoint: Registering block manager spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc:7079 with 593.9 MiB RAM, BlockManagerId(driver, spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc, 7079, None)
  20/09/17 11:07:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc, 7079, None)
  20/09/17 11:07:00 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : 
  20/09/17 11:07:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc, 7079, None)
  20/09/17 11:07:00 INFO BasicExecutorFeatureStep: Adding decommission script to lifecycle
  20/09/17 11:07:00 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : 
  20/09/17 11:07:00 INFO BasicExecutorFeatureStep: Adding decommission script to lifecycle
  20/09/17 11:07:00 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : 
  20/09/17 11:07:05 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.6:50176) with ID 2,  ResourceProfileId 0
  20/09/17 11:07:05 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.5:35624) with ID 1,  ResourceProfileId 0
  20/09/17 11:07:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.0.6:33547 with 593.9 MiB RAM, BlockManagerId(2, 172.17.0.6, 33547, None)
  20/09/17 11:07:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.0.5:46327 with 593.9 MiB RAM, BlockManagerId(1, 172.17.0.5, 46327, None)
  20/09/17 11:07:29 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
  20/09/17 11:07:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/work-dir/spark-warehouse').
  20/09/17 11:07:30 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
  20/09/17 11:07:32 INFO SparkContext: Starting job: collect at /opt/spark/tests/decommissioning.py:44
  20/09/17 11:07:32 INFO DAGScheduler: Registering RDD 2 (groupByKey at /opt/spark/tests/decommissioning.py:43) as input to shuffle 0
  20/09/17 11:07:32 INFO DAGScheduler: Got job 0 (collect at /opt/spark/tests/decommissioning.py:44) with 5 output partitions
  20/09/17 11:07:32 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/spark/tests/decommissioning.py:44)
  20/09/17 11:07:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
  20/09/17 11:07:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
  20/09/17 11:07:32 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[2] at groupByKey at /opt/spark/tests/decommissioning.py:43), which has no missing parents
  20/09/17 11:07:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.6 KiB, free 593.9 MiB)
  20/09/17 11:07:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 593.9 MiB)
  20/09/17 11:07:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc:7079 (size: 6.5 KiB, free: 593.9 MiB)
  20/09/17 11:07:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1348
  20/09/17 11:07:32 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 0 (PairwiseRDD[2] at groupByKey at /opt/spark/tests/decommissioning.py:43) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
  20/09/17 11:07:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 5 tasks resource profile 0
  20/09/17 11:07:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.17.0.6, executor 2, partition 0, PROCESS_LOCAL, 7341 bytes) taskResourceAssignments Map()
  20/09/17 11:07:32 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.17.0.5, executor 1, partition 1, PROCESS_LOCAL, 7341 bytes) taskResourceAssignments Map()
  20/09/17 11:07:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.5:46327 (size: 6.5 KiB, free: 593.9 MiB)
  20/09/17 11:07:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.6:33547 (size: 6.5 KiB, free: 593.9 MiB)
  20/09/17 11:07:34 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.17.0.5, executor 1, partition 2, PROCESS_LOCAL, 7341 bytes) taskResourceAssignments Map()
  20/09/17 11:07:34 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1825 ms on 172.17.0.5 (executor 1) (1/5)
  20/09/17 11:07:34 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 47109
  20/09/17 11:07:34 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.17.0.6, executor 2, partition 3, PROCESS_LOCAL, 7341 bytes) taskResourceAssignments Map()
  20/09/17 11:07:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1960 ms on 172.17.0.6 (executor 2) (2/5)
  20/09/17 11:07:34 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.17.0.5, executor 1, partition 4, PROCESS_LOCAL, 7341 bytes) taskResourceAssignments Map()
  20/09/17 11:07:34 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 136 ms on 172.17.0.5 (executor 1) (3/5)
  20/09/17 11:07:34 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 119 ms on 172.17.0.6 (executor 2) (4/5)
  20/09/17 11:07:34 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 117 ms on 172.17.0.5 (executor 1) (5/5)
  20/09/17 11:07:34 INFO DAGScheduler: ShuffleMapStage 0 (groupByKey at /opt/spark/tests/decommissioning.py:43) finished in 2.352 s
  20/09/17 11:07:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
  20/09/17 11:07:34 INFO DAGScheduler: looking for newly runnable stages
  20/09/17 11:07:34 INFO DAGScheduler: running: Set()
  20/09/17 11:07:34 INFO DAGScheduler: waiting: Set(ResultStage 1)
  20/09/17 11:07:34 INFO DAGScheduler: failed: Set()
  20/09/17 11:07:34 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at collect at /opt/spark/tests/decommissioning.py:44), which has no missing parents
  20/09/17 11:07:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KiB, free 593.9 MiB)
  20/09/17 11:07:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 593.9 MiB)
  20/09/17 11:07:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc:7079 (size: 5.4 KiB, free: 593.9 MiB)
  20/09/17 11:07:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1348
  20/09/17 11:07:34 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 1 (PythonRDD[5] at collect at /opt/spark/tests/decommissioning.py:44) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
  20/09/17 11:07:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 5 tasks resource profile 0
  20/09/17 11:07:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 5) (172.17.0.6, executor 2, partition 0, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:07:34 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 6) (172.17.0.5, executor 1, partition 1, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:07:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.6:33547 (size: 5.4 KiB, free: 593.9 MiB)
  20/09/17 11:07:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.5:46327 (size: 5.4 KiB, free: 593.9 MiB)
  20/09/17 11:07:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.17.0.5:35624
  20/09/17 11:07:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.17.0.6:50176
  20/09/17 11:07:35 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 7) (172.17.0.6, executor 2, partition 2, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:07:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 5) in 729 ms on 172.17.0.6 (executor 2) (1/5)
  20/09/17 11:07:35 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 8) (172.17.0.5, executor 1, partition 3, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:07:35 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 6) in 755 ms on 172.17.0.5 (executor 1) (2/5)
  20/09/17 11:07:35 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 9) (172.17.0.6, executor 2, partition 4, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:07:35 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 7) in 113 ms on 172.17.0.6 (executor 2) (3/5)
  20/09/17 11:07:35 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 8) in 104 ms on 172.17.0.5 (executor 1) (4/5)
  20/09/17 11:07:35 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 9) in 82 ms on 172.17.0.6 (executor 2) (5/5)
  20/09/17 11:07:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
  20/09/17 11:07:35 INFO DAGScheduler: ResultStage 1 (collect at /opt/spark/tests/decommissioning.py:44) finished in 0.943 s
  20/09/17 11:07:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
  20/09/17 11:07:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
  20/09/17 11:07:35 INFO DAGScheduler: Job 0 finished: collect at /opt/spark/tests/decommissioning.py:44, took 3.420388 s
  1st accumulator value is: 100
  Waiting to give nodes time to finish migration, decom exec 1.
  ...
  20/09/17 11:07:36 WARN KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Received executor 1 decommissioned message
  20/09/17 11:07:36 INFO ShuffleStatus: Updating map output for 4 to BlockManagerId(2, 172.17.0.6, 33547, None)
  20/09/17 11:07:36 INFO ShuffleStatus: Updating map output for 1 to BlockManagerId(2, 172.17.0.6, 33547, None)
  20/09/17 11:07:36 INFO ShuffleStatus: Updating map output for 2 to BlockManagerId(2, 172.17.0.6, 33547, None)
  20/09/17 11:07:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc:7079 in memory (size: 5.4 KiB, free: 593.9 MiB)
  20/09/17 11:07:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.17.0.5:46327 in memory (size: 5.4 KiB, free: 593.9 MiB)
  20/09/17 11:07:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.17.0.6:33547 in memory (size: 5.4 KiB, free: 593.9 MiB)
  20/09/17 11:07:37 ERROR TaskSchedulerImpl: Lost executor 1 on 172.17.0.5: Executor decommission.
  20/09/17 11:07:37 INFO DAGScheduler: Executor lost: 1 (epoch 1)
  20/09/17 11:07:37 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
  20/09/17 11:07:37 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 172.17.0.5, 46327, None)
  20/09/17 11:07:37 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
  20/09/17 11:07:37 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 1)
  20/09/17 11:07:41 INFO ExecutorPodsAllocator: Going to request 1 executors from Kubernetes.
  20/09/17 11:07:41 INFO BasicExecutorFeatureStep: Adding decommission script to lifecycle
  20/09/17 11:07:41 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : 
  20/09/17 11:07:43 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.5:35848) with ID 3,  ResourceProfileId 0
  20/09/17 11:07:43 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.0.5:34299 with 593.9 MiB RAM, BlockManagerId(3, 172.17.0.5, 34299, None)
  20/09/17 11:08:05 INFO SparkContext: Starting job: count at /opt/spark/tests/decommissioning.py:49
  20/09/17 11:08:05 INFO DAGScheduler: Got job 1 (count at /opt/spark/tests/decommissioning.py:49) with 5 output partitions
  20/09/17 11:08:05 INFO DAGScheduler: Final stage: ResultStage 3 (count at /opt/spark/tests/decommissioning.py:49)
  20/09/17 11:08:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
  20/09/17 11:08:05 INFO DAGScheduler: Missing parents: List()
  20/09/17 11:08:05 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[6] at count at /opt/spark/tests/decommissioning.py:49), which has no missing parents
  20/09/17 11:08:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.6 KiB, free 593.9 MiB)
  20/09/17 11:08:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 593.9 MiB)
  20/09/17 11:08:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc:7079 (size: 5.9 KiB, free: 593.9 MiB)
  20/09/17 11:08:05 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1348
  20/09/17 11:08:05 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 3 (PythonRDD[6] at count at /opt/spark/tests/decommissioning.py:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
  20/09/17 11:08:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 5 tasks resource profile 0
  20/09/17 11:08:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 10) (172.17.0.6, executor 2, partition 0, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:08:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.17.0.6:33547 (size: 5.9 KiB, free: 593.9 MiB)
  20/09/17 11:08:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.17.0.6:50176
  20/09/17 11:08:05 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 10) in 133 ms on 172.17.0.6 (executor 2) (1/5)
  20/09/17 11:08:05 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 11) (172.17.0.6, executor 2, partition 1, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:08:05 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 12) (172.17.0.6, executor 2, partition 2, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:08:05 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 11) in 81 ms on 172.17.0.6 (executor 2) (2/5)
  20/09/17 11:08:05 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 13) (172.17.0.6, executor 2, partition 3, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:08:05 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 12) in 85 ms on 172.17.0.6 (executor 2) (3/5)
  20/09/17 11:08:05 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 14) (172.17.0.6, executor 2, partition 4, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:08:05 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 13) in 73 ms on 172.17.0.6 (executor 2) (4/5)
  20/09/17 11:08:06 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 14) in 91 ms on 172.17.0.6 (executor 2) (5/5)
  20/09/17 11:08:06 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
  20/09/17 11:08:06 INFO DAGScheduler: ResultStage 3 (count at /opt/spark/tests/decommissioning.py:49) finished in 0.478 s
  20/09/17 11:08:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
  20/09/17 11:08:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
  20/09/17 11:08:06 INFO DAGScheduler: Job 1 finished: count at /opt/spark/tests/decommissioning.py:49, took 0.489355 s
  20/09/17 11:08:06 INFO SparkContext: Starting job: collect at /opt/spark/tests/decommissioning.py:50
  20/09/17 11:08:06 INFO DAGScheduler: Got job 2 (collect at /opt/spark/tests/decommissioning.py:50) with 5 output partitions
  20/09/17 11:08:06 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/spark/tests/decommissioning.py:50)
  20/09/17 11:08:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
  20/09/17 11:08:06 INFO DAGScheduler: Missing parents: List()
  20/09/17 11:08:06 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc:7079 in memory (size: 5.9 KiB, free: 593.9 MiB)
  20/09/17 11:08:06 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[5] at collect at /opt/spark/tests/decommissioning.py:44), which has no missing parents
  20/09/17 11:08:06 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.17.0.6:33547 in memory (size: 5.9 KiB, free: 593.9 MiB)
  20/09/17 11:08:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.3 KiB, free 593.9 MiB)
  20/09/17 11:08:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 593.9 MiB)
  20/09/17 11:08:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc:7079 (size: 5.4 KiB, free: 593.9 MiB)
  20/09/17 11:08:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1348
  20/09/17 11:08:06 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 5 (PythonRDD[5] at collect at /opt/spark/tests/decommissioning.py:44) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
  20/09/17 11:08:06 INFO TaskSchedulerImpl: Adding task set 5.0 with 5 tasks resource profile 0
  20/09/17 11:08:06 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 15) (172.17.0.6, executor 2, partition 0, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:08:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.17.0.6:33547 (size: 5.4 KiB, free: 593.9 MiB)
  20/09/17 11:08:06 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 16) (172.17.0.6, executor 2, partition 1, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:08:06 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 15) in 105 ms on 172.17.0.6 (executor 2) (1/5)
  20/09/17 11:08:06 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 16) in 84 ms on 172.17.0.6 (executor 2) (2/5)
  20/09/17 11:08:06 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 17) (172.17.0.6, executor 2, partition 2, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:08:06 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 18) (172.17.0.6, executor 2, partition 3, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:08:06 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 17) in 76 ms on 172.17.0.6 (executor 2) (3/5)
  20/09/17 11:08:06 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 19) (172.17.0.6, executor 2, partition 4, NODE_LOCAL, 7162 bytes) taskResourceAssignments Map()
  20/09/17 11:08:06 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 18) in 72 ms on 172.17.0.6 (executor 2) (4/5)
  20/09/17 11:08:06 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 19) in 90 ms on 172.17.0.6 (executor 2) (5/5)
  20/09/17 11:08:06 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
  20/09/17 11:08:06 INFO DAGScheduler: ResultStage 5 (collect at /opt/spark/tests/decommissioning.py:50) finished in 0.448 s
  20/09/17 11:08:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
  20/09/17 11:08:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
  20/09/17 11:08:06 INFO DAGScheduler: Job 2 finished: collect at /opt/spark/tests/decommissioning.py:50, took 0.460430 s
  Final accumulator value is: 100
  Finished waiting, stopping Spark.
  20/09/17 11:08:06 INFO SparkUI: Stopped Spark web UI at http://spark-test-app-08853d749bbee080-driver-svc.a0af92633bef4a91b5f7e262e919afd9.svc:4040
  20/09/17 11:08:06 INFO KubernetesClusterSchedulerBackend: Shutting down all executors
  20/09/17 11:08:06 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down
  20/09/17 11:08:06 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)
  20/09/17 11:08:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
  20/09/17 11:08:06 INFO MemoryStore: MemoryStore cleared
  20/09/17 11:08:06 INFO BlockManager: BlockManager stopped
  20/09/17 11:08:06 INFO BlockManagerMaster: BlockManagerMaster stopped
  20/09/17 11:08:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
  20/09/17 11:08:06 INFO SparkContext: Successfully stopped SparkContext
  Done, exiting Python
  20/09/17 11:08:07 INFO ShutdownHookManager: Shutdown hook called
  20/09/17 11:08:07 INFO ShutdownHookManager: Deleting directory /var/data/spark-7985c075-3b02-42ec-9111-cefba535adf0/spark-d5ac2f3e-fe8b-4122-8026-807d265f3a69/pyspark-62a6caeb-b2e5-4b8f-8eb3-e7b2c5fb155c
  20/09/17 11:08:07 INFO ShutdownHookManager: Deleting directory /var/data/spark-7985c075-3b02-42ec-9111-cefba535adf0/spark-d5ac2f3e-fe8b-4122-8026-807d265f3a69
  20/09/17 11:08:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-b74e6224-3fa7-40d2-abc4-6622bd524e65
  "" did not contain ""Received decommission executor message"" The application did not complete, did not find str Received decommission executor message. (KubernetesSuite.scala:387)
Run completed in 12 minutes, 29 seconds.
Total number of tests run: 18
Suites: completed 2, aborted 0
Tests: succeeded 17, failed 1, canceled 0, ignored 0, pending 0
*** 1 TEST FAILED ***
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Spark Project Parent POM 3.1.0-SNAPSHOT:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [  4.094 s]
[INFO] Spark Project Tags ................................. SUCCESS [  8.630 s]
[INFO] Spark Project Local DB ............................. SUCCESS [  4.062 s]
[INFO] Spark Project Networking ........................... SUCCESS [  5.891 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  3.059 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [ 10.869 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  3.432 s]
[INFO] Spark Project Core ................................. SUCCESS [02:26 min]
[INFO] Spark Project Kubernetes Integration Tests ......... FAILURE [15:13 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  18:21 min
[INFO] Finished at: 2020-09-17T04:10:08-07:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.scalatest:scalatest-maven-plugin:2.0.0:test (integration-test) on project spark-kubernetes-integration-tests_2.12: There are test failures -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :spark-kubernetes-integration-tests_2.12
+ retcode3=1
+ kill -9 82255
+ minikube stop
:   Stopping ""minikube"" in kvm2 ...
-   ""minikube"" stopped.
/tmp/hudson6767824981271828433.sh: line 66: 82255 Killed                  minikube mount ${PVC_TESTS_HOST_PATH}:${PVC_TESTS_VM_PATH} --9p-version=9p2000.L --gid=0 --uid=185
+ [[ 1 = 0 ]]
+ test_status=failure
+ /home/jenkins/bin/post_github_pr_comment.py
Attempting to post to Github...
 > Post successful.
+ rm -rf /tmp/tmp.epTpFHp0Dl
+ exit 1
Build step 'Execute shell' marked build as failure
{code}",,apachespark,dongjoon,holden,Ngone51,prashant,,,,,,,,,,,,,,,,,,,SPARK-32979,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 22:39:52 UTC 2020,,,,,,,,,,"0|z0ips8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/20 01:50;gurwls223;Oops, I left a comment in the wrong JIRA. I removed it back [~prashant].;;;","20/Sep/20 01:51;gurwls223;cc [~holden]can you take a look please?;;;","20/Sep/20 02:18;holden;It looks like someone changed the logging message again without running the K8s tests. The commit that caused this is 56ae95053df4afa9764df3f1d88f300896ca0183. I will ask the committer who committed it to revert the PR.;;;","20/Sep/20 02:26;gurwls223;[~Ngone51] can you take a look? If this isn't easy to fix, let's revert for now as Holden said.;;;","20/Sep/20 02:32;holden;I want to be clear, I'm asking for a revert then fix. I do not believe attempt to fix in dev branch is the correct path forward. I believe the failure of this test demonstrates the original PR requires additional review.;;;","20/Sep/20 02:45;gurwls223;This is a usual approach to make a followup if that can be easily done when it's not the part of the regular PR builder. I can give you a lot of references.
If you think this is not the correct path forward in the dev branch, we'll have to discuss and revert PRs whenever the test fails even when it's not in the regular PR builder.

If this specific test failure looks not the case, please go ahead and revert [~holden] after commenting in the original PR.;;;","20/Sep/20 02:52;holden;So there are additional proposed changes to this chunk of code being reviewed. I'll go ahead and put a pending -1 on them because I don't want us in the situation where it's no longer a simple revert to fix this test suite. If it's not resolved by Monday I'll go ahead and make the revert myself.;;;","20/Sep/20 03:43;gurwls223;Sure, if there's a technical reason to revert, I believe that's right to revert. I trust your judgement.

 

What I meant is that: practically the reverts dont happen often when what we need is just a simple fix (e.g., fixing few lines in the tests) if it doesn't break the regular PR builder.;;;","21/Sep/20 02:28;Ngone51;I'm looking at it. Thanks for reporting!;;;","23/Sep/20 19:01;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29854;;;","23/Sep/20 22:39;dongjoon;Issue resolved by pull request 29854
[https://github.com/apache/spark/pull/29854];;;",,,,,,,,,,,,,,,
Web UI sort on duration is wrong,SPARK-32924,13328107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Baohe Zhang,toopt4,toopt4,17/Sep/20 18:10,12/Dec/22 18:11,13/Jul/23 08:50,04/Mar/21 23:39,2.4.6,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,Web UI,,,,0,,,,"See attachment, 9 s(econds) is showing as larger than 8.1min",,apachespark,Baohe Zhang,dongjoon,pralabhkumar,rakson,tgraves,toopt4,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/20 18:10;toopt4;ui_sort.png;https://issues.apache.org/jira/secure/attachment/13011708/ui_sort.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 23:43:12 UTC 2021,,,,,,,,,,"0|z0ip14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/20 07:34;gurwls223;Which codes did you run?;;;","09/Oct/20 13:48;rakson;I think its due to string sorting. One similar issue is fixed here SPARK-31983;;;","21/Nov/20 17:40;pralabhkumar;[~rakson] [~hyukjin.kwon]

Can I open PR for this ?;;;","21/Nov/20 18:17;toopt4;[~pralabhkumar] yes;;;","04/Mar/21 20:29;apachespark;User 'baohe-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31743;;;","04/Mar/21 23:39;dongjoon;Issue resolved by pull request 31743
[https://github.com/apache/spark/pull/31743];;;","04/Mar/21 23:43;Baohe Zhang;[~dongjoon] This is my Jira id.;;;",,,,,,,,,,,,,,,,,,,
UnsafeExternalSorter.SpillableIterator cannot spill after reading all records,SPARK-32911,13328043,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tomvanbussel,tomvanbussel,tomvanbussel,17/Sep/20 12:06,31/May/22 22:57,13/Jul/23 08:50,18/Sep/20 11:49,2.4.7,3.0.1,,,,,,,,,,3.0.4,3.1.0,,,Spark Core,,,,0,,,,"No memory is freed after calling {{UnsafeExternalSorter.SpillableIterator.spill()}} when all records have been read, even though it is still holding onto some memory. This may starve other {{MemoryConsumer}}s of memory.",,apachespark,cloud_fan,joshrosen,tomvanbussel,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 31 22:57:05 UTC 2022,,,,,,,,,,"0|z0iomw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/20 14:58;apachespark;User 'tomvanbussel' has created a pull request for this issue:
https://github.com/apache/spark/pull/29787;;;","18/Sep/20 11:49;cloud_fan;Issue resolved by pull request 29787
[https://github.com/apache/spark/pull/29787];;;","31/May/22 22:57;joshrosen;Also backported this to 3.0.4 since it's the only UnsafeExternalSorter change not in that branch and it was creating conflicts for other backports.;;;",,,,,,,,,,,,,,,,,,,,,,,
percentile_approx() returns incorrect results,SPARK-32908,13327984,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,17/Sep/20 07:22,12/Dec/22 18:10,13/Jul/23 08:50,18/Sep/20 01:50,2.3.4,2.4.7,3.0.1,3.1.0,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,correctness,,,"Read input data from the attached CSV file:
{code:scala}
      val df = spark.read.option(""header"", ""true"")
        .option(""inferSchema"", ""true"")
        .csv(""/Users/maximgekk/tmp/percentile_approx-input.csv"")
        .repartition(1)
      df.createOrReplaceTempView(table)
{code}
Calculate the 0.77 percentile with accuracy 1e-05:
{code:Scala}
      spark.sql(
        s""""""SELECT
           |  percentile_approx(tr_rat_resampling_score, 0.77, 100000)
           |FROM $table
           """""".stripMargin).show
{code}
{code}
+------------------------------------------------------------------------+
|percentile_approx(tr_rat_resampling_score, CAST(0.77 AS DOUBLE), 100000)|
+------------------------------------------------------------------------+
|                                                                    1000|
+------------------------------------------------------------------------+
{code}
 The same for smaller accuracy 0.001:
{code}
+----------------------------------------------------------------------+
|percentile_approx(tr_rat_resampling_score, CAST(0.77 AS DOUBLE), 1000)|
+----------------------------------------------------------------------+
|                                                                    18|
+----------------------------------------------------------------------+
{code} 
and better accuracy 1e-06:
{code}
+-------------------------------------------------------------------------+
|percentile_approx(tr_rat_resampling_score, CAST(0.77 AS DOUBLE), 1000000)|
+-------------------------------------------------------------------------+
|                                                                       17|
+-------------------------------------------------------------------------+
{code}

For the accuracy 1e-05, the result must be around 17-18 but not 1000.

Here is percentile calculation in Google Sheets for the same input:
https://docs.google.com/spreadsheets/d/1Y1i4Td6s9jZQ-bD4IRTESLXP3UxKpqJSXGtmx0Q5TA0/edit?usp=sharing",,apachespark,dongjoon,maxgekk,xkrogen,,,,,,,,,,,,,,,,,,,,SPARK-30882,SPARK-31430,,,,,SPARK-30882,,,,,,"17/Sep/20 07:25;maxgekk;percentile_approx-input.csv;https://issues.apache.org/jira/secure/attachment/13011675/percentile_approx-input.csv",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 21 18:51:45 UTC 2020,,,,,,,,,,"0|z0io9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/20 07:23;maxgekk;I am preparing a fix for the issue.;;;","17/Sep/20 10:19;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29784;;;","17/Sep/20 10:20;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29784;;;","17/Sep/20 14:47;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29786;;;","17/Sep/20 14:47;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29786;;;","18/Sep/20 01:50;gurwls223;Issue resolved by pull request 29786
[https://github.com/apache/spark/pull/29786];;;","21/Sep/20 18:36;dongjoon;Hi, All. I added a correctness label.;;;","21/Sep/20 18:51;dongjoon;I also added `2.3.4` to the affected version and 2.2.x looks okay for now.

cc [~viirya];;;",,,,,,,,,,,,,,,,,,
Struct field names should not change after normalizing floats,SPARK-32906,13327957,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,maropu,maropu,17/Sep/20 04:37,18/Sep/20 05:09,13/Jul/23 08:50,18/Sep/20 05:08,3.0.2,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"This ticket aims at fixing a minor bug when normalizing floats for struct types;
{code}
scala> import org.apache.spark.sql.execution.aggregate.HashAggregateExec
scala> val df = Seq(Tuple1(Tuple1(-0.0d)), Tuple1(Tuple1(0.0d))).toDF(""k"")
scala> val agg = df.distinct()
scala> agg.explain()
== Physical Plan ==
*(2) HashAggregate(keys=[k#40], functions=[])
+- Exchange hashpartitioning(k#40, 200), true, [id=#62]
   +- *(1) HashAggregate(keys=[knownfloatingpointnormalized(if (isnull(k#40)) null else named_struct(col1, knownfloatingpointnormalized(normalizenanandzero(k#40._1)))) AS k#40], functions=[])
      +- *(1) LocalTableScan [k#40]

scala> val aggOutput = agg.queryExecution.sparkPlan.collect { case a: HashAggregateExec => a.output.head }
scala> aggOutput.foreach { attr => println(attr.prettyJson) }
### Final Aggregate ###
[ {
  ""class"" : ""org.apache.spark.sql.catalyst.expressions.AttributeReference"",
  ""num-children"" : 0,
  ""name"" : ""k"",
  ""dataType"" : {
    ""type"" : ""struct"",
    ""fields"" : [ {
      ""name"" : ""_1"",
                ^^^
      ""type"" : ""double"",
      ""nullable"" : false,
      ""metadata"" : { }
    } ]
  },
  ""nullable"" : true,
  ""metadata"" : { },
  ""exprId"" : {
    ""product-class"" : ""org.apache.spark.sql.catalyst.expressions.ExprId"",
    ""id"" : 40,
    ""jvmId"" : ""a824e83f-933e-4b85-a1ff-577b5a0e2366""
  },
  ""qualifier"" : [ ]
} ]

### Partial Aggregate ###
[ {
  ""class"" : ""org.apache.spark.sql.catalyst.expressions.AttributeReference"",
  ""num-children"" : 0,
  ""name"" : ""k"",
  ""dataType"" : {
    ""type"" : ""struct"",
    ""fields"" : [ {
      ""name"" : ""col1"",
                ^^^^
      ""type"" : ""double"",
      ""nullable"" : true,
      ""metadata"" : { }
    } ]
  },
  ""nullable"" : true,
  ""metadata"" : { },
  ""exprId"" : {
    ""product-class"" : ""org.apache.spark.sql.catalyst.expressions.ExprId"",
    ""id"" : 40,
    ""jvmId"" : ""a824e83f-933e-4b85-a1ff-577b5a0e2366""
  },
  ""qualifier"" : [ ]
} ]
{code}",,apachespark,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 18 05:09:21 UTC 2020,,,,,,,,,,"0|z0io3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/20 04:40;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29780;;;","17/Sep/20 04:41;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29780;;;","18/Sep/20 05:09;viirya;This was resolved by https://github.com/apache/spark/pull/29780.;;;",,,,,,,,,,,,,,,,,,,,,,,
ApplicationMaster fails to receive UpdateDelegationTokens message,SPARK-32905,13327944,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,17/Sep/20 02:22,18/Sep/20 07:41,13/Jul/23 08:50,18/Sep/20 07:41,3.0.1,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,Spark Core,YARN,,,0,,,,"
{code:java}
20-09-15 18:53:01 INFO yarn.YarnAllocator: Received 22 containers from YARN, launching executors on 0 of them.
20-09-16 12:52:28 ERROR netty.Inbox: Ignoring error
org.apache.spark.SparkException: NettyRpcEndpointRef(spark-client://YarnAM) does not implement 'receive'
	at org.apache.spark.rpc.RpcEndpoint$$anonfun$receive$1.applyOrElse(RpcEndpoint.scala:70)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20-09-17 06:52:28 ERROR netty.Inbox: Ignoring error
org.apache.spark.SparkException: NettyRpcEndpointRef(spark-client://YarnAM) does not implement 'receive'
	at org.apache.spark.rpc.RpcEndpoint$$anonfun$receive$1.applyOrElse(RpcEndpoint.scala:70)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}


With a long-running application in kerberized mode, the AMEndpiont handles the token updating message wrong.",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 18 07:41:54 UTC 2020,,,,,,,,,,"0|z0io0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/20 02:34;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29777;;;","18/Sep/20 07:41;cloud_fan;Issue resolved by pull request 29777
[https://github.com/apache/spark/pull/29777];;;",,,,,,,,,,,,,,,,,,,,,,,,
UnsafeExternalSorter may cause a SparkOutOfMemoryError to be thrown while spilling,SPARK-32901,13327832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tomvanbussel,tomvanbussel,tomvanbussel,16/Sep/20 09:47,08/Oct/20 07:59,13/Jul/23 08:50,29/Sep/20 11:10,2.4.7,3.0.1,,,,,,,,,,2.4.8,3.0.2,3.1.0,,Spark Core,,,,0,,,,"Consider the following sequence of events:
 # {{UnsafeExternalSorter}} runs out of space in its pointer array and attempts to allocate a large array to replace the current one.
 # {{TaskMemoryManager}} tries to allocate the memory backing the large array using {{MemoryManager}}, but {{MemoryManager}} is only willing to return most but not all of the memory requested.
 # {{TaskMemoryManager}} asks {{UnsafeExternalSorter}} to spill, which causes {{UnsafeExternalSorter}} to spill the current run to disk, to free its record pages and to reset its {{UnsafeInMemorySorter}}.
 # {{UnsafeInMemorySorter}} frees its pointer array, and tries to allocate a new small pointer array.
 # {{TaskMemoryManager}} tries to allocate the memory backing the small array using {{MemoryManager}}, but {{MemoryManager}} is unwilling to give it any memory, as the {{TaskMemoryManager}} is still holding on to the memory it got for the large array.
 # {{TaskMemoryManager}} again asks {{UnsafeExternalSorter}} to spill, but this time there is nothing to spill.
 # {{UnsafeInMemorySorter}} receives less memory than it requested, and causes a {{SparkOutOfMemoryError}} to be thrown, which causes the current task to fail.

A simple way to fix this is to avoid allocating a new array in {{UnsafeInMemorySorter.reset()}} and to do this on-demand instead.",,apachespark,tomvanbussel,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 30 11:25:56 UTC 2020,,,,,,,,,,"0|z0incg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/20 11:41;apachespark;User 'tomvanbussel' has created a pull request for this issue:
https://github.com/apache/spark/pull/29785;;;","17/Sep/20 11:41;apachespark;User 'tomvanbussel' has created a pull request for this issue:
https://github.com/apache/spark/pull/29785;;;","30/Sep/20 11:25;apachespark;User 'tomvanbussel' has created a pull request for this issue:
https://github.com/apache/spark/pull/29910;;;",,,,,,,,,,,,,,,,,,,,,,,
UnsafeExternalSorter.SpillableIterator cannot spill when there are NULLs in the input and radix sorting is used.,SPARK-32900,13327828,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tomvanbussel,tomvanbussel,tomvanbussel,16/Sep/20 09:19,07/Jul/21 10:44,13/Jul/23 08:50,17/Sep/20 10:38,2.4.7,3.0.1,,,,,,,,,,2.4.8,3.0.2,3.1.0,,Spark Core,,,,0,,,,"In order to determine whether {{UnsafeExternalSorter.SpillableIterator}} has spilled already it checks whether {{upstream}} is an instance of {{UnsafeInMemorySorter.SortedIterator}}. When radix sorting is used (added by SPARK-14851) and there are NULLs in the input however, upstream will be an instance of {{UnsafeExternalSorter.ChainedIterator}} instead, but should still be spilled.",,apachespark,tomvanbussel,viirya,,,,,,,,,,,,,,,,,,,SPARK-29657,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 11:59:45 UTC 2020,,,,,,,,,,"0|z0inbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/20 11:59;apachespark;User 'tomvanbussel' has created a pull request for this issue:
https://github.com/apache/spark/pull/29772;;;",,,,,,,,,,,,,,,,,,,,,,,,,
totalExecutorRunTimeMs is too big,SPARK-32898,13327787,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,linhongliu-db,linhongliu-db,16/Sep/20 05:18,22/Sep/20 06:20,13/Jul/23 08:50,18/Sep/20 21:06,2.4.7,3.0.1,,,,,,,,,,2.4.8,3.0.2,3.1.0,,Spark Core,,,,0,,,,"This might be because of incorrectly calculating executorRunTimeMs in Executor.scala
 The function collectAccumulatorsAndResetStatusOnFailure(taskStartTimeNs) can be called when taskStartTimeNs is not set yet (it is 0).

As of now in master branch, here is the problematic code: 

[https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala#L470]

 

There is a throw exception before this line. The catch branch still updates the metric.
 However the query shows as SUCCESSful. Maybe this task is speculative. Not sure.

 

submissionTime in LiveExecutionData may also have similar problem.

[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala#L449]

 ",,apachespark,linhongliu-db,Ngone51,tgraves,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 06:05:38 UTC 2020,,,,,,,,,,"0|z0in2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/20 13:25;tgraves;[~linhongliu-db] can you please provide more of a description. You say this was too big, did it cause an error for your job or you just noticed the time was to big?  Do you have a reproducible case?

You have some details there about what might be wrong with  taskStartTimeNs possibly not initialized, if you can give more details there in generally that would be great as its a bit hard to follow your description.  If you have spent the time to debug you and have a fix in mind please feel free to put up a pull request.;;;","17/Sep/20 03:17;Ngone51;I think the issue is(for executorRunTimeMs): Before a task reaches to ""taskStartTimeNs = System.nanoTime()"", it might be already killed(e.g., by another successful attempt).  So, taskStartTimeNs can not get initialized and remains 0. However, the executorRunTimeMs is calculated by ""System.nanoTime() - taskStartTimeNs"" in collectAccumulatorsAndResetStatusOnFailure, which is obviously a wrong big result when taskStartTimeNs = 0.

 

I haven't taken a detail look for the submissionTime, but it sounds like it's a different issue? Though, it may be due to the same logic hole.

 

I'd like to make a fix for the executorRunTimeMs first if [~linhongliu-db] doesn't mind.;;;","17/Sep/20 15:34;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29789;;;","17/Sep/20 15:35;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29789;;;","22/Sep/20 06:05;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29832;;;",,,,,,,,,,,,,,,,,,,,,
SparkSession.builder.getOrCreate should not show deprecation warning of SQLContext,SPARK-32897,13327768,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,16/Sep/20 03:14,12/Dec/22 17:51,13/Jul/23 08:50,16/Sep/20 17:16,3.0.1,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,PySpark,,,,0,,,,"In PySpark shell:

{code}
import warnings
from pyspark.sql import SparkSession, SQLContext
warnings.simplefilter('always', DeprecationWarning)
spark.stop()
SparkSession.builder.getOrCreate()
{code}

shows a deprecation warning from {{SQLContext}}

{code}
/.../spark/python/pyspark/sql/context.py:72: DeprecationWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.
  DeprecationWarning)
{code}",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 17:16:10 UTC 2020,,,,,,,,,,"0|z0imy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/20 03:19;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29768;;;","16/Sep/20 17:16;ueshin;Issue resolved by pull request 29768
https://github.com/apache/spark/pull/29768;;;",,,,,,,,,,,,,,,,,,,,,,,,
Murmur3 and xxHash64 implementations do not produce the correct results on big-endian platforms,SPARK-32892,13327682,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mundaym,mundaym,mundaym,15/Sep/20 13:45,23/Sep/20 17:37,13/Jul/23 08:50,23/Sep/20 17:37,3.0.1,,,,,,,,,,,3.1.0,,,,Spark Core,SQL,,,0,big-endian,,,"The Murmur3 and xxHash64 implementations in Spark do not produce the correct results on big-endian systems. This causes test failures on my target platform (s390x).

These hash functions require that multi-byte chunks be interpreted as integers encoded in *little-endian* byte order. This requires byte reversal when using multi-byte unsafe operations on big-endian platforms.

I have a PR ready for discussion and review.",,apachespark,mundaym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 17:37:05 UTC 2020,,,,,,,,,,"0|z0imfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/20 13:58;apachespark;User 'mundaym' has created a pull request for this issue:
https://github.com/apache/spark/pull/29762;;;","15/Sep/20 13:59;apachespark;User 'mundaym' has created a pull request for this issue:
https://github.com/apache/spark/pull/29762;;;","23/Sep/20 17:37;srowen;Issue resolved by pull request 29762
[https://github.com/apache/spark/pull/29762];;;",,,,,,,,,,,,,,,,,,,,,,,
Example command in https://spark.apache.org/docs/latest/sql-ref-syntax-aux-show-table.html to be changed,SPARK-32887,13327617,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Udbhav Agrawal,chetdb,chetdb,15/Sep/20 08:18,15/Nov/21 18:26,13/Jul/23 08:50,17/Sep/20 16:25,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,Documentation,,,,0,,,,"In the link [https://spark.apache.org/docs/latest/sql-ref-syntax-aux-show-table.html] the below command example mentioned is wrong.

SHOW TABLE EXTENDED IN default LIKE 'employee' PARTITION ('grade=1');

 

Complete example executed throws below error.

CREATE TABLE employee(name STRING)PARTITIONED BY (grade int) stored as parquet;

INSERT INTO employee PARTITION (grade = 1) VALUES ('sam');

INSERT INTO employee PARTITION (grade = 2) VALUES ('suj');

spark-sql> SHOW TABLE EXTENDED IN default LIKE 'employee' PARTITION ('grade=1');

**Error in query:**

```
 mismatched input ''grade=1'' expecting \{'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DATA', 'DATABASE', DATABASES, 'DAY', 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SCHEMA', 'SECOND', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'DIV', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 59)

== SQL ==
 SHOW TABLE EXTENDED IN default LIKE 'employee' PARTITION ('grade=1')
 -----------------------------------------------------------^^^

```

 

Expected : - If that partition value is string we can give like this grade ='abc'","Spark 2.4.5, Spark 3.0.0",apachespark,chetdb,dongjoon,Udbhav Agrawal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 17 16:25:57 UTC 2020,,,,,,,,,,"0|z0im0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/20 08:20;Udbhav Agrawal;Thanks for reporting, seems to be a documenting typo error. and since it is misleading i will raise a MR to correct it;;;","15/Sep/20 09:58;apachespark;User 'Udbhav30' has created a pull request for this issue:
https://github.com/apache/spark/pull/29758;;;","15/Sep/20 09:59;apachespark;User 'Udbhav30' has created a pull request for this issue:
https://github.com/apache/spark/pull/29758;;;","17/Sep/20 16:25;dongjoon;Issue resolved by pull request 29758
[https://github.com/apache/spark/pull/29758];;;",,,,,,,,,,,,,,,,,,,,,,
"'.../jobs/undefined' link from ""Event Timeline"" in jobs page",SPARK-32886,13327609,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhli,zhli,zhli,15/Sep/20 07:55,25/Sep/20 13:35,13/Jul/23 08:50,21/Sep/20 14:06,2.4.4,3.0.0,3.1.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,Web UI,,,,0,,,,"In event timeline view of jobs page, clicking job item would redirect you to corresponding job page. when there are two many jobs, some job items' link would redirect to wrong link like '.../jobs/undefined'",,apachespark,zhli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/20 07:56;zhli;undefinedlink.JPG;https://issues.apache.org/jira/secure/attachment/13011536/undefinedlink.JPG",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 07:35:40 UTC 2020,,,,,,,,,,"0|z0ilzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/20 08:23;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/29757;;;","21/Sep/20 14:06;srowen;Issue resolved by pull request 29757
[https://github.com/apache/spark/pull/29757];;;","21/Sep/20 16:13;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29825;;;","21/Sep/20 16:13;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29825;;;","22/Sep/20 07:35;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/29833;;;","22/Sep/20 07:35;apachespark;User 'zhli1142015' has created a pull request for this issue:
https://github.com/apache/spark/pull/29833;;;",,,,,,,,,,,,,,,,,,,,
NoSuchElementException occurs during decommissioning,SPARK-32881,13327508,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,dongjoon,dongjoon,14/Sep/20 19:24,21/Oct/20 06:13,13/Jul/23 08:50,21/Oct/20 06:13,3.1.0,,,,,,,,,,,3.1.0,,,,Spark Core,,,,0,,,,"`BlockManagerMasterEndpoint` seems to fail at `getReplicateInfoForRDDBlocks` due to `java.util.NoSuchElementException`. This happens on K8s IT testing, but the main code seems to need a graceful handling of `NoSuchElementException` instead of showing a naive error message.
{code}
private def getReplicateInfoForRDDBlocks(blockManagerId: BlockManagerId): Seq[ReplicateBlock] = {
    val info = blockManagerInfo(blockManagerId)
   ...
}
{code}
{code}
  20/09/14 18:56:54 INFO ExecutorPodsAllocator: Going to request 1 executors from Kubernetes.
  20/09/14 18:56:54 INFO BasicExecutorFeatureStep: Adding decommission script to lifecycle
  20/09/14 18:56:55 ERROR TaskSchedulerImpl: Lost executor 1 on 172.17.0.4: Executor decommission.
  20/09/14 18:56:55 INFO BlockManagerMaster: Removal of executor 1 requested
  20/09/14 18:56:55 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asked to remove non-existent executor 1
  20/09/14 18:56:55 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
  20/09/14 18:56:55 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 172.17.0.4, 41235, None)
  20/09/14 18:56:55 INFO DAGScheduler: Executor lost: 1 (epoch 1)
  20/09/14 18:56:55 ERROR Inbox: Ignoring error
  java.util.NoSuchElementException
  	at scala.collection.concurrent.TrieMap.apply(TrieMap.scala:833)
  	at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$getReplicateInfoForRDDBlocks(BlockManagerMasterEndpoint.scala:383)
  	at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:171)
  	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
  	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)
  	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
  	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
  	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  20/09/14 18:56:55 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
  20/09/14 18:56:55 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
  20/09/14 18:56:55 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 1)
  20/09/14 18:56:58 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.7:46674) with ID 4,  ResourceProfileId 0
  20/09/14 18:56:58 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.0.7:40495 with 593.9 MiB RAM, BlockManagerId(4, 172.17.0.7, 40495, None)
  20/09/14 18:57:23 INFO SparkContext: Starting job: count at /opt/spark/tests/decommissioning.py:49
{code}",,apachespark,dongjoon,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 21 06:13:50 UTC 2020,,,,,,,,,,"0|z0ilcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/20 19:33;holden;Thanks for the catch, I'll take a look at this issue.;;;","09/Oct/20 21:08;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29992;;;","21/Oct/20 06:13;dongjoon;This is resolved via https://github.com/apache/spark/pull/29992;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix Hive UDF not support decimal type in complex type,SPARK-32877,13327409,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ulysses,ulysses,ulysses,14/Sep/20 10:34,25/Sep/20 05:16,13/Jul/23 08:50,25/Sep/20 05:16,3.1.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"This pr aims to support Hive UDF when input complex type contains decimal type.

Before this pr, we failed in this code.
{code:java}
class ArraySumUDF extends UDF {
 import scala.collection.JavaConverters._
 def evaluate(values: java.util.List[java.lang.Double]): java.lang.Double = {
 var r = 0d
 for (v <- values.asScala) {
 r += v
 }
 r
 }
}

sql(s""CREATE FUNCTION testArraySum AS '${classOf[ArraySumUDF].getName}'"")
sql(""SELECT testArraySum(array(1, 1.1, 1.2))"")
-- failed msg
Error in query: No handler for UDF/UDAF/UDTF 'ArraySumUDF': org.apache.hadoop.hive.ql.exec.NoMatchingMethodException: No matching method for class ArraySumUDF with (array<decimal(11,1)>). Possible choices: _FUNC_(array<double>) ; line 1 pos 7
{code}",,apachespark,dongjoon,maropu,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 25 05:16:57 UTC 2020,,,,,,,,,,"0|z0ikqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/20 10:43;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/29749;;;","14/Sep/20 10:44;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/29749;;;","24/Sep/20 10:33;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/29863;;;","24/Sep/20 10:34;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/29863;;;","25/Sep/20 05:16;dongjoon;Issue resolved by pull request 29863
[https://github.com/apache/spark/pull/29863];;;",,,,,,,,,,,,,,,,,,,,,
BytesToBytesMap at MAX_CAPACITY exceeds growth threshold,SPARK-32872,13327312,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,ankurd,ankurd,13/Sep/20 20:51,15/Sep/20 06:38,13/Jul/23 08:50,14/Sep/20 21:01,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.7,3.0.1,,,,,2.4.8,3.0.2,3.1.0,,Spark Core,,,,0,,,,"When BytesToBytesMap is at {{MAX_CAPACITY}} and reaches the growth threshold, {{numKeys >= growthThreshold}} is true but {{longArray.size() / 2 < MAX_CAPACITY}} is false. This correctly prevents the map from growing, but {{canGrowArray}} incorrectly remains true. Therefore the map keeps accepting new keys and exceeds its growth threshold. If we attempt to spill the map in this state, the UnsafeKVExternalSorter will not be able to reuse the long array for sorting, causing grouping aggregations to fail with the following error:

{{2020-09-13 18:33:48,765 ERROR Executor - Exception in task 0.0 in stage 7.0 (TID 69)
org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 12982025696 bytes of memory, got 0
	at org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:160)
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:100)
	at org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:118)
	at org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:253)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:733)
	at org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)
	at org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:187)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
	at org.apache.spark.scheduler.Task.run(Task.scala:117)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)}}",,ankurd,apachespark,dongjoon,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 06:38:53 UTC 2020,,,,,,,,,,"0|z0ik5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/20 21:29;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/29744;;;","13/Sep/20 21:29;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/29744;;;","14/Sep/20 06:17;dongjoon;Hi, [~ankurd]. According to the patch, I updated the `Affected Version` field because this is another independent long-standing issue. Please let me know if the new affected version is wrong.;;;","14/Sep/20 07:29;ankurd;Thanks, [~dongjoon]! Based on a quick look at the history, I believe this issue was introduced by [PR #9241|https://github.com/apache/spark/pull/9241] ([SPARK-10342|https://issues.apache.org/jira/browse/SPARK-10342]). If this is true, it dates back to Spark 1.6.0. I augmented the ""Affects Version"" field accordingly.;;;","14/Sep/20 21:01;dongjoon;Issue resolved by pull request 29744
[https://github.com/apache/spark/pull/29744];;;","14/Sep/20 21:49;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/29753;;;","14/Sep/20 21:50;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/29753;;;","15/Sep/20 06:38;dongjoon;Thank you for updating, [~ankurd]!;;;",,,,,,,,,,,,,,,,,,
Docker buildx now requires --push,SPARK-32866,13327256,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,holden,holden,holden,13/Sep/20 00:06,28/Jan/21 19:58,13/Jul/23 08:50,28/Jan/21 19:58,3.0.0,3.0.1,3.1.0,3.1.1,3.2.0,,,,,,,3.1.1,3.2.0,,,Build,,,,0,starter,,,The buildx command has been updated and now requires --push to be added to ensure the image is pushed. To fix this please edit `./bin/docker-image-tool.sh` and verify that your images are pushed with the latest docker buildx,,apachespark,holden,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 22 20:39:08 UTC 2021,,,,,,,,,,"0|z0ijsw:",9223372036854775807,,,,,holden,,,,,,,,,,,,,,,,,,"22/Jan/21 20:38;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31299;;;","22/Jan/21 20:39;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31299;;;",,,,,,,,,,,,,,,,,,,,,,,,
python section in quickstart page doesn't display SPARK_VERSION correctly,SPARK-32865,13327248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,phoenixjiangnan,phoenixjiangnan,phoenixjiangnan,12/Sep/20 22:16,15/Nov/21 18:26,13/Jul/23 08:50,13/Sep/20 04:47,2.2.3,2.3.4,2.4.7,3.0.0,3.0.1,,,,,,,2.4.8,3.0.2,3.1.0,,Documentation,,,,0,,,,"[https://github.com/apache/spark/blame/master/docs/quick-start.md#L402]

It should be {{{{site.SPARK_VERSION}}}} rather than {site.SPARK_VERSION}",,apachespark,dongjoon,phoenixjiangnan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 13 04:47:54 UTC 2020,,,,,,,,,,"0|z0ijr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/20 22:19;phoenixjiangnan;can someone help to assign this ticket to me? seems that I cannot assign it to myself;;;","12/Sep/20 22:54;apachespark;User 'bowenli86' has created a pull request for this issue:
https://github.com/apache/spark/pull/29738;;;","13/Sep/20 04:47;dongjoon;Issue resolved by pull request 29738
[https://github.com/apache/spark/pull/29738];;;",,,,,,,,,,,,,,,,,,,,,,,
GenerateExec should require output column ordering,SPARK-32861,13327170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,12/Sep/20 00:25,08/Dec/20 03:15,13/Jul/23 08:50,08/Dec/20 03:15,3.0.1,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"{{GenerateExec}} should require column ordering since it binds its input rows directly with its {{requiredChildOutput}} without using the child's output schema. In this case, changing input column ordering will result in {{GenerateExec}} binding the wrong schema to the input columns. ",,allisonwang-db,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 08 03:15:41 UTC 2020,,,,,,,,,,"0|z0ij9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/20 00:40;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/29734;;;","08/Dec/20 03:15;dongjoon;This is resolved via https://github.com/apache/spark/pull/29734;;;",,,,,,,,,,,,,,,,,,,,,,,,
Flaky o.a.s.scheduler.BarrierTaskContextSuite.throw exception if the number of barrier() calls are not the same on every task,SPARK-32857,13327109,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,11/Sep/20 15:37,06/Oct/20 21:19,13/Jul/23 08:50,06/Oct/20 21:19,2.4.6,3.0.1,3.1.0,,,,,,,,,3.1.0,,,,Spark Core,,,,0,,,,"[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/128548/testReport/org.apache.spark.scheduler/BarrierTaskContextSuite/throw_exception_if_the_number_of_barrier___calls_are_not_the_same_on_every_task/]
{code:java}
Error Message
org.scalatest.exceptions.TestFailedException: Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown
Stacktrace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
	at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1562)
	at org.scalatest.Assertions.intercept(Assertions.scala:766)
	at org.scalatest.Assertions.intercept$(Assertions.scala:746)
	at org.scalatest.funsuite.AnyFunSuite.intercept(AnyFunSuite.scala:1562)
	at org.apache.spark.scheduler.BarrierTaskContextSuite.$anonfun$new$23(BarrierTaskContextSuite.scala:211)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:176)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:61)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:61)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:61)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:61)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}",,apachespark,dongjoon,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 21:19:04 UTC 2020,,,,,,,,,,"0|z0iiw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/20 15:56;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29732;;;","11/Sep/20 15:57;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29732;;;","06/Oct/20 21:19;dongjoon;Issue resolved by pull request 29732
[https://github.com/apache/spark/pull/29732];;;",,,,,,,,,,,,,,,,,,,,,,,
consecutive load/save calls should be allowed,SPARK-32853,13327004,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Sep/20 05:00,11/Sep/20 13:16,13/Jul/23 08:50,11/Sep/20 13:16,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,,,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 11 13:16:14 UTC 2020,,,,,,,,,,"0|z0ii8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/20 05:42;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29723;;;","11/Sep/20 05:43;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29723;;;","11/Sep/20 13:16;dongjoon;Issue resolved by pull request 29723
[https://github.com/apache/spark/pull/29723];;;",,,,,,,,,,,,,,,,,,,,,,,
Add sinkParameter to check sink options robustly in DataStreamReaderWriterSuite,SPARK-32845,13326851,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,cloud_fan,cloud_fan,10/Sep/20 11:24,11/Sep/20 19:01,13/Jul/23 08:50,11/Sep/20 19:00,2.4.6,3.0.1,3.1.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,,,,"{code:java}
org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite.SPARK-32832: later option should override earlier options for start()

java.util.NoSuchElementException: key not found: path
{code}
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/128498/testReport/org.apache.spark.sql.streaming.test/DataStreamReaderWriterSuite/SPARK_32832__later_option_should_override_earlier_options_for_start__/",,apachespark,cloud_fan,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 11 19:00:46 UTC 2020,,,,,,,,,,"0|z0ihaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/20 11:24;cloud_fan;[~dongjoon] can you help to take a look? thanks!;;;","10/Sep/20 23:21;dongjoon;[~cloud_fan]. The analysis is already done in the original PR. The current structured streaming test way using `LastOption` seems to has some issues with `stop` method, but it's not a new one definitely. Let's see. I've been monitoring the Jenkins, but this failure doesn't happen until now across branches. I'll continue the investigation when I hit this again in Jenkins.;;;","10/Sep/20 23:23;dongjoon;I'm wondering if this happens only at PRBuilder or not. Anyway, thank you for more evidence. It's helpful for me.;;;","11/Sep/20 14:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29730;;;","11/Sep/20 15:00;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29730;;;","11/Sep/20 19:00;dongjoon;Issue resolved by pull request 29730
[https://github.com/apache/spark/pull/29730];;;",,,,,,,,,,,,,,,,,,,,
Invalid interval value can happen to be just adhesive with the unit,SPARK-32840,13326795,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,10/Sep/20 07:03,11/Sep/20 15:20,13/Jul/23 08:50,10/Sep/20 11:20,3.0.0,3.0.1,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,correctness,,,"select interval '1 day 2' day; 

it should be invalid but not result in 3 days",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 10 13:13:22 UTC 2020,,,,,,,,,,"0|z0igyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/20 07:14;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29708;;;","10/Sep/20 11:20;cloud_fan;Issue resolved by pull request 29708
[https://github.com/apache/spark/pull/29708];;;","10/Sep/20 13:12;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29716;;;","10/Sep/20 13:13;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29716;;;",,,,,,,,,,,,,,,,,,,,,,
Make Spark scripts working with the spaces in paths on Windows,SPARK-32839,13326790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,10/Sep/20 06:44,12/Dec/22 18:10,13/Jul/23 08:50,14/Sep/20 04:16,3.0.0,,,,,,,,,,,3.1.0,,,,Windows,,,,0,,,,"Currently, Spark on Windows does not work with the error messages below:

{code}
>>> SparkSession.builder.getOrCreate()
Presence of build for multiple Scala versions detected (C:\...\assembly\target\scala-2.13 and C:\...\assembly\target\scala-2.12).
Remove one of them or, set SPARK_SCALA_VERSION=2.13 in spark-env.cmd.
Visit https://spark.apache.org/docs/latest/configuration.html#environment-variables for more details about setting environment variables in spark-env.cmd.
Either clean one of them or, set SPARK_SCALA_VERSION in spark-env.cmd.
{code}

We should handle whitespaces in the cmd scripts at Spark.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 14 04:16:03 UTC 2020,,,,,,,,,,"0|z0igxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/20 06:50;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29706;;;","14/Sep/20 04:16;gurwls223;Issue resolved by pull request 29706
[https://github.com/apache/spark/pull/29706];;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix DataStreamReaderWriterSuite to check writer options correctly,SPARK-32836,13326763,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,10/Sep/20 00:41,10/Sep/20 02:48,13/Jul/23 08:50,10/Sep/20 02:47,2.0.2,2.1.3,2.2.3,2.3.4,2.4.6,3.0.1,3.1.0,,,,,2.4.8,3.0.2,3.1.0,,Structured Streaming,Tests,,,0,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 10 02:47:59 UTC 2020,,,,,,,,,,"0|z0igrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/20 00:47;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29701;;;","10/Sep/20 02:47;dongjoon;Issue resolved by pull request 29701
[https://github.com/apache/spark/pull/29701];;;",,,,,,,,,,,,,,,,,,,,,,,,
Use CaseInsensitiveMap for DataStreamReader/Writer options,SPARK-32832,13326706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,09/Sep/20 16:28,11/Sep/20 04:07,13/Jul/23 08:50,10/Sep/20 06:48,2.0.2,2.1.3,2.2.3,2.3.4,2.4.6,3.0.1,,,,,,2.4.8,3.0.2,3.1.0,,Structured Streaming,,,,0,,,,"When a user have multiple options like path, paTH, and PATH for the same key path, option/options is non-deterministic because extraOptions is HashMap. This issue aims to use *CaseInsensitiveMap* instead of *HashMap* to fix this bug fundamentally.

{code}
spark.readStream
  .option(""paTh"", ""1"")
  .option(""PATH"", ""2"")
  .option(""Path"", ""3"")
  .option(""patH"", ""4"")
  .option(""path"", ""5"")
  .load()
...
org.apache.spark.sql.AnalysisException:
Path does not exist: file:/.../1;
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32364,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 10 07:05:57 UTC 2020,,,,,,,,,,"0|z0igeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/20 01:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29702;;;","10/Sep/20 01:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29702;;;","10/Sep/20 06:48;dongjoon;Issue resolved by pull request 29702
[https://github.com/apache/spark/pull/29702];;;","10/Sep/20 07:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29707;;;","10/Sep/20 07:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29707;;;",,,,,,,,,,,,,,,,,,,,,
Cast from a derived user-defined type to a base type,SPARK-32828,13326616,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,maropu,maropu,09/Sep/20 07:28,10/Sep/20 23:47,13/Jul/23 08:50,10/Sep/20 10:21,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"This ticket targets at fixing an existing bug below in `UserDefinedTypeSuite`;This PR intends to fix a existing bug below in `UserDefinedTypeSuite`;
{code:java}
[info] - SPARK-19311: UDFs disregard UDT type hierarchy (931 milliseconds)16:22:35.936 WARN org.apache.spark.sql.catalyst.expressions.SafeProjection: Expr codegen error and falling back to interpreter modeorg.apache.spark.SparkException: Cannot cast org.apache.spark.sql.ExampleSubTypeUDT@46b1771f to org.apache.spark.sql.ExampleBaseTypeUDT@31e8d979. at org.apache.spark.sql.catalyst.expressions.CastBase.nullSafeCastFunction(Cast.scala:891) at org.apache.spark.sql.catalyst.expressions.CastBase.doGenCode(Cast.scala:852) at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:147)    ...{code}
 

 ",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 10 12:30:45 UTC 2020,,,,,,,,,,"0|z0ifvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/20 07:31;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29691;;;","10/Sep/20 10:21;maropu;Resolved by [https://github.com/apache/spark/pull/29691|https://github.com/apache/spark/pull/29691#];;;","10/Sep/20 12:30;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29714;;;","10/Sep/20 12:30;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29714;;;",,,,,,,,,,,,,,,,,,,,,,
The error is confusing when resource .amount not provided ,SPARK-32824,13326513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,08/Sep/20 15:22,12/Dec/22 18:10,13/Jul/23 08:50,09/Sep/20 01:29,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,Spark Core,,,,0,,,,"If the user forgets to specify the .amount when specifying a resource, the error that comes out is confusing, we should improve.

 

$ $SPARK_HOME/bin/spark-shell  --master spark://host9:7077 --conf spark.executor.resource.gpu=1

 
{code:java}
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to ""WARN"".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).20/09/08 08:19:35 ERROR SparkContext: Error initializing SparkContext.java.lang.StringIndexOutOfBoundsException: String index out of range: -1 at java.lang.String.substring(String.java:1967) at org.apache.spark.resource.ResourceUtils$.$anonfun$listResourceIds$1(ResourceUtils.scala:151) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238) at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198) at scala.collection.TraversableLike.map(TraversableLike.scala:238) at scala.collection.TraversableLike.map$(TraversableLike.scala:231) at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198) at org.apache.spark.resource.ResourceUtils$.listResourceIds(ResourceUtils.scala:150) at org.apache.spark.resource.ResourceUtils$.parseAllResourceRequests(ResourceUtils.scala:158) at org.apache.spark.SparkContext$.checkResourcesPerTask$1(SparkContext.scala:2773) at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2884) at org.apache.spark.SparkContext.<init>(SparkContext.scala:528) at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2555) at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$1(SparkSession.scala:930) at scala.Option.getOrElse(Option.scala:189) at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921) at org.apache.spark.repl.Main$.createSparkSession(Main.scala:106) at $line3.$read$$iw$$iw.<init>(<console>:15) at $line3.$read$$iw.<init>(<console>:42) at $line3.$read.<init>(<console>:44){code}
'",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 01:29:38 UTC 2020,,,,,,,,,,"0|z0if8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/20 18:38;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/29685;;;","08/Sep/20 18:39;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/29685;;;","09/Sep/20 01:29;gurwls223;Issue resolved by pull request 29685
[https://github.com/apache/spark/pull/29685];;;",,,,,,,,,,,,,,,,,,,,,,,
Standalone Master UI resources in use wrong,SPARK-32823,13326507,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,08/Sep/20 14:53,12/Dec/22 18:10,13/Jul/23 08:50,09/Sep/20 01:34,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,Web UI,,,,0,,,,"I was using the standalone deployment with workers with GPUs and the master ui was wrong for:
 * *Resources in use:* 0 / 4 gpu

In this case I had 2 workers, each with 4 gpus, so this total should have been 8.  It seems like its just looking at a single worker.",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 01:34:35 UTC 2020,,,,,,,,,,"0|z0if74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/20 14:53;tgraves;I'm looking into this.;;;","08/Sep/20 16:07;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/29683;;;","09/Sep/20 01:34;gurwls223;Issue resolved by pull request 29683
[https://github.com/apache/spark/pull/29683];;;",,,,,,,,,,,,,,,,,,,,,,,
Spark SQL aggregate() fails on nested string arrays,SPARK-32819,13326398,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,laurikoobas,laurikoobas,08/Sep/20 08:07,10/Sep/20 07:07,13/Jul/23 08:50,10/Sep/20 02:53,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"The aggregate() function seems to fail if the initial state is array(array( some string )). Seems to work if it's an INT for example.

Example that works with a simple array:

{{select aggregate(split('abcdefgh',''), array(''), (acc, x) -> array( x ) )}}

Example that works with nested array and INTs:

{{select aggregate(sequence(0,9), array(array(0)), (acc, x) -> array(array}}{{( x ) ) }}{{)}}

Example that errors:

{{select aggregate(split('abcdefgh',''), array(array('')), (acc, x) -> array(array}}{{( x ) ) }}{{)}}

Producing the following (shortened) error:

{{data type mismatch: argument 3 requires array<array<string>> type, however ... is of array<array<string>> type.}}

 ",Spark 3.0.0 on Databricks (DBR 7.2).,apachespark,cloud_fan,laurikoobas,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 10 05:08:03 UTC 2020,,,,,,,,,,"0|z0ieiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/20 19:25;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29698;;;","10/Sep/20 02:53;cloud_fan;Issue resolved by pull request 29698
[https://github.com/apache/spark/pull/29698];;;","10/Sep/20 05:07;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29705;;;","10/Sep/20 05:08;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29705;;;",,,,,,,,,,,,,,,,,,,,,,
DPP throws error when broadcast side is empty,SPARK-32817,13326390,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhenhuawang,zhenhuawang,zhenhuawang,08/Sep/20 07:25,08/Sep/20 12:39,13/Jul/23 08:50,08/Sep/20 12:37,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"In `SubqueryBroadcastExec.relationFuture`, if the `broadcastRelation` is an `EmptyHashedRelation`, then `broadcastRelation.keys()` will throw `UnsupportedOperationException`.",,apachespark,maropu,zhenhuawang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 08 12:37:56 UTC 2020,,,,,,,,,,"0|z0ieh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/20 07:34;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/29671;;;","08/Sep/20 12:37;maropu;Resolved by https://github.com/apache/spark/pull/29671;;;",,,,,,,,,,,,,,,,,,,,,,,,
Planner error when aggregating multiple distinct DECIMAL columns,SPARK-32816,13326384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linhongliu-db,linhongliu-db,linhongliu-db,08/Sep/20 06:34,15/Oct/20 08:46,13/Jul/23 08:50,16/Sep/20 16:53,3.0.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"Running different DISTINCT decimal aggregations causes a query planner error:
{code:java}
java.lang.RuntimeException: You hit a query analyzer bug. Please report your query to Spark user mailing list.
at scala.sys.package$.error(package.scala:30)
	at org.apache.spark.sql.execution.SparkStrategies$Aggregation$.apply(SparkStrategies.scala:473)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:67)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:97)
	at org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:74)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:82)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:162)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:162)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
{code}
example failing query
{code:java}
import org.apache.spark.util.Utils

// Changing decimal(9, 0) to decimal(8, 0) fixes the problem. Root cause seems to have to do with
// UnscaledValue being used in one of the expressions but not the other.
val df = spark.range(0, 50000, 1, 1).selectExpr(
      ""id"",
      ""cast(id as decimal(9, 0)) as ss_ext_list_price"")
val cacheDir = Utils.createTempDir().getCanonicalPath
df.write.parquet(cacheDir)

spark.read.parquet(cacheDir).createOrReplaceTempView(""test_table"")

spark.sql(""""""
select
avg(distinct ss_ext_list_price), sum(distinct ss_ext_list_price)
from test_table"""""").explain
{code}",,apachespark,cloud_fan,linhongliu-db,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 08:46:16 UTC 2020,,,,,,,,,,"0|z0iefs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/20 07:56;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/29673;;;","16/Sep/20 16:53;cloud_fan;Issue resolved by pull request 29673
[https://github.com/apache/spark/pull/29673];;;","15/Oct/20 08:46;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30053;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix LibSVM data source loading error on file paths with glob metacharacters,SPARK-32815,13326382,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,08/Sep/20 06:28,09/Sep/20 21:01,13/Jul/23 08:50,08/Sep/20 14:15,2.4.6,3.0.1,3.1.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,MLlib,,,,0,,,,"SPARK-32810 fixed a long standing bug in a few Spark built-in data sources that fails to read files whose names contain glob metacharacters, such as [, ], \{, }, etc.

CSV and JSON data source on the Spark side were affected. We've also noticed that the LibSVM data source had the same code pattern that leads to the bug, so the fix https://github.com/apache/spark/pull/29659 included a fix for that data source as well, but it did not include a test for the LibSVM data source.

This ticket tracks adding a test case for LibSVM, similar to the ones for CSV/JSON, to verify whether or not the fix works as intended.",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32810,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 08 14:15:40 UTC 2020,,,,,,,,,,"0|z0iefc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/20 06:28;maxgekk;I am working on this.;;;","08/Sep/20 07:03;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29670;;;","08/Sep/20 09:24;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29675;;;","08/Sep/20 09:25;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29675;;;","08/Sep/20 09:31;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29676;;;","08/Sep/20 09:32;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29676;;;","08/Sep/20 09:57;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29678;;;","08/Sep/20 14:15;cloud_fan;Issue resolved by pull request 29670
[https://github.com/apache/spark/pull/29670];;;",,,,,,,,,,,,,,,,,,
Metaclasses are broken for a few classes in Python 3,SPARK-32814,13326379,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,08/Sep/20 05:57,12/Dec/22 18:10,13/Jul/23 08:50,16/Sep/20 11:22,2.4.0,3.0.0,3.1.0,,,,,,,,,3.1.0,,,,ML,PySpark,SQL,,0,,,,"As of Python 3 {{__metaclass__}} is no longer supported https://www.python.org/dev/peps/pep-3115/.

However, we have multiple classes which where never migrated to Python 3 compatible syntax:

- A number of ML {{Params}}} with {{__metaclass__ = ABCMeta}}
- Some of the SQL {{types}} with {{__metaclass__ = DataTypeSingleton}}


As a result some functionalities are broken in Python 3. For example 


{code:python}
>>> from pyspark.sql.types import BooleanType                                                                                                                                                                      
>>> BooleanType() is BooleanType()                                                                                                                                                                                 
False
{code}

or

{code:python}
>>> import inspect                                                                                                                                                                                                 
>>> from pyspark.ml import Estimator                                                                                                                                                                               
>>> inspect.isabstract(Estimator)                                                                                                                                                                                  
False
{code}

where in both cases we expect to see {{True}}.",,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32138,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 11:22:55 UTC 2020,,,,,,,,,,"0|z0ieeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/20 06:00;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/29664;;;","08/Sep/20 06:00;zero323;https://github.com/apache/spark/pull/29664;;;","08/Sep/20 06:01;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/29664;;;","16/Sep/20 11:22;gurwls223;Issue resolved by pull request 29664
[https://github.com/apache/spark/pull/29664];;;",,,,,,,,,,,,,,,,,,,,,,
Reading parquet rdd in non columnar mode fails in multithreaded environment,SPARK-32813,13326338,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,vklyushnikov,vklyushnikov,07/Sep/20 16:31,12/Dec/22 18:10,13/Jul/23 08:50,09/Sep/20 03:24,3.0.0,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"Reading parquet rdd in non columnar mode (i.e. with list fields)  if Spark session was  created in one thread and rdd is being read in another  - so InheritableThreadLocal  with active session is not propagated. Code below was working perfectly in Spark 2.X, but fails in Spark 3  
{code:scala}
import java.util.concurrent.Executors

import org.apache.spark.sql.SparkSession

import scala.concurrent.{Await, ExecutionContext, Future}
import scala.concurrent.duration._

object Main {

  final case class Data(list: List[Int])

  def main(args: Array[String]): Unit = {

    val executor1 = Executors.newSingleThreadExecutor()
    val executor2 = Executors.newSingleThreadExecutor()
    try {
      val ds = Await.result(Future {
        val session = SparkSession.builder().appName(""test"").master(""local[*]"").getOrCreate()
        import session.implicits._

        val path = ""test.parquet""
        session.createDataset(Data(1 :: Nil) :: Nil).write.parquet(path)
        session.read.parquet(path).as[Data]
      }(ExecutionContext.fromExecutorService(executor1)), 1.minute)

      Await.result(Future {
        ds.rdd.collect().foreach(println(_))
      }(ExecutionContext.fromExecutorService(executor2)), 1.minute)

    } finally {
      executor1.shutdown()
      executor2.shutdown()
    }
  }
}
{code}
This code fails with following exception:
{code}
Exception in thread ""main"" java.util.NoSuchElementException: None.getException in thread ""main"" java.util.NoSuchElementException: None.get at scala.None$.get(Option.scala:529) at scala.None$.get(Option.scala:527) at org.apache.spark.sql.execution.FileSourceScanExec.needsUnsafeRowConversion$lzycompute(DataSourceScanExec.scala:178) at org.apache.spark.sql.execution.FileSourceScanExec.needsUnsafeRowConversion(DataSourceScanExec.scala:176) at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:462) at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171) at org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96) at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121) at org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3198) at org.apache.spark.sql.Dataset.rdd(Dataset.scala:3196)
{code}
","Spark 3.0.0, Scala 2.12.12",apachespark,viirya,vklyushnikov,,,,,,,,,,,,,,,,,,,,,SPARK-32589,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 03:24:08 UTC 2020,,,,,,,,,,"0|z0ie5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/20 01:40;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29667;;;","09/Sep/20 03:24;gurwls223;Issue resolved by pull request 29667
[https://github.com/apache/spark/pull/29667];;;",,,,,,,,,,,,,,,,,,,,,,,,
Run tests script for Python fails in certain environments,SPARK-32812,13326307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,07/Sep/20 13:30,12/Dec/22 18:10,13/Jul/23 08:50,08/Sep/20 03:24,3.1.0,,,,,,,,,,,2.4.7,3.0.2,3.1.0,,PySpark,Tests,,,0,,,,"When running PySpark test in the local environment with ""python/run-tests"" command, the following error could occur.

 {code}

Traceback (most recent call last):
 File ""<string>"", line 1, in <module>
...

raise RuntimeError('''
RuntimeError:
 An attempt has been made to start a new process before the
 current process has finished its bootstrapping phase.

This probably means that you are not using fork to start your
 child processes and you have forgotten to use the proper idiom
 in the main module:

if __name__ == '__main__':
 freeze_support()
 ...

The ""freeze_support()"" line can be omitted if the program
 is not going to be frozen to produce an executable.
Traceback (most recent call last):
...
 raise EOFError
EOFError

 {code}",,apachespark,itholic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 08 03:24:46 UTC 2020,,,,,,,,,,"0|z0idyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/20 01:18;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/29666;;;","08/Sep/20 01:19;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/29666;;;","08/Sep/20 03:24;gurwls223;Issue resolved by pull request 29666
[https://github.com/apache/spark/pull/29666];;;",,,,,,,,,,,,,,,,,,,,,,,
CSV/JSON data sources should avoid globbing paths when inferring schema,SPARK-32810,13326254,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,07/Sep/20 07:15,12/Dec/22 18:10,13/Jul/23 08:50,08/Sep/20 00:44,2.4.6,3.0.0,3.0.1,3.1.0,,,,,,,,2.4.7,3.0.2,3.1.0,,SQL,,,,0,,,,"The problem is that when the user doesn't specify the schema when reading a CSV table, The CSV file format and data source needs to infer schema, and it does so by creating a base DataSource relation, and there's a mismatch: *FileFormat.inferSchema* expects actual file paths without glob patterns, but *DataSource.paths* expects file paths in glob patterns.
 An example is demonstrated below:
{code:java}
^
|         DataSource.resolveRelation    tries to glob again (incorrectly) on glob pattern """"""[abc].csv""""""
|         DataSource.apply                      ^
|       CSVDataSource.inferSchema               |
|     CSVFileFormat.inferSchema                 |
|   ...                                         |
|   DataSource.resolveRelation          globbed into """"""[abc].csv"""""", should be treated as verbatim path, not as glob pattern
|   DataSource.apply                            ^
| DataFrameReader.load                          |
|                                       input """"""\[abc\].csv""""""
{code}
The same problem exists in the JSON data source as well. Ditto for MLlib's LibSVM data source.",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32815,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 07:03:36 UTC 2020,,,,,,,,,,"0|z0idmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/20 07:18;maxgekk;I am working on this.;;;","07/Sep/20 08:45;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29659;;;","07/Sep/20 14:54;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29662;;;","07/Sep/20 14:55;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29662;;;","07/Sep/20 17:36;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29663;;;","08/Sep/20 00:44;gurwls223;Issue resolved by pull request 29659
[https://github.com/apache/spark/pull/29659];;;","08/Sep/20 17:15;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29684;;;","09/Sep/20 05:04;gurwls223;Thanks [~dongjoon] for fixing it here and in other JIRAs.;;;","09/Sep/20 07:02;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29690;;;","09/Sep/20 07:03;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29690;;;",,,,,,,,,,,,,,,,
run-example failed in standalone cluster mode,SPARK-32804,13326152,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,KevinSmile,KevinSmile,KevinSmile,05/Sep/20 13:54,16/Sep/20 03:55,13/Jul/23 08:50,12/Sep/20 21:13,2.4.0,3.0.0,,,,,,,,,,3.1.0,,,,Deploy,Examples,,,0,,,,"run-example failed in standalone cluster mode (seems like something wrong in SparkSubmitCommand Build): 

 

  !image-2020-09-05-21-55-00-227.png!",Spark 3.0 ,apachespark,KevinSmile,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/20 13:55;KevinSmile;image-2020-09-05-21-55-00-227.png;https://issues.apache.org/jira/secure/attachment/13011094/image-2020-09-05-21-55-00-227.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 03:55:05 UTC 2020,,,,,,,,,,"0|z0id08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/20 14:14;apachespark;User 'KevinSmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/29653;;;","05/Sep/20 14:15;apachespark;User 'KevinSmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/29653;;;","12/Sep/20 21:13;srowen;Issue resolved by pull request 29653
[https://github.com/apache/spark/pull/29653];;;","16/Sep/20 03:54;apachespark;User 'KevinSmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/29769;;;","16/Sep/20 03:55;apachespark;User 'KevinSmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/29769;;;",,,,,,,,,,,,,,,,,,,,,
Rare corner case error in micro-batch engine with some stateful queries + no-data-batches + V1 streaming sources ,SPARK-32794,13325993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,03/Sep/20 22:21,11/Sep/20 19:01,13/Jul/23 08:50,11/Sep/20 07:15,2.3.4,2.4.6,3.0.0,3.0.1,,,,,,,,2.4.8,3.0.2,3.1.0,,Structured Streaming,,,,0,,,,"Structured Streaming micro-batch engine has the contract with V1 data sources that, after a restart, it will call `source.getBatch()` on the last batch attempted before the restart. However, a very rare combination of sequences violates this contract. It occurs only when 
- The streaming query has specific types of stateful operations with watermarks (e.g., aggregation in append, mapGroupsWithState with timeouts). 
    - These queries can execute a batch even without new data when the previous updates the watermark and the stateful ops are such that the new watermark can cause new output/cleanup. Such batches are called no-data-batches.
- The last batch before termination was an incomplete no-data-batch. Upon restart, the micro-batch engine fails to call `source.getBatch` when attempting to re-execute the incomplete no-data-batch.

This occurs because no-data-batches has the same and end offsets, and when a batch is executed, if the start and end offset is same then calling `source.getBatch` is skipped as it is assumed the generated plan will be empty. This only affects V1 data sources which rely on this invariant to initialize differently when the query is being started from scratch or restarted. How will a source misbehave is very source-specific. 
",,apachespark,tdas,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 11 07:15:10 UTC 2020,,,,,,,,,,"0|z0ic0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/20 17:51;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/29651;;;","04/Sep/20 17:51;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/29651;;;","09/Sep/20 18:02;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/29696;;;","09/Sep/20 18:03;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/29696;;;","10/Sep/20 00:00;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/29700;;;","11/Sep/20 07:15;tdas;Issue resolved by pull request 29700
[https://github.com/apache/spark/pull/29700];;;",,,,,,,,,,,,,,,,,,,,
non-partitioned table scan should not have partition filter,SPARK-32788,13325895,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,03/Sep/20 10:27,09/Sep/20 02:08,13/Jul/23 08:50,03/Sep/20 15:50,3.0.1,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,,,apachespark,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32620,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 03 15:50:55 UTC 2020,,,,,,,,,,"0|z0ibf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/20 10:35;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29637;;;","03/Sep/20 10:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29637;;;","03/Sep/20 15:50;yumwang;Issue resolved by pull request 29637
[https://github.com/apache/spark/pull/29637];;;",,,,,,,,,,,,,,,,,,,,,,,
interval with dangling part should not results null,SPARK-32785,13325882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,03/Sep/20 09:21,21/Oct/20 07:32,13/Jul/23 08:50,07/Sep/20 05:11,3.0.0,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"bin/spark-sql -S -e ""select interval '1', interval '+', interval '1 day -'""

NULL	NULL	NULL

we should fail these cases correctly",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 21 07:32:07 UTC 2020,,,,,,,,,,"0|z0ibc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/20 09:34;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29635;;;","07/Sep/20 05:11;cloud_fan;Issue resolved by pull request 29635
[https://github.com/apache/spark/pull/29635];;;","07/Sep/20 07:19;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29658;;;","07/Sep/20 07:20;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29658;;;","21/Oct/20 03:32;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30113;;;","21/Oct/20 07:32;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/30117;;;",,,,,,,,,,,,,,,,,,,,
Spark/Hive3 interaction potentially causes deadlock,SPARK-32779,13325818,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandeep.katta2007,bersprockets,bersprockets,02/Sep/20 23:19,12/Dec/22 18:10,13/Jul/23 08:50,07/Sep/20 06:11,3.0.0,3.1.0,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"This is an issue for applications that share a Spark Session across multiple threads.

sessionCatalog.loadPartition (after checking that the table exists) grabs locks in this order:
 - HiveExternalCatalog
 - HiveSessionCatalog (in Shim_v3_0)

Other operations (e.g., sessionCatalog.tableExists), grab locks in this order:
 - HiveSessionCatalog
 - HiveExternalCatalog

[This|https://github.com/apache/spark/blob/ad6b887541bf90cc3ea830a1a3322b71ccdd80ee/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala#L1332] appears to be the culprit. Maybe db name should be defaulted _before_ the call to HiveClient so that Shim_v3_0 doesn't have to call back into SessionCatalog. Or possibly this is not needed at all, since loadPartition in Shim_v2_1 doesn't worry about the default db name, but that might be because of differences between Hive client libraries.

Reproduction case:
 - You need to have a running Hive 3.x HMS instance and the appropriate hive-site.xml for your Spark instance
 - Adjust your spark.sql.hive.metastore.version accordingly
 - It might take more than one try to hit the deadlock

Launch Spark:
{noformat}
bin/spark-shell --conf ""spark.sql.hive.metastore.jars=${HIVE_HOME}/lib/*"" --conf spark.sql.hive.metastore.version=3.1
{noformat}
Then use the following code:
{noformat}
import scala.collection.mutable.ArrayBuffer
import scala.util.Random

val tableCount = 4
for (i <- 0 until tableCount) {
  val tableName = s""partitioned${i+1}""
  sql(s""drop table if exists $tableName"")
  sql(s""create table $tableName (a bigint) partitioned by (b bigint) stored as orc"")
}

val threads = new ArrayBuffer[Thread]
for (i <- 0 until tableCount) {
  threads.append(new Thread( new Runnable {
    override def run: Unit = {
      val tableName = s""partitioned${i + 1}""
      val rand = Random
      val df = spark.range(0, 20000).toDF(""a"")
      val location = s""/tmp/${rand.nextLong.abs}""
      df.write.mode(""overwrite"").orc(location)
      sql(
        s""""""
        LOAD DATA LOCAL INPATH '$location' INTO TABLE $tableName partition (b=$i)"""""")
    }
  }, s""worker$i""))
  threads(i).start()
}

for (i <- 0 until tableCount) {
  println(s""Joining with thread $i"")
  threads(i).join()
}
println(""All done"")
{noformat}
The job often gets stuck after one or two ""Joining..."" lines.

{{kill -3}} shows something like this:
{noformat}
Found one Java-level deadlock:
=============================
""worker3"":
  waiting to lock monitor 0x00007fdc3cde6798 (object 0x0000000784d98ac8, a org.apache.spark.sql.hive.HiveSessionCatalog),
  which is held by ""worker0""
""worker0"":
  waiting to lock monitor 0x00007fdc441d1b88 (object 0x00000007861d1208, a org.apache.spark.sql.hive.HiveExternalCatalog),
  which is held by ""worker3""
{noformat}",,apachespark,bersprockets,JinxinTang,petertoth,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 12 05:48:04 UTC 2020,,,,,,,,,,"0|z0iay0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/20 09:46;JinxinTang;Hi [~bersprockets],

Seems spark cannot work with hive-3.1.x currently due to type imcompatible, for example:

in `org.apache.spark.sql.catalyst.util.DateTimeUtils$#fromJavaTimestamp` the method `fromJavaTimestamp` accept `java.sql.Timestamp` type,  it works fine in hive-2.x, but in `hive-3.1.x`, the `org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector#getPrimitiveJavaObject` return `org.apache.hadoop.hive.common.type.Timestamp` not `java.sql.Timestamp`. Could you please provide related infos.;;;","04/Sep/20 08:25;sandeep.katta2007;[~bersprockets] thanks for raising this issue, I am working on this. I will raise the patch soon;;;","04/Sep/20 10:39;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/29649;;;","07/Sep/20 06:11;gurwls223;Issue resolved by pull request 29649
[https://github.com/apache/spark/pull/29649];;;","12/Sep/20 05:47;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/29736;;;","12/Sep/20 05:48;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/29736;;;",,,,,,,,,,,,,,,,,,,,
Limit in streaming should not be optimized away by PropagateEmptyRelation,SPARK-32776,13325646,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liwensun,liwen,liwen,02/Sep/20 01:31,12/Dec/22 18:10,13/Jul/23 08:50,02/Sep/20 09:07,3.1.0,,,,,,,,,,,3.0.2,3.1.0,,,Structured Streaming,,,,0,,,,"Right now, the limit operator in a streaming query may get optimized away when the relation is empty. This can be problematic for stateful streaming, as this empty batch will not write any state store files, and the next batch will fail when trying to read these state store files and throw a file not found error.

We should not let PropagateEmptyRelation optimize away the Limit operator for streaming queries.

This ticket is intended to apply a small and safe fix for PropagateEmptyRelation. A fundamental fix that can prevent this from happening again in the future and in other optimizer rules is more desirable, but that's a much larger task.

 ",,apachespark,kabhwan,liwen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 02 10:09:09 UTC 2020,,,,,,,,,,"0|z0i9vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/20 01:43;apachespark;User 'liwensun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29623;;;","02/Sep/20 09:06;gurwls223;Fixed in https://github.com/apache/spark/pull/29623;;;","02/Sep/20 10:03;kabhwan;It sounds to be safer to mark 3.0.x to 3.0.2 until the vote is open - it's relatively easier to find issues marked as 3.0.2 and make correction, assuming the case if the vote isn't going to the predicted way.;;;","02/Sep/20 10:09;gurwls223;Sure, thanks [~kabhwan].;;;",,,,,,,,,,,,,,,,,,,,,,
The example of expressions.Aggregator in Javadoc / Scaladoc is wrong,SPARK-32771,13325594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,01/Sep/20 19:25,12/Dec/22 18:10,13/Jul/23 08:50,02/Sep/20 01:04,3.0.0,3.1.0,,,,,,,,,,2.4.7,3.0.2,3.1.0,,Documentation,,,,0,,,,"There is an example of expressions.Aggregator in Javadoc and Scaladoc like as follows.
{code:java}
val customSummer =  new Aggregator[Data, Int, Int] {
  def zero: Int = 0
  def reduce(b: Int, a: Data): Int = b + a.i
  def merge(b1: Int, b2: Int): Int = b1 + b2
  def finish(r: Int): Int = r
}.toColumn(){code}
But this example doesn't work because it doesn't define bufferEncoder and outputEncoder.",,apachespark,maropu,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 03 04:14:51 UTC 2020,,,,,,,,,,"0|z0i9k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/20 19:37;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29617;;;","01/Sep/20 19:38;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29617;;;","02/Sep/20 01:04;gurwls223;Issue resolved by pull request 29617
[https://github.com/apache/spark/pull/29617];;;","03/Sep/20 04:14;maropu;Since v3.0.1 does not include this fix, I reset ""Target Version/s"" from 3.0.1 to 3.0.2.;;;",,,,,,,,,,,,,,,,,,,,,,
Bucket join should work if spark.sql.shuffle.partitions larger than bucket number,SPARK-32767,13325556,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,01/Sep/20 16:37,04/Sep/20 00:01,13/Jul/23 08:50,04/Sep/20 00:01,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"How to reproduce this issue:
{code:scala}
    spark.range(1000).write.bucketBy(432, ""id"").saveAsTable(""t1"")
    spark.range(1000).write.bucketBy(34, ""id"").saveAsTable(""t2"")
    sql(""set spark.sql.shuffle.partitions=600"")
    sql(""set spark.sql.autoBroadcastJoinThreshold=-1"")
    sql(""select * from t1 join t2 on t1.id = t2.id"").explain()
{code}

{noformat}
== Physical Plan ==
*(5) SortMergeJoin [id#26L], [id#27L], Inner
:- *(2) Sort [id#26L ASC NULLS FIRST], false, 0
:  +- Exchange hashpartitioning(id#26L, 600), true, [id=#65]
:     +- *(1) Filter isnotnull(id#26L)
:        +- *(1) ColumnarToRow
:           +- FileScan parquet default.t1[id#26L] Batched: true, DataFilters: [isnotnull(id#26L)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-32444/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 432 out of 432
+- *(4) Sort [id#27L ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(id#27L, 600), true, [id=#74]
      +- *(3) Filter isnotnull(id#27L)
         +- *(3) ColumnarToRow
            +- FileScan parquet default.t2[id#27L] Batched: true, DataFilters: [isnotnull(id#27L)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-32444/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 34 out of 34

{noformat}

*Expected*:
{noformat}
== Physical Plan ==
*(4) SortMergeJoin [id#26L], [id#27L], Inner
:- *(1) Sort [id#26L ASC NULLS FIRST], false, 0
:  +- *(1) Filter isnotnull(id#26L)
:     +- *(1) ColumnarToRow
:        +- FileScan parquet default.t1[id#26L] Batched: true, DataFilters: [isnotnull(id#26L)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-32444/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 432 out of 432
+- *(3) Sort [id#27L ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(id#27L, 432), true, [id=#69]
      +- *(2) Filter isnotnull(id#27L)
         +- *(2) ColumnarToRow
            +- FileScan parquet default.t2[id#27L] Batched: true, DataFilters: [isnotnull(id#27L)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-32444/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 34 out of 34

{noformat}

",,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 04 00:01:57 UTC 2020,,,,,,,,,,"0|z0i9bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/20 16:55;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29612;;;","01/Sep/20 16:56;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29612;;;","02/Sep/20 04:39;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29624;;;","04/Sep/20 00:01;yumwang;Issue resolved by pull request 29624
[https://github.com/apache/spark/pull/29624];;;",,,,,,,,,,,,,,,,,,,,,,
compare of -0.0 < 0.0 return true,SPARK-32764,13325499,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,igreenfi,igreenfi,01/Sep/20 10:30,08/Sep/20 03:46,13/Jul/23 08:50,08/Sep/20 03:45,3.0.0,3.0.1,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,correctness,,,"{code:scala}
 val spark: SparkSession = SparkSession
      .builder()
      .master(""local"")
      .appName(""SparkByExamples.com"")
      .getOrCreate()
    spark.sparkContext.setLogLevel(""ERROR"")

    import spark.sqlContext.implicits._

    val df = Seq((-0.0, 0.0)).toDF(""neg"", ""pos"")
      .withColumn(""comp"", col(""neg"") < col(""pos""))
      df.show(false)

======

+----+---+----+
|neg |pos|comp|
+----+---+----+
|-0.0|0.0|true|
+----+---+----+{code}

I think that result should be false.

**Apache Spark 2.4.6 RESULT**
{code}
scala> spark.version
res0: String = 2.4.6

scala> Seq((-0.0, 0.0)).toDF(""neg"", ""pos"").withColumn(""comp"", col(""neg"") < col(""pos"")).show
+----+---+-----+
| neg|pos| comp|
+----+---+-----+
|-0.0|0.0|false|
+----+---+-----+
{code}",,apachespark,cloud_fan,dongjoon,igreenfi,maropu,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,SPARK-30009,,,,,,HIVE-11174,,,"03/Sep/20 17:46;sandeep.katta2007;2.4_codegen.txt;https://issues.apache.org/jira/secure/attachment/13011002/2.4_codegen.txt","03/Sep/20 17:46;sandeep.katta2007;3.0_Codegen.txt;https://issues.apache.org/jira/secure/attachment/13011001/3.0_Codegen.txt",,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 08 03:45:51 UTC 2020,,,,,,,,,,"0|z0i8zc:",9223372036854775807,,,,,,,,,,,,,3.0.2,,,,,,,,,,"02/Sep/20 21:29;dongjoon;Thank you for reporting, [~igreenfi]. I also confirm this regression.;;;","02/Sep/20 21:31;dongjoon;cc [~cloud_fan] and [~smilegator];;;","03/Sep/20 17:42;sandeep.katta2007;[~cloud_fan] and [~smilegator]

I did some analysis on this, it is because as a part of SPARK-30009 org.apache.spark.util.Utils.nanSafeCompareDoubles is replaced with java.lang.Double.compare. Same can be confirmed from the codegen code, I have attached those for referrence.

 

If you see the implementation of java.lang.Double.compare
 
*java.lang.Double.compare(-0.0, 0.0) < 0 evaluates to true*

*java.lang.Double.compare(0.0, -0.0) < 0 evaluates to false*

 ;;;","03/Sep/20 17:48;sandeep.katta2007;So it's edge case scenario, how should we fix it or we can leave it as limitation ?;;;","04/Sep/20 05:03;dongjoon;Thank you for analysis, [~sandeep.katta2007]. Also, cc [~srowen].
;;;","04/Sep/20 09:05;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29647;;;","04/Sep/20 09:06;cloud_fan;Thanks for your analysis, [~sandeep.katta2007]

I think we need to add back the `OrderingUtil` to implement custom compare methods to take care of 0.0 vs -0.0. I've opened a PR for it: [https://github.com/apache/spark/pull/29647];;;","08/Sep/20 03:45;dongjoon;Issue resolved by pull request 29647
[https://github.com/apache/spark/pull/29647];;;",,,,,,,,,,,,,,,,,,
Planner error when aggregating multiple distinct Constant columns,SPARK-32761,13325448,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liulinhong,linhongliu-db,linhongliu-db,01/Sep/20 05:09,16/Oct/20 03:37,13/Jul/23 08:50,01/Sep/20 13:06,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"SELECT COUNT(DISTINCT 2), COUNT(DISTINCT 2, 3) will trigger this bug.

The problematic code is:

 
{code:java}
val distinctAggGroups = aggExpressions.filter(_.isDistinct).groupBy { e =>
  val unfoldableChildren = e.aggregateFunction.children.filter(!_.foldable).toSet
  if (unfoldableChildren.nonEmpty) {
    // Only expand the unfoldable children
     unfoldableChildren
  } else {
    // If aggregateFunction's children are all foldable
    // we must expand at least one of the children (here we take the first child),
    // or If we don't, we will get the wrong result, for example:
    // count(distinct 1) will be explained to count(1) after the rewrite function.
    // Generally, the distinct aggregateFunction should not run
    // foldable TypeCheck for the first child.
    e.aggregateFunction.children.take(1).toSet
  }
}
{code}",,apachespark,linhongliu-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 08:45:14 UTC 2020,,,,,,,,,,"0|z0i8o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/20 05:28;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/29607;;;","15/Oct/20 08:44;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30052;;;","15/Oct/20 08:45;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30052;;;",,,,,,,,,,,,,,,,,,,,,,,
Deduplicating and repartitioning the same column create duplicate rows with AQE,SPARK-32753,13325277,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mauzhang,mauzhang,mauzhang,31/Aug/20 06:51,08/Sep/20 14:43,13/Jul/23 08:50,07/Sep/20 16:09,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,correctness,,,"To reproduce:
{code:java}
spark.range(10).union(spark.range(10)).createOrReplaceTempView(""v1"")
val df = spark.sql(""select id from v1 group by id distribute by id"") 
println(df.collect().toArray.mkString("",""))
println(df.queryExecution.executedPlan)

// With AQE
[4],[0],[3],[2],[1],[7],[6],[8],[5],[9],[4],[0],[3],[2],[1],[7],[6],[8],[5],[9]
AdaptiveSparkPlan(isFinalPlan=true)
+- CustomShuffleReader local
   +- ShuffleQueryStage 0
      +- Exchange hashpartitioning(id#183L, 10), true
         +- *(3) HashAggregate(keys=[id#183L], functions=[], output=[id#183L])
            +- Union
               :- *(1) Range (0, 10, step=1, splits=2)
               +- *(2) Range (0, 10, step=1, splits=2)

// Without AQE
[4],[7],[0],[6],[8],[3],[2],[5],[1],[9]
*(4) HashAggregate(keys=[id#206L], functions=[], output=[id#206L])
+- Exchange hashpartitioning(id#206L, 10), true
   +- *(3) HashAggregate(keys=[id#206L], functions=[], output=[id#206L])
      +- Union
         :- *(1) Range (0, 10, step=1, splits=2)
         +- *(2) Range (0, 10, step=1, splits=2){code}",,apachespark,cloud_fan,maropu,mauzhang,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 08 14:43:03 UTC 2020,,,,,,,,,,"0|z0i7ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/20 07:14;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29593;;;","31/Aug/20 07:15;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29593;;;","07/Sep/20 16:09;cloud_fan;Issue resolved by pull request 29593
[https://github.com/apache/spark/pull/29593];;;","07/Sep/20 23:42;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29665;;;","08/Sep/20 14:42;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29682;;;","08/Sep/20 14:43;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29682;;;",,,,,,,,,,,,,,,,,,,,
thread safe endpoints may hang due to fatal error,SPARK-32738,13325187,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhenhuawang,zhenhuawang,zhenhuawang,30/Aug/20 04:20,31/Aug/21 08:18,13/Jul/23 08:50,15/Sep/20 06:46,2.3.4,2.4.6,3.0.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,Spark Core,,,,0,,,,"Processing for `ThreadSafeRpcEndpoint` is controlled by 'numActiveThreads' in `Inbox`. Now if any fatal error happens during `Inbox.process`, 'numActiveThreads' is not reduced. Then other threads can not process messages in that inbox, which causes the endpoint to ""hang"".

This problem is more serious in previous Spark 2.x versions since the driver, executor and block manager endpoints are all thread safe endpoints.",,apachespark,cloud_fan,zhenhuawang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 31 08:18:10 UTC 2021,,,,,,,,,,"0|z0i71s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/20 04:33;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/29580;;;","15/Sep/20 06:46;cloud_fan;Issue resolved by pull request 29580
[https://github.com/apache/spark/pull/29580];;;","15/Sep/20 17:03;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/29763;;;","15/Sep/20 17:03;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/29763;;;","15/Sep/20 17:42;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/29764;;;","15/Sep/20 17:43;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/29764;;;","31/Aug/21 08:17;apachespark;User 'sleep1661' has created a pull request for this issue:
https://github.com/apache/spark/pull/33872;;;","31/Aug/21 08:18;apachespark;User 'sleep1661' has created a pull request for this issue:
https://github.com/apache/spark/pull/33872;;;",,,,,,,,,,,,,,,,,,
Upgrade to jQuery 3.5.1,SPARK-32723,13324967,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,ashish23aks,ashish23aks,28/Aug/20 06:09,01/Oct/20 15:21,13/Jul/23 08:50,01/Oct/20 04:30,3.0.0,,,,,,,,,,,3.1.0,,,,Spark Core,,,,0,Security,,,"Spark 3.0, Spark 2.4.x uses JQuery version < 3.5 which has known security vulnerability in Spark Master UI and Spark Worker UI.

Can we please upgrade JQuery to 3.5 and above ?

 [https://www.tenable.com/plugins/nessus/136929]

??According to the self-reported version in the script, the version of JQuery hosted on the remote web server is greater than or equal to 1.2 and prior to 3.5.0. It is, therefore, affected by multiple cross site scripting vulnerabilities.??

 

 ",,apachespark,ashish23aks,dongjoon,nmarion,rohitmishr1484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 15:21:57 UTC 2020,,,,,,,,,,"0|z0i5ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/20 09:17;rohitmishr1484;[~ashish23aks], Please refrain from marking the target version. This is reserved for committers. ;;;","29/Sep/20 12:51;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/29902;;;","01/Oct/20 04:30;dongjoon;Issue resolved by pull request 29902
[https://github.com/apache/spark/pull/29902];;;","01/Oct/20 15:21;apachespark;User 'n-marion' has created a pull request for this issue:
https://github.com/apache/spark/pull/29922;;;",,,,,,,,,,,,,,,,,,,,,,
"Update document type conversion for Pandas UDFs (pyarrow 1.0.1, pandas 1.1.1, Python 3.7)",SPARK-32722,13324960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,28/Aug/20 04:47,12/Dec/22 18:11,13/Jul/23 08:50,28/Aug/20 06:39,3.1.0,,,,,,,,,,,3.1.0,,,,PySpark,,,,0,,,,Update the chart generated by SPARK-25798.,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25798,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 28 06:39:02 UTC 2020,,,,,,,,,,"0|z0i5nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/20 05:04;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29569;;;","28/Aug/20 06:39;gurwls223;Issue resolved by pull request 29569
[https://github.com/apache/spark/pull/29569];;;",,,,,,,,,,,,,,,,,,,,,,,,
Broadcast block pieces may memory leak,SPARK-32715,13324830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,27/Aug/20 11:58,15/Sep/20 01:27,13/Jul/23 08:50,15/Sep/20 01:27,2.4.6,3.0.0,3.1.0,,,,,,,,,2.4.8,3.0.2,3.1.0,,Spark Core,,,,0,,,,"We use Spark thrift-server as a long-running service. A bad query submitted a heavy BroadcastNestLoopJoin operation and made driver full GC. We killed the bad query but we found the driver's memory usage was still high and full GCs had very frequency. By investigating with GC dump and log, we found the broadcast may memory leak.

2020-08-19T18:54:02.824-0700: [Full GC (Allocation Failure) 2020-08-19T18:54:02.824-0700: [Class Histogram (before full gc):
116G->112G(170G), 184.9121920 secs]
[Eden: 32.0M(7616.0M)->0.0B(8704.0M) Survivors: 1088.0M->0.0B Heap: 116.4G(170.0G)->112.9G(170.0G)], [Metaspace: 177285K->177270K(182272K)]

num #instances #bytes class name
----------------------------------------------
1: 676531691 72035438432 [B
2: 676502528 32472121344 org.apache.spark.sql.catalyst.expressions.UnsafeRow
3: 99551 12018117568 [Ljava.lang.Object;
4: 26570 4349629040 [I
5: 6 3264536688 [Lorg.apache.spark.sql.catalyst.InternalRow;
6: 1708819 256299456 [C
7: 2338 179615208 [J
8: 1703669 54517408 java.lang.String
9: 103860 34896960 org.apache.spark.status.TaskDataWrapper
10: 177396 25545024 java.net.URI
...",,apachespark,cltlfcjin,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 01:27:07 UTC 2020,,,,,,,,,,"0|z0i4uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/20 12:33;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29558;;;","27/Aug/20 12:34;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29558;;;","15/Sep/20 01:27;dongjoon;Issue resolved by pull request 29558
[https://github.com/apache/spark/pull/29558];;;",,,,,,,,,,,,,,,,,,,,,,,
Query optimization fails to reuse exchange with DataSourceV2,SPARK-32708,13324732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mingjial,mingjial,mingjial,26/Aug/20 23:44,22/Sep/20 02:51,13/Jul/23 08:50,14/Sep/20 07:55,2.4.7,,,,,,,,,,,2.4.8,,,,SQL,,,28/Aug/20 00:00,0,,,,"Repro query:
{code:java}
spark.conf.set(""spark.sql.exchange.reuse"",""true"")
spark.read.format('parquet').load('gs://dataproc-kokoro-tests-us-central1/tpcds/1G/parquet/date_dim')
#spark.read.format(""com.google.cloud.spark.bigquery.v2.BigQueryDataSourceV2"").option(""table"",'tpcds_1G.date_dim').load() 
df.createOrReplaceTempView(table)
 
df = spark.sql("""""" 
WITH t1 AS (
 SELECT 
 d_year, d_month_seq
 FROM (
 SELECT t1.d_year , t2.d_month_seq 
 FROM 
 date_dim t1
 cross join
 date_dim t2
 where t1.d_day_name = ""Monday"" and t1.d_fy_year > 2000
 and t2.d_day_name = ""Monday"" and t2.d_fy_year > 2000
 )
 GROUP BY d_year, d_month_seq)
 
 SELECT
 prev_yr.d_year AS prev_year, curr_yr.d_year AS year, curr_yr.d_month_seq
 FROM t1 curr_yr cross join t1 prev_yr
 WHERE curr_yr.d_year=2002 AND prev_yr.d_year=2002 
 ORDER BY d_month_seq
 LIMIT 100
 
 """""")
df.explain()
#df.show()
{code}
 

*The above query has different plans with Parquet and DataSourceV2. Both plans are correct tho. However, the DataSourceV2 plan is less optimized :*

*Sub-plan [5-7] is exactly the same as sub-plan [1-3]( Aggregate on BHJed dataset of two tables that are filtered, projected the same way).* 

*Therefore, in the below parquet plan, exchange that happens after [1-3] is reused to replace [5-6].*

 *However, the DataSourceV2 plan failed to do so.*

 

Parquet:
{code:java}
== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[d_month_seq#21456L ASC NULLS FIRST], output=[prev_year#21451L,year#21452L,d_month_seq#21456L])
+- *(9) Project [d_year#21487L AS prev_year#21451L, d_year#20481L AS year#21452L, d_month_seq#21456L]
   +- CartesianProduct
      :- *(4) HashAggregate(keys=[d_year#20481L, d_month_seq#21456L], functions=[])
      :  +- Exchange hashpartitioning(d_year#20481L, d_month_seq#21456L, 200)
      :     +- *(3) HashAggregate(keys=[d_year#20481L, d_month_seq#21456L], functions=[])
      :        +- BroadcastNestedLoopJoin BuildRight, Cross
      :           :- *(1) Project [d_year#20481L]
      :           :  +- *(1) Filter (((((isnotnull(d_year#20481L) && isnotnull(d_day_name#20489)) && isnotnull(d_fy_year#20486L)) && (d_day_name#20489 = Monday)) && (d_fy_year#20486L > 2000)) && (d_year#20481L = 2002))
      :           :     +- *(1) FileScan parquet [d_year#20481L,d_fy_year#20486L,d_day_name#20489] Batched: true, Format: Parquet, Location: InMemoryFileIndex[gs://dataproc-kokoro-tests-us-central1/tpcds/1G/parquet/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_day_name), IsNotNull(d_fy_year), EqualTo(d_day_name,Monday), Grea..., ReadSchema: struct<d_year:bigint,d_fy_year:bigint,d_day_name:string>
      :           +- BroadcastExchange IdentityBroadcastMode
      :              +- *(2) Project [d_month_seq#21456L]
      :                 +- *(2) Filter (((isnotnull(d_day_name#21467) && isnotnull(d_fy_year#21464L)) && (d_day_name#21467 = Monday)) && (d_fy_year#21464L > 2000))
      :                    +- *(2) FileScan parquet [d_month_seq#21456L,d_fy_year#21464L,d_day_name#21467] Batched: true, Format: Parquet, Location: InMemoryFileIndex[gs://dataproc-kokoro-tests-us-central1/tpcds/1G/parquet/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_day_name), IsNotNull(d_fy_year), EqualTo(d_day_name,Monday), GreaterThan(d_fy_year,2..., ReadSchema: struct<d_month_seq:bigint,d_fy_year:bigint,d_day_name:string>
      +- *(8) HashAggregate(keys=[d_year#21487L, d_month_seq#21540L], functions=[])
         +- ReusedExchange [d_year#21487L, d_month_seq#21540L], Exchange hashpartitioning(d_year#20481L, d_month_seq#21456L, 200){code}
 

DataSourceV2:
{code:java}
== Physical Plan ==
 TakeOrderedAndProject(limit=100, orderBy=d_month_seq#22325L ASC NULLS FIRST, output=prev_year#22320L,year#22321L,d_month_seq#22325L)
 +- *(9) Project d_year#22356L AS prev_year#22320L, d_year#21696L AS year#22321L, d_month_seq#22325L
 +- CartesianProduct
 :- *(4) HashAggregate(keys=d_year#21696L, d_month_seq#22325L, functions=[])
 : +- Exchange hashpartitioning(d_year#21696L, d_month_seq#22325L, 200)
 : +- *(3) HashAggregate(keys=d_year#21696L, d_month_seq#22325L, functions=[])
 : +- BroadcastNestedLoopJoin BuildRight, Cross
 : :- *(1) Project d_year#21696L
 : : +- *(1) ScanV2 BigQueryDataSourceV2d_year#21696L (Filters: [isnotnull(d_day_name#21704), (d_day_name#21704 = Monday), isnotnull(d_fy_year#21701L), (d_fy_yea..., Options: [table=tpcds_1G.date_dim,paths=[]])
 : +- BroadcastExchange IdentityBroadcastMode
 : +- *(2) Project d_month_seq#22325L
 : +- *(2) ScanV2 BigQueryDataSourceV2d_month_seq#22325L (Filters: [isnotnull(d_day_name#22336), (d_day_name#22336 = Monday), isnotnull(d_fy_year#22333L), (d_fy_yea..., Options: [table=tpcds_1G.date_dim,paths=[]])
 +- *(8) HashAggregate(keys=d_year#22356L, d_month_seq#22409L, functions=[])
 +- Exchange hashpartitioning(d_year#22356L, d_month_seq#22409L, 200)
 +- *(7) HashAggregate(keys=d_year#22356L, d_month_seq#22409L, functions=[])
 +- BroadcastNestedLoopJoin BuildRight, Cross
 :- *(5) Project d_year#22356L
 : +- *(5) ScanV2 BigQueryDataSourceV2d_year#22356L (Filters: [isnotnull(d_day_name#22364), (d_day_name#22364 = Monday), isnotnull(d_fy_year#22361L), (d_fy_yea..., Options: [table=tpcds_1G.date_dim,paths=[]])
 +- BroadcastExchange IdentityBroadcastMode
 +- *(6) Project d_month_seq#22409L
 +- *(6) ScanV2 BigQueryDataSourceV2d_month_seq#22409L (Filters: [isnotnull(d_day_name#22420), (d_day_name#22420 = Monday), isnotnull(d_fy_year#22417L), (d_fy_yea..., Options: [table=tpcds_1G.date_dim,paths=[]]){code}
 ",,apachespark,emkornfield@gmail.com,Gengliang.Wang,maropu,mingjial,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 14 07:55:59 UTC 2020,,,,,,,,,,"0|z0i48w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/20 00:01;mingjial;This difference happened at ReuseExchange rule apply. I can't repro the issue at Spark 3.0

*Direct reason:*

equals function of DataSourceV2ScanExec returns 'false' as comparing [6] [5] VS [2][1] in DataSourceV2 plan.

*Actual cause* :

DataSourceV2ScanExec.pushedFilters are defined as array of Expressions whose equal function has  expression_id in its scope. So for example,  *isnotnull(d_day_name#22364)*  is not considered equal to  *isnotnull(d_day_name#22420)*
  
 *Fix:* 

2.4 needs  to partially follow what 3.0 does. Basically pushedFilter should be of Filter class instead of Expression class.  

mingjialiu- I am currently working on a fix.;;;","27/Aug/20 00:05;mingjial;This bug is brought to the surface by [link SPARK-32609|https://issues.apache.org/jira/browse/SPARK-32609]
Previously, two DataSourceV2ScanExec are incorrectly considered equal no matter what pushedFilters they have.;;;","27/Aug/20 23:37;apachespark;User 'mingjialiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29564;;;","14/Sep/20 07:55;Gengliang.Wang;The issue is resolved in https://github.com/apache/spark/pull/29564;;;",,,,,,,,,,,,,,,,,,,,,,
"EmptyHashedRelation$; no valid constructor",SPARK-32705,13324600,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,leanken,leanken,leanken,26/Aug/20 07:34,27/Aug/20 06:25,13/Jul/23 08:50,27/Aug/20 06:25,3.0.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"Current EmptyHashedRelation is object and it will cause JavaDeserialization Exception as follow

 
{code:java}
// Stack
20/08/26 11:13:30 WARN [task-result-getter-2] TaskSetManager: Lost task 34.0 in stage 57.0 (TID 18076, emr-worker-5.cluster-183257, executor 18): java
.io.InvalidClassException: org.apache.spark.sql.execution.joins.EmptyHashedRelation$; no valid constructor
        at java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:169)
        at java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:874)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1572)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:430)
        at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
        at org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$unBlockifyObject$4(TorrentBroadcast.scala:328)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:330)
        at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:249)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:223)
        at org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)
        at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:218)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1343)

{code}
using case object instead to fix serialization issue. And also change EmptyHashedRelation not to extend NullAwareHashedRelation since it's already being used in other non-NAAJ joins.

 ",,apachespark,cloud_fan,leanken,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 27 06:25:03 UTC 2020,,,,,,,,,,"0|z0i3fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/20 07:48;apachespark;User 'leanken' has created a pull request for this issue:
https://github.com/apache/spark/pull/29547;;;","27/Aug/20 06:25;cloud_fan;Issue resolved by pull request 29547
[https://github.com/apache/spark/pull/29547];;;",,,,,,,,,,,,,,,,,,,,,,,,
Compare two dataframes with same schema except nullable property,SPARK-32693,13324296,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,dbe_75,dbe_75,24/Aug/20 15:28,03/Sep/20 04:10,13/Jul/23 08:50,28/Aug/20 01:37,2.4.4,3.1.0,,,,,,,,,,2.4.7,3.0.2,3.1.0,,SQL,,,,0,,,,"My aim is to compare two dataframes with very close schemas : same number of fields, with the same names, types and metadata. The only difference comes from the fact that a given field might be nullable in one dataframe and not in the other.

Here is the code that i used :
{code:java}
val session = SparkSession.builder().getOrCreate()
import org.apache.spark.sql.Row
import java.sql.Timestamp
import scala.collection.JavaConverters._

case class A(g: Timestamp, h: Option[Timestamp], i: Int)
case class B(e: Int, f: Seq[A])
case class C(g: Timestamp, h: Option[Timestamp], i: Option[Int])
case class D(e: Option[Int], f: Seq[C])

val schema1 = StructType(Array(StructField(""a"", IntegerType, false), StructField(""b"", IntegerType, false), StructField(""c"", IntegerType, false)))
val rowSeq1: List[Row] = List(Row(10, 1, 1), Row(10, 50, 2))
val df1 = session.createDataFrame(rowSeq1.asJava, schema1)
df1.printSchema()

val schema2 = StructType(Array(StructField(""a"", IntegerType), StructField(""b"", IntegerType), StructField(""c"", IntegerType)))
val rowSeq2: List[Row] = List(Row(10, 1, 1))
val df2 = session.createDataFrame(rowSeq2.asJava, schema2)
df2.printSchema()

println(s""Number of records for first case : ${df1.except(df2).count()}"")
val schema3 = StructType(
 Array(
 StructField(""a"", IntegerType, false),
 StructField(""b"", IntegerType, false), 
 StructField(""c"", IntegerType, false), 
 StructField(""d"", ArrayType(StructType(Array(StructField(""e"", IntegerType, false), StructField(""f"", ArrayType(StructType(Array(StructField(""g"", TimestampType), StructField(""h"", TimestampType), StructField(""i"", IntegerType, false)
 ))))
 ))))
 )
 )
val date1 = new Timestamp(1597589638L)
val date2 = new Timestamp(1597599638L)
val rowSeq3: List[Row] = List(Row(10, 1, 1, Seq(B(100, Seq(A(date1, None, 1))))), Row(10, 50, 2, Seq(B(101, Seq(A(date2, None, 2))))))
val df3 = session.createDataFrame(rowSeq3.asJava, schema3)
df3.printSchema()

val schema4 = StructType(
 Array(
 StructField(""a"", IntegerType), 
 StructField(""b"", IntegerType), 
 StructField(""b"", IntegerType), 
 StructField(""d"", ArrayType(StructType(Array(StructField(""e"", IntegerType), StructField(""f"", ArrayType(StructType(Array(StructField(""g"", TimestampType), StructField(""h"", TimestampType), StructField(""i"", IntegerType)
 ))))
 ))))
 )
 )
val rowSeq4: List[Row] = List(Row(10, 1, 1, Seq(D(Some(100), Seq(C(date1, None, Some(1)))))))
val df4 = session.createDataFrame(rowSeq4.asJava, schema3)
df4.printSchema()
println(s""Number of records for second case : ${df3.except(df4).count()}"")
{code}
The preceeding code shows what seems to me a bug in Spark :
 * If you consider two dataframes (df1 and df2) having exactly the same schema, except fields are not nullable for the first dataframe and are nullable for the second. Then, doing df1.except(df2).count() works well.
 * Now, if you consider two other dataframes (df3 and df4) having the same schema (with fields nullable on one side and not on the other). If these two dataframes contain nested fields, then, this time, the action df3.except(df4).count gives the following exception : java.lang.IllegalArgumentException: requirement failed: Join keys from two sides should have same types",,apachespark,dbe_75,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 03 04:09:59 UTC 2020,,,,,,,,,,"0|z0i1kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/20 06:07;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29555;;;","28/Aug/20 01:37;maropu;Resolved by [https://github.com/apache/spark/pull/29555];;;","28/Aug/20 12:44;dbe_75;thanks;;;","29/Aug/20 00:10;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29575;;;","29/Aug/20 00:11;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29575;;;","29/Aug/20 00:21;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29576;;;","29/Aug/20 00:21;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29576;;;","29/Aug/20 21:56;maropu;Since the release vote for v3.0.1 (RC3) is on-going now, I set 3.0.2 to ""Fix Version/s"". If it fails, I will reset it to 3.0.1. ;;;","01/Sep/20 00:15;maropu;I changed 3.0.2 to 3.0.1 in ""Fix Version/s"" because the RC3 vote for v3.0.1 seemed to fail: [http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Spark-3-0-1-RC3-td30092.html];;;","01/Sep/20 00:17;viirya;Ok. Thanks [~maropu];;;","03/Sep/20 04:09;maropu;Unfortunately, v3.0.1 does not include this fix...., so I reset it back: [http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-RESULT-Release-Spark-3-0-1-RC3-td30114.html];;;",,,,,,,,,,,,,,,
Update commons-crypto to v1.1.0,SPARK-32691,13324259,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huangtianhua,huangtianhua,huangtianhua,24/Aug/20 12:04,12/Jan/21 21:18,13/Jul/23 08:50,09/Nov/20 22:33,2.4.7,3.0.0,3.0.1,3.1.0,,,,,,,,3.0.2,3.1.0,,,Spark Core,Tests,,,1,,,,"Tests of org.apache.spark.DistributedSuite are failed on arm64 jenkins: https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-arm/ 

    - caching in memory and disk, replicated (encryption = on) (with replication as stream) *** FAILED ***
  3 did not equal 2; got 3 replicas instead of 2 (DistributedSuite.scala:191)
    - caching in memory and disk, serialized, replicated (encryption = on) (with replication as stream) *** FAILED ***
  3 did not equal 2; got 3 replicas instead of 2 (DistributedSuite.scala:191)
    - caching in memory, serialized, replicated (encryption = on) (with replication as stream) *** FAILED ***
  3 did not equal 2; got 3 replicas instead of 2 (DistributedSuite.scala:191)
    .......
    ",ARM64,apachespark,dongjoon,ganeshraju,huangtianhua,maropu,podongfeng,RuiChen,v_ganeshraju,,,,,,,,,,,,,,,,,,,CRYPTO-139,,,,,,,,,"28/Sep/20 15:49;dongjoon;Screen Shot 2020-09-28 at 8.49.04 AM.png;https://issues.apache.org/jira/secure/attachment/13012252/Screen+Shot+2020-09-28+at+8.49.04+AM.png","01/Sep/20 03:11;huangtianhua;failure.log;https://issues.apache.org/jira/secure/attachment/13010804/failure.log","01/Sep/20 03:11;huangtianhua;success.log;https://issues.apache.org/jira/secure/attachment/13010803/success.log",,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 07 03:03:52 UTC 2021,,,,,,,,,,"0|z0i1c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/20 12:08;huangtianhua;[~dongjoon], I took tests for several times locally, the different tests are failed. ;;;","25/Aug/20 15:18;dongjoon;Thank you for reporting, [~huangtianhua]. Yes. I suspect ""with replication as stream"" code path is related to this on ARM.;;;","31/Aug/20 06:37;huangtianhua;[~dongjoon] Seems it doesn't with 'with replication as stream', see https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-arm/392/testReport/junit/org.apache.spark/DistributedSuite/caching_on_disk__replicated_2__encryption___on_/;;;","31/Aug/20 16:16;dongjoon;Yes. That failure looks like that, but it's still irrelevant to DISK_3, isn't it? You can attach that to the PR description and make the scope broader.
{code}
caching on disk, replicated 2 (encryption = on)
{code};;;","01/Sep/20 03:22;huangtianhua;> testOnly *DistributedSuite -- -z ""caching in memory and disk, replicated (encryption = on) (with replication as stream)""
I test only for case ""caching in memory and disk, replicated (encryption = on) (with replication as stream)"", it's not fail always.
I am so sorry I can't fix this issue, and the arm jenkins failed for a few days, I am uploaded the success.log and failure.log to attach files, so if anybody can help to analysis, and I can provide the arm64 instance if need, thanks all!;;;","02/Sep/20 03:54;dongjoon;You don't need to be sorry for not fixing the issue. Reporting a bug is an invaluable contribution. It's just difficult for most community members to reproduce/develop/test on ARM.;;;","03/Sep/20 01:35;huangtianhua;[~dongjoon], ok, thanks. And if anyone wants to reproduce the failure on ARM, I can provide an arm instance:);;;","27/Sep/20 10:09;podongfeng;[~huangtianhua]  [~dongjoon] I just see that the last two {{org.apache.spark.DistributedSuite}} tests pass, is this issue revolved?;;;","28/Sep/20 15:49;dongjoon;[~podongfeng], it failed again. It seems that they are flaky tests.

 !Screen Shot 2020-09-28 at 8.49.04 AM.png! ;;;","15/Oct/20 01:59;huangtianhua;[~podongfeng] Seems the tests failed after https://issues.apache.org/jira/browse/SPARK-32517 merged, please help me to check this, thanks very much.;;;","15/Oct/20 03:14;dongjoon;[~huangtianhua] As I posted at the initial analysis, it's irrelevant to the newly added code. Instead, it's relevant to the older code path which handles `with replication as stream`.;;;","15/Oct/20 07:47;huangtianhua;[~dongjoon] Sorry, but I have tested the case: remove the commit of SPRK-32517 , and then the tests success. ;;;","15/Oct/20 07:50;huangtianhua;
[~dongjoon] And I found the failed tests are not 'with replication as stream'. like this https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-arm/435/testReport/junit/org.apache.spark/DistributedSuite/caching_on_disk__replicated_2__encryption___on_/;;;","15/Oct/20 08:39;podongfeng;[~huangtianhua] It looks like that this is an already existing issue before SPARK-32517, which is triggered by changing #executors. ({{val clusterUrl = ""local-cluster[2,1,1024]""}} -> {{val clusterUrl = ""local-cluster[3,1,1024]""}}).

 

in the attached failure.log, block {{rdd_0_0}} was added three time, but it should be added only twice:
{code:java}
...
20/08/31 20:39:21.361 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_0_0 in memory on 192.168.1.225:36305 (size: 416.0 B, free: 546.3 MiB)
...
20/08/31 20:41:21.844 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_0_0 in memory on 192.168.1.225:35957 (size: 416.0 B, free: 546.3 MiB)
...
20/08/31 20:41:27.771 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_0_0 in memory on 192.168.1.225:34623 (size: 416.0 B, free: 546.3 MiB)
...

 {code}
 

IIUC, the result of {{testCaching}} in {{DistributedSuite}} should be irrelevant to the number of executor? [~dongjoon]

 

 ;;;","15/Oct/20 09:11;huangtianhua;[~podongfeng] Thanks, there are twice in success.log;;;","15/Oct/20 11:53;huangtianhua;I took test to remove the commit 32517 and just modify clusterUrl = ""local-cluster[2,1,1024]"" -> ""local-cluster[3,1,1024]"") then the tests failed.;;;","15/Oct/20 11:54;huangtianhua;So yes it's not related with commit 32517.;;;","18/Oct/20 04:19;dongjoon;Thank you for confirming, [~huangtianhua]. :);;;","06/Nov/20 06:44;huangtianhua;We have found the problem, it is take long time to replicate remote over the default timeout 120 seconds, so it try again to another executor, but in fact the replication is complete, so there are 3 replications total. Then we found the progress hang in CryptoRandomFactory.getCryptoRandom(properties), we found the jar commons-crypto v1.0.0 doesn't support aarch64, after we change to use v1.1.0 then the tests pass and the time is short. 
So I plan to propose a PR to change to use commons-crypto v1.1.0 which support aarch64: http://commons.apache.org/proper/commons-crypto/changes-report.html  https://issues.apache.org/jira/browse/CRYPTO-139;;;","06/Nov/20 07:02;apachespark;User 'huangtianhua' has created a pull request for this issue:
https://github.com/apache/spark/pull/30275;;;","06/Nov/20 07:03;apachespark;User 'huangtianhua' has created a pull request for this issue:
https://github.com/apache/spark/pull/30275;;;","09/Nov/20 22:33;dongjoon;Issue resolved by pull request 30275
[https://github.com/apache/spark/pull/30275];;;","12/Nov/20 02:25;huangtianhua;The ARM Jenkins CI still fail after this merged, it because we didn't install libssl-dev package. Now I think it's ok, let's wait today's Jenkins status.;;;","27/Nov/20 07:17;RuiChen;[~huangtianhua] looks Spark ARM CI passed in these days, the issue have been fixed, right?

https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-arm/;;;","07/Jan/21 03:02;apachespark;User 'huangtianhua' has created a pull request for this issue:
https://github.com/apache/spark/pull/31078;;;","07/Jan/21 03:03;apachespark;User 'huangtianhua' has created a pull request for this issue:
https://github.com/apache/spark/pull/31078;;;"
Datetime Pattern F not working as expected,SPARK-32683,13323949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,lamanus,lamanus,21/Aug/20 12:34,25/Aug/20 13:27,13/Jul/23 08:50,25/Aug/20 13:17,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"h3. Background

From the [documentation|https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html], the pattern F should give a week of the month.
|*Symbol*|*Meaning*|*Presentation*|*Example*|
|F|week-of-month|number(1)|3|
h3. Test Data

Here is my test data, that is a csv file.
{code:java}
date
2020-08-01
2020-08-02
2020-08-03
2020-08-04
2020-08-05
2020-08-06
2020-08-07
2020-08-08
2020-08-09
2020-08-10 {code}
h3. Steps to the bug

I have tested in the scala spark 3.0.0 and pyspark 3.0.0:
{code:java}
// Spark

df.withColumn(""date"", to_timestamp('date, ""yyyy-MM-dd""))
  .withColumn(""week"", date_format('date, ""F"")).show

+-------------------+----+
|               date|week|
+-------------------+----+
|2020-08-01 00:00:00|   1|
|2020-08-02 00:00:00|   2|
|2020-08-03 00:00:00|   3|
|2020-08-04 00:00:00|   4|
|2020-08-05 00:00:00|   5|
|2020-08-06 00:00:00|   6|
|2020-08-07 00:00:00|   7|
|2020-08-08 00:00:00|   1|
|2020-08-09 00:00:00|   2|
|2020-08-10 00:00:00|   3|
+-------------------+----+


# pyspark

df.withColumn('date', to_timestamp('date', 'yyyy-MM-dd')) \
  .withColumn('week', date_format('date', 'F')) \
  .show(10, False)

+-------------------+----+
|date               |week|
+-------------------+----+
|2020-08-01 00:00:00|1   |
|2020-08-02 00:00:00|2   |
|2020-08-03 00:00:00|3   |
|2020-08-04 00:00:00|4   |
|2020-08-05 00:00:00|5   |
|2020-08-06 00:00:00|6   |
|2020-08-07 00:00:00|7   |
|2020-08-08 00:00:00|1   |
|2020-08-09 00:00:00|2   |
|2020-08-10 00:00:00|3   |
+-------------------+----+{code}
h3. Expected result

The `week` column is not the week of the month. It is a day of the week as a number.

  !comment.png!

From my calendar, the first day of August should have 1 for the week-of-month and from 2nd to 8th should have 2 and so on.
{code:java}
+-------------------+----+
|date               |week|
+-------------------+----+
|2020-08-01 00:00:00|1   |
|2020-08-02 00:00:00|2   |
|2020-08-03 00:00:00|2   |
|2020-08-04 00:00:00|2   |
|2020-08-05 00:00:00|2   |
|2020-08-06 00:00:00|2   |
|2020-08-07 00:00:00|2   |
|2020-08-08 00:00:00|2   |
|2020-08-09 00:00:00|3   |
|2020-08-10 00:00:00|3   |
+-------------------+----+{code}","Windows 10 Pro
 * with Jupyter Lab - Docker Image 
 ** jupyter/all-spark-notebook:f1811928b3dd 
 *** spark 3.0.0
 *** python 3.8.5
 *** openjdk 11.0.8",apachespark,cloud_fan,lamanus,maropu,Qin Yao,Varun Wachaspati,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/20 12:35;lamanus;comment.png;https://issues.apache.org/jira/secure/attachment/13010214/comment.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 13:21:40 UTC 2020,,,,,,,,,,"0|z0hzfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/20 10:09;Varun Wachaspati;Stumbled upon the same issue, upon initial investigation looks like change in behaviour in java.lang.DateTimeFormatterBuilder.
Literal character 'F' currently maps to ChronoField.ALIGNED_DAY_OF_WEEK_IN_MONTH instead of ChronoField.ALIGNED_WEEK_OF_MONTH

According to ALIGNED_DAY_OF_WEEK_IN_MONTH definition it formats the timestamp to give the count of the date within that week. Defintion - [https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/time/temporal/ChronoField.java#L373]



Tried the same snippet in Spark 2.4.6, and it's behaviour was inline with the spark documentation. This looks like a regression for Spark 3.x. ;;;","23/Aug/20 12:25;maropu;cc: [~maxgekk] [~Qin Yao];;;","24/Aug/20 03:48;Qin Yao;This is a bug of the doc that we inherited from JDK https://bugs.openjdk.java.net/browse/JDK-8169482

The SimpleDateFormatter(*F	Day of week in month*) we used in 2.x and the DatetimeFormatter(*F       week-of-month*) we use now both have the opposite meanings to what they declared in the java docs. And unfortunately, this also leads to silent data change in Spark too.

The  *`week-of-month`* is actually  the pattern `W` in DatetimeFormatter, which is banned to use in Spark 3.x

If we want to keep pattern `F`, we need to accept the behavior change and fix the doc in Spark. cc [~cloud_fan];;;","24/Aug/20 08:58;cloud_fan;It's unfortunate that we missed it before the 3.0 release. I think we should not introduce behavior change in 3.0.1 again, and let's update the doc and migration guide instead.;;;","25/Aug/20 06:41;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29538;;;","25/Aug/20 06:41;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/29538;;;","25/Aug/20 13:17;cloud_fan;Issue resolved by pull request 29538
[https://github.com/apache/spark/pull/29538];;;","25/Aug/20 13:21;lamanus;I did not mean to change the doc but the source or recover the DateFormatter W, anyway, the function is gone (even before) and the documentation is now clear, not confused. ;;;",,,,,,,,,,,,,,,,,,
CTAS with V2 catalog wrongly accessed unresolved query,SPARK-32680,13323913,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linhongliu-db,linhongliu-db,linhongliu-db,21/Aug/20 08:47,08/Dec/20 03:45,13/Jul/23 08:50,07/Dec/20 13:26,3.0.0,,,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"Case:

{{CREATE TABLE t USING delta AS SELECT * from nonexist }}

 

Expected:

throw AnalysisException with ""Table or view not found""

 

Actual:

{{throw UnresolvedException with ""org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: *""}}",,apachespark,cloud_fan,linhongliu-db,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 14:54:44 UTC 2020,,,,,,,,,,"0|z0hz7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/20 14:10;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/29507;;;","07/Dec/20 03:06;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30637;;;","07/Dec/20 13:26;cloud_fan;Issue resolved by pull request 30637
[https://github.com/apache/spark/pull/30637];;;","07/Dec/20 14:54;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/30649;;;",,,,,,,,,,,,,,,,,,,,,,
--py-files option is appended without passing value for it,SPARK-32675,13323862,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,farhan5900,farhan5900,farhan5900,21/Aug/20 00:58,21/Oct/20 04:56,13/Jul/23 08:50,24/Aug/20 00:25,3.0.0,,,,,,,,,,,3.1.0,,,,Mesos,,,,0,,,,"Submitted application passing --py-files option in a hardcoded manner for a Mesos Cluster in cluster mode using REST Submission API. It is causing a simple Java-based SparkPi job to fail.

This Bug is introduced by SPARK-26466.

Here is the example job submission:
{code:bash}
curl -X POST http://localhost:7077/v1/submissions/create --header ""Content-Type:application/json"" --data '{
""action"": ""CreateSubmissionRequest"",
""appResource"": ""file:///opt/spark-3.0.0-bin-3.2.0/examples/jars/spark-examples_2.12-3.0.0.jar"",
""clientSparkVersion"": ""3.0.0"",
""appArgs"": [""30""],
""environmentVariables"": {},
""mainClass"": ""org.apache.spark.examples.SparkPi"",
""sparkProperties"": {
  ""spark.jars"": ""file:///opt/spark-3.0.0-bin-3.2.0/examples/jars/spark-examples_2.12-3.0.0.jar"",
  ""spark.driver.supervise"": ""false"",
  ""spark.executor.memory"": ""512m"",
  ""spark.driver.memory"": ""512m"",
  ""spark.submit.deployMode"": ""cluster"",
  ""spark.app.name"": ""SparkPi"",
  ""spark.master"": ""mesos://localhost:5050""
}}'
{code}
Expected Driver log would contain:
{code:bash}
20/08/20 20:19:57 WARN DependencyUtils: Local jar /var/lib/mesos/slaves/e6779377-08ec-4765-9bfc-d27082fbcfa1-S0/frameworks/e6779377-08ec-4765-9bfc-d27082fbcfa1-0000/executors/driver-20200820201954-0002/runs/d9d734e8-a299-4d87-8f33-b134c65c422b/spark.driver.memory=512m does not exist, skipping.
Error: Failed to load class org.apache.spark.examples.SparkPi.
20/08/20 20:19:57 INFO ShutdownHookManager: Shutdown hook called
{code}",,amandeep.kaur,apachespark,dongjoon,farhan5900,rrusso2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 21 04:56:47 UTC 2020,,,,,,,,,,"0|z0hyw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/20 01:15;apachespark;User 'farhan5900' has created a pull request for this issue:
https://github.com/apache/spark/pull/29499;;;","24/Aug/20 00:25;dongjoon;Issue resolved by pull request 29499
[https://github.com/apache/spark/pull/29499];;;","21/Oct/20 04:56;rrusso2007;I see this is targeted for 3.1.0, but is this also going into 3.0.2? This completely breaks the mesos dispatcher service for 3.0.x for anyone trying to upgrade. We just had to hunt down the same issue;;;",,,,,,,,,,,,,,,,,,,,,,,
Data corruption in some cached compressed boolean columns,SPARK-32672,13323837,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,revans2,revans2,revans2,20/Aug/20 21:24,12/Dec/22 18:11,13/Jul/23 08:50,22/Aug/20 02:09,2.3.4,2.4.6,3.0.0,3.0.1,3.1.0,,,,,,,2.4.7,3.0.1,3.1.0,,SQL,,,,0,correctness,,,"I found that when sorting some boolean data into the cache that the results can change when the data is read back out.

It needs to be a non-trivial amount of data, and it is highly dependent on the order of the data.  If I disable compression in the cache the issue goes away.  I was able to make this happen in 3.0.0.  I am going to try and reproduce it in other versions too.

I'll attach the parquet file with boolean data in an order that causes this to happen. As you can see after the data is cached a single null values switches over to be false.

{code}
scala> val bad_order = spark.read.parquet(""./bad_order.snappy.parquet"")
bad_order: org.apache.spark.sql.DataFrame = [b: boolean]                        

scala> bad_order.groupBy(""b"").count.show
+-----+-----+
|    b|count|
+-----+-----+
| null| 7153|
| true|54334|
|false|54021|
+-----+-----+


scala> bad_order.cache()
res1: bad_order.type = [b: boolean]

scala> bad_order.groupBy(""b"").count.show
+-----+-----+
|    b|count|
+-----+-----+
| null| 7152|
| true|54334|
|false|54022|
+-----+-----+


scala> 

{code}",,apachespark,cltlfcjin,dongjoon,kabhwan,maropu,revans2,tgraves,,,,,,,,,,,,,,,,,,,,SPARK-20783,,,,,,,,,"20/Aug/20 21:24;revans2;bad_order.snappy.parquet;https://issues.apache.org/jira/secure/attachment/13010159/bad_order.snappy.parquet","21/Aug/20 13:02;revans2;small_bad.snappy.parquet;https://issues.apache.org/jira/secure/attachment/13010217/small_bad.snappy.parquet",,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 22 12:41:53 UTC 2020,,,,,,,,,,"0|z0hyqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/20 21:40;revans2;I verified that this is still happening on 3.0.2-SNAPSHOT;;;","20/Aug/20 21:54;revans2;I verified that this is still happening on 3.1.0-SNAPSHOT  too;;;","20/Aug/20 21:58;tgraves;[~cloud_fan] [~ruifengz];;;","20/Aug/20 22:20;revans2;I did a little debugging and found that `BooleanBitSet$Encoder` is being used for compression.  There are other data orderings that use the same encoder and produce correct results though.;;;","20/Aug/20 22:45;revans2;I added some debugging to the compression code and it looks like in the 8th CompressedBatch of 10,000 entries the number of nulls seen was different from the number expected.

619 expected and 618 seen.  I'll try to debug this a bit more tomorrow.;;;","21/Aug/20 02:51;cltlfcjin;Changed to Critical, Blocker is reserved for committer;;;","21/Aug/20 05:56;kabhwan;Just FYI, he's a PMC member. And correctness issue goes normally a blocker unless there's some strong reason to not address the issue right now.;;;","21/Aug/20 13:00;tgraves;[~cltlfcjin]  Please do not be changing priority just because people are not committers. You should first evaluate what they are reporting.  If you don't think its a blocker then we should state why the reason.

I looked at this after it was filed and added correctness tag and it was already marked as Blocker so I didn't need to change it.  As you can see from [https://spark.apache.org/contributing.html,|https://spark.apache.org/contributing.html] correctness issues should be marked as a blocker at least until it's investigated and discussed. ;;;","21/Aug/20 13:00;revans2;So I am able to reduce the corruption down to just a single 10,000 row chunk, and still get it to happen. I'll post a new parquet file soon that will hopefully make debugging a little simpler.

{code}
scala> val bad_order = spark.read.parquet(""/home/roberte/src/rapids-plugin-4-spark/integration_tests/bad_order.snappy.parquet"").selectExpr(""b"", ""monotonically_increasing_id() as id"").where(col(""id"")>=70000 and col(""id"") < 80000)
bad_order: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [b: boolean, id: bigint]

scala> bad_order.groupBy(""b"").count.show
+-----+-----+
|    b|count|
+-----+-----+
| null|  619|
| true| 4701|
|false| 4680|
+-----+-----+


scala> bad_order.cache()
res2: bad_order.type = [b: boolean, id: bigint]

scala> bad_order.groupBy(""b"").count.show
+-----+-----+
|    b|count|
+-----+-----+
| null|  618|
| true| 4701|
|false| 4681|
+-----+-----+

{code};;;","21/Aug/20 13:15;revans2;OK reading through the code I understand what is happening now.  The compression format ignores nulls, which are stored separately.  As such the bit set stored is only for non-null boolean values/bits. The number of entries stored in the compression format is  the number of non-null boolean values that are stored.

So the stopping condition on a batch decompress.

{code}
while (visitedLocal < countLocal) {
{code}

skips all of null values at the end.  But because the length of the column is known ahead of time it falls back to the default value which is false.

I'll try to get a patch up shortly to fix this.;;;","21/Aug/20 14:03;apachespark;User 'revans2' has created a pull request for this issue:
https://github.com/apache/spark/pull/29506;;;","21/Aug/20 14:03;apachespark;User 'revans2' has created a pull request for this issue:
https://github.com/apache/spark/pull/29506;;;","21/Aug/20 21:19;dongjoon;Thank you, [~revans2]. ;;;","22/Aug/20 02:09;gurwls223;Issue resolved by pull request 29506
[https://github.com/apache/spark/pull/29506];;;","22/Aug/20 02:24;gurwls223;[~revans2] is a PMC, and it is a correctness issue. This indeed is a blocker. I think the initial action was a mistake.

I think [~cltlfcjin] referred:

{quote}
Set to Major or below; higher priorities are generally reserved for committers to set. 
{quote}

I fully agree that ideally we should first evaluate what they are reporting with stating the reason.

The problem is that we don't have a lot of manpower here in triaging/managing JIRAs. It's just that there are not so many people who do.
Given this situation, I would like to encourage to aggressively triage - there are many JIRAs that set priority incorrectly.

For example, many JIRAs just ask questions and/or investigations with setting the priority as a blocker, presumably expecting quicker responses and actions. Such blockers matter for release managers.

If we want more fine-grained and ideal evaluation of JIRAs, I would encourage our PMC members/committers/contributors to take a look more often.

We also ended up with introducing auto-resolving JIRAs that sets affect versions as EOL Spark versions, due to the lack of the manpower in JIRA management (which I don't think this is ideal but I think it was inevitable) for the same reason.;;;","22/Aug/20 12:41;revans2;Honestly, it is not a big deal what happened.  I have worked on enough open-source projects that I know that all of this is best-effort run by volunteers.  Plus my involvement in the Spark project has not been frequent enough for a lot of people to know that I am a PMC member and honestly I have not been involved enough lately to know the process myself.  So I am happy to have people correct me or treat me like I am a contributor instead. The important thing is that we fixed the bug, and it should start to roll out soon.;;;",,,,,,,,,,
Getting local shuffle block clutters the executor logs,SPARK-32664,13323659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dmoore62,csingh,csingh,20/Aug/20 06:05,12/Dec/22 18:11,13/Jul/23 08:50,25/Aug/20 13:47,3.1.0,,,,,,,,,,,3.1.0,,,,Shuffle,,,,0,,,,"The below log statement in {{BlockManager.getLocalBlockData}} should be at debug level
{code:java}
logInfo(s""Getting local shuffle block ${blockId}"")
{code}
Currently, the executor logs get cluttered with this
{code:java}
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6103_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6132_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6137_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6312_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6323_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6402_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6413_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6694_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6709_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6753_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6822_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6894_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_6913_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_7052_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_7073_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_7167_4964
20/08/20 02:07:52 INFO storage.BlockManager: Getting local shuffle block shuffle_0_7194_4964
{code}

This was added with SPARK-20629.
cc. [~holden]
",,apachespark,csingh,dmoore62,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 13:47:44 UTC 2020,,,,,,,,,,"0|z0hxn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/20 04:21;dmoore62;PR open - [https://github.com/apache/spark/pull/29527];;;","24/Aug/20 04:22;apachespark;User 'dmoore62' has created a pull request for this issue:
https://github.com/apache/spark/pull/29527;;;","25/Aug/20 13:47;gurwls223;Issue resolved by pull request 29527
[https://github.com/apache/spark/pull/29527];;;",,,,,,,,,,,,,,,,,,,,,,,
TransportClient getting closed when there are outstanding requests to the server,SPARK-32663,13323638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,csingh,csingh,20/Aug/20 00:47,21/Aug/20 06:05,13/Jul/23 08:50,21/Aug/20 06:05,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,Shuffle,,,,0,,,,"The implementation of {{removeBlocks}} and {{getHostLocalDirs}} in {{ExternalBlockStoreClient}} closes the client after processing a response in the callback. 

This is a cached client which will be re-used for other responses. There could be other outstanding request to the shuffle service, so it should not be closed after processing a response. 
Seems like this is a bug introduced with SPARK-27651 and SPARK-27677. 

The older methods  {{registerWithShuffleServer}} and {{fetchBlocks}} didn't close the client.

cc [~attilapiros] [~vanzin] [~mridulm80]",,apachespark,attilapiros,csingh,mridulm80,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 21 06:05:57 UTC 2020,,,,,,,,,,"0|z0hxig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/20 12:00;attilapiros;You are right. The clients will be closed via [TransportClientFactory#close()|https://github.com/attilapiros/spark/blob/e6795cd3416bbe32505efd4c1fa3202f451bf74d/common/network-common/src/main/java/org/apache/spark/network/client/TransportClientFactory.java#L324].

;;;","20/Aug/20 13:16;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/29492;;;","20/Aug/20 13:16;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/29492;;;","21/Aug/20 06:05;mridulm80;Issue resolved by pull request 29492
[https://github.com/apache/spark/pull/29492];;;",,,,,,,,,,,,,,,,,,,,,,
Fix the data issue of inserted DPP on non-atomic type,SPARK-32659,13323516,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,19/Aug/20 08:41,22/Sep/20 16:01,13/Jul/23 08:50,26/Aug/20 06:58,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,correctness,,,"DPP has data issue when pruning on non-atomic type. for example:
{noformat}
 spark.range(1000)
 .select(col(""id""), col(""id"").as(""k""))
 .write
 .partitionBy(""k"")
 .format(""parquet"")
 .mode(""overwrite"")
 .saveAsTable(""df1"");

spark.range(100)
 .select(col(""id""), col(""id"").as(""k""))
 .write
 .partitionBy(""k"")
 .format(""parquet"")
 .mode(""overwrite"")
 .saveAsTable(""df2"")

spark.sql(""set spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio=2"")
spark.sql(""set spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly=false"")
spark.sql(""SELECT df1.id, df2.k FROM df1 JOIN df2 ON struct(df1.k) = struct(df2.k) AND df2.id < 2"").show
{noformat}


 It should return two records, but it returns empty.",,apachespark,cloud_fan,maropu,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 16:01:44 UTC 2020,,,,,,,,,,"0|z0hwrc:",9223372036854775807,,,,,,,,,,,,,3.0.1,,,,,,,,,,"19/Aug/20 08:51;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29475;;;","19/Aug/20 08:52;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29475;;;","26/Aug/20 06:58;cloud_fan;Issue resolved by pull request 29475
[https://github.com/apache/spark/pull/29475];;;","31/Aug/20 10:38;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29595;;;","22/Sep/20 13:07;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29838;;;","22/Sep/20 16:01;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29840;;;",,,,,,,,,,,,,,,,,,,,
Partition length number overflow in `PartitionWriterStream`,SPARK-32658,13323492,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,19/Aug/20 06:48,20/Aug/20 07:09,13/Jul/23 08:50,20/Aug/20 07:09,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,Spark Core,,,,0,,,,"A Spark user reported  `FetchFailedException: Stream is corrupted` error when they upgraded their workload to 3.0. The issue happens when the shuffle output data size from a single task is very large (~5GB). The issue is introduced by https://github.com/apache/spark/commit/abef84a868e9e15f346eea315bbab0ec8ac8e389 , the `PartitionWriterStream` defined the partition length to be an int value, while it should be a long value.",,apachespark,jiangxb1987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 19 06:55:39 UTC 2020,,,,,,,,,,"0|z0hwm0:",9223372036854775807,,,,,,,,,,,,,3.0.1,3.1.0,,,,,,,,,"19/Aug/20 06:55;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/29474;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ObjectSerializerPruning fails for RowEncoder,SPARK-32652,13323390,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,18/Aug/20 15:55,12/Dec/22 18:10,13/Jul/23 08:50,19/Aug/20 04:51,3.0.0,3.1.0,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 19 04:51:40 UTC 2020,,,,,,,,,,"0|z0hvzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/20 16:01;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29467;;;","18/Aug/20 16:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29467;;;","19/Aug/20 04:51;gurwls223;Issue resolved by pull request 29467
[https://github.com/apache/spark/pull/29467];;;",,,,,,,,,,,,,,,,,,,,,,,
ORC predicate pushdown should work with case-insensitive analysis,SPARK-32646,13323148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,18/Aug/20 03:33,25/Aug/20 04:44,13/Jul/23 08:50,25/Aug/20 04:44,3.0.0,3.1.0,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"Currently ORC predicate pushdown doesn't work with case-insensitive analysis, see SPARK-32622 for the test case.

We should make ORC predicate pushdown work with case-insensitive analysis too.",,apachespark,cloud_fan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 24 16:04:58 UTC 2020,,,,,,,,,,"0|z0huhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/20 04:01;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29457;;;","18/Aug/20 04:02;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29457;;;","20/Aug/20 23:26;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/29498;;;","21/Aug/20 07:57;cloud_fan;Issue resolved by pull request 29457
[https://github.com/apache/spark/pull/29457];;;","22/Aug/20 02:00;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29513;;;","23/Aug/20 02:20;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29518;;;","23/Aug/20 02:21;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29518;;;","23/Aug/20 05:44;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29519;;;","23/Aug/20 07:45;viirya;Due to issue of hive-1.2 profile, reverted merged diff and so reopened this.;;;","24/Aug/20 05:14;viirya;All failed tests in hive-1.2 profile are fixed now. I will create a new PR for this.;;;","24/Aug/20 16:04;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29530;;;","24/Aug/20 16:04;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29530;;;",,,,,,,,,,,,,,
withField + getField on null structs returns incorrect results,SPARK-32641,13323110,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fqaiser94,fqaiser94,fqaiser94,17/Aug/20 20:52,25/Aug/20 05:00,13/Jul/23 08:50,25/Aug/20 05:00,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,," 

There is a bug in the way the optimizer rule in SimplifyExtractValueOps (sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ComplexTypes.scala) is currently coded which yields incorrect results in scenarios like the following: 
{code:java}
sql(""SELECT CAST(NULL AS struct<a:int,b:int>) struct_col"")
.select($""struct_col"".withField(""d"", lit(4)).getField(""d"").as(""d""))

// currently returns this:
+---+
|d  |
+---+
|4  |
+---+

// when in fact it should return this: 
+----+
|d   |
+----+
|null|
+----+
{code}
 

 ",,apachespark,cloud_fan,fqaiser94,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 05:00:12 UTC 2020,,,,,,,,,,"0|z0hu94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/20 15:38;apachespark;User 'fqaiser94' has created a pull request for this issue:
https://github.com/apache/spark/pull/29522;;;","23/Aug/20 15:39;apachespark;User 'fqaiser94' has created a pull request for this issue:
https://github.com/apache/spark/pull/29522;;;","25/Aug/20 05:00;cloud_fan;Issue resolved by pull request 29522
[https://github.com/apache/spark/pull/29522];;;",,,,,,,,,,,,,,,,,,,,,,,
Spark 3.1 log(NaN) returns null instead of NaN,SPARK-32640,13323102,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,tgraves,tgraves,17/Aug/20 20:03,20/Aug/20 21:44,13/Jul/23 08:50,20/Aug/20 20:27,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,correctness,,,"I was testing Spark 3.1.0 and I noticed that if you take the log(NaN) it now returns a null whereas in Spark 3.0 it returned a NaN.  I'm not an expert in this but I thought NaN was correct.

Spark 3.1.0 Example:

>>> df.selectExpr([""value"", ""log1p(value)""]).show()

+--------------+-----------------+
|        value|      LOG1P(value)|

+--------------+-----------------+
|-3.4028235E38|              null|
|3.4028235E38|88.72283906194683|
|          0.0|               0.0|
|         -0.0|              -0.0|
|          1.0|0.6931471805599453|
|         -1.0|              null|
|          NaN|              null|

+--------------+-----------------+

 

Spark 3.0.0 example:

 

+-------------+------------------+
| value| LOG1P(value)|
+-------------+------------------+
|-3.4028235E38| null|
| 3.4028235E38| 88.72283906194683|
| 0.0| 0.0|
| -0.0| -0.0|
| 1.0|0.6931471805599453|
| -1.0| null|
| NaN| NaN|
+-------------+------------------+

 

Note it also does the same for log1p, log2, log10",,apachespark,cloud_fan,dongjoon,maropu,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 20 20:27:14 UTC 2020,,,,,,,,,,"0|z0hu7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/20 15:46;srowen;I wouldn't expect a behavior change. But I also don't know what the right answer is meant to be in this case, so it might be a 'bug fix'. But then I don't know of any changes along these lines. [~cloud_fan] do you have any ideas about the right answer here?

Under the hood we use StrictMath.log1p, which would return NaN for NaN.

BTW I'd expect log1p(-1) to be negative infinity? Just like log(0) should.;;;","18/Aug/20 16:20;cloud_fan;Just checked with Postgres: log('NaN') returns NaN. log(0) or log(-1) fails.

I think it makes sense for Spark to return null for 0 or negative values, but `log(NaN)` should return NaN.

I don't really know why the behavior was changed. The only change I see is we use `StrictMath` instead of `Math`, but the doc of `StrictMath` says that NaN will be returned if input is NaN.

The `log` implementation is:
{code:java}
if (double <= 0.0) return null else return StrictMath.log(double){code}
Maybe it's due to the scala 2.12 upgrade and the result of `NaN <= 0.0` becomes true?;;;","19/Aug/20 09:38;cloud_fan;I quickly tried 3.0.0
{code:java}
scala> sql(""select log1p(double('nan'))"").show
+--------------------------+
|LOG1P(CAST(nan AS DOUBLE))|
+--------------------------+
|                       NaN|
+--------------------------+
{code}
 

Looks like it's fine. [~tgraves] can you provide more details about how to reproduce the bug? It's a correctness issue that we should fix ASAP.;;;","19/Aug/20 13:16;tgraves;The problem is with 3.1.0 not, 3.0.0. The description shows the columns input data and output;;;","19/Aug/20 14:58;cloud_fan;master branch works too
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.0-SNAPSHOT
      /_/Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_161)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sql(""select log1p(double('nan'))"").show
+--------------------------+
|LOG1P(CAST(nan AS DOUBLE))|
+--------------------------+
|                       NaN|
+--------------------------+
{code}
Maybe there is a special NaN value that cause the null result?;;;","19/Aug/20 19:13;tgraves;hmm, interesting, this is how my test was reproducing with paysark:

 

 
{code:java}
special_cases = [0.0, -0.0, 1.0, -1.0]
special_cases.append(float('nan'))
from pyspark.sql.types import *
df = spark.createDataFrame(special_cases, DoubleType())
df.selectExpr('log(value)').show()
{code}
 

+---------------+
|LOG(E(), value)|

+---------------+
|null|

+---------------+

 

>>> df.show()
 +-----+
|value|

+-----+
|0.0|
|-0.0|
|1.0|
|-1.0|
|NaN|

+-----+

>>> df.printSchema()
 root
|– value: double (nullable = true)|

 ;;;","20/Aug/20 17:14;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29495;;;","20/Aug/20 17:15;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29495;;;","20/Aug/20 20:27;dongjoon;Issue resolved by pull request 29495
[https://github.com/apache/spark/pull/29495];;;",,,,,,,,,,,,,,,,,
Support GroupType parquet mapkey field,SPARK-32639,13323071,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Chen Zhang,Chen Zhang,Chen Zhang,17/Aug/20 15:10,28/Aug/20 16:52,13/Jul/23 08:50,28/Aug/20 16:51,2.4.6,3.0.0,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"I have a parquet file, and the MessageType recorded in the file is:
{code:java}
message parquet_schema {
  optional group value (MAP) {
    repeated group key_value {
      required group key {
        optional binary first (UTF8);
        optional binary middle (UTF8);
        optional binary last (UTF8);
      }
      optional binary value (UTF8);
    }
  }
}{code}
 

Use +spark.read.parquet(""000.snappy.parquet"")+ to read the file. Spark will throw an exception when converting Parquet MessageType to Spark SQL StructType:
{code:java}
AssertionError(Map key type is expected to be a primitive type, but found...)
{code}
 

Use +spark.read.schema(""value MAP<STRUCT<first:STRING, middle:STRING, last:STRING>, STRING>"").parquet(""000.snappy.parquet"")+ to read the file, spark returns the correct result .

According to the parquet project document (https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#maps), the mapKey in the parquet format does not need to be a primitive type.

 
Note: This parquet file is not written by spark, because spark will write additional sparkSchema string information in the parquet file. When Spark reads, it will directly use the additional sparkSchema information in the file instead of converting Parquet MessageType to Spark SQL StructType.


I will submit a PR later.",,apachespark,Chen Zhang,cloud_fan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/20 15:12;Chen Zhang;000.snappy.parquet;https://issues.apache.org/jira/secure/attachment/13009979/000.snappy.parquet",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 28 16:51:23 UTC 2020,,,,,,,,,,"0|z0hu0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/20 15:36;apachespark;User 'izchen' has created a pull request for this issue:
https://github.com/apache/spark/pull/29451;;;","28/Aug/20 16:51;cloud_fan;Issue resolved by pull request 29451
[https://github.com/apache/spark/pull/29451];;;",,,,,,,,,,,,,,,,,,,,,,,,
WidenSetOperationTypes in subquery  attribute  missing,SPARK-32638,13323034,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,kkyong,kkyong,17/Aug/20 11:16,08/Sep/20 23:23,13/Jul/23 08:50,03/Sep/20 14:49,2.3.4,2.4.5,3.0.0,,,,,,,,,3.0.2,3.1.0,,,SQL,,,,0,,,,"I am migrating sql from mysql to spark sql, meet a very strange case. Below is code to reproduce the exception:

 
{code:java}
val spark = SparkSession.builder()
 .master(""local"")
 .appName(""Word Count"")
 .getOrCreate()
spark.sparkContext.setLogLevel(""TRACE"")
val DecimalType = DataTypes.createDecimalType(20, 2)
val schema = StructType(List(
 StructField(""a"", DecimalType, true)
))
val dataList = new util.ArrayList[Row]()


val df=spark.createDataFrame(dataList,schema)
df.printSchema()
df.createTempView(""test"")
val sql=
 """"""
 |SELECT t.kpi_04 FROM
 |(
 | SELECT a as `kpi_04` FROM test
 | UNION ALL
 | SELECT a+a as `kpi_04` FROM test
 |) t
 |
 """""".stripMargin
spark.sql(sql)
{code}
 

Exception Message:

 
{code:java}
Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Resolved attribute(s) kpi_04#2 missing from kpi_04#4 in operator !Project [kpi_04#2]. Attribute(s) with the same name appear in the operation: kpi_04. Please check if the right attribute(s) are used.;;
!Project [kpi_04#2]
+- SubqueryAlias t
 +- Union
 :- Project [cast(kpi_04#2 as decimal(21,2)) AS kpi_04#4]
 : +- Project [a#0 AS kpi_04#2]
 : +- SubqueryAlias test
 : +- LocalRelation <empty>, [a#0]
 +- Project [kpi_04#3]
 +- Project [CheckOverflow((promote_precision(cast(a#0 as decimal(21,2))) + promote_precision(cast(a#0 as decimal(21,2)))), DecimalType(21,2)) AS kpi_04#3]
 +- SubqueryAlias test
 +- LocalRelation <empty>, [a#0]{code}
 

 

Base the trace log ,seemly the WidenSetOperationTypes add new outer project layer. It caused the parent query lose the reference to subquery. 

 

 
{code:java}
 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.TypeCoercion$WidenSetOperationTypes ===
!'Project [kpi_04#2] !Project [kpi_04#2]
!+- 'SubqueryAlias t +- SubqueryAlias t
! +- 'Union +- Union
! :- Project [a#0 AS kpi_04#2] :- Project [cast(kpi_04#2 as decimal(21,2)) AS kpi_04#4]
! : +- SubqueryAlias test : +- Project [a#0 AS kpi_04#2]
! : +- LocalRelation <empty>, [a#0] : +- SubqueryAlias test
! +- Project [CheckOverflow((promote_precision(cast(a#0 as decimal(21,2))) + promote_precision(cast(a#0 as decimal(21,2)))), DecimalType(21,2)) AS kpi_04#3] : +- LocalRelation <empty>, [a#0]
! +- SubqueryAlias test +- Project [kpi_04#3]
! +- LocalRelation <empty>, [a#0] +- Project [CheckOverflow((promote_precision(cast(a#0 as decimal(21,2))) + promote_precision(cast(a#0 as decimal(21,2)))), DecimalType(21,2)) AS kpi_04#3]
! +- SubqueryAlias test
! +- LocalRelation <empty>, [a#0]
{code}
 

  in the source code ,WidenSetOperationTypes.scala. it is  a intent behavior, but  possibly  miss this edge case. 

I hope someone can help me out to fix it . 

 

 
{code:java}
if (targetTypes.nonEmpty) {
 // Add an extra Project if the targetTypes are different from the original types.
 children.map(widenTypes(_, targetTypes))
} else {
 // Unable to find a target type to widen, then just return the original set.
 children
}{code}
 

 

 

 

 

 ",,apachespark,cloud_fan,cltlfcjin,kkyong,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 08 10:14:10 UTC 2020,,,,,,,,,,"0|z0hts8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/20 10:39;cloud_fan;2.3 is not maintained anymore, can you check with 2.4/3.0/master?;;;","19/Aug/20 08:46;cltlfcjin;Yes. This problem exists in 3.0 and master.

The problem occurs follow these steps:
In ResolveReference.apply()
When resolving the Project, its children are all resolved
'Project ['t.kpi_04]
+- SubqueryAlias t
   +- Union
      :- Project [a#44 AS kpi_04#46]
      :  +- SubqueryAlias test
      :     +- LocalRelation <empty>, [a#44]
      +- Project [(a#44 + a#44) AS kpi_04#47]
         +- SubqueryAlias test
            +- LocalRelation <empty>, [a#44]

-> 
Project [kpi_04#46]
+- SubqueryAlias t
   +- Union
      :- Project [a#44 AS kpi_04#46]
      :  +- SubqueryAlias test
      :     +- LocalRelation <empty>, [a#44]
      +- Project [(a#44 + a#44) AS kpi_04#47]
         +- SubqueryAlias test
            +- LocalRelation <empty>, [a#44]

After Project resolved. It child Union changes the children by WidenSetOperationTypes.

In the next iteration, Project won't be resolved again.
!Project [kpi_04#46]
+- SubqueryAlias t
   +- Union
      :- Project [cast(kpi_04#46 as decimal(22,1)) AS kpi_04#48]
      :  +- Project [a#44 AS kpi_04#46]
      :     +- SubqueryAlias test
      :        +- LocalRelation <empty>, [a#44]
      +- Project [kpi_04#47]
         +- Project [CheckOverflow((promote_precision(cast(a#44 as decimal(22,1))) + promote_precision(cast(a#44 as decimal(22,1)))), DecimalType(22,1), true) AS kpi_04#47]
            +- SubqueryAlias test
               +- LocalRelation <empty>, [a#44];;;","19/Aug/20 08:55;kkyong;I reproduce it on 2.4.5 .  and find  it was reported before on https://issues.apache.org/jira/browse/SPARK-18622

But developer just fix it  by changing out type  not the suggesting solution .   When WidenSetOperationTypes adding extra new project , it need to  make sure the existing reference still valid.   

the issue is too hard for me to fix ,  really hope  some guys can help me out .  :) 

 

 

 ;;;","19/Aug/20 09:50;maropu;Looks like a minor bug. I'll make a PR later.;;;","20/Aug/20 06:50;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29485;;;","03/Sep/20 14:49;cloud_fan;Issue resolved by pull request 29485
[https://github.com/apache/spark/pull/29485];;;","03/Sep/20 18:05;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29643;;;","03/Sep/20 18:06;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29643;;;","08/Sep/20 10:13;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29680;;;","08/Sep/20 10:14;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29680;;;",,,,,,,,,,,,,,,,
"When pyspark.sql.functions.lit() function is used with dataframe cache, it returns wrong result",SPARK-32635,13323001,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,petertoth,vinodkc,vinodkc,17/Aug/20 06:34,18/Sep/20 17:29,13/Jul/23 08:50,17/Sep/20 23:23,2.1.3,2.2.3,2.3.4,2.4.7,3.0.0,,,,,,,2.4.8,3.0.2,3.1.0,,SQL,,,,0,correctness,,,"When pyspark.sql.functions.lit() function is used with dataframe cache, it returns wrong result

eg:lit() function with cache() function.
 -----------------------------------
{code:java}
from pyspark.sql import Row
from pyspark.sql import functions as F

df_1 = spark.createDataFrame(Row(**x) for x in [{'col1': 'b'}]).withColumn(""col2"", F.lit(str(2)))
df_2 = spark.createDataFrame(Row(**x) for x in [{'col1': 'a', 'col3': 8}]).withColumn(""col2"", F.lit(str(1)))
df_3 = spark.createDataFrame(Row(**x) for x in [{'col1': 'b', 'col3': 9}]).withColumn(""col2"", F.lit(str(2)))
df_23 = df_2.union(df_3)
df_4 = spark.createDataFrame(Row(**x) for x in [{'col3': 9}]).withColumn(""col2"", F.lit(str(2)))

sel_col3 = df_23.select('col3', 'col2')
df_4 = df_4.join(sel_col3, on=['col3', 'col2'], how = ""inner"")
df_23_a = df_23.join(df_1, on=[""col1"", 'col2'], how=""inner"").cache() 
finaldf = df_23_a.join(df_4, on=['col2', 'col3'], how='left').filter(F.col('col3') == 9)
finaldf.show()
finaldf.select('col2').show() #Wrong result
{code}
 

Output
 -----------
{code:java}
>>> finaldf.show()
+----+----+----+
|col2|col3|col1|
+----+----+----+
| 2| 9| b|
+----+----+----+
>>> finaldf.select('col2').show() #Wrong result, instead of 2, got 1
+----+
|col2|
+----+
| 1|
+----+
+----+{code}
 lit() function without cache() function.
{code:java}
from pyspark.sql import Row
from pyspark.sql import functions as F

df_1 = spark.createDataFrame(Row(**x) for x in [{'col1': 'b'}]).withColumn(""col2"", F.lit(str(2)))
df_2 = spark.createDataFrame(Row(**x) for x in [{'col1': 'a', 'col3': 8}]).withColumn(""col2"", F.lit(str(1)))
df_3 = spark.createDataFrame(Row(**x) for x in [{'col1': 'b', 'col3': 9}]).withColumn(""col2"", F.lit(str(2)))
df_23 = df_2.union(df_3)
df_4 = spark.createDataFrame(Row(**x) for x in [{'col3': 9}]).withColumn(""col2"", F.lit(str(2)))

sel_col3 = df_23.select('col3', 'col2')
df_4 = df_4.join(sel_col3, on=['col3', 'col2'], how = ""inner"")
df_23_a = df_23.join(df_1, on=[""col1"", 'col2'], how=""inner"")
finaldf = df_23_a.join(df_4, on=['col2', 'col3'], how='left').filter(F.col('col3') == 9)
finaldf.show() 
finaldf.select('col2').show() #Correct result
{code}
 

Output
{code:java}
----------
>>> finaldf.show()
+----+----+----+
|col2|col3|col1|
+----+----+----+
| 2| 9| b|
+----+----+----+
>>> finaldf.select('col2').show() #Correct result, when df_23_a is not cached 
+----+
|col2|
+----+
| 2|
+----+
{code}
 ",,apachespark,bersprockets,maropu,petertoth,sascha.baumanns,vinodkc,xzrspark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 18 10:01:45 UTC 2020,,,,,,,,,,"0|z0htkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/20 09:44;sascha.baumanns;Please close the brackets in the first code section: final.show();;;","21/Aug/20 10:38;vinodkc;[~sascha.baumanns], updated code section. Thanks;;;","09/Sep/20 09:36;petertoth;I was able to repro the issue and see the root cause. Started working on a fix.;;;","16/Sep/20 11:38;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/29771;;;","17/Sep/20 23:23;maropu;Resolved by https://github.com/apache/spark/pull/29771;;;","18/Sep/20 08:40;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/29802;;;","18/Sep/20 10:01;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/29805;;;",,,,,,,,,,,,,,,,,,,
Log error message when falling back to interpreter mode,SPARK-32625,13322876,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,15/Aug/20 13:49,15/Aug/20 19:32,13/Jul/23 08:50,15/Aug/20 19:32,3.0.0,,,,,,,,,,,2.4.7,3.0.1,3.1.0,,SQL,,,,0,,,,"Current:
{noformat}
21:45:52.787 WARN org.apache.spark.sql.catalyst.expressions.Predicate: Expr codegen error and falling back to interpreter mode
+---+----+
| id|   k|
+---+----+
|  0|0.00|
|  1|1.00|
+---+----+
{noformat}

Excepted:
{noformat}
21:48:44.612 WARN org.apache.spark.sql.catalyst.expressions.Predicate: Expr codegen error and falling back to interpreter mode
java.lang.IllegalArgumentException: Can not interpolate org.apache.spark.sql.types.Decimal into code block.
	at org.apache.spark.sql.catalyst.expressions.codegen.Block$BlockHelper$.$anonfun$code$1(javaCode.scala:240)
	at org.apache.spark.sql.catalyst.expressions.codegen.Block$BlockHelper$.$anonfun$code$1$adapted(javaCode.scala:236)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.expressions.codegen.Block$BlockHelper$.code$extension(javaCode.scala:236)
	at org.apache.spark.sql.execution.MixedFilterSubqueryExec.doGenCode(subquery.scala:284)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:147)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:142)
	at org.apache.spark.sql.catalyst.expressions.DynamicPruningExpression.doGenCode(DynamicPruning.scala:93)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:147)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:142)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:35)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:26)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1263)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:76)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:73)
	at org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:52)
	at org.apache.spark.sql.catalyst.expressions.Predicate$.create(predicates.scala:89)
	at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions$lzycompute(DataSourceScanExec.scala:239)
	at org.apache.spark.sql.execution.FileSourceScanExec.dynamicallySelectedPartitions(DataSourceScanExec.scala:227)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:411)
	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:396)
	at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:491)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)
	at org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:520)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)
	at org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:203)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.inputRDDs(BroadcastHashJoinExec.scala:178)
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:747)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:434)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3680)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2710)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3671)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3669)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2710)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2917)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:824)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:783)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:792)
	at org.apache.spark.sql.DataFrameSuite.$anonfun$new$543(DataFrameSuite.scala:2570)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1403)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:305)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:303)
	at org.apache.spark.sql.DataFrameSuite.withTable(DataFrameSuite.scala:53)
	at org.apache.spark.sql.DataFrameSuite.$anonfun$new$542(DataFrameSuite.scala:2549)
	at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54)
	at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38)
	at org.apache.spark.sql.DataFrameSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(DataFrameSuite.scala:53)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:246)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:244)
	at org.apache.spark.sql.DataFrameSuite.withSQLConf(DataFrameSuite.scala:53)
	at org.apache.spark.sql.DataFrameSuite.$anonfun$new$541(DataFrameSuite.scala:2549)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:164)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:60)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:60)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:60)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:60)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1320)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1314)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1314)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:993)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:971)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1480)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:971)
	at org.scalatest.tools.Runner$.run(Runner.scala:798)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:133)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:27)
+---+----+
| id|   k|
+---+----+
|  0|0.00|
|  1|1.00|
+---+----+
{noformat}


",,apachespark,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 15 19:32:55 UTC 2020,,,,,,,,,,"0|z0hst4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/20 13:58;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29440;;;","15/Aug/20 13:58;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29440;;;","15/Aug/20 19:32;dongjoon;Issue resolved by pull request 29440
[https://github.com/apache/spark/pull/29440];;;",,,,,,,,,,,,,,,,,,,,,,,
Replace getClass.getName with getClass.getCanonicalName in CodegenContext.addReferenceObj,SPARK-32624,13322873,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,15/Aug/20 13:33,31/Aug/20 19:36,13/Jul/23 08:50,19/Aug/20 12:21,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"{code:java}
scala> Array[Byte](1, 2).getClass.getName
res13: String = [B

scala> Array[Byte](1, 2).getClass.getCanonicalName
res14: String = byte[]
{code}

{{[B}} is not a correct java type. We should use {{byte[]}}. Otherwise will hit compile issue:
{noformat}
20:49:54.885 ERROR org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: 
...
/* 029 */     if (!isNull_2) {
/* 030 */       value_1 = org.apache.spark.sql.catalyst.util.TypeUtils.compareBinary(value_2, (([B) references[0] /* min */)) >= 0 && org.apache.spark.sql.catalyst.util.TypeUtils.compareBinary(value_2, (([B) references[1] /* max */)) <= 0 ).mightContainBinary(value_2);
...

20:49:54.886 WARN org.apache.spark.sql.catalyst.expressions.Predicate: Expr codegen error and falling back to interpreter mode
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 30, Column 81: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 30, Column 81: Unexpected token ""["" in primary
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
{noformat}",,apachespark,maropu,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 31 19:36:01 UTC 2020,,,,,,,,,,"0|z0hssg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/20 13:39;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29439;;;","16/Aug/20 17:01;srowen;Lots of your JIRAs are lacking descriptions or no detail beyond some code. Could you please explain what the issue is and your proposed solution if you have one [~yumwang];;;","17/Aug/20 03:43;yumwang;Thank you [~srowen] I have updated the description.;;;","19/Aug/20 12:21;yumwang;Issue resolved by pull request 29439
[https://github.com/apache/spark/pull/29439];;;","31/Aug/20 19:36;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/29602;;;",,,,,,,,,,,,,,,,,,,,,
"""path"" option is added again to input paths during infer()",SPARK-32621,13322839,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,imback82,imback82,imback82,15/Aug/20 01:27,20/Aug/20 17:19,13/Jul/23 08:50,19/Aug/20 16:23,2.4.6,3.0.0,3.0.1,3.1.0,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"When ""path"" option is used when creating a DataFrame, it can cause issues during infer.
{code:java}
class TestFileFilter extends PathFilter {
  override def accept(path: Path): Boolean = path.getParent.getName != ""p=2""
}

val path = ""/tmp""
val df = spark.range(2)
df.write.json(path + ""/p=1"")
df.write.json(path + ""/p=2"")

val extraOptions = Map(
  ""mapred.input.pathFilter.class"" -> classOf[TestFileFilter].getName,
  ""mapreduce.input.pathFilter.class"" -> classOf[TestFileFilter].getName
)

// This works fine.
assert(spark.read.options(extraOptions).json(path).count == 2)

// The following with ""path"" option fails with the following:
// assertion failed: Conflicting directory structures detected. Suspicious paths
//	file:/tmp
//	file:/tmp/p=1
assert(spark.read.options(extraOptions).format(""json"").option(""path"", path).load.count() === 2)
{code}",,apachespark,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 19 17:39:16 UTC 2020,,,,,,,,,,"0|z0hskw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/20 01:56;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/29437;;;","19/Aug/20 16:23;cloud_fan;Issue resolved by pull request 29437
[https://github.com/apache/spark/pull/29437];;;","19/Aug/20 17:39;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/29478;;;",,,,,,,,,,,,,,,,,,,,,,,
Reset the numPartitions metric when DPP is enabled,SPARK-32620,13322835,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,15/Aug/20 00:20,09/Sep/20 02:07,13/Jul/23 08:50,26/Aug/20 01:47,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"This pr reset the {{numPartitions}} metric when DPP is enabled. Otherwise, it is always a [static value|https://github.com/apache/spark/blob/18cac6a9f0bf4a6d449393f1ee84004623b3c893/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L215].",,apachespark,maropu,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32788,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 26 01:47:08 UTC 2020,,,,,,,,,,"0|z0hsk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/20 00:54;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/29436;;;","15/Aug/20 07:45;maropu;Please fill the description.;;;","26/Aug/20 01:47;yumwang;Issue resolved by pull request 29436
[https://github.com/apache/spark/pull/29436];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix AQE aggregateMetrics java.util.NoSuchElementException,SPARK-32615,13322677,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,leanken,leanken,leanken,14/Aug/20 06:53,17/Aug/20 14:28,13/Jul/23 08:50,17/Aug/20 14:28,3.0.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"{code:java}
// Reproduce Step
sql/test-only org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite -- -z ""SPARK-32573: Eliminate NAAJ when BuildSide is EmptyHashedRelationWithAllNullKeys""
{code}
{code:java}
// Error Message
14:40:44.089 ERROR org.apache.spark.util.Utils: Uncaught exception in thread element-tracking-store-worker
14:40:44.089 ERROR org.apache.spark.util.Utils: Uncaught exception in thread element-tracking-store-worker java.util.NoSuchElementException: key not found: 12 
at scala.collection.immutable.Map$Map1.apply(Map.scala:114) 
at org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$aggregateMetrics$11(SQLAppStatusListener.scala:257) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238) at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149) at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237) at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44) at scala.collection.mutable.HashMap.foreach(HashMap.scala:149) at scala.collection.TraversableLike.map(TraversableLike.scala:238) at scala.collection.TraversableLike.map$(TraversableLike.scala:231) at scala.collection.AbstractTraversable.map(Traversable.scala:108) at org.apache.spark.sql.execution.ui.SQLAppStatusListener.aggregateMetrics(SQLAppStatusListener.scala:256) at org.apache.spark.sql.execution.ui.SQLAppStatusListener.$anonfun$onExecutionEnd$2(SQLAppStatusListener.scala:365) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.util.Utils$.tryLog(Utils.scala:1971) at org.apache.spark.status.ElementTrackingStore$$anon$1.run(ElementTrackingStore.scala:117) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)[info] - SPARK-32573: Eliminate NAAJ when BuildSide is EmptyHashedRelationWithAllNullKeys (2 seconds, 14 milliseconds)
{code}
This issue is mainly because during AQE, while sub-plan changed, the metrics update is overwrite. for example, in this UT, change from BroadcastHashJoinExec into a LocalTableScanExec, and in the onExecutionEnd action it will try aggregate all metrics including old ones during the execution, which will cause NoSuchElementException, since the metricsType is already updated with plan rewritten. So we need to filter out those outdated metrics.",,apachespark,cloud_fan,leanken,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 17 14:28:34 UTC 2020,,,,,,,,,,"0|z0hrkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/20 07:13;apachespark;User 'leanken' has created a pull request for this issue:
https://github.com/apache/spark/pull/29431;;;","17/Aug/20 14:28;cloud_fan;Issue resolved by pull request 29431
[https://github.com/apache/spark/pull/29431];;;",,,,,,,,,,,,,,,,,,,,,,,,
"Support for treating the line as valid record if it starts with \u0000 or null character, or starts with any character mentioned as comment",SPARK-32614,13322662,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,Chanduhawk,Chanduhawk,14/Aug/20 04:42,12/Dec/22 18:11,13/Jul/23 08:50,25/Aug/20 15:29,2.2.3,2.4.5,3.0.0,,,,,,,,,3.0.1,3.1.0,,,Spark Core,SQL,,,0,correctness,,,"In most of the data ware housing scenarios files does not have comment records and every line needs to be treated as a valid record even though it starts with default comment character as \u0000 or null character.Though user can set any comment character other than \u0000, but there is a chance the actual record can start with those characters.

Currently for the below piece of code and the given testdata where first row starts with null \u0000
character it will throw the below error.

*eg: val df = spark.read.option(""delimiter"","","").csv(""file:/E:/Data/Testdata.dat"");
      df.show(false);*

*+TestData+*
 
 !screenshot-1.png! 

Internal state when error was thrown: line=1, column=0, record=0, charIndex=7
	at com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:339)
	at com.univocity.parsers.common.AbstractParser.parseLine(AbstractParser.java:552)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:160)
	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:148)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:62)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)

*Note:*

Though its the limitation of the univocity parser and the workaround is to provide any other comment character by mentioning .option(""comment"",""#""), but if my actual data starts with this character then the particular row will be discarded.

Currently I pushed the code in univocity parser to handle this scenario as part of the below PR
https://github.com/uniVocity/univocity-parsers/pull/412

please accept the jira so that we can enable this feature in spark-csv by adding a parameter in spark csvoptions.
 ",,apachespark,Chanduhawk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/20 04:56;Chanduhawk;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13009827/screenshot-1.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 15:29:24 UTC 2020,,,,,,,,,,"0|z0hrhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/20 17:03;srowen;I dont' really understand this. Are you just saying you need to treat the null byte as a comment character? so set that as the comment char? what is the Spark issue here.;;;","16/Aug/20 17:35;Chanduhawk;[~srowen]

*currently  spark cannt process the row that starts with null character.*
If one of the rows the data file(CSV) starts with null or \u0000 character like below(PFA screenshot)
like below
*null*,abc,test

then spark will throw the error as mentioned in the description. i.e spark cannt process any row that starts with null character. It can only process the row if we set the options like below

option(""comment"",""a character"")

*comment* - it will take a character that needs to be treated as comment character so spark wont process the row that starts with this character

The above is a work around to process the row that starts with null. But this process is also flawed and subject to skip a valid row of data that may start with the comment character.
In data ware house most of the time we dont have comment charaters concept and all the rows needs to be processed.

So there should be an option in spark which will disable the processing of comment characters like below

option(""enableProcessingComments"", false)

this option will disable checking for any comment character processing and can also process the rows that starts with the null or \u0000 character



;;;","17/Aug/20 15:29;srowen;I see, you're not saying that nulls are comment characters in your data.
Are you just saying we need to update univocity?;;;","17/Aug/20 16:23;Chanduhawk;[~srowen]

yes the row that starts with null character is not a comment character in actual data warehouse projects. We have to process all rows irrespective of any startign chars.

Univocity parser i already raised PR and its merged successfully which will enable the option whether to process comment characters or not.
https://github.com/uniVocity/univocity-parsers/pull/412

in spark we need to add the respective option in CSVoptions class and csvutils class

*proposed changes in spark*

val df = spark.read.option(""delimiter"","","").option(""*processComments*"",""false"").csv(""file:/E:/Data/Testdata.dat"");

this *processComments *option when set to false spark should not check for any comment characters and will process all rows even if it started with null or any other comment character.

Please let me know if I can raise a PR on this, once this enhancement is accepted


;;;","17/Aug/20 17:44;srowen;Why a new option then? how about you don't process comments if the comment char is not set?
I'm still not quite clear on why this depends on processing comments or not?;;;","17/Aug/20 17:50;Chanduhawk;[~srowen]

In spark if the comment option is not set then it will take \u0000 as default comment character. So in our case the data file, few rows starts with \u0000 character, so it will ignore those rows.

so currently in spark csv options there is no option present to disable processing the comment characters at all so that it will treat all rows as valid rows. 

You can recreate this issue by using the data file screen shot that i attached in the jira.

please try to process the csv file and keep any row that starts with \u0000;;;","22/Aug/20 21:29;srowen;Sort of; Spark uses '\u0000' to mean ""no comment processing"", but then does not apply that consistently. In your case I think it sets this as the comment char to Univocity even when ""not set"", and defaulting to ""\u0000"". I think that's the issue here because the univocity will try to ignore lines starting that way. 
What I don't think we want to do is add a new flag for this, just deal with no setting of comment differently.
I wonder why the univocity change doesn't work that way?;;;","22/Aug/20 22:09;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/29516;;;","22/Aug/20 22:10;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/29516;;;","23/Aug/20 05:11;Chanduhawk;[~srowen]

Univocity parser always takes default comment character as #. It seems spark updates the comment settings to \u0000 character. Please see the https://github.com/uniVocity/univocity-parsers/pull/412 that raised which involves adding one new option which enable/disable the comment processing. Currently as per the PR still I think there should be an option which enable or disable the comment processing in spark CVS so that the parameter Boolean value can be passed to univocity parser

As per PR If we will change that way then it might impact existing users for which \u0000 is a comment character by default. So I would say a separate optional config is a better solution. What I am saying here is that we need to wait for univocity 3.0.0 to be available where the new changes will be available then we can add spark changes in a proper manner.;;;","25/Aug/20 15:29;gurwls223;Issue resolved by pull request 29516
[https://github.com/apache/spark/pull/29516];;;",,,,,,,,,,,,,,,
Incorrect exchange reuse with DataSourceV2,SPARK-32609,13322579,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mingjial,mingjial,mingjial,13/Aug/20 17:18,17/Aug/20 16:40,13/Jul/23 08:50,17/Aug/20 15:24,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,2.4.6,,,,,2.4.7,,,,SQL,,,,0,correctness,,," 
{code:java}
spark.conf.set(""spark.sql.exchange.reuse"",""true"")
spark.read.format(""com.google.cloud.spark.bigquery.v2.BigQueryDataSourceV2"").option(""table"",'tpcds_1G.date_dim').load()
df.createOrReplaceTempView(table)
    
df = spark.sql("""""" 
WITH t1 AS (
    SELECT 
        d_year, d_month_seq
    FROM (
        SELECT t1.d_year , t2.d_month_seq          
        FROM 
        date_dim t1
        cross join
        date_dim t2
        where t1.d_day_name = ""Monday"" and t1.d_fy_year > 2000
        and t2.d_day_name = ""Monday"" and t2.d_fy_year > 2000
        )
    GROUP BY d_year, d_month_seq)
   
 SELECT
    prev_yr.d_year AS prev_year, curr_yr.d_year AS year, curr_yr.d_month_seq
 FROM t1 curr_yr cross join t1 prev_yr
 WHERE curr_yr.d_year=2002 AND prev_yr.d_year=2002-1
 ORDER BY d_month_seq
 LIMIT 100
 
 """""")

df.explain()
df.show(){code}
 

the repro query :
A. defines a temp table t1  
B. cross join t1 (year 2002)  and  t2 (year 2001)

With reuse exchange enabled, the plan incorrectly ""decides"" to re-use persisted shuffle writes of A filtering on year 2002 , for year 2001.
{code:java}
== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[d_month_seq#24371L ASC NULLS FIRST], output=[prev_year#24366L,year#24367L,d_month_seq#24371L])
+- *(9) Project [d_year#24402L AS prev_year#24366L, d_year#23551L AS year#24367L, d_month_seq#24371L]
   +- CartesianProduct
      :- *(4) HashAggregate(keys=[d_year#23551L, d_month_seq#24371L], functions=[])
      :  +- Exchange hashpartitioning(d_year#23551L, d_month_seq#24371L, 200)
      :     +- *(3) HashAggregate(keys=[d_year#23551L, d_month_seq#24371L], functions=[])
      :        +- BroadcastNestedLoopJoin BuildRight, Cross
      :           :- *(1) Project [d_year#23551L]
      :           :  +- *(1) ScanV2 BigQueryDataSourceV2[d_year#23551L] (Filters: [isnotnull(d_day_name#23559), (d_day_name#23559 = Monday), isnotnull(d_fy_year#23556L), (d_fy_yea..., Options: [table=tpcds_1G.date_dim,paths=[]])
      :           +- BroadcastExchange IdentityBroadcastMode
      :              +- *(2) Project [d_month_seq#24371L]
      :                 +- *(2) ScanV2 BigQueryDataSourceV2[d_month_seq#24371L] (Filters: [isnotnull(d_day_name#24382), (d_day_name#24382 = Monday), isnotnull(d_fy_year#24379L), (d_fy_yea..., Options: [table=tpcds_1G.date_dim,paths=[]])
      +- *(8) HashAggregate(keys=[d_year#24402L, d_month_seq#24455L], functions=[])
         +- ReusedExchange [d_year#24402L, d_month_seq#24455L], Exchange hashpartitioning(d_year#23551L, d_month_seq#24371L, 200){code}
 

 

And the result is obviously incorrect because prev_year should be 2001
{code:java}
+---------+----+-----------+
|prev_year|year|d_month_seq|
+---------+----+-----------+
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
|     2002|2002|       1212|
+---------+----+-----------+
only showing top 20 rows
{code}
 

 ",,apachespark,cloud_fan,emkornfield@gmail.com,kjung520,mingjial,rohitmishr1484,viirya,xkrogen,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,Mon Aug 17 15:24:02 UTC 2020,,,,,,,,,,"0|z0hqz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/20 17:29;mingjial;Mitigation:

Turn off  spark.sql.exchange.reuse. eg: spark.conf.set(""spark.sql.exchange.reuse"", ""false"")

 

Root cause: 

bug at [https://github.com/apache/spark/blob/e5bef51826dc2ff4020879e35ae7eb9019aa7fcd/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala#L48]

 

Fix:

Add pushedfilters comparison in equals function. verified that applying the fix brings right plan and result.

 ;;;","13/Aug/20 18:04;mingjial;I am currently working on a fix &  unit test;;;","13/Aug/20 18:26;rohitmishr1484;[~mingjial], Please refrain from adding Priority as ""Critical"". These are reserved for committers. Changing it to ""Major"".

Also please don't populate Target and Fix version field. These are also set by committers.;;;","14/Aug/20 06:13;apachespark;User 'mingjialiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29430;;;","14/Aug/20 19:40;apachespark;User 'mingjialiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29435;;;","14/Aug/20 19:41;apachespark;User 'mingjialiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29435;;;","17/Aug/20 15:24;cloud_fan;Issue resolved by pull request 29430
[https://github.com/apache/spark/pull/29430];;;",,,,,,,,,,,,,,,,,,,
Not able to see driver logs in spark history server in standalone mode,SPARK-32598,13322196,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,KevinSmile,sriramgr,sriramgr,12/Aug/20 06:19,15/Jan/21 15:03,13/Jul/23 08:50,15/Jan/21 15:03,2.3.0,2.4.4,,,,,,,,,,3.0.2,3.1.1,,,Spark Core,,,,1,,,,"Driver logs are not coming in history server in spark standalone mode. Checked in the spark events logs it is not there. Is this by design or can I fix it by creating a patch?. Not able to see any proper documentation regarding this.

 

!image-2020-08-12-11-50-01-899.png!",,apachespark,cltlfcjin,KevinSmile,leighklotz,sriramgr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/20 06:20;sriramgr;image-2020-08-12-11-50-01-899.png;https://issues.apache.org/jira/secure/attachment/13009524/image-2020-08-12-11-50-01-899.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 15 15:03:05 UTC 2021,,,,,,,,,,"0|z0homw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/20 09:09;cltlfcjin;Does this problem exist in Spark3.0? I think branch-2.3 is not maintained.;;;","12/Aug/20 09:27;sriramgr;I haven't run and checked the spark 3.0. But by the looks of the implementation, it is the same as 2.3.0
https://github.com/apache/spark/blob/a418548dad57775fbb10b4ea690610bad1a8bfb0/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala#L97;;;","12/Aug/20 10:47;sriramgr;Even spark 2.4 has the same issue.;;;","14/Aug/20 02:08;cltlfcjin;[~sriramgr] PullRequest is welcome. Please commit on the master branch. Thanks.;;;","04/Sep/20 03:06;apachespark;User 'KevinSmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/29644;;;","04/Sep/20 03:07;KevinSmile;Facing this bug too, I'm trying to fix it at [https://github.com/apache/spark/pull/29644]

[~sriramgr]

 ;;;","15/Jan/21 15:03;srowen;Resolved by https://github.com/apache/spark/pull/29644;;;",,,,,,,,,,,,,,,,,,,
Clear Ivy resolution files as part of the finally block,SPARK-32596,13322149,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vsowrirajan,vsowrirajan,vsowrirajan,11/Aug/20 23:19,12/Dec/22 18:10,13/Jul/23 08:50,12/Aug/20 06:11,3.0.0,,,,,,,,,,,3.1.0,,,,Spark Core,,,,0,,,,Clear ivy resolution files as part of the finally block in SparkSubmit without which failures while resolving packages can leave resolution files around.,,apachespark,vsowrirajan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 12 06:12:52 UTC 2020,,,,,,,,,,"0|z0hocg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/20 23:26;apachespark;User 'venkata91' has created a pull request for this issue:
https://github.com/apache/spark/pull/29411;;;","11/Aug/20 23:27;apachespark;User 'venkata91' has created a pull request for this issue:
https://github.com/apache/spark/pull/29411;;;","12/Aug/20 06:11;gurwls223;Fixed in https://github.com/apache/spark/pull/29411;;;","12/Aug/20 06:12;gurwls223;[~mridulm80], I resolved the JIRA manually. FYI;;;",,,,,,,,,,,,,,,,,,,,,,
Insert wrong dates to Hive tables,SPARK-32594,13322111,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,11/Aug/20 20:18,12/Dec/22 18:10,13/Jul/23 08:50,12/Aug/20 04:33,3.0.1,3.1.0,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"Run *spark-sql -S*:
{code:sql}
spark-sql> CREATE TABLE table1 (d date);
20/08/11 23:17:20 WARN HiveMetaStore: Location: file:/Users/maximgekk/proj/insert-date-into-hive-table/spark-warehouse/table1 specified for non-external table:table1
spark-sql> INSERT INTO table1 VALUES (date '2020-08-11');
spark-sql> SELECT * FROM table1;
1970-01-01
{code}
The expected result of the last statement is *2020-08-11* but got *1970-01-01*.",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 23 16:07:20 UTC 2020,,,,,,,,,,"0|z0ho40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/20 20:19;maxgekk;I am working on a fix;;;","11/Aug/20 20:37;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29409;;;","12/Aug/20 04:33;gurwls223;Issue resolved by pull request 29409
[https://github.com/apache/spark/pull/29409];;;","23/Aug/20 16:06;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29523;;;","23/Aug/20 16:07;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29523;;;",,,,,,,,,,,,,,,,,,,,,
SizeEstimator class not always properly initialized in tests.,SPARK-32588,13321998,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mundaym,mundaym,mundaym,11/Aug/20 11:59,24/Aug/20 15:20,13/Jul/23 08:50,24/Aug/20 15:20,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,Tests,,,,0,OpenJ9,,,"The SizeEstimator class reads some system properties when it is initialized. Some tests override these system properties in order to force the SizeEstimator class to produce the same results on all platforms. However there are multiple places where the SizeEstimator class is either not re-initialized after system properties have been changed or system properties are not correctly set.

This causes test failures when using OpenJ9 compiled in large heap mode. This build of OpenJ9 does not use compressed references which affects the settings that SizeEstimator is originally initialized with.

I have a PR that fixes this and will submit it shortly.","openjdk version ""1.8.0_265""
OpenJDK Runtime Environment (build 1.8.0_265-b01)
Eclipse OpenJ9 VM (build openj9-0.21.0, JRE 1.8.0 Linux s390x-64-Bit 20200728_255 (JIT enabled, AOT enabled)
OpenJ9 - 34cf4c075
OMR - 113e54219
JCL - c82ff0c20f based on jdk8u265-b01)",apachespark,mundaym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 24 15:20:05 UTC 2020,,,,,,,,,,"0|z0hnew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/20 12:08;apachespark;User 'mundaym' has created a pull request for this issue:
https://github.com/apache/spark/pull/29407;;;","24/Aug/20 15:20;srowen;Issue resolved by pull request 29407
[https://github.com/apache/spark/pull/29407];;;",,,,,,,,,,,,,,,,,,,,,,,,
Issue accessing a column values after 'explode' function,SPARK-32580,13321773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,airatsa,airatsa,10/Aug/20 11:13,12/Dec/22 18:10,13/Jul/23 08:50,11/Aug/20 12:06,3.0.0,,,,,,,,,,,3.0.1,,,,SQL,,,,0,,,,"An exception occurs when trying to flatten double nested arrays

The schema is
{code:none}
root
 |-- data: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- item_id: string (nullable = true)
 |    |    |-- timestamp: string (nullable = true)
 |    |    |-- values: array (nullable = true)
 |    |    |    |-- element: struct (containsNull = true)
 |    |    |    |    |-- sample: double (nullable = true)
{code}
The target schema is
{code:none}
root
 |-- item_id: string (nullable = true)
 |-- timestamp: string (nullable = true)
 |-- sample: double (nullable = true)
{code}
The code (in Java)
{code:java}
package com.skf.streamer.spark;

import java.util.concurrent.TimeoutException;

import org.apache.spark.SparkConf;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

public class ExplodeTest {

   public static void main(String[] args) throws TimeoutException {

      SparkConf conf = new SparkConf()
         .setAppName(""SimpleApp"")
         .set(""spark.scheduler.mode"", ""FAIR"")
         .set(""spark.master"", ""local[1]"")
         .set(""spark.sql.streaming.checkpointLocation"", ""checkpoint"");

      SparkSession spark = SparkSession.builder()
         .config(conf)
         .getOrCreate();

      Dataset<Row> d0 = spark
         .read()
         .format(""json"")
         .option(""multiLine"", ""true"")
         .schema(getSchema())
         .load(""src/test/resources/explode/data.json"");

      d0.printSchema();

      d0 = d0.withColumn(""item"", functions.explode(d0.col(""data"")));
      d0 = d0.withColumn(""value"", functions.explode(d0.col(""item.values"")));
      d0.printSchema();
      d0 = d0.select(
         d0.col(""item.item_id""),
         d0.col(""item.timestamp""),
         d0.col(""value.sample"")
      );

      d0.printSchema();

      d0.show(); // Failes

      spark.stop();
   }

   private static StructType getSchema() {
      StructField[] level2Fields = {
         DataTypes.createStructField(""sample"", DataTypes.DoubleType, false),
      };

      StructField[] level1Fields = {
         DataTypes.createStructField(""item_id"", DataTypes.StringType, false),
         DataTypes.createStructField(""timestamp"", DataTypes.StringType, false),
         DataTypes.createStructField(""values"", DataTypes.createArrayType(DataTypes.createStructType(level2Fields)), false)
      };

      StructField[] fields = {
         DataTypes.createStructField(""data"", DataTypes.createArrayType(DataTypes.createStructType(level1Fields)), false)
      };

      return DataTypes.createStructType(fields);
   }
}
{code}
The data file
{code:json}
{
  ""data"": [
    {
      ""item_id"": ""item_1"",
      ""timestamp"": ""2020-07-01 12:34:89"",
      ""values"": [
        {
          ""sample"": 1.1
        },
        {
          ""sample"": 1.2
        }
      ]
    },
    {
      ""item_id"": ""item_2"",
      ""timestamp"": ""2020-07-02 12:34:89"",
      ""values"": [
        {
          ""sample"": 2.2
        }
      ]
    }
  ]
}
{code}
Dataset.show() method fails with an exception
{code:none}
Caused by: java.lang.RuntimeException: Couldn't find _gen_alias_30#30 in [_gen_alias_28#28,_gen_alias_29#29]
	at scala.sys.package$.error(package.scala:30)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 37 more
{code}",,airatsa,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/20 13:31;airatsa;ExplodeTest.java;https://issues.apache.org/jira/secure/attachment/13009392/ExplodeTest.java","10/Aug/20 13:31;airatsa;data.json;https://issues.apache.org/jira/secure/attachment/13009393/data.json",,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 11 16:15:29 UTC 2020,,,,,,,,,,"0|z0hm1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/20 12:54;maropu;Please do not use ""Blocker"" in the priority and it is reserved for committers. Anyway, thanks for the report.;;;","10/Aug/20 14:38;maropu;I made the given query simpler;
{code}
scala> import org.apache.spark.sql.functions
scala> val df1 = spark.range(1).selectExpr(""array(named_struct('item_id', 1, 'values', array(named_struct('sample', 0.1)))) data"")
scala> val df2 = df1.withColumn(""item"", functions.explode(df1.col(""data"")))
scala> val df3 = df2.withColumn(""value"", functions.explode(df2.col(""item.values"")))
scala> val df4 = df3.select(df3.col(""item.item_id""), df3.col(""value.sample""))
scala> df4.show()
20/08/10 22:53:26 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 1]
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: _gen_alias_26#26
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
	at org.apache.spark.
{code}

Looks like this issue only happens in v3.0.0. It passed in master/v2.4.6.;;;","11/Aug/20 04:46;airatsa;If you select values from one of the arrays, it works

{code:scala}
scala> val df5 = df3.select(df3.col(""value.sample""))
df5: org.apache.spark.sql.DataFrame = [sample: decimal(1,1)]

scala> df5.show()
+------+
|sample|
+------+
|   0.1|
+------+


scala> val df6 = df3.select(df3.col(""item.item_id""))
df6: org.apache.spark.sql.DataFrame = [item_id: int]

scala> df6.show()
+-------+
|item_id|
+-------+
|      1|
+-------+

{code};;;","11/Aug/20 06:28;gurwls223;[~airatsa] can you try to turn off {{spark.sql.optimizer.expression.nestedPruning.enabled}} and see if it works?;;;","11/Aug/20 07:02;gurwls223;Seems it was accidentally fixed somewhere in generator column pruning cc [~viirya] FYI;;;","11/Aug/20 12:06;maropu;Yea, in branch-3.0, it seems this issue already has been fixed. I looked around related commits though, I couldn't find which commit fix it. Anyway, I will make this resolved. If anyone knows about the commit that fixes the issue, please update it.;;;","11/Aug/20 13:03;airatsa;When it will be published?

The error is present in the 3.0.0 distribution.

 ;;;","11/Aug/20 13:16;maropu;We are planning to release v3.0.1 soon. Please see: http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Apache-Spark-3-0-1-Release-td29969.html;;;","11/Aug/20 16:15;viirya;Yeah, this looks like related to nested column pruning. You can disable ""spark.sql.optimizer.nestedSchemaPruning.enabled and spark.sql.optimizer.expression.nestedPruning.enabled as workaround.;;;",,,,,,,,,,,,,,,,,
Support PostgreSQL `bpchar` type and array of char type,SPARK-32576,13321715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kujon,kujon,dongjoon,10/Aug/20 01:50,10/Aug/20 07:42,13/Jul/23 08:50,10/Aug/20 02:04,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"Attempting to read the following table:
{code:java}
CREATE TABLE test_table (
  test_column char(64)[]
)
{code}
results in the following exception:
{code:java}
java.sql.SQLException: Unsupported type ARRAY
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getCatalystType(JdbcUtils.scala:256)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getSchema$1(JdbcUtils.scala:321)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:321)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:63)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)
{code}
non-array and varchar equivalents are fine.

 

I've tracked it down to an internal function of the postgres dialect, that accounts for the special 1-byte char, but doesn't deal with different length ones, which postgres represents as bpchar: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala#L60-L61].

Relevant driver code can be found here: [https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/jdbc/TypeInfoCache.java#L85-L87]

 

I'll submit a fix shortly",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,SPARK-32393,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 10 07:42:15 UTC 2020,,,,,,,,,,"0|z0hloo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/20 01:52;apachespark;User 'kujon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29192;;;","10/Aug/20 02:04;dongjoon;Issue resolved by pull request 29192
[https://github.com/apache/spark/pull/29192];;;","10/Aug/20 07:41;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29397;;;","10/Aug/20 07:42;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29397;;;",,,,,,,,,,,,,,,,,,,,,,
BlockManagerDecommissionIntegrationSuite is still flaky,SPARK-32575,13321714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dagrawal3409,dagrawal3409,dagrawal3409,10/Aug/20 01:05,10/Aug/20 03:01,13/Jul/23 08:50,10/Aug/20 03:01,3.1.0,,,,,,,,,,,3.1.0,,,,Spark Core,,,,0,,,,"As reported by [~gurwls223], BlockManagerDecommissionIntegrationSuite test is apparently still flaky (even after https://github.com/apache/spark/pull/29226): https://github.com/apache/spark/pull/29226#issuecomment-670286829.

The new flakyness is because the executors are not launching in the 6 seconds time out I had given them when run under github checks.",,apachespark,dagrawal3409,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 10 03:01:10 UTC 2020,,,,,,,,,,"0|z0hlog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/20 01:08;apachespark;User 'agrawaldevesh' has created a pull request for this issue:
https://github.com/apache/spark/pull/29388;;;","10/Aug/20 01:09;apachespark;User 'agrawaldevesh' has created a pull request for this issue:
https://github.com/apache/spark/pull/29388;;;","10/Aug/20 03:01;dongjoon;Issue resolved by pull request 29388
[https://github.com/apache/spark/pull/29388];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix the trim logic in UTF8String.toInt/toLong did't handle Chinese characters correctly,SPARK-32559,13321267,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,EdisonWang,EdisonWang,EdisonWang,06/Aug/20 09:02,12/Jul/23 09:47,13/Jul/23 08:50,09/Aug/20 19:13,3.0.0,,,,,,,,,,,3.0.1,,,,SQL,,,,0,correctness,,,"The trim logic in Cast expression introduced in [https://github.com/apache/spark/pull/26622] will trim chinese characters unexpectly.

For example,  sql  select cast(""1中文"" as float) gives 1 instead of null

 ",,apachespark,EdisonWang,kwafor,snoot,,,,,,,,,,,,,,,,,,,,,,,SPARK-28023,,,SPARK-44383,,,,,,"09/Jun/23 08:42;kwafor;error.log;https://issues.apache.org/jira/secure/attachment/13058886/error.log",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 12 03:41:22 UTC 2023,,,,,,,,,,"0|z0hixc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/20 09:07;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29375;;;","09/Aug/20 02:00;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29393;;;","09/Jun/23 08:40;kwafor;The expressions `getByte(s) <= ' '` and `Character.isWhitespace(getByte(s))` are different, which causes the `trimAll()` function to fail to remove characters other than spaces, deviating from the original intention of the `trimAll` function (to be consistent with Java's String trim).

This will cause the `date()` function to fail, for example: executing `select date(""today\u0003"");` would result in an error instead of returning null.

```java
/**
* Trims whitespaces (\{@literal <=} ASCII 32) from both ends of this string.
*
* Note that, this method is the same as java's \{@link String#trim}, and different from
* \{@link UTF8String#trim()} which remove only spaces(= ASCII 32) from both ends.
*
* @return A UTF8String whose value is this UTF8String, with any leading and trailing white
* space removed, or this UTF8String if it has no leading or trailing whitespace.
*
*/
public UTF8String trimAll() {
int s = 0;
// skip all of the whitespaces (<=0x20) in the left side
while (s < this.numBytes && Character.isWhitespace(getByte(s))) s++;
if (s == this.numBytes) {
// Everything trimmed
return EMPTY_UTF8;
}
// skip all of the whitespaces (<=0x20) in the right side
int e = this.numBytes - 1;
while (e > s && Character.isWhitespace(getByte(e))) e--;
if (s == 0 && e == numBytes - 1) {
// Nothing trimmed
return this;
}
return copyUTF8String(s, e);
}
```;;;","09/Jun/23 08:42;kwafor;This is the detailed error log.

[^error.log];;;","12/Jun/23 03:41;snoot;User 'Kwafoor' has created a pull request for this issue:
https://github.com/apache/spark/pull/41535;;;",,,,,,,,,,,,,,,,,,,,,
Fix release script to uri encode the user provided passwords.,SPARK-32556,13321233,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,prashant,prashant,06/Aug/20 05:33,13/Aug/20 10:18,13/Jul/23 08:50,07/Aug/20 08:35,2.4.6,3.0.0,3.1.0,,,,,,,,,2.4.7,3.0.1,3.1.0,,Build,Project Infra,,,0,,,,"As I was trying to do the release using the docker
{code:java}
 dev/create-release/do-release-docker.sh{code}
script, there were some failures.

 
 # If the release manager password contains a char, that is not allowed in URL, then it fails the build at the clone spark step.
 # If the .gitignore file is missing, it fails the build at rm .gitignore step.",,apachespark,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 07 08:35:14 UTC 2020,,,,,,,,,,"0|z0hips:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/20 06:19;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/29371;;;","06/Aug/20 06:47;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/29373;;;","07/Aug/20 08:35;prashant;Issue resolved by pull request 29373
[https://github.com/apache/spark/pull/29373];;;",,,,,,,,,,,,,,,,,,,,,,,
Ambiguous self join error in non self join with window,SPARK-32551,13321192,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kdhuria,kdhuria,05/Aug/20 23:35,12/Dec/22 18:10,13/Jul/23 08:50,06/Aug/20 18:04,3.0.0,,,,,,,,,,,3.0.1,,,,Spark Core,,,,0,,,,"Following code fails ambiguous self join analysis, even when it doesn't have self join 

val v1 = spark.range(3).toDF(""m"")
 val v2 = spark.range(3).toDF(""d"")
 val v3 = v1.join(v2, v1(""m"").===(v2(""d"")))
 val v4 = v3(""d"");
 val w1 = Window.partitionBy(v4)
 val out = v3.select(v4.as(""a""), sum(v4).over(w1).as(""b""))

org.apache.spark.sql.AnalysisException: Column a#45L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(""a"").join(df.as(""b""), $""a.id"" > $""b.id"")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.;

 ",,cloud_fan,kdhuria,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31956,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 06 18:04:37 UTC 2020,,,,,,,,,,"0|z0hih4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/20 02:50;gurwls223;{{spark.sql.analyzer.failAmbiguousSelfJoin}} isn't a completely correct check but roughly detects. You can turn off that configuration for now as guided. cc [~cloud_fan] FYI;;;","06/Aug/20 14:21;cloud_fan;Can you try the latest 3.0 branch? There are some bug fixes for this self-join check.;;;","06/Aug/20 18:03;kdhuria;Thanks [~cloud_fan], it is fixed in latest 3.0 branch. 

Fixed as part of https://issues.apache.org/jira/browse/SPARK-31956.;;;","06/Aug/20 18:04;kdhuria;Closing as duplicate.;;;",,,,,,,,,,,,,,,,,,,,,,
Add Application attemptId support to SQL Rest API,SPARK-32548,13321150,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,erenavsarogullari,erenavsarogullari,erenavsarogullari,05/Aug/20 19:22,06/Sep/20 11:31,13/Jul/23 08:50,06/Sep/20 11:31,3.1.0,,,,,,,,,,,,,,,SQL,,,,0,,,,"Currently, Spark Public Rest APIs support Application attemptId except SQL API. This causes *no such app: application_X* issue when the application has *{{attemptId}}*.

Please find existing and supported Rest endpoints with {{attemptId}}.
{code:java}
// Existing Rest Endpoints
applications/{appId}/sql
applications/{appId}/sql/{executionId}

// Rest Endpoints required support
applications/{appId}/{attemptId}/sql 
applications/{appId}/{attemptId}/sql/{executionId}{code}",,apachespark,erenavsarogullari,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 06 11:31:40 UTC 2020,,,,,,,,,,"0|z0hi88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/20 21:33;apachespark;User 'erenavsarogullari' has created a pull request for this issue:
https://github.com/apache/spark/pull/29364;;;","06/Sep/20 11:31;Gengliang.Wang;The issue is resolved in https://github.com/apache/spark/pull/29364;;;",,,,,,,,,,,,,,,,,,,,,,,,
SHOW VIEWS fails with MetaException ... ClassNotFoundException,SPARK-32546,13321129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,05/Aug/20 16:16,06/Aug/20 13:32,13/Jul/23 08:50,06/Aug/20 08:37,3.1.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"SHOW VIEWS can fail with the error:
{code}
java.lang.RuntimeException: MetaException(message:java.lang.ClassNotFoundException Class com.ibm.spss.hive.serde2.xml.XmlSerDe not found)
at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:290)
at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:281)
at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:631)
at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:486)
at org.apache.spark.sql.hive.client.HiveClientImpl.convertHiveTableToCatalogTable(HiveClientImpl.scala:485)
at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTablesByName$2(HiveClientImpl.scala:472)
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
at scala.collection.TraversableLike.map(TraversableLike.scala:238)
at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
at scala.collection.AbstractTraversable.map(Traversable.scala:108)
at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTablesByName$1(HiveClientImpl.scala:472)
at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:349)
at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$retryLocked$1(HiveClientImpl.scala:252)
at org.apache.spark.sql.hive.client.HiveClientImpl.synchronizeOnObject(HiveClientImpl.scala:288)
at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:244)
at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:331)
at org.apache.spark.sql.hive.client.HiveClientImpl.getTablesByName(HiveClientImpl.scala:472)
at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$listTablesByType$1(HiveClientImpl.scala:873)
at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:349)
at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$retryLocked$1(HiveClientImpl.scala:252)
at org.apache.spark.sql.hive.client.HiveClientImpl.synchronizeOnObject(HiveClientImpl.scala:288)
at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:244)
at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:331)
at org.apache.spark.sql.hive.client.HiveClientImpl.listTablesByType(HiveClientImpl.scala:866)
at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$listTablesByType$1(PoolingHiveClient.scala:266)
at org.apache.spark.sql.hive.client.PoolingHiveClient.withHiveClient(PoolingHiveClient.scala:112)
at org.apache.spark.sql.hive.client.PoolingHiveClient.listTablesByType(PoolingHiveClient.scala:266)
at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$listViews$1(HiveExternalCatalog.scala:940)
{code}
 ",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 06 11:19:15 UTC 2020,,,,,,,,,,"0|z0hi40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/20 16:31;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29363;;;","05/Aug/20 16:31;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29363;;;","06/Aug/20 10:38;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29377;;;","06/Aug/20 11:19;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29379;;;",,,,,,,,,,,,,,,,,,,,,,
Use local time zone for the timestamp logged in unit-tests.log,SPARK-32538,13321032,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,05/Aug/20 07:46,07/Aug/20 02:32,13/Jul/23 08:50,07/Aug/20 02:32,3.0.0,3.1.0,,,,,,,,,,3.0.1,,,,Tests,,,,0,,,,"SparkFunSuite fixes the default time zone to America/Los_Angeles so the timestamp logged in unit-tests.log is also based on the fixed time zone.

It's confusable for developers whose time zone is not America/Los_Angeles.",,apachespark,maropu,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 07 02:32:14 UTC 2020,,,,,,,,,,"0|z0hhig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/20 07:57;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29356;;;","05/Aug/20 07:58;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29356;;;","07/Aug/20 02:32;maropu;Resolved by [https://github.com/apache/spark/pull/29356];;;",,,,,,,,,,,,,,,,,,,,,,,
Spark 3.0 History Server May Never Finish One Round Log Dir Scan,SPARK-32529,13320889,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanxl,yanxl,yanxl,04/Aug/20 15:08,05/Aug/20 18:02,13/Jul/23 08:50,05/Aug/20 18:02,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,Spark Core,,,,0,,,,"If there are a large number (>100k) of applications log dir, listing the log dir will take a few seconds. After getting the path list some applications might have finished already, and the filename will change from ""foo.inprogress"" to ""foo"".

It leads to a problem when adding an entry to the listing, querying file status like `fileSizeForLastIndex` will throw out a `FileNotFoundException` exception if the application was finished. And the exception will abort current loop, in a busy cluster, it will make history server couldn't list and load any application log.

 

 
{code:java}
20/08/03 15:17:23 ERROR FsHistoryProvider: Exception in checking for event log updates
 java.io.FileNotFoundException: File does not exist: hdfs://xx/logs/spark/application_11111111111111.lz4.inprogress
 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1527)
 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1520)
 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
 at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1520)
 at org.apache.spark.deploy.history.SingleFileEventLogFileReader.status$lzycompute(EventLogFileReaders.scala:170)
 at org.apache.spark.deploy.history.SingleFileEventLogFileReader.status(EventLogFileReaders.scala:170)
 at org.apache.spark.deploy.history.SingleFileEventLogFileReader.fileSizeForLastIndex(EventLogFileReaders.scala:174)
 at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$checkForLogs$7(FsHistoryProvider.scala:523)
 at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$checkForLogs$7$adapted(FsHistoryProvider.scala:466)
 at scala.collection.TraversableLike.$anonfun$filterImpl$1(TraversableLike.scala:256)
 at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
 at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
 at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
 at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:255)
 at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:249)
 at scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)
 at scala.collection.TraversableLike.filter(TraversableLike.scala:347)
 at scala.collection.TraversableLike.filter$(TraversableLike.scala:347)
 at scala.collection.AbstractTraversable.filter(Traversable.scala:108)
 at org.apache.spark.deploy.history.FsHistoryProvider.checkForLogs(FsHistoryProvider.scala:466)
 at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$startPolling$3(FsHistoryProvider.scala:287)
 at org.apache.spark.util.Utils$.tryOrExit(Utils.scala:1302)
 at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$getRunner$1(FsHistoryProvider.scala:210)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748){code}
 

 

 

 

 

 ",,apachespark,dongjoon,yanxl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 05 18:02:20 UTC 2020,,,,,,,,,,"0|z0hgnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/20 16:19;apachespark;User 'yanxiaole' has created a pull request for this issue:
https://github.com/apache/spark/pull/29350;;;","04/Aug/20 16:19;apachespark;User 'yanxiaole' has created a pull request for this issue:
https://github.com/apache/spark/pull/29350;;;","05/Aug/20 18:02;dongjoon;Issue resolved by pull request 29350
[https://github.com/apache/spark/pull/29350];;;",,,,,,,,,,,,,,,,,,,,,,,
The layout of monitoring.html is broken,SPARK-32525,13320789,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,04/Aug/20 05:46,15/Aug/20 23:29,13/Jul/23 08:50,04/Aug/20 15:30,3.1.0,,,,,,,,,,,3.1.0,,,,Documentation,,,,0,,,,The layout of monitoring.html is broken because there are 2 <td> tags not closed in monitoring.md.,,apachespark,Gengliang.Wang,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 04 15:30:45 UTC 2020,,,,,,,,,,"0|z0hg14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/20 05:56;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29345;;;","04/Aug/20 15:30;Gengliang.Wang;This issue is resolved in https://github.com/apache/spark/pull/29345;;;",,,,,,,,,,,,,,,,,,,,,,,,
SharedSparkSession should clean up InMemoryRelation.ser ,SPARK-32524,13320781,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,04/Aug/20 04:39,12/Dec/22 18:10,13/Jul/23 08:50,04/Aug/20 08:50,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,Tests,,,0,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 04 08:50:32 UTC 2020,,,,,,,,,,"0|z0hfzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/20 04:45;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29344;;;","04/Aug/20 04:45;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29344;;;","04/Aug/20 05:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29346;;;","04/Aug/20 08:50;gurwls223;Issue resolved by pull request 29346
[https://github.com/apache/spark/pull/29346];;;",,,,,,,,,,,,,,,,,,,,,,
WithFields Expression should not be foldable,SPARK-32521,13320691,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fqaiser94,fqaiser94,fqaiser94,03/Aug/20 14:58,15/Aug/20 23:30,13/Jul/23 08:50,15/Aug/20 23:30,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"The following query currently fails on master brach: 
{code:scala}
sql(""SELECT named_struct('a', 1, 'b', 2) a"")
.select($""a"".withField(""c"", lit(3)).as(""a""))
.show(false)

// java.lang.UnsupportedOperationException: Cannot evaluate expression: with_fields(named_struct(a, 1, b, 2), c, 3)
{code}
This happens because the Catalyst optimizer tries to statically evaluate the {{WithFields Expression}} (via the {{ConstantFolding}} rule), however it cannot do so because {{WithFields Expression}} is {{Unevaluable}}. 

The likely solution here is to change {{WithFields Expression}} so that it is not {{foldable}} under any circumstance. This should solve the issue because then Catalyst will no longer attempt to statically evaluate {{WithFields Expression}} anymore. 

 

 ",,apachespark,fqaiser94,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 03 15:28:59 UTC 2020,,,,,,,,,,"0|z0hffk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/20 15:28;apachespark;User 'fqaiser94' has created a pull request for this issue:
https://github.com/apache/spark/pull/29338;;;","03/Aug/20 15:28;apachespark;User 'fqaiser94' has created a pull request for this issue:
https://github.com/apache/spark/pull/29338;;;",,,,,,,,,,,,,,,,,,,,,,,,
CoarseGrainedSchedulerBackend.maxNumConcurrentTasks should consider all kinds of resources,SPARK-32518,13320595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,03/Aug/20 05:53,18/Aug/20 06:51,13/Jul/23 08:50,06/Aug/20 05:40,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,Spark Core,,,,0,,,,"Currently, CoarseGrainedSchedulerBackend.maxNumConcurrentTasks only considers the CPU for the max concurrent tasks. This can cause the application to hang when a barrier stage requires extra custom resources but the cluster doesn't have enough corresponding resources. Because, without the checking for other custom resources in maxNumConcurrentTasks, the barrier stage can be submitted to the TaskSchedulerImpl. But the TaskSchedulerImpl can not launch tasks for the barrier stage due to the insufficient task slots calculated by calculateAvailableSlots(which does check all kinds of resources). ",,apachespark,cloud_fan,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 10 06:44:07 UTC 2020,,,,,,,,,,"0|z0heu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/20 06:11;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29332;;;","03/Aug/20 06:12;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29332;;;","06/Aug/20 05:40;cloud_fan;Issue resolved by pull request 29332
[https://github.com/apache/spark/pull/29332];;;","10/Aug/20 06:44;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29395;;;",,,,,,,,,,,,,,,,,,,,,,
"path option is treated differently for 'format(""parquet"").load(path)' vs. 'parquet(path)'",SPARK-32516,13320567,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,imback82,imback82,imback82,02/Aug/20 20:59,09/Sep/20 18:43,13/Jul/23 08:50,24/Aug/20 16:30,2.4.6,3.0.0,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"When data is read, ""path"" option is treated differently depending on how dataframe is created:
{code:java}
scala> Seq(1).toDF.write.mode(""overwrite"").parquet(""/tmp/test"")
                                                                                
scala> spark.read.option(""path"", ""/tmp/test"").format(""parquet"").load(""/tmp/test"").show
+-----+
|value|
+-----+
|    1|
+-----+


scala> spark.read.option(""path"", ""/tmp/test"").parquet(""/tmp/test"").show
+-----+
|value|
+-----+
|    1|
|    1|
+-----+
{code}",,apachespark,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 09 18:43:54 UTC 2020,,,,,,,,,,"0|z0heo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/20 21:13;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/29328;;;","02/Aug/20 21:14;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/29328;;;","24/Aug/20 16:30;cloud_fan;Issue resolved by pull request 29328
[https://github.com/apache/spark/pull/29328];;;","25/Aug/20 22:45;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/29543;;;","09/Sep/20 18:43;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/29697;;;",,,,,,,,,,,,,,,,,,,,,
JDBC doesn't check duplicate column names in nested structures ,SPARK-32510,13320396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,31/Jul/20 12:12,03/Aug/20 03:20,13/Jul/23 08:50,03/Aug/20 03:20,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"JdbcUtils.getCustomSchema calls checkColumnNameDuplication() which checks duplicates on top-level but not in nested structures as other built-in datasources do, see

[https://github.com/apache/spark/blob/8bc799f92005c903868ef209f5aec8deb6ccce5a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala#L822-L823]",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32431,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 03 03:20:42 UTC 2020,,,,,,,,,,"0|z0hdm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/20 12:36;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29317;;;","03/Aug/20 03:20;cloud_fan;Issue resolved by pull request 29317
[https://github.com/apache/spark/pull/29317];;;",,,,,,,,,,,,,,,,,,,,,,,,
Unused DPP Filter causes issue in canonicalization and prevents reuse exchange,SPARK-32509,13320394,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prakharjain09,prakharjain09,prakharjain09,31/Jul/20 12:05,03/Aug/20 03:26,13/Jul/23 08:50,03/Aug/20 03:26,3.0.0,3.0.1,3.1.0,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"As part of PlanDynamicPruningFilter rule, the unused DPP Filter are simply replaced by `DynamicPruningExpression(TrueLiteral)` so that they can be avoided. But these unnecessary`DynamicPruningExpression(TrueLiteral)` partition filter inside the FileSourceScanExec affects the canonicalization of the node and so in many cases, this can prevent ReuseExchange from happening.",,apachespark,cloud_fan,prakharjain09,rohitmishr1484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 03 03:26:41 UTC 2020,,,,,,,,,,"0|z0hdlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/20 12:36;apachespark;User 'prakharjain09' has created a pull request for this issue:
https://github.com/apache/spark/pull/29318;;;","31/Jul/20 13:12;rohitmishr1484;[~prakharjain09], Meanwhile you are working on the pull request, can you please attach code sample, Environment detail or output snapshot related to the bug for reference of the wider audience and to avoid any duplicate issue. Thanks.;;;","03/Aug/20 03:26;cloud_fan;Issue resolved by pull request 29318
[https://github.com/apache/spark/pull/29318];;;",,,,,,,,,,,,,,,,,,,,,,,
RecordBinaryComparatorSuite test failures on big-endian systems,SPARK-32485,13319982,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mundaym,mundaym,mundaym,29/Jul/20 14:05,05/Aug/20 16:11,13/Jul/23 08:50,05/Aug/20 16:11,3.0.0,,,,,,,,,,,3.1.0,,,,Tests,,,,0,endianness,,,"The fix for SPARK-29918 broke two tests on big-endian systems:
 * testBinaryComparatorWhenSubtractionIsDivisibleByMaxIntValue
 * testBinaryComparatorWhenSubtractionCanOverflowLongValue

These tests date from a time where subtraction was being used to do multi-byte comparisons. They try to trigger old bugs by feeding specific values into the comparison. However the fix for SPARK-29918 modified the order in which bytes are compared when comparing 8 bytes at a time on little-endian systems (to match the normal byte-by-byte comparison). This fix did not affect big-endian systems. However the expected output of the tests was modified for all systems regardless of endianness. So the tests broke on big-endian systems.

It is also not clear that the values compared in the tests match the original intent of the tests now that the bytes in those values are compared in order (equivalent to the bytes in the values being reversed).",,apachespark,cloud_fan,mundaym,rohitmishr1484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 05 16:11:29 UTC 2020,,,,,,,,,,"0|z0hb2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/20 18:33;rohitmishr1484;[~mundaym], can you please add environment detail and steps useful for recreating the scenario. It will be helpful.;;;","30/Jul/20 12:25;mundaym;This is for Linux running on the IBM Z platform (s390x) which is a big endian platform. I have a PR open to fix this, I will update it with this issue number shortly. Thanks.;;;","05/Aug/20 11:22;apachespark;User 'mundaym' has created a pull request for this issue:
https://github.com/apache/spark/pull/29259;;;","05/Aug/20 11:23;apachespark;User 'mundaym' has created a pull request for this issue:
https://github.com/apache/spark/pull/29259;;;","05/Aug/20 16:11;cloud_fan;Issue resolved by pull request 29259
[https://github.com/apache/spark/pull/29259];;;",,,,,,,,,,,,,,,,,,,,,
Remove task result size check for shuffle map stage,SPARK-32470,13319820,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,28/Jul/20 18:04,15/Sep/21 02:04,13/Jul/23 08:50,11/Aug/20 06:31,2.4.6,3.0.0,3.1.0,,,,,,,,,3.1.0,,,,Spark Core,,,,0,,,,"The task result of a shuffle map stage is not the query result but instead is only map status and metrics accumulator updates. Aside from the metrics that can vary in size, the total task result size solely depends on the number of tasks. And the number of tasks can get large regardless of the stage's output size. For example, the number of tasks generated by `CartesianProduct` is square of ""spark.sql.shuffle.partitions"", say if ""spark.sql.shuffle.partitions"" is set to 200, you get 40,000 tasks, if set to 500, you get 250,000 tasks, which can easily error on the default limit of `spark.driver.maxResultSize`:

 
{code:java}
org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 66496 tasks (4.0 GiB) is bigger than spark.driver.maxResultSize (4.0 GiB)
{code}
 

However, map status and accumulator updates are used by the driver to update the overall map stats and metrics of the query, and they are not cached on the driver, so they won't cause catastrophic memory issues on the driver. So we should remove this check for shuffle map stage tasks.",,apachespark,cloud_fan,maryannxue,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-36071,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 11 06:31:37 UTC 2020,,,,,,,,,,"0|z0ha2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/20 19:13;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/29276;;;","29/Jul/20 13:58;tgraves;Please add a description to the Jira as to why;;;","11/Aug/20 06:31;cloud_fan;Issue resolved by pull request 29276
[https://github.com/apache/spark/pull/29276];;;",,,,,,,,,,,,,,,,,,,,,,,
Fix timeout config issue in Kafka connector tests,SPARK-32468,13319783,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gsomogyi,gsomogyi,gsomogyi,28/Jul/20 15:07,04/Aug/20 02:43,13/Jul/23 08:50,31/Jul/20 05:53,3.1.0,,,,,,,,,,,3.1.0,,,,Structured Streaming,Tests,,,0,,,,While I'm implementing SPARK-32032 I've found a bug in Kafka: https://issues.apache.org/jira/browse/KAFKA-10318. This will cause issues only later when it's fixed but it would be good to fix it now because SPARK-32032 would like to bring in AdminClient where the code blows up with the mentioned ConfigException. This would reduce the code changes in the mentioned jira.,,apachespark,gsomogyi,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 04 02:43:56 UTC 2020,,,,,,,,,,"0|z0h9ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/20 15:15;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/29272;;;","28/Jul/20 15:15;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/29272;;;","31/Jul/20 05:53;kabhwan;Issue resolved by pull request 29272
[https://github.com/apache/spark/pull/29272];;;","04/Aug/20 02:43;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29343;;;","04/Aug/20 02:43;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29343;;;",,,,,,,,,,,,,,,,,,,,,
Avoid encoding URL twice on https redirect,SPARK-32467,13319773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,28/Jul/20 14:31,06/Jul/21 13:16,13/Jul/23 08:50,01/Aug/20 05:11,3.0.1,3.1.0,,,,,,,,,,,,,,Web UI,,,,0,,,,"Currently, on https redirect, the original URL is encoded as an HTTPS URL. However, the original URL could be encoded already, so that the return result of method
UriInfo.getQueryParameters will contain encoded keys and values. For example, a parameter
order[0][dir] will become order%255B0%255D%255Bcolumn%255D after encoded twice, and the decoded
key in the result of UriInfo.getQueryParameters will be order%5B0%5D%5Bcolumn%5D.

To fix the problem, we try decoding the query parameters before encoding it. This is to make sure we encode the URL exactly once.",,apachespark,dzcxzl,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33195,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 23 09:56:13 UTC 2021,,,,,,,,,,"0|z0h9s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/20 14:52;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/29271;;;","01/Aug/20 05:11;Gengliang.Wang;The issue is resolved in https://github.com/apache/spark/pull/29271;;;","23/Jun/21 09:56;dzcxzl;YARN-3239. WebAppProxy does not support a final tracking url which has query fragments and params.

If the Yarn cluster does not use the YARN-3217 YARN-3239 patch, the running spark job still encounters the NPE problem when accessing the task page.

Does spark need to do URL decode twice to avoid NPE?;;;",,,,,,,,,,,,,,,,,,,,,,,
Mismatched row access sizes in tests,SPARK-32458,13319520,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mundaym,mundaym,mundaym,27/Jul/20 10:18,28/Jul/20 17:37,13/Jul/23 08:50,28/Jul/20 17:37,3.0.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,catalyst,endianness,,"The RowEncoderSuite and UnsafeMapSuite tests fail on big-endian systems. This is because the test data is written into the row using unsafe operations with one size and then read back using a different size. For example, in UnsafeMapSuite the test data is written using putLong and then read back using getInt. This happens to work on little-endian systems but these differences appear to be typos and cause the tests to fail on big-endian systems.

I have a patch that fixes the issue.",,apachespark,dongjoon,mundaym,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 28 17:37:07 UTC 2020,,,,,,,,,,"0|z0h880:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/20 10:38;apachespark;User 'mundaym' has created a pull request for this issue:
https://github.com/apache/spark/pull/29258;;;","28/Jul/20 17:37;dongjoon;Issue resolved by pull request 29258
[https://github.com/apache/spark/pull/29258];;;",,,,,,,,,,,,,,,,,,,,,,,,
Support Apache Arrow 1.0.0 in SparkR,SPARK-32451,13319413,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,26/Jul/20 23:47,19/Aug/20 14:23,13/Jul/23 08:50,27/Jul/20 01:51,3.1.0,,,,,,,,,,,3.0.1,3.1.0,,,R,Tests,,,0,,,,Apache Arrow released 1.0.0 and this causes GitHub Action SparkR test failures. This issue aims to fix that.,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 18 07:47:36 UTC 2020,,,,,,,,,,"0|z0h7k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/20 23:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29251;;;","27/Jul/20 00:06;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29252;;;","27/Jul/20 01:51;dongjoon;Issue resolved by pull request 29252
[https://github.com/apache/spark/pull/29252];;;","18/Aug/20 07:46;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29462;;;","18/Aug/20 07:47;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29462;;;",,,,,,,,,,,,,,,,,,,,,
The .schema() API behaves incorrectly for nested schemas that have column duplicates in case-insensitive mode,SPARK-32431,13319187,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,mswit,mswit,24/Jul/20 15:20,31/Jul/20 12:12,13/Jul/23 08:50,30/Jul/20 06:06,2.4.6,3.0.0,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"The code below throws org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the data schema: `camelcase`; for multiple file formats due to a duplicate column in the requested schema.
{code:java}
import org.apache.spark.sql.types._
spark.conf.set(""spark.sql.caseSensitive"", ""false"")
val formats = Seq(""parquet"", ""orc"", ""avro"", ""json"")
val caseInsensitiveSchema = new StructType().add(""LowerCase"", LongType).add(""camelcase"", LongType).add(""CamelCase"", LongType)
formats.map{ format =>
    val path = s""/tmp/$format""
    spark
    .range(1L)
    .selectExpr(""id AS lowercase"", ""id + 1 AS camelCase"")
    .write.mode(""overwrite"").format(format).save(path) 
    spark.read.schema(caseInsensitiveSchema).format(format).load(path).show
}
{code}
Similar code with nested schema behaves inconsistently across file formats and sometimes returns incorrect results:
{code:java}
import org.apache.spark.sql.types._
spark.conf.set(""spark.sql.caseSensitive"", ""false"")
val formats = Seq(""parquet"", ""orc"", ""avro"", ""json"")
val caseInsensitiveSchema = new StructType().add(""StructColumn"", new StructType().add(""LowerCase"", LongType).add(""camelcase"", LongType).add(""CamelCase"", LongType))
formats.map{ format =>
    val path = s""/tmp/$format""
    spark
    .range(1L)
    .selectExpr(""NAMED_STRUCT('lowercase', id, 'camelCase', id + 1) AS StructColumn"")
    .write.mode(""overwrite"").format(format).save(path)
    
    spark.read.schema(caseInsensitiveSchema).format(format).load(path).show
}
{code}
The desired behavior likely should be returning an exception just like in the flat schema scenario.",,apachespark,cloud_fan,maxgekk,mswit,xabriel,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20460,SPARK-32510,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 30 06:06:15 UTC 2020,,,,,,,,,,"0|z0h660:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/20 15:45;maxgekk;I am working on this.;;;","25/Jul/20 09:51;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29234;;;","30/Jul/20 06:06;cloud_fan;Issue resolved by pull request 29234
[https://github.com/apache/spark/pull/29234];;;",,,,,,,,,,,,,,,,,,,,,,,
Allow plugins to inject rules into AQE query stage preparation,SPARK-32430,13319185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andygrove,tgraves,tgraves,24/Jul/20 14:51,24/Jul/20 18:04,13/Jul/23 08:50,24/Jul/20 18:04,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"Provide a generic mechanism for plugins to inject rules into the AQE ""query prep"" stage that happens before query stage creation.

this goes along with https://issues.apache.org/jira/browse/SPARK-32332 where the current AQE implementation doesn't allow for users to properly extend it for columnar processing. 

The issue here is that we create new query stages but we do not have access to the parent plan of the new query stage so certain things can not be determined because you have to know what the parent did.  With this change it would allow you to add TAGs to be able to figure out what is going on.

 ",,apachespark,dongjoon,nartal1,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32332,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 18:04:36 UTC 2020,,,,,,,,,,"0|z0h65k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/20 15:00;apachespark;User 'andygrove' has created a pull request for this issue:
https://github.com/apache/spark/pull/29224;;;","24/Jul/20 15:01;apachespark;User 'andygrove' has created a pull request for this issue:
https://github.com/apache/spark/pull/29224;;;","24/Jul/20 18:04;dongjoon;Issue resolved by pull request 29224
[https://github.com/apache/spark/pull/29224];;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky test: BlockManagerDecommissionIntegrationSuite.verify that an already running task which is going to cache data succeeds on a decommissioned executor,SPARK-32417,13319032,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dagrawal3409,gsomogyi,gsomogyi,23/Jul/20 19:00,30/Jul/20 19:02,13/Jul/23 08:50,30/Jul/20 19:02,3.1.0,,,,,,,,,,,3.1.0,,,,Spark Core,Tests,,,0,,,,"https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/126424/testReport/
{code:java}
Error Message
org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 2759 times over 30.001772248 seconds. Last failure message: Map() was empty We should have a block that has been on multiple BMs in rdds:  ArrayBuffer(SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_2,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(0, localhost, 42041, None),rdd_1_1,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_0,StorageLevel(memory, deserialized, 1 replicas),56,0))) from: ArrayBuffer(SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, amp-jenkins-worker-05.amp, 45854, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(2, localhost, 42805, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(0, localhost, 42041, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_2,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(0, localhost, 42041, None),rdd_1_1,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_0,StorageLevel(memory, deserialized, 1 replicas),56,0)))  but instead we got:  Map(rdd_1_0 -> 1, rdd_1_2 -> 1, rdd_1_1 -> 1).
Stacktrace
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 2759 times over 30.001772248 seconds. Last failure message: Map() was empty We should have a block that has been on multiple BMs in rdds:
 ArrayBuffer(SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_2,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(0, localhost, 42041, None),rdd_1_1,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_0,StorageLevel(memory, deserialized, 1 replicas),56,0))) from:
ArrayBuffer(SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, amp-jenkins-worker-05.amp, 45854, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(2, localhost, 42805, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(0, localhost, 42041, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_2,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(0, localhost, 42041, None),rdd_1_1,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_0,StorageLevel(memory, deserialized, 1 replicas),56,0)))
 but instead we got:
 Map(rdd_1_0 -> 1, rdd_1_2 -> 1, rdd_1_1 -> 1).
	at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432)
	at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439)
	at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391)
	at org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite.eventually(BlockManagerDecommissionIntegrationSuite.scala:33)
	at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:308)
	at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:307)
	at org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite.eventually(BlockManagerDecommissionIntegrationSuite.scala:33)
	at org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite.runDecomTest(BlockManagerDecommissionIntegrationSuite.scala:169)
	at org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite.$anonfun$new$1(BlockManagerDecommissionIntegrationSuite.scala:41)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:157)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:59)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:59)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1124)
	at org.scalatest.Suite.run$(Suite.scala:1106)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:518)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:59)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:59)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: Map() was empty We should have a block that has been on multiple BMs in rdds:
 ArrayBuffer(SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_2,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(0, localhost, 42041, None),rdd_1_1,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_0,StorageLevel(memory, deserialized, 1 replicas),56,0))) from:
ArrayBuffer(SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, amp-jenkins-worker-05.amp, 45854, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(2, localhost, 42805, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(0, localhost, 42041, None),broadcast_1_piece0,StorageLevel(memory, 1 replicas),2695,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_2,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(0, localhost, 42041, None),rdd_1_1,StorageLevel(memory, deserialized, 1 replicas),56,0)), SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(1, localhost, 37968, None),rdd_1_0,StorageLevel(memory, deserialized, 1 replicas),56,0)))
 but instead we got:
 Map(rdd_1_0 -> 1, rdd_1_2 -> 1, rdd_1_1 -> 1)
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:503)
	at org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite.$anonfun$runDecomTest$7(BlockManagerDecommissionIntegrationSuite.scala:179)
	at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395)
	at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409)
	... 53 more
{code}
",,apachespark,dagrawal3409,gsomogyi,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 27 21:59:26 UTC 2020,,,,,,,,,,"0|z0h57k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/20 06:34;apachespark;User 'agrawaldevesh' has created a pull request for this issue:
https://github.com/apache/spark/pull/29226;;;","27/Jul/20 21:59;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/29263;;;",,,,,,,,,,,,,,,,,,,,,,,,
sparksql cannot access hive table while data in hbase,SPARK-32380,13318475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,meimile,meimile,21/Jul/20 11:10,12/Dec/22 18:10,13/Jul/23 08:50,05/Nov/22 12:31,3.0.0,,,,,,,,,,,3.2.3,3.3.2,,,SQL,,,,0,,,,"* step1: create hbase table

{code:java}
 hbase(main):001:0>create 'hbase_test1', 'cf1'
 hbase(main):001:0> put 'hbase_test', 'r1', 'cf1:c1', '123'
{code}
 * step2: create hive table related to hbase table

 
{code:java}
hive> 
CREATE EXTERNAL TABLE `hivetest.hbase_test`(
  `key` string COMMENT '', 
  `value` string COMMENT '')
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.hbase.HBaseSerDe' 
STORED BY 
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ( 
  'hbase.columns.mapping'=':key,cf1:v1', 
  'serialization.format'='1')
TBLPROPERTIES (
  'hbase.table.name'='hbase_test')
 {code}
 * step3: sparksql query hive table while data in hbase

{code:java}
spark-sql --master yarn -e ""select * from hivetest.hbase_test""
{code}
 

The error log as follow: 

java.io.IOException: Cannot create a record reader because of a previous error. Please look at the previous logs lines from the task's full log for more details.
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:270)
 at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:131)
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)
 at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
 at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
 at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)
 at org.apache.spark.rdd.RDD.collect(RDD.scala:1003)
 at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)
 at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:412)
 at org.apache.spark.sql.execution.HiveResult$.hiveResultString(HiveResult.scala:58)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.$anonfun$run$1(SparkSQLDriver.scala:65)
 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
 at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
 at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
 at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:65)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:377)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:496)
 at scala.collection.Iterator.foreach(Iterator.scala:941)
 at scala.collection.Iterator.foreach$(Iterator.scala:941)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
 at scala.collection.IterableLike.foreach(IterableLike.scala:74)
 at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
 at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:490)
 at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336)
 at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:474)
 at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:490)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:206)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
 Caused by: java.lang.IllegalStateException: The input format instance has not been properly initialized. Ensure you call initializeTable either in your constructor or initialize method
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getTable(TableInputFormatBase.java:652)
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:265)
 ... 62 more
 java.io.IOException: Cannot create a record reader because of a previous error. Please look at the previous logs lines from the task's full log for more details.
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:270)
 at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:131)
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)
 at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
 at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
 at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)
 at org.apache.spark.rdd.RDD.collect(RDD.scala:1003)
 at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)
 at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:412)
 at org.apache.spark.sql.execution.HiveResult$.hiveResultString(HiveResult.scala:58)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.$anonfun$run$1(SparkSQLDriver.scala:65)
 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
 at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
 at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
 at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:65)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:377)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:496)
 at scala.collection.Iterator.foreach(Iterator.scala:941)
 at scala.collection.Iterator.foreach$(Iterator.scala:941)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
 at scala.collection.IterableLike.foreach(IterableLike.scala:74)
 at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
 at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:490)
 at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336)
 at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:474)
 at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:490)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:206)
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
 Caused by: java.lang.IllegalStateException: The input format instance has not been properly initialized. Ensure you call initializeTable either in your constructor or initialize method
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getTable(TableInputFormatBase.java:652)
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:265)
 ... 62 more","||component||version||
|hadoop|2.8.5|
|hive|2.3.7|
|spark|3.0.0|
|hbase|1.4.9|",4u.PremSagar,apachespark,mehulthakkar,meimile,rangareddy.avula@gmail.com,,,,,,259200,259200,,0%,259200,259200,,,,,,,,SPARK-34210,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 05 12:31:17 UTC 2022,,,,,,,,,,"0|z0h1s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/20 11:37;meimile;I have solved this bug by modified TableReader.scala.

The solution is when the inputformat class is org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat,  will create OldHadoopRDD. I have tested in my product env as well.

Can I submit a pr to spark ?

[~apachespark]

 ;;;","21/Jul/20 13:58;apachespark;User 'DeyinZhong' has created a pull request for this issue:
https://github.com/apache/spark/pull/29178;;;","21/Jul/20 13:58;apachespark;User 'DeyinZhong' has created a pull request for this issue:
https://github.com/apache/spark/pull/29178;;;","12/Jan/21 08:28;apachespark;User 'yangBottle' has created a pull request for this issue:
https://github.com/apache/spark/pull/31147;;;","12/Jan/21 08:29;apachespark;User 'yangBottle' has created a pull request for this issue:
https://github.com/apache/spark/pull/31147;;;","22/Apr/21 13:26;4u.PremSagar;[~meimile] Will there be any permanent fix for this issues spark3 and HIve on top of Hbase tables access;;;","21/Oct/22 21:30;mehulthakkar;[~apachespark] 

Since changes were merged on Jan 11, 2021, I expect this issue to be fixed in version 3.0.2 but it is not fixed even in Spark 3.2.2. 

 

The build.sbt looks as follows.

 

val scalaVersion := ""2.12.3""

val sparkVersion = ""3.2.2""

 

lazy val root = (project in file("".*))

    .settings(

    libraryDependencies ++= Seq(""org.apache.spark"" %% ""spark-core"" % sparkVersion % Provided,

                                                    ""org.apache.spark"" %% ""spark-hive"" % sparkVersion % Provided,

                                                   ""org.apache.spark"" %% ""spark-sql"" % sparkVersion % Provided)

 ;;;","02/Nov/22 13:27;rangareddy.avula@gmail.com;The below pull request will solve the issue but needs to check if there are any other issues.

[https://github.com/apache/spark/pull/29178]

 ;;;","05/Nov/22 01:57;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/38516;;;","05/Nov/22 12:31;gurwls223;Fixed in https://github.com/apache/spark/commit/7009ef0510dae444c72e7513357e681b08379603;;;",,,,,,,,,,,,,,,,
docker based spark release script should use correct CRAN repo.,SPARK-32379,13318417,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,prashant,prashant,prashant,21/Jul/20 07:51,12/Dec/22 18:10,13/Jul/23 08:50,21/Jul/20 10:10,2.4.6,,,,,,,,,,,2.4.7,,,,Build,,,,0,,,,"While running, dev/create-release/do-release-docker.sh script, it is failing with following errors

{code}
[root@kyok-test-1 ~]# tail docker-build.log 
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 r-base : Depends: r-base-core (>= 4.0.2-1.1804.0) but it is not going to be installed
          Depends: r-recommended (= 4.0.2-1.1804.0) but it is not going to be installed
 r-base-dev : Depends: r-base-core (>= 4.0.2-1.1804.0) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
The command '/bin/sh -c apt-get clean && apt-get update && $APT_INSTALL gnupg ca-certificates apt-transport-https &&   echo 'deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/' >> /etc/apt/sources.list &&   gpg --keyserver keyserver.ubuntu.com --recv-key E298A3A825C0D65DFD57CBB651716619E084DAB9 &&   gpg -a --export E084DAB9 | apt-key add - &&   apt-get clean &&   rm -rf /var/lib/apt/lists/* &&   apt-get clean &&   apt-get update &&   $APT_INSTALL software-properties-common &&   apt-add-repository -y ppa:brightbox/ruby-ng &&   apt-get update &&   $APT_INSTALL openjdk-8-jdk &&   update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java &&   $APT_INSTALL curl wget git maven ivy subversion make gcc lsof libffi-dev     pandoc pandoc-citeproc libssl-dev libcurl4-openssl-dev libxml2-dev &&   ln -s -T /usr/share/java/ivy.jar /usr/share/ant/lib/ivy.jar &&   curl -sL https://deb.nodesource.com/setup_4.x | bash &&   $APT_INSTALL nodejs &&   $APT_INSTALL libpython2.7-dev libpython3-dev python-pip python3-pip &&   pip install --upgrade pip && hash -r pip &&   pip install setuptools &&   pip install $BASE_PIP_PKGS &&   pip install $PIP_PKGS &&   cd &&   virtualenv -p python3 /opt/p35 &&   . /opt/p35/bin/activate &&   pip install setuptools &&   pip install $BASE_PIP_PKGS &&   pip install $PIP_PKGS &&   $APT_INSTALL r-base r-base-dev &&   $APT_INSTALL texlive-latex-base texlive texlive-fonts-extra texinfo qpdf &&   Rscript -e ""install.packages(c('curl', 'xml2', 'httr', 'devtools', 'testthat', 'knitr', 'rmarkdown', 'roxygen2', 'e1071', 'survival'), repos='https://cloud.r-project.org/')"" &&   Rscript -e ""devtools::install_github('jimhester/lintr')"" &&   $APT_INSTALL ruby2.3 ruby2.3-dev mkdocs &&   gem install jekyll --no-rdoc --no-ri -v 3.8.6 &&   gem install jekyll-redirect-from -v 0.15.0 &&   gem install pygments.rb' returned a non-zero code: 100

{code}
",,apachespark,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 10:10:09 UTC 2020,,,,,,,,,,"0|z0h1fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/20 08:05;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/29177;;;","21/Jul/20 08:06;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/29177;;;","21/Jul/20 10:10;gurwls223;Issue resolved by pull request 29177
[https://github.com/apache/spark/pull/29177];;;",,,,,,,,,,,,,,,,,,,,,,,
CaseInsensitiveMap should be deterministic for addition,SPARK-32377,13318369,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,girish_pandit,dongjoon,21/Jul/20 00:39,21/Jul/20 22:58,13/Jul/23 08:50,21/Jul/20 06:03,2.1.3,2.2.3,2.3.4,2.4.6,3.0.0,,,,,,,2.4.7,3.0.1,3.1.0,,SQL,,,,0,,,,"{code}
import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap
var m = CaseInsensitiveMap(Map.empty[String, String])
Seq((""paTh"", ""1""), (""PATH"", ""2""), (""Path"", ""3""), (""patH"", ""4""), (""path"", ""5"")).foreach { kv =>
  m = (m + kv).asInstanceOf[CaseInsensitiveMap[String]]
  println(m.get(""path""))
}

Some(1)
Some(2)
Some(3)
Some(4)
Some(1)
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 06:03:23 UTC 2020,,,,,,,,,,"0|z0h14o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/20 00:45;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29172;;;","21/Jul/20 05:37;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29175;;;","21/Jul/20 05:37;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29175;;;","21/Jul/20 06:03;dongjoon;This is resolved via https://github.com/apache/spark/pull/29172;;;",,,,,,,,,,,,,,,,,,,,,,
"""Resolved attribute(s) XXX missing"" after dudup conflict references",SPARK-32372,13318276,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,20/Jul/20 14:48,24/Jul/20 04:27,13/Jul/23 08:50,23/Jul/20 14:38,2.2.2,2.3.4,2.4.6,3.0.0,,,,,,,,2.4.7,3.0.1,3.1.0,,SQL,,,,0,,,,"{code:java}
// case class Person(id: Int, name: String, age: Int)

sql(""SELECT name, avg(age) as avg_age FROM person GROUP BY name"").createOrReplaceTempView(""person_a"")
sql(""SELECT p1.name, p2.avg_age FROM person p1 JOIN person_a p2 ON p1.name = p2.name"").createOrReplaceTempView(""person_b"")
sql(""SELECT * FROM person_a UNION SELECT * FROM person_b"")   .createOrReplaceTempView(""person_c"")
sql(""SELECT p1.name, p2.avg_age FROM person_c p1 JOIN person_c p2 ON p1.name = p2.name"").show
{code}
error:
{code:java}
[info]   Failed to analyze query: org.apache.spark.sql.AnalysisException: Resolved attribute(s) avg_age#235 missing from name#233,avg_age#231 in operator !Project [name#233, avg_age#235]. Attribute(s) with the same name appear in the operation: avg_age. Please check if the right attribute(s) are used.;;
...{code}",,apachespark,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 17:08:23 UTC 2020,,,,,,,,,,"0|z0h0k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/20 15:38;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29166;;;","20/Jul/20 15:39;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29166;;;","23/Jul/20 17:08;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29208;;;",,,,,,,,,,,,,,,,,,,,,,,
Options in PartitioningAwareFileIndex should respect case insensitivity,SPARK-32368,13318231,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,20/Jul/20 11:27,12/Dec/22 17:51,13/Jul/23 08:50,20/Jul/20 20:56,3.0.1,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,The datasource options such as {{recursiveFileLookup}} or {{pathglobFilter}} currently don't respect case insensitivity.,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 20 20:56:44 UTC 2020,,,,,,,,,,"0|z0h0a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/20 11:47;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29165;;;","20/Jul/20 20:56;dongjoon;Issue resolved by pull request 29165
[https://github.com/apache/spark/pull/29165];;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix java.lang.IndexOutOfBoundsException: No group -1,SPARK-32365,13318173,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,20/Jul/20 06:27,22/Jul/20 14:18,13/Jul/23 08:50,21/Jul/20 03:35,3.0.1,3.1.0,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"The current implement of regexp_extract will throws a unprocessed exception show below:

SELECT regexp_extract('1a 2b 14m', 'd+' -1)


{code:java}
java.lang.IndexOutOfBoundsException: No group -1
java.util.regex.Matcher.group(Matcher.java:538)
org.apache.spark.sql.catalyst.expressions.RegExpExtract.nullSafeEval(regexpExpressions.scala:455)
org.apache.spark.sql.catalyst.expressions.TernaryExpression.eval(Expression.scala:704)
org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:52)
org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:45)
{code}
",,apachespark,beliefer,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 03:35:29 UTC 2020,,,,,,,,,,"0|z0gzx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/20 06:34;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/29161;;;","21/Jul/20 03:35;dongjoon;Issue resolved by pull request 29161
[https://github.com/apache/spark/pull/29161];;;",,,,,,,,,,,,,,,,,,,,,,,,
Use CaseInsensitiveMap for DataFrameReader/Writer options,SPARK-32364,13318164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,girish_pandit,dongjoon,20/Jul/20 05:57,09/Sep/20 21:02,13/Jul/23 08:50,22/Jul/20 15:00,2.0.2,2.1.3,2.2.3,2.3.4,2.4.6,3.0.0,,,,,,2.4.7,3.0.1,3.1.0,,SQL,,,,0,,,,"When a user have multiple options like path, paTH, and PATH for the same key path, option/options is non-deterministic because extraOptions is HashMap. This issue aims to use *CaseInsensitiveMap* instead of *HashMap* to fix this bug fundamentally.

{code}
spark.read
  .option(""paTh"", ""1"")
  .option(""PATH"", ""2"")
  .option(""Path"", ""3"")
  .option(""patH"", ""4"")
  .load(""5"")
...
org.apache.spark.sql.AnalysisException:
Path does not exist: file:/.../1;
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32756,SPARK-32832,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 30 07:45:55 UTC 2020,,,,,,,,,,"0|z0gzv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/20 06:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29160;;;","22/Jul/20 15:00;dongjoon;Issue resolved by pull request 29160
[https://github.com/apache/spark/pull/29160];;;","22/Jul/20 16:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29191;;;","22/Jul/20 16:08;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29191;;;","23/Jul/20 18:40;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29209;;;","30/Aug/20 07:45;apachespark;User 'karolchmist' has created a pull request for this issue:
https://github.com/apache/spark/pull/29584;;;",,,,,,,,,,,,,,,,,,,,
AdaptiveQueryExecSuite misses verifying AE results,SPARK-32362,13318153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,20/Jul/20 04:05,12/Dec/22 18:10,13/Jul/23 08:50,21/Jul/20 03:48,3.0.0,3.1.0,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"{code}
QueryTest.sameRows(result.toSeq, df.collect().toSeq)
{code}
Even the results are different, no fail.",,apachespark,cltlfcjin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 03:48:32 UTC 2020,,,,,,,,,,"0|z0gzso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/20 04:22;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29158;;;","20/Jul/20 04:23;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29158;;;","21/Jul/20 03:48;gurwls223;Issue resolved by pull request 29158
[https://github.com/apache/spark/pull/29158];;;",,,,,,,,,,,,,,,,,,,,,,,
Unevaluable expr is set to FIRST/LAST ignoreNullsExpr in distinct aggregates,SPARK-32344,13317336,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,maropu,maropu,17/Jul/20 07:04,12/Dec/22 18:10,13/Jul/23 08:50,19/Jul/20 02:12,2.2.3,2.3.4,2.4.6,3.0.0,,,,,,,,2.4.7,3.0.1,3.1.0,,SQL,,,,0,,,,"{code}
scala> sql(""SELECT FIRST(DISTINCT v) FROM VALUES 1, 2, 3 t(v)"").show()
...
Caused by: java.lang.UnsupportedOperationException: Cannot evaluate expression: false#37
  at org.apache.spark.sql.catalyst.expressions.Unevaluable$class.eval(Expression.scala:258)
  at org.apache.spark.sql.catalyst.expressions.AttributeReference.eval(namedExpressions.scala:226)
  at org.apache.spark.sql.catalyst.expressions.aggregate.First.ignoreNulls(First.scala:68)
  at org.apache.spark.sql.catalyst.expressions.aggregate.First.updateExpressions$lzycompute(First.scala:82)
  at org.apache.spark.sql.catalyst.expressions.aggregate.First.updateExpressions(First.scala:81)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$15.apply(HashAggregateExec.scala:268)
{code}",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 20 03:33:05 UTC 2020,,,,,,,,,,"0|z0gur4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/20 07:23;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29143;;;","19/Jul/20 02:12;gurwls223;Issue resolved by pull request 29143
[https://github.com/apache/spark/pull/29143];;;","20/Jul/20 03:33;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29157;;;",,,,,,,,,,,,,,,,,,,,,,,
AQE doesn't adequately allow for Columnar Processing extension ,SPARK-32332,13317182,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,tgraves,tgraves,16/Jul/20 14:50,31/Jul/20 16:15,13/Jul/23 08:50,29/Jul/20 19:25,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"In SPARK-27396 we added support to extended Columnar Processing. We did the initial work as to what we thought was sufficient but adaptive query execution was being developed at the same time.

We have discovered that the changes made to AQE are not sufficient for users to properly extend it for columnar processing because AQE hardcodes to look for specific classes/execs.

 ",,apachespark,nartal1,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32430,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 30 16:11:53 UTC 2020,,,,,,,,,,"0|z0gttc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/20 15:05;apachespark;User 'andygrove' has created a pull request for this issue:
https://github.com/apache/spark/pull/29134;;;","16/Jul/20 15:06;apachespark;User 'andygrove' has created a pull request for this issue:
https://github.com/apache/spark/pull/29134;;;","27/Jul/20 18:20;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29262;;;","27/Jul/20 18:21;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29262;;;","30/Jul/20 16:11;apachespark;User 'andygrove' has created a pull request for this issue:
https://github.com/apache/spark/pull/29310;;;",,,,,,,,,,,,,,,,,,,,,
Aggression that use map type input UDF as group expression can fail,SPARK-32307,13316732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,14/Jul/20 16:14,17/Dec/21 14:02,13/Jul/23 08:50,14/Jul/20 19:19,3.0.1,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"{code:java}
spark.udf.register(""key"", udf((m: Map[String, String]) => m.keys.head.toInt))
Seq(Map(""1"" -> ""one"", ""2"" -> ""two"")).toDF(""a"").createOrReplaceTempView(""t"")
checkAnswer(sql(""SELECT key(a) AS k FROM t GROUP BY key(a)""), Row(1) :: Nil)

[info]   org.apache.spark.sql.AnalysisException: expression 't.`a`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;
[info] Aggregate [UDF(a#6)], [UDF(a#6) AS k#8]
[info] +- SubqueryAlias t
[info]    +- Project [value#3 AS a#6]
[info]       +- LocalRelation [value#3]
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:49)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:48)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:130)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:257)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10(CheckAnalysis.scala:259)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10$adapted(CheckAnalysis.scala:259)
[info]   at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[info]   at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[info]   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:259)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10(CheckAnalysis.scala:259)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10$adapted(CheckAnalysis.scala:259)
[info]   at scala.collection.immutable.List.foreach(List.scala:392)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:259)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13(CheckAnalysis.scala:286)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13$adapted(CheckAnalysis.scala:286)
[info]   at scala.collection.immutable.List.foreach(List.scala:392)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:286)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:92)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:177)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:92)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:89)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:130)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:156)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:153)
[info]   at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:70)
[info]   at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[info]   at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:135)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
[info]   at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:135)
[info]   at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:70)
[info]   at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:68)
[info]   at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:60)
[info]   at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
[info]   at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
[info]   at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:606)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
[info]   at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:601)
...
{code}
",,apachespark,dongjoon,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31826,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 17 14:02:13 UTC 2021,,,,,,,,,,"0|z0gr1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/20 16:47;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29106;;;","14/Jul/20 16:48;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29106;;;","14/Jul/20 18:55;dongjoon;Hi, [~Ngone51].
I cannot reproduce the bug with the example. Did I miss something?
{code}
scala> spark.version
res0: String = 3.0.0

scala> spark.udf.register(""key"", udf((m: Map[String, String]) => m.keys.head.toInt))
res1: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$1954/1561881364@8937f62,IntegerType,List(Some(class[value[0]: map<string,string>])),None,false,true)

scala> Seq(Map(""1"" -> ""one"", ""2"" -> ""two"")).toDF(""a"").createOrReplaceTempView(""t"")

scala> sql(""SELECT key(a) AS k FROM t GROUP BY key(a)"").collect()
res3: Array[org.apache.spark.sql.Row] = Array([1])
{code};;;","14/Jul/20 19:06;dongjoon;If this exists at `branch-3.0` and doesn't released yet, please use `3.0.1` instead of `3.0.0` in the `Affected Versions`, [~Ngone51].;;;","14/Jul/20 19:16;dongjoon;I'll update it.;;;","14/Jul/20 19:19;dongjoon;Issue resolved by pull request 29106
[https://github.com/apache/spark/pull/29106];;;","16/Jul/20 00:46;dongjoon;This is reverted from `branch-3.0` due to the UT failure.;;;","16/Jul/20 00:47;dongjoon;Hi, [~Ngone51]. It seems that we need to pass a full Jenkins run on branch-4.0. Could you make a backporting PR please?;;;","16/Jul/20 08:42;Ngone51;Hi [~dongjoon] Please see my response here https://github.com/apache/spark/pull/29106#issuecomment-659253331. We may don't need to backport it to 3.0. ;;;","16/Jul/20 16:50;dongjoon;Got it. Thanks, [~Ngone51]!;;;","17/Dec/21 14:02;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/34937;;;",,,,,,,,,,,,,,,
toPandas with no partitions should work,SPARK-32300,13316639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,14/Jul/20 08:18,12/Dec/22 17:51,13/Jul/23 08:50,14/Jul/20 20:29,2.4.6,,,,,,,,,,,2.4.7,,,,PySpark,,,,0,,,,"{code}
>>> spark.sparkContext.emptyRDD().toDF(""col1 int"").toPandas()
  An error occurred while calling o158.getResult.
: org.apache.spark.SparkException: Exception thrown in awaitResult:
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:874)
	at org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:870)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NegativeArraySizeException
	at org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3293)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3287)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1.apply(Dataset.scala:3287)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1.apply(Dataset.scala:3286)
	at org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply$mcV$sp(PythonRDD.scala:456)
	at org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)
	at org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:457)
	at org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:453)
	at org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:994)
	at org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:988)
	at org.apache.spark.api.python.PythonServer$$anonfun$11$$anonfun$apply$9.apply(PythonRDD.scala:853)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:853)
	at org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:852)
	at org.apache.spark.api.python.PythonServer$$anon$1.run(PythonRDD.scala:908)

  warnings.warn(msg)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/hyukjin.kwon/workspace/forked/spark/python/pyspark/sql/dataframe.py"", line 2132, in toPandas
    batches = self.toDF(*tmp_column_names)._collectAsArrow()
  File ""/Users/hyukjin.kwon/workspace/forked/spark/python/pyspark/sql/dataframe.py"", line 2223, in _collectAsArrow
    jsocket_auth_server.getResult()  # Join serving thread and raise any exceptions
  File ""/Users/hyukjin.kwon/workspace/forked/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
  File ""/Users/hyukjin.kwon/workspace/forked/spark/python/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/Users/hyukjin.kwon/workspace/forked/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py"", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o158.getResult.
: org.apache.spark.SparkException: Exception thrown in awaitResult:
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:874)
	at org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:870)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NegativeArraySizeException
	at org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3293)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3287)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1.apply(Dataset.scala:3287)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1.apply(Dataset.scala:3286)
	at org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply$mcV$sp(PythonRDD.scala:456)
	at org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)
	at org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:457)
	at org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:453)
	at org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:994)
	at org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:988)
	at org.apache.spark.api.python.PythonServer$$anonfun$11$$anonfun$apply$9.apply(PythonRDD.scala:853)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:853)
	at org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:852)
	at org.apache.spark.api.python.PythonServer$$anon$1.run(PythonRDD.scala:908)
{code}",,apachespark,bryanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32301,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 14 20:29:18 UTC 2020,,,,,,,,,,"0|z0gqh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/20 08:41;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29098;;;","14/Jul/20 08:42;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29098;;;","14/Jul/20 20:29;bryanc;Issue resolved by pull request 29098
[https://github.com/apache/spark/pull/29098];;;",,,,,,,,,,,,,,,,,,,,,,,
Multiple Kryo registrators can't be used anymore,SPARK-32283,13316330,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cltlfcjin,LorenzB,LorenzB,12/Jul/20 11:26,29/Jul/20 03:58,13/Jul/23 08:50,29/Jul/20 03:58,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,Spark Core,,,,0,,,,"This is a regression in Spark 3.0 as it is working with Spark 2.

According to the docs, it should be possible to register multiple Kryo registrators via Spark config option spark.kryo.registrator . 

In Spark 3.0 the code to parse Kryo config options has been refactored into Scala class [Kryo|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/config/Kryo.scala]. The code to parse the registrators is in [Line 29-32|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/config/Kryo.scala#L29-L32]

{code:scala}
val KRYO_USER_REGISTRATORS = ConfigBuilder(""spark.kryo.registrator"")
    .version(""0.5.0"")
    .stringConf
    .createOptional
{code}
but it should be
{code:scala}
val KRYO_USER_REGISTRATORS = ConfigBuilder(""spark.kryo.registrator"")
    .version(""0.5.0"")
    .stringConf
    .toSequence
    .createOptional
{code}
 to split the comma seprated list.

In previous Spark 2.x it was done differently directly in [KryoSerializer Line 77-79|https://github.com/apache/spark/blob/branch-2.4/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala#L77-L79]

{code:scala}
private val userRegistrators = conf.get(""spark.kryo.registrator"", """")
    .split(',').map(_.trim)
    .filter(!_.isEmpty)
{code}

Hope this helps.",,apachespark,cloud_fan,cltlfcjin,LorenzB,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12080,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 29 03:58:36 UTC 2020,,,,,,,,,,"0|z0gokg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/20 09:28;cltlfcjin;Thanks for reporting this. Will file a patch.;;;","15/Jul/20 09:53;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29123;;;","29/Jul/20 03:58;cloud_fan;Issue resolved by pull request 29123
[https://github.com/apache/spark/pull/29123];;;",,,,,,,,,,,,,,,,,,,,,,,
AnalysisException thrown when query contains several JOINs,SPARK-32280,13316295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,dlindelof,dlindelof,11/Jul/20 16:40,24/Jul/20 04:27,13/Jul/23 08:50,23/Jul/20 14:28,2.4.5,,,,,,,,,,,2.4.7,3.0.1,3.1.0,,PySpark,,,,0,,,,"I've come across a curious {{AnalysisException}} thrown in one of my SQL queries, even though the SQL appears legitimate. I was able to reduce it to this example:
{code:python}
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

spark.sql('SELECT 1 AS id').createOrReplaceTempView('A')

spark.sql('''
 SELECT id,
 'foo' AS kind
 FROM A''').createOrReplaceTempView('B')

spark.sql('''
 SELECT l.id
 FROM B AS l
 JOIN B AS r
 ON l.kind = r.kind''').createOrReplaceTempView('C')

spark.sql('''
 SELECT 0
 FROM (
   SELECT *
   FROM B
   JOIN C
   USING (id))
 JOIN (
   SELECT *
   FROM B
   JOIN C
   USING (id))
 USING (id)''')
{code}
Running this yields the following error:

{code}
 py4j.protocol.Py4JJavaError: An error occurred while calling o20.sql.
: org.apache.spark.sql.AnalysisException: Resolved attribute(s) kind#11 missing from id#10,kind#2,id#7,kind#5 in operator !Join Inner, (kind#11 = kind#5). Attribute(s) with the same name appear in the operation: kind. Please check if the right attribute(s) are used.;;
Project [0 AS 0#15]
+- Project [id#0, kind#2, kind#11]
   +- Join Inner, (id#0 = id#14)
      :- SubqueryAlias `__auto_generated_subquery_name`
      :  +- Project [id#0, kind#2]
      :     +- Project [id#0, kind#2]
      :        +- Join Inner, (id#0 = id#9)
      :           :- SubqueryAlias `b`
      :           :  +- Project [id#0, foo AS kind#2]
      :           :     +- SubqueryAlias `a`
      :           :        +- Project [1 AS id#0]
      :           :           +- OneRowRelation
      :           +- SubqueryAlias `c`
      :              +- Project [id#9]
      :                 +- Join Inner, (kind#2 = kind#5)
      :                    :- SubqueryAlias `l`
      :                    :  +- SubqueryAlias `b`
      :                    :     +- Project [id#9, foo AS kind#2]
      :                    :        +- SubqueryAlias `a`
      :                    :           +- Project [1 AS id#9]
      :                    :              +- OneRowRelation
      :                    +- SubqueryAlias `r`
      :                       +- SubqueryAlias `b`
      :                          +- Project [id#7, foo AS kind#5]
      :                             +- SubqueryAlias `a`
      :                                +- Project [1 AS id#7]
      :                                   +- OneRowRelation
      +- SubqueryAlias `__auto_generated_subquery_name`
         +- Project [id#14, kind#11]
            +- Project [id#14, kind#11]
               +- Join Inner, (id#14 = id#10)
                  :- SubqueryAlias `b`
                  :  +- Project [id#14, foo AS kind#11]
                  :     +- SubqueryAlias `a`
                  :        +- Project [1 AS id#14]
                  :           +- OneRowRelation
                  +- SubqueryAlias `c`
                     +- Project [id#10]
                        +- !Join Inner, (kind#11 = kind#5)
                           :- SubqueryAlias `l`
                           :  +- SubqueryAlias `b`
                           :     +- Project [id#10, foo AS kind#2]
                           :        +- SubqueryAlias `a`
                           :           +- Project [1 AS id#10]
                           :              +- OneRowRelation
                           +- SubqueryAlias `r`
                              +- SubqueryAlias `b`
                                 +- Project [id#7, foo AS kind#5]
                                    +- SubqueryAlias `a`
                                       +- Project [1 AS id#7]
                                          +- OneRowRelation
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:43)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:369)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
{code}",,apachespark,cloud_fan,dlindelof,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 17:07:40 UTC 2020,,,,,,,,,,"0|z0goco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/20 02:00;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29166;;;","23/Jul/20 14:28;cloud_fan;Issue resolved by pull request 29166
[https://github.com/apache/spark/pull/29166];;;","23/Jul/20 17:07;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29208;;;",,,,,,,,,,,,,,,,,,,,,,,
Hive may fail to detect Hadoop version when using isolated classloader,SPARK-32256,13315934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,zsxwing,zsxwing,09/Jul/20 15:50,12/Dec/22 18:10,13/Jul/23 08:50,10/Jul/20 12:15,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"Spark allows the user to set `spark.sql.hive.metastore.jars` to specify jars to access Hive Metastore. These jars are loaded by the isolated classloader. Because we also share Hadoop classes with the isolated classloader, the user doesn't need to add Hadoop jars to `spark.sql.hive.metastore.jars`, which means when we are using the isolated classloader, hadoop-common jar is not available in this case. If Hadoop VersionInfo is not initialized before we switch to the isolated classloader, and we try to initialize it using the isolated classloader (the current thread context classloader), it will fail and report `Unknown` which causes Hive to throw the following exception:

{code}
java.lang.RuntimeException: Illegal Hadoop Version: Unknown (expected A.B.* format)
	at org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion(ShimLoader.java:147)
	at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:122)
	at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:88)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(ObjectStore.java:377)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:268)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:517)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:482)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:544)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:370)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:219)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:67)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1548)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3080)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3108)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3349)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:217)
	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:204)
	at org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:331)
	at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:292)
	at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:262)
	at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:247)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:543)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:511)
	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:175)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:128)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:301)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:431)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:324)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:71)
	at org.apache.spark.sql.hive.client.HadoopVersionInfoSuite.$anonfun$new$1(HadoopVersionInfoSuite.scala:63)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)

{code}

Technically, This is indeed an issue of Hadoop VersionInfo which has been fixed: https://issues.apache.org/jira/browse/HADOOP-14067. But since we are still supporting old Hadoop versions, we should fix it.

Why this issue starts to happen in Spark 3.0.0?

In Spark 2.4.x, we use Hive 1.2.1 by default. It will trigger `VersionInfo` initialization in the static codes of `Hive` class. This will happen when we load `HiveClientImpl` class because `HiveClientImpl.clent` method refers to `Hive` class. At this moment, the thread context classloader is not using the isolcated classloader, so it can access hadoop-common jar on the classpath and initialize it correctly.

In Spark 3.0.0, we use Hive 2.3.7. The static codes of `Hive` class are not accessing `VersionInfo` because of the change in https://issues.apache.org/jira/browse/HIVE-11657. Instead, accessing `VersionInfo` happens when creating a `Hive` object (See the above stack trace). This happens here https://github.com/apache/spark/blob/v3.0.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala#L260. But we switch to the isolated classloader before calling `HiveClientImpl.client` (See https://github.com/apache/spark/blob/v3.0.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala#L283). This is exactly what I mentioned above: `If Hadoop VersionInfo is not initialized before we switch to the isolated classloader, and we try to initialize it using the isolated classloader (the current thread context classloader), it will fail`

I marked this is a blocker because it's a regression in 3.0.0 caused by upgrading Hive execution version from 1.2.1 to 2.3.7.",,apachespark,codingcat,dongjoon,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-14067,SPARK-32058,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 10 12:15:08 UTC 2020,,,,,,,,,,"0|z0gm48:",9223372036854775807,,,,,,,,,,,,,3.0.1,,,,,,,,,,"09/Jul/20 16:24;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/29059;;;","09/Jul/20 16:25;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/29059;;;","09/Jul/20 16:48;dongjoon;Thank you so much, [~zsxwing].
According to the PR description, this doesn't happen with Apache Spark 3.0.0 with Hadoop 3.2 distribution.;;;","10/Jul/20 00:49;zsxwing;Yep. This doesn't happen in Hadoop 3.1.0 and above ;;;","10/Jul/20 12:15;gurwls223;Issue resolved by pull request 29059
[https://github.com/apache/spark/pull/29059];;;",,,,,,,,,,,,,,,,,,,,,
fix SQL keyword document,SPARK-32251,13315907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,09/Jul/20 13:51,23/Jul/20 06:50,13/Jul/23 08:50,10/Jul/20 22:11,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,,,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 06:50:59 UTC 2020,,,,,,,,,,"0|z0gly8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/20 13:55;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29055;;;","09/Jul/20 13:55;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29055;;;","10/Jul/20 22:11;dongjoon;Issue resolved by pull request 29055
[https://github.com/apache/spark/pull/29055];;;","23/Jul/20 06:50;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29200;;;",,,,,,,,,,,,,,,,,,,,,,
HiveSessionCatalog call super.makeFunctionExpression should show error message,SPARK-32243,13315890,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,09/Jul/20 12:59,06/Oct/20 14:17,13/Jul/23 08:50,06/Oct/20 13:09,3.0.0,,,,,,,,,,,3.1.0,,,,SQL,,,,0,,,,"When we create a UDAF function use class extended UserDefinedAggregetFunction,  when we call the function,  in support hive mode, in HiveSessionCatalog, it will call super.makeFunctionExpression, 

but it will catch error  such as the function need 2 parameter and we only give 1, throw exception only show 
{code:java}
No handler for UDF/UDAF/UDTF xxxxxxxx
{code}
This is confused , we should show error thrown by super method too.",,angerszhuuu,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 14:17:39 UTC 2020,,,,,,,,,,"0|z0glug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/20 13:10;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29054;;;","09/Jul/20 13:11;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29054;;;","06/Oct/20 13:09;cloud_fan;Issue resolved by pull request 29054
[https://github.com/apache/spark/pull/29054];;;","06/Oct/20 14:17;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29955;;;","06/Oct/20 14:17;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29955;;;",,,,,,,,,,,,,,,,,,,,,
Fix flakiness of CliSuite,SPARK-32242,13315883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,09/Jul/20 12:44,10/Jul/20 04:12,13/Jul/23 08:50,10/Jul/20 04:12,3.1.0,,,,,,,,,,,3.1.0,,,,SQL,Tests,,,0,,,,"A lot of build failures happened in recent builds, which showed that CliSuite is very flaky.

While the flakiness depends on the power of the machine, it'd be better if we can make the suite less flaky even on slow machine.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 10 04:12:56 UTC 2020,,,,,,,,,,"0|z0glsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/20 12:58;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29036;;;","09/Jul/20 12:59;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/29036;;;","10/Jul/20 04:12;kabhwan;Issue resolved by pull request 29036
[https://github.com/apache/spark/pull/29036];;;",,,,,,,,,,,,,,,,,,,,,,,
Use Utils.getSimpleName to avoid hitting Malformed class name in ScalaUDF,SPARK-32238,13315809,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Ngone51,Ngone51,Ngone51,09/Jul/20 07:19,11/Jul/20 13:46,13/Jul/23 08:50,11/Jul/20 13:46,2.0.2,2.1.3,2.2.3,2.3.4,2.4.6,3.0.0,,,,,,2.4.7,3.0.1,3.1.0,,SQL,,,,0,,,,"*BEFORE*
{code:java}
object MalformedClassObject extends Serializable {
    class MalformedFunction extends (String => Int) with Serializable {
      override def apply(v1: String): Int = v1.toInt / 0
    }
}
Seq(""20"").toDF(""col"").select(udf(new MalformedFunction).apply(Column(""col""))).collect()

An exception or error caused a run to abort: Malformed class name 
java.lang.InternalError: Malformed class name
	at java.lang.Class.getSimpleName(Class.java:1330)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF.udfErrorMessage$lzycompute(ScalaUDF.scala:1157)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF.udfErrorMessage(ScalaUDF.scala:1155)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF.doGenCode(ScalaUDF.scala:1077)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:147)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:142)
	at org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:160)
	at org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:69)
	....
{code}

*AFTER*
{code}
org.apache.spark.SparkException: Failed to execute user defined function(UDFSuite$MalformedClassObject$MalformedNonPrimitiveFunction: (string) => int)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:753)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:464)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:467)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ArithmeticException: / by zero
	at org.apache.spark.sql.UDFSuite$MalformedClassObject$MalformedNonPrimitiveFunction.apply(UDFSuite.scala:677)
	at org.apache.spark.sql.UDFSuite$MalformedClassObject$MalformedNonPrimitiveFunction.apply(UDFSuite.scala:676)
	... 17 more
{code}",,apachespark,dongjoon,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 11 13:46:48 UTC 2020,,,,,,,,,,"0|z0glcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/20 07:30;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/29050;;;","11/Jul/20 13:46;dongjoon;Issue resolved by pull request 29050
[https://github.com/apache/spark/pull/29050];;;",,,,,,,,,,,,,,,,,,,,,,,,
Cannot resolve column when put hint in the views of common table expression,SPARK-32237,13315797,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,kernelforce,kernelforce,09/Jul/20 06:38,05/Aug/20 07:02,13/Jul/23 08:50,24/Jul/20 03:48,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"Suppose we have a table:
{code:sql}
CREATE TABLE DEMO_DATA (
  ID VARCHAR(10),
  NAME VARCHAR(10),
  BATCH VARCHAR(10),
  TEAM VARCHAR(1)
) STORED AS PARQUET;
{code}
and some data in it:
{code:sql}
0: jdbc:hive2://HOSTNAME:10000> SELECT T.* FROM DEMO_DATA T;
+-------+---------+-------------+---------+
| t.id  | t.name  |   t.batch   | t.team  |
+-------+---------+-------------+---------+
| 1     | mike    | 2020-07-08  | A       |
| 2     | john    | 2020-07-07  | B       |
| 3     | rose    | 2020-07-06  | B       |
| ....                                    |
+-------+---------+-------------+---------+
{code}
If I put query hint in va or vb and run it in spark-shell:
{code:sql}
sql(""""""
WITH VA AS
 (SELECT T.ID, T.NAME, T.BATCH, T.TEAM 
    FROM DEMO_DATA T WHERE T.TEAM = 'A'),
VB AS
 (SELECT /*+ REPARTITION(3) */ T.ID, T.NAME, T.BATCH, T.TEAM
    FROM VA T)
SELECT T.ID, T.NAME, T.BATCH, T.TEAM 
  FROM VB T
"""""").show
{code}
In Spark-2.4.4 it works fine.
 But in Spark-3.0.0, it throws AnalysisException with Unrecognized hint warning:
{code:scala}
20/07/09 13:51:14 WARN analysis.HintErrorLogger: Unrecognized hint: REPARTITION(3)
org.apache.spark.sql.AnalysisException: cannot resolve '`T.ID`' given input columns: [T.BATCH, T.ID, T.NAME, T.TEAM]; line 8 pos 7;
'Project ['T.ID, 'T.NAME, 'T.BATCH, 'T.TEAM]
+- SubqueryAlias T
   +- SubqueryAlias VB
      +- Project [ID#0, NAME#1, BATCH#2, TEAM#3]
         +- SubqueryAlias T
            +- SubqueryAlias VA
               +- Project [ID#0, NAME#1, BATCH#2, TEAM#3]
                  +- Filter (TEAM#3 = A)
                     +- SubqueryAlias T
                        +- SubqueryAlias spark_catalog.default.demo_data
                           +- Relation[ID#0,NAME#1,BATCH#2,TEAM#3] parquet

  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:143)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:140)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:333)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:333)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:106)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:118)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:118)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:129)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:134)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.immutable.List.map(List.scala:298)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:134)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:139)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:139)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:106)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:140)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:92)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:177)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:92)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:89)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:130)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:156)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:153)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:68)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:133)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:133)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:68)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:606)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:601)
  ... 56 elided
{code}
I think the analysis procedure should not be disturbed even though there was any hint could not be recognized.","Hadoop-2.7.7
Hive-2.3.6
Spark-3.0.0",apachespark,cloud_fan,cltlfcjin,kernelforce,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,SPARK-32347,SPARK-32535,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 03:48:44 UTC 2020,,,,,,,,,,"0|z0gl9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/20 04:09;cltlfcjin;Thanks to report this. I am going to fix it.;;;","10/Jul/20 04:37;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29062;;;","23/Jul/20 07:50;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29201;;;","23/Jul/20 07:50;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29201;;;","24/Jul/20 03:48;cloud_fan;Issue resolved by pull request 29201
[https://github.com/apache/spark/pull/29201];;;",,,,,,,,,,,,,,,,,,,,,
Spark sql commands are failing on select Queries for the  orc tables,SPARK-32234,13315713,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,saurabhc100,saurabhc100,saurabhc100,08/Jul/20 19:38,14/Aug/20 11:22,13/Jul/23 08:50,16/Jul/20 13:12,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"Spark sql commands are failing on select Queries for the orc tables

Steps to reproduce

 
{code:java}
val table = """"""CREATE TABLE `date_dim` (
  `d_date_sk` INT,
  `d_date_id` STRING,
  `d_date` TIMESTAMP,
  `d_month_seq` INT,
  `d_week_seq` INT,
  `d_quarter_seq` INT,
  `d_year` INT,
  `d_dow` INT,
  `d_moy` INT,
  `d_dom` INT,
  `d_qoy` INT,
  `d_fy_year` INT,
  `d_fy_quarter_seq` INT,
  `d_fy_week_seq` INT,
  `d_day_name` STRING,
  `d_quarter_name` STRING,
  `d_holiday` STRING,
  `d_weekend` STRING,
  `d_following_holiday` STRING,
  `d_first_dom` INT,
  `d_last_dom` INT,
  `d_same_day_ly` INT,
  `d_same_day_lq` INT,
  `d_current_day` STRING,
  `d_current_week` STRING,
  `d_current_month` STRING,
  `d_current_quarter` STRING,
  `d_current_year` STRING)
USING orc
LOCATION '/Users/test/tpcds_scale5data/date_dim'
TBLPROPERTIES (
  'transient_lastDdlTime' = '1574682806')""""""

spark.sql(table).collect

val u = """"""select date_dim.d_date_id from date_dim limit 5""""""

spark.sql(u).collect
{code}
 

 

Exception

 
{code:java}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, 192.168.0.103, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1
    at org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader.initBatch(OrcColumnarBatchReader.java:156)
    at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.$anonfun$buildReaderWithPartitionValues$7(OrcFileFormat.scala:258)
    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:141)
    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:203)
    at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)
    at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:620)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:343)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:895)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:895)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:372)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:336)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:133)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:445)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1489)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:448)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)

{code}
 

 

The reason behind this initBatch is not getting the schema that is needed to find out the column value in OrcFileFormat.scala

 
{code:java}
batchReader.initBatch(
 TypeDescription.fromString(resultSchemaString){code}
 

Query is working if 
{code:java}
val u = """"""select * from date_dim limit 5""""""{code}
 ",,apachespark,cloud_fan,ramks,saurabhc100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/20 16:41;saurabhc100;e17f6887c06d47f6a62c0140c1ad569c_000000;https://issues.apache.org/jira/secure/attachment/13007637/e17f6887c06d47f6a62c0140c1ad569c_000000",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 14 11:21:14 UTC 2020,,,,,,,,,,"0|z0gkrs:",9223372036854775807,,,,,,,,,,,,,3.0.1,,,,,,,,,,"08/Jul/20 19:42;saurabhc100;creating the PR for that change [https://github.com/apache/spark/pull/29045];;;","08/Jul/20 19:42;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/29045;;;","08/Jul/20 19:43;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/29045;;;","14/Jul/20 16:41;saurabhc100;Attached the tpcds date_dim orc file created by the hive for testing purpose;;;","16/Jul/20 13:12;cloud_fan;Issue resolved by pull request 29045
[https://github.com/apache/spark/pull/29045];;;","25/Jul/20 07:58;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/29232;;;","09/Aug/20 17:13;ramks;[~saurabhc100] has the fix for this bug been verified? We are observing the same issue as reported here when we upgraded to Spark_3.0 and would like to patch the fix on our product.

Our ORC source file contains three fields:  _col1 string,_col2 string,_col3 string and reading it fails with the below exception:

java.lang.ArrayIndexOutOfBoundsException: 1
 at org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader.initBatch(OrcColumnarBatchReader.java:183)
 at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.$anonfun$buildReaderWithPartitionValues$2(OrcFileFormat.scala:216)

Thanks and Regards,

Ramakrishna;;;","10/Aug/20 04:15;saurabhc100;[~ramks] - We faced the similar issue in spark3.0.0. After applying this fix , we are not facing this issue.;;;","14/Aug/20 11:21;ramks;Thanks [~saurabhc100] I am going ahead and merging these changes to our product which is on Spark_3.0. I hope there is no regression or side effects due to these changes. Just wanted to know why this bug is still in resolved state. Is any test still pending to be run? Thank you.;;;",,,,,,,,,,,,,,,,,
IllegalArgumentException: MultilayerPerceptronClassifier_... parameter solver given invalid value auto,SPARK-32232,13315667,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,st_60,st_60,08/Jul/20 15:57,11/Jul/20 15:38,13/Jul/23 08:50,11/Jul/20 15:38,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,ML,,,,0,,,,"I believe I have discovered a bug when loading MultilayerPerceptronClassificationModel in spark 3.0.0, scala 2.1.2 which I have tested and can see is not there in at least Spark 2.4.3, Scala 2.11.  (I'm not sure if the Scala version is important).

 

I am using pyspark on a databricks cluster and importing the library  ""from pyspark.ml.classification import MultilayerPerceptronClassificationModel""

 

When running model=MultilayerPerceptronClassificationModel.(""load"") and then model. transform (df) I get the following error: IllegalArgumentException: MultilayerPerceptronClassifier_8055d1368e78 parameter solver given invalid value auto.

 

 

This issue can be easily replicated by running the example given on the spark documents: [http://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier]

 

Then adding a save model, load model and transform statement as such:

 

*from* *pyspark.ml.classification* *import* MultilayerPerceptronClassifier

*from* *pyspark.ml.evaluation* *import* MulticlassClassificationEvaluator

 

_# Load training data_

data = spark.read.format(""libsvm"")\

    .load(""data/mllib/sample_multiclass_classification_data.txt"")

 

_# Split the data into train and test_

splits = data.randomSplit([0.6, 0.4], 1234)

train = splits[0]

test = splits[1]

 

_# specify layers for the neural network:_

_# input layer of size 4 (features), two intermediate of size 5 and 4_

_# and output of size 3 (classes)_

layers = [4, 5, 4, 3]

 

_# create the trainer and set its parameters_

trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)

 

_# train the model_

model = trainer.fit(train)

 

_# compute accuracy on the test set_

result = model.transform(test)

predictionAndLabels = result.select(""prediction"", ""label"")

evaluator = MulticlassClassificationEvaluator(metricName=""accuracy"")

*print*(""Test set accuracy = "" + str(evaluator.evaluate(predictionAndLabels)))

 

*from* *pyspark.ml.classification* *import* MultilayerPerceptronClassifier, MultilayerPerceptronClassificationModel

model.save(Save_location)

model2. MultilayerPerceptronClassificationModel.load(Save_location)

 

result_from_loaded = model2.transform(test)

 ",,apachespark,huaxingao,st_60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 11 15:38:00 UTC 2020,,,,,,,,,,"0|z0gkhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/20 17:13;srowen;A few more notes: the equivalent code in Scala works fine, so it's related to the Pyspark wrappers.
The serialized model looks fine even in this case; it records a default solver param of 'l-bfgs' and no overridden value.
When loaded back, the solver is 'auto', the default in HasSolver. I think something in MultilayerPerceptronParams doesn't quite override this as intended. It does have a solver member but not sure if something else is missing.;;;","09/Jul/20 08:02;huaxingao;I think the problem is this:

model2.transform(df) -> wrapper._transfer_params_to_java -> get the default value for solver which is 'auto', this causes Exception because it is an invalid solver.

On python side, we set solver default value to 'l-bfgs' in MultilayerPerceptron, but we don't set solver default value in MultilayerPerceptronModel. This makes sense because for fitted model, we don't need the solver value any more, so no need to set it. However, since we don't set default value for solver in MultilayerPerceptronModel, when calling wrapper._transfer_params_to_java, the default value for solver in HasSolver is used, which is 'auto', and this caused Exception because it is an invalid solver. 
Seems to me that we don't need to set solver or set any of the params for MultilayerPerceptronModel. I will fix this problem. 
;;;","09/Jul/20 08:46;st_60;[~huaxingao]  will your intended fix remove the ability to retrieve the{color:#172b4d} MultilayerPerceptronModel{color} params using the get<Param> method or will that still exist? (e.g. model.getSolver() {color:#555555}Out[12]: 'l-bfgs'{color})

--this is still quite useful functionality ;;;","09/Jul/20 17:36;huaxingao;[~st_60]No, I will not remove the getter.
I think over, seems all I need to do is to make sure that Python and Scala have the same solver default value for MultilayerPerceptronModel.
I will submit a PR soon.;;;","09/Jul/20 19:15;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/29060;;;","11/Jul/20 15:38;srowen;Issue resolved by pull request 29060
[https://github.com/apache/spark/pull/29060];;;",,,,,,,,,,,,,,,,,,,,
Bug in load-spark-env.cmd  with Spark 3.0.0,SPARK-32227,13315593,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ibobak,ibobak,ibobak,08/Jul/20 09:55,12/Dec/22 18:11,13/Jul/23 08:50,30/Jul/20 12:48,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,Spark Shell,,,,0,,,,"spark-env.cmd  which is located in conf  is not loaded by load-spark-env.cmd.

 

*How to reproduce:*

1) download spark 3.0.0 without hadoop and extract it

2) put a file conf/spark-env.cmd with the following contents (paths are relative to where my hadoop is - in C:\opt\hadoop\hadoop-3.2.1, you may need to change):

 

SET JAVA_HOME=C:\opt\Java\jdk1.8.0_241
 SET HADOOP_HOME=C:\opt\hadoop\hadoop-3.2.1
 SET HADOOP_CONF_DIR=C:\opt\hadoop\hadoop-3.2.1\conf
 SET SPARK_DIST_CLASSPATH=C:\opt\hadoop\hadoop-3.2.1\etc\hadoop;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\common;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\common\lib*;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\common*;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\hdfs;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\hdfs\lib*;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\hdfs*;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\yarn;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\yarn\lib*;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\yarn*;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\mapreduce\lib*;C:\opt\hadoop\hadoop-3.2.1\share\hadoop\mapreduce*

 

3) go to the bin directory and run pyspark.   You will get an error that log4j can't be found, etc. (reason: the environment was not loaded indeed, it doesn't see where hadoop with all its jars is).

 

*How to fix:*

just take the load-spark-env.cmd  from Spark version 2.4.3, and everything will work.

[UPDATE]:  I attached a fixed version of load-spark-env.cmd  that works fine.

 

*What is the difference?*

I am not a good specialist in Windows batch, but doing a function

:LoadSparkEnv
 if exist ""%SPARK_CONF_DIR%\spark-env.cmd"" (
  call ""%SPARK_CONF_DIR%\spark-env.cmd""
 )

and then calling it (as it was in 2.4.3) helps.

 

 ",Windows 10,apachespark,ibobak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/20 10:02;ibobak;load-spark-env.cmd;https://issues.apache.org/jira/secure/attachment/13007288/load-spark-env.cmd",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 30 12:48:14 UTC 2020,,,,,,,,,,"0|z0gk20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/20 16:20;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/29044;;;","08/Jul/20 16:21;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/29044;;;","30/Jul/20 12:48;gurwls223;Fixed in https://github.com/apache/spark/pull/29044;;;",,,,,,,,,,,,,,,,,,,,,,,
Cartesian Product Hint cause data error,SPARK-32220,13315551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,08/Jul/20 06:12,14/Jul/20 01:30,13/Jul/23 08:50,10/Jul/20 16:05,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,correctness,,,"{code:java}
spark-sql> select * from test4 order by a asc;
1 2
Time taken: 1.063 seconds, Fetched 4 row(s)20/07/08 14:11:25 INFO SparkSQLCLIDriver: Time taken: 1.063 seconds, Fetched 4 row(s)
spark-sql>select * from test5 order by a asc
1 2
2 2
Time taken: 1.18 seconds, Fetched 24 row(s)20/07/08 14:13:59 INFO SparkSQLCLIDriver: Time taken: 1.18 seconds, Fetched 24 row(s)spar
spark-sql>select /*+ shuffle_replicate_nl(test4) */ * from test4 join test5 where test4.a = test5.a order by test4.a asc ;
1 2 1 2
1 2 2 2
Time taken: 0.351 seconds, Fetched 2 row(s)
20/07/08 14:18:16 INFO SparkSQLCLIDriver: Time taken: 0.351 seconds, Fetched 2 row(s){code}",,angerszhuuu,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 14 01:30:43 UTC 2020,,,,,,,,,,"0|z0gjso:",9223372036854775807,,,,,,,,,,,,,3.0.1,,,,,,,,,,"08/Jul/20 06:18;angerszhuuu;raise a pr soon;;;","08/Jul/20 07:07;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29035;;;","09/Jul/20 22:25;dongjoon;According to the PR test case, I labeled this as a correctness issue and raised to `Blocker` for 3.0.1. Thanks, [~angerszhuuu].;;;","10/Jul/20 16:05;dongjoon;This is resolved via https://github.com/apache/spark/pull/29035;;;","10/Jul/20 23:16;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29070;;;","11/Jul/20 13:25;dongjoon;This lands at branch-3.0 via https://github.com/apache/spark/pull/29070;;;","13/Jul/20 08:14;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29084;;;","14/Jul/20 01:30;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/29093;;;",,,,,,,,,,,,,,,,,,
"The type conversion function generated in makeFromJava for ""other""  type uses a wrong variable.",SPARK-32214,13315498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,07/Jul/20 21:59,12/Dec/22 18:11,13/Jul/23 08:50,08/Jul/20 08:48,2.4.6,3.0.0,3.1.0,,,,,,,,,2.4.7,3.0.1,3.1.0,,SQL,,,,0,,,,"`makeFromJava` in `EvaluatePython` create a type conversion function for some Java/Scala types.

For `other` type, the parameter of the type conversion function is named `obj` but `other` is mistakenly used rather than `obj` in the function body.
{code:java}
case other => (obj: Any) => nullSafeConvert(other)(PartialFunction.empty) {code}",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 08:48:08 UTC 2020,,,,,,,,,,"0|z0gjh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/20 22:10;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29029;;;","07/Jul/20 22:10;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29029;;;","08/Jul/20 08:48;gurwls223;Fixed in https://github.com/apache/spark/pull/29029;;;",,,,,,,,,,,,,,,,,,,,,,,
Failed to serialize large MapStatuses,SPARK-32210,13315412,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kazuyukitanimura,yumwang,yumwang,07/Jul/20 13:45,16/Aug/21 16:13,13/Jul/23 08:50,16/Aug/21 16:13,2.3.4,2.4.8,3.0.3,3.1.2,3.3.0,,,,,,,3.2.0,,,,Spark Core,,,,0,,,,"Driver side exception:
{noformat}
20/07/07 02:22:26,366 ERROR [map-output-dispatcher-3] spark.MapOutputTrackerMaster:91 :
java.lang.NegativeArraySizeException
        at org.apache.commons.io.output.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:322)
        at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:984)
        at org.apache.spark.ShuffleStatus$$anonfun$serializedMapStatus$2.apply$mcV$sp(MapOutputTracker.scala:228)
        at org.apache.spark.ShuffleStatus$$anonfun$serializedMapStatus$2.apply(MapOutputTracker.scala:222)
        at org.apache.spark.ShuffleStatus$$anonfun$serializedMapStatus$2.apply(MapOutputTracker.scala:222)
        at org.apache.spark.ShuffleStatus.withWriteLock(MapOutputTracker.scala:72)
        at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:222)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:493)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
20/07/07 02:22:26,366 ERROR [map-output-dispatcher-5] spark.MapOutputTrackerMaster:91 :
java.lang.NegativeArraySizeException
        at org.apache.commons.io.output.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:322)
        at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:984)
        at org.apache.spark.ShuffleStatus$$anonfun$serializedMapStatus$2.apply$mcV$sp(MapOutputTracker.scala:228)
        at org.apache.spark.ShuffleStatus$$anonfun$serializedMapStatus$2.apply(MapOutputTracker.scala:222)
        at org.apache.spark.ShuffleStatus$$anonfun$serializedMapStatus$2.apply(MapOutputTracker.scala:222)
        at org.apache.spark.ShuffleStatus.withWriteLock(MapOutputTracker.scala:72)
        at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:222)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:493)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
20/07/07 02:22:26,366 ERROR [map-output-dispatcher-2] spark.MapOutputTrackerMaster:91 :
{noformat}",,apachespark,dongjoon,sandeep.katta2007,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 16 16:13:01 UTC 2021,,,,,,,,,,"0|z0giy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/21 15:10;dongjoon;There is a new observation of this situation in 3.1 from [~kazuyukitanimura].;;;","16/Jul/21 15:11;dongjoon;Please feel free to work on this, [~kazuyukitanimura] and ping me after you make a PR.;;;","12/Aug/21 11:03;apachespark;User 'kazuyukitanimura' has created a pull request for this issue:
https://github.com/apache/spark/pull/33721;;;","16/Aug/21 16:13;dongjoon;Issue resolved by pull request 33721
[https://github.com/apache/spark/pull/33721];;;",,,,,,,,,,,,,,,,,,,,,,
Fix the order between initialization for ExecutorPlugin and starting heartbeat thread,SPARK-32175,13315046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,05/Jul/20 20:59,31/Jul/20 01:35,13/Jul/23 08:50,29/Jul/20 13:47,3.0.0,3.1.0,,,,,,,,,,3.0.1,3.1.0,,,Spark Core,,,,0,,,,"In the current master, heartbeat thread in a executor starts after plugin initialization so if the initialization takes long time, heartbeat is not sent to driver and the executor will be removed from cluster.",,apachespark,lucacanali,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 31 01:35:29 UTC 2020,,,,,,,,,,"0|z0ggoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/20 21:09;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29002;;;","05/Jul/20 21:10;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29002;;;","30/Jul/20 22:08;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29313;;;","31/Jul/20 01:35;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/29314;;;",,,,,,,,,,,,,,,,,,,,,,
DSv2 SQL overwrite incorrectly uses static plan with hidden partitions,SPARK-32168,13314947,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rdblue,rdblue,rdblue,03/Jul/20 20:45,24/Nov/20 00:06,13/Jul/23 08:50,08/Jul/20 23:09,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,correctness,,,"The v2 analyzer rule {{ResolveInsertInto}} tries to detect when a static overwrite and a dynamic overwrite would produce the same result and will choose to use static overwrite in that case. It will only use a dynamic overwrite if there is a partition column without a static value and the SQL mode is set to dynamic.

{code:lang=scala}
val dynamicPartitionOverwrite = partCols.size > staticPartitions.size &&
          conf.partitionOverwriteMode == PartitionOverwriteMode.DYNAMIC
{code}

The problem is that {{partCols}} are the names of only partitions that are in the column list (identity partitions) and does not include hidden partitions, like {{days(ts)}}. As a result, this doesn't detect hidden partitions and use dynamic overwrite. Static overwrite is used instead; when a table has only hidden partitions, the static filter drops all table data.

This is a correctness bug because Spark will overwrite more data than just the set of partitions being written to in dynamic mode. The impact is limited because this rule is only used for SQL queries (not plans from DataFrameWriters) and only affects tables with hidden partitions.",,apachespark,dongjoon,rdblue,viirya,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33524,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 23:09:40 UTC 2020,,,,,,,,,,"0|z0gg2o:",9223372036854775807,,,,,,,,,,,,,3.0.1,,,,,,,,,,"03/Jul/20 20:57;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/28993;;;","08/Jul/20 23:09;dongjoon;Issue resolved by pull request 28993
[https://github.com/apache/spark/pull/28993];;;",,,,,,,,,,,,,,,,,,,,,,,,
nullability of GetArrayStructFields is incorrect,SPARK-32167,13314849,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,cloud_fan,cloud_fan,03/Jul/20 08:21,08/Jul/20 20:32,13/Jul/23 08:50,07/Jul/20 03:10,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.6,3.0.0,,,,,2.4.7,3.0.1,3.1.0,,SQL,,,,0,correctness,,,"The following should be `Array([WrappedArray(1, null)])` instead of `Array([WrappedArray(1, 0)])`
{code:java}
import scala.collection.JavaConverters._
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{ArrayType, StructType}

val innerStruct = new StructType().add(""i"", ""int"", nullable = true)
val schema = new StructType().add(""arr"", ArrayType(innerStruct, containsNull = false))
val df = spark.createDataFrame(List(Row(Seq(Row(1), Row(null)))).asJava, schema)
df.select($""arr"".getField(""i"")).collect
{code}
 ",,apachespark,cloud_fan,dongjoon,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 20:32:33 UTC 2020,,,,,,,,,,"0|z0gfgw:",9223372036854775807,,,,,,,,,,,,,2.4.7,3.0.1,,,,,,,,,"03/Jul/20 08:31;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/28992;;;","04/Jul/20 17:38;dongjoon;Thank you. I raised the issue as a `Blocker` for `2.4.7/3.0.1` since this is a correctness issue.;;;","04/Jul/20 18:03;dongjoon;With the above example, I verified 2.4.0 and 2.4.5 also have this bug while 2.3.4 doesn't have this. I updated the `Affected Versions`.;;;","04/Jul/20 18:11;dongjoon;After looking at the code, I realize that this is a very old code path from 1.5.0 or older. This should be the following. But 2.3.4 and older also have this bug.
{code:java}
scala> df.select($""arr"".getField(""i"")).printSchema
root
 |-- arr.i: array (nullable = true)
 |    |-- element: integer (containsNull = true){code};;;","07/Jul/20 03:10;dongjoon;This is resolved via https://github.com/apache/spark/pull/28992;;;","07/Jul/20 06:43;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29019;;;","07/Jul/20 06:44;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/29019;;;","08/Jul/20 20:32;dongjoon;This lands at branch-2.4 via https://github.com/apache/spark/pull/29019 .;;;",,,,,,,,,,,,,,,,,,
Nested pruning should still work for nested column extractors of attributes with cosmetic variations,SPARK-32163,13314805,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,03/Jul/20 02:14,08/Jul/20 02:19,13/Jul/23 08:50,07/Jul/20 18:18,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"If the expressions extracting nested fields have cosmetic variations like qualifier difference, currently nested column pruning cannot work well.

For example, two attributes which are semantically the same, are referred in a query, but the nested column extractors of them are treated differently when we deal with nested column pruning.




",,apachespark,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 02:19:21 UTC 2020,,,,,,,,,,"0|z0gf74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/20 02:20;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/28988;;;","07/Jul/20 18:18;dongjoon;Issue resolved by pull request 28988
[https://github.com/apache/spark/pull/28988];;;","07/Jul/20 18:46;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/29027;;;","08/Jul/20 02:19;dongjoon;This lands at branch-3.0 via https://github.com/apache/spark/pull/29027 .;;;",,,,,,,,,,,,,,,,,,,,,,
Executors should not be able to create SparkContext.,SPARK-32160,13314787,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,02/Jul/20 23:24,12/Dec/22 18:10,13/Jul/23 08:50,09/Jul/20 06:53,2.4.6,3.0.0,,,,,,,,,,3.0.1,3.1.0,,,Spark Core,,,,0,,,,"Currently executors can create SparkContext, but shouldn't be able to create it.
{code:scala}
sc.range(0, 1).foreach { _ =>
  new SparkContext(new SparkConf().setAppName(""test"").setMaster(""local""))
}
{code}",,apachespark,dongjoon,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 03 21:34:02 UTC 2020,,,,,,,,,,"0|z0gf34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/20 23:34;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28986;;;","02/Jul/20 23:35;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28986;;;","09/Jul/20 06:53;gurwls223;Issue resolved by pull request 28986
[https://github.com/apache/spark/pull/28986];;;","29/Jul/20 01:53;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29278;;;","29/Jul/20 01:54;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29278;;;","29/Jul/20 23:49;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29294;;;","29/Jul/20 23:49;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29294;;;","03/Aug/20 18:34;dongjoon;This seems to exist at all old Spark versions. Could you confirm that, [~ueshin] and [~hyukjin.kwon]?;;;","03/Aug/20 21:33;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29341;;;","03/Aug/20 21:34;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/29340;;;",,,,,,,,,,,,,,,,
New udaf(Aggregator) has an integration bug with UnresolvedMapObjects serialization,SPARK-32159,13314757,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eje,eje,eje,02/Jul/20 18:05,09/Jul/20 08:42,13/Jul/23 08:50,09/Jul/20 08:42,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,,,,0,,,,"The new user defined aggregator feature (SPARK-27296) based on calling 'functions.udaf(aggregator)' works fine when the aggregator input type is atomic, e.g. 'Aggregator[Double, _, _]', however if the input type is an array, like 'Aggregator[Array[Double], _, _]',  it is tripping over the following:

/**
 * When constructing [[MapObjects]], the element type must be given, which may not be available
 * before analysis. This class acts like a placeholder for [[MapObjects]], and will be replaced by
 * [[MapObjects]] during analysis after the input data is resolved.
 * Note that, ideally we should not serialize and send unresolved expressions to executors, but
 * users may accidentally do this(e.g. mistakenly reference an encoder instance when implementing
 * Aggregator). Here we mark `function` as transient because it may reference scala Type, which is
 * not serializable. Then even users mistakenly reference unresolved expression and serialize it,
 * it's just a performance issue(more network traffic), and will not fail.
 */
 case class UnresolvedMapObjects(
 {color:#de350b}@transient function: Expression => Expression{color},
 child: Expression,
 customCollectionCls: Option[Class[_]] = None) extends UnaryExpression with Unevaluable {
 override lazy val resolved = false

override def dataType: DataType = customCollectionCls.map(ObjectType.apply).getOrElse

{ throw new UnsupportedOperationException(""not resolved"") }

}

 

*The '@transient' is causing the function to be unpacked as 'null' over on the executors, and it is causing a null-pointer exception here, when it tries to do 'function(loopVar)'*

object MapObjects {
 def apply(
 function: Expression => Expression,
 inputData: Expression,
 elementType: DataType,
 elementNullable: Boolean = true,
 customCollectionCls: Option[Class[_]] = None): MapObjects =

{ val loopVar = LambdaVariable(""MapObject"", elementType, elementNullable) MapObjects(loopVar, {color:#de350b}function(loopVar){color}, inputData, customCollectionCls) }

}

*I believe it may be possible to just use 'loopVar' instead of 'function(loopVar)', whenever 'function' is null, but need second opinion from catalyst developers on what a robust fix should be*",,apachespark,cloud_fan,dongjoon,eje,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 08:42:59 UTC 2020,,,,,,,,,,"0|z0gewg:",9223372036854775807,,,,,,,,,,,,,3.0.1,,,,,,,,,,"02/Jul/20 18:11;eje;cc [~cloud_fan];;;","02/Jul/20 21:22;apachespark;User 'erikerlandson' has created a pull request for this issue:
https://github.com/apache/spark/pull/28983;;;","02/Jul/20 21:23;apachespark;User 'erikerlandson' has created a pull request for this issue:
https://github.com/apache/spark/pull/28983;;;","02/Jul/20 21:33;dongjoon;Hi, [~eje]. Shall we set `Target Version` to `3.0.1`?;;;","09/Jul/20 08:42;cloud_fan;Issue resolved by pull request 28983
[https://github.com/apache/spark/pull/28983];;;",,,,,,,,,,,,,,,,,,,,,
.m2 repository corruption happens,SPARK-32153,13314612,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,shaneknapp,sarutak,sarutak,02/Jul/20 07:57,25/Sep/20 01:34,13/Jul/23 08:50,25/Sep/20 01:34,2.4.8,3.0.1,3.1.0,,,,,,,,,,,,,Project Infra,,,,0,,,,"Build task on Jenkins-worker4 often fails with dependency problem.
[https://github.com/apache/spark/pull/28971#issuecomment-652570066]
[https://github.com/apache/spark/pull/28971#issuecomment-652611025]
[https://github.com/apache/spark/pull/28971#issuecomment-652690849] [https://github.com/apache/spark/pull/28971#issuecomment-652611025]
[https://github.com/apache/spark/pull/28942#issuecomment-652842960]
[https://github.com/apache/spark/pull/28942#issuecomment-652835679]

These can be related to .m2 corruption.

 ",,sarutak,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 25 01:10:56 UTC 2020,,,,,,,,,,"0|z0ge5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/20 07:57;sarutak;[~shaneknapp] Could you look into this?;;;","06/Jul/20 17:09;shaneknapp;i wiped the m2 directory from worker-04;;;","06/Jul/20 17:56;sarutak;Thanks [~shaneknapp]!;;;","22/Sep/20 14:26;sarutak;[~shaneknapp]This issue seems to happen again especially for branch-2.4.

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/128982/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/128981/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/128976/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/128966/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/128982/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/128966/

 Could you help us?|;;;","24/Sep/20 17:12;shaneknapp;i wiped all of the .m2 dirs on the centos workers.  seems like that cleared things up.;;;","25/Sep/20 01:10;sarutak;[~shaneknapp]Thank you so much!;;;",,,,,,,,,,,,,,,,,,,,
./bin/spark-sql got error with reading hive metastore,SPARK-32152,13314611,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jodang99,jodang99,02/Jul/20 07:47,23/Jul/20 04:50,13/Jul/23 08:50,23/Jul/20 04:50,3.0.0,,,,,,,,,,,,,,,SQL,,,,13,,,,"1. Fist of all, I built Spark3.0.0 from source with below command.
{quote}{{./build/mvn -Pyarn -Phive -Phive-thriftserver -Dskip Tests clean package}}
{quote}
2. I set the ${SPARK_HOME}/conf/spark-defaults.conf as below.
{quote}spark.sql.hive.metastore.version    2.1.1

spark.sql.hive.metastore.jars    {color:#ff0000}maven{color}
{quote}
3. There is no problem to run ""${SPARK_HOME}/bin/spark-sql""

4. For production environment, I copied all downloaded jar files from maven to ${SPARK_HOME}/lib/

5. I changed ${SPARK_HOME}/conf/spark-defaluts.conf as below.
{quote}spark.sql.hive.metastore.jars   {color:#ff0000}${SPARK_HOME}/lib/{color}
{quote}
6. Then I got error running command ./bin/spark-sql as below.
{quote}Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/metadata/HiveException
{quote}
I found out that HiveException class is in the hive-exec-XXX.jar...

Spark 3.0.0 was built with hive 2.3.7 by default, and I could find ""hive-exec-2.3.7-core.jar"" after I finished build. And I could find hive-exec-2.1.1.jar downloaded from maven when I use ""spark.sql.hive.metastore.jars maven"" in the spark-defaults.conf.

 

I thought that there are some conflict between hive 2.1.1 and hive 2.3.7 when I set the {color:#7a869a}spark.sql.hive.metastore.jars   ${SPARK_HOME}/lib/.{color}

 ","Spark 3.0.0

Hive 2.1.1",jodang99,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,Thu Jul 23 04:49:47 UTC 2020,,,,,,,,,,"0|z0ge54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/20 04:49;jodang99;I got the hint in manual below.

[https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html#interacting-with-different-versions-of-hive-metastore]

You can close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
LEFT JOIN generating non-deterministic and unexpected result (regression in Spark 3.0),SPARK-32148,13314462,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kabhwan,hiddenbit,hiddenbit,01/Jul/20 12:01,09/Jul/20 07:39,13/Jul/23 08:50,09/Jul/20 07:39,3.0.0,,,,,,,,,,,3.0.1,3.1.0,,,SQL,Structured Streaming,,,0,correctness,,,"When upgrading from Spark 2.4.6 to 3.0.0 I found that previously working LEFT JOINs now output unexpected results.

Below is a minimal example to run in {{spark-shell}} to demonstrate this. In it there are 3 events on the left side of the join and two on the right.
 The expected output should contain two matching pairs and one item on the left side without a matching right side, so that it should be output with the right side fields being {{NULL}}. The join condition is that event times must be max. 30sec apart and the IDs must match.
{code:scala}
import spark.implicits._
import org.apache.spark.sql.Encoders
import org.apache.spark.sql.functions.expr
import org.apache.spark.sql.streaming.OutputMode
import java.sql.Timestamp
import java.util.UUID

// Structure of left and right data items
case class LeftEntry(eventTime: Timestamp, id: String, comment: String)
case class RightEntry(eventTime: Timestamp, id: String, name: String)

// Some test data
val leftData = Vector(
  LeftEntry(Timestamp.valueOf(""2020-01-01 00:00:00""), ""abc"", ""has no join partner""),
  LeftEntry(Timestamp.valueOf(""2020-01-02 00:00:00""), ""abc"", ""joined with A""),
  LeftEntry(Timestamp.valueOf(""2020-01-02 01:00:00""), ""abc"", ""joined with B"")
)

val rightData = Vector(
  RightEntry(Timestamp.valueOf(""2020-01-02 00:00:10""), ""abc"", ""A""),
  RightEntry(Timestamp.valueOf(""2020-01-02 00:59:59""), ""abc"", ""B"")
)

// Write test data, that we will stream from later (random output directories; alternatively we could delete the directories after each run)
val leftFilePath = s""/tmp/demo-left-data-${UUID.randomUUID()}""
spark.createDataset(leftData).write.format(""parquet"").save(leftFilePath)
val rightFilePath = s""/tmp/demo-right-data-${UUID.randomUUID()}""
spark.createDataset(rightData).write.format(""parquet"").save(rightFilePath)

// Read data from Parquet as stream
val leftStream = spark.readStream
  .schema(Encoders.product[LeftEntry].schema)
  .parquet(leftFilePath)
  .withWatermark(""eventTime"", ""2 minutes"")
val rightStream = spark.readStream
  .schema(Encoders.product[RightEntry].schema)
  .parquet(rightFilePath)
  .withWatermark(""eventTime"", ""4 minutes"")

// Define Join
val joinExpression = expr(
  s""""""
     |leftStream.id = rightStream.id AND
     |leftStream.eventTime BETWEEN
     |  rightStream.eventTime - INTERVAL 30 seconds AND
     |  rightStream.eventTime + INTERVAL 30 seconds
    """""".stripMargin
)
val joinedData = leftStream.as(""leftStream"")
  .join(
    rightStream.as(""rightStream""),
    joinExpression,
    ""left""
  )

// Run query
val query = joinedData.writeStream
  .format(""memory"")
  .queryName(""myQuery"")
  .outputMode(OutputMode.Append())
  .start()
query.processAllAvailable()

// Print results
spark
  .table(query.name)
  .show(truncate = false)
{code}
When this is executed with Spark 2.4.6, the result is as expected and deterministic:
{noformat}
+-------------------+---+-------------------+-------------------+----+----+
|eventTime          |id |comment            |eventTime          |id  |name|
+-------------------+---+-------------------+-------------------+----+----+
|2020-01-02 00:00:00|abc|joined with A      |2020-01-02 00:00:10|abc |A   |
|2020-01-02 01:00:00|abc|joined with B      |2020-01-02 00:59:59|abc |B   |
|2020-01-01 00:00:00|abc|has no join partner|null               |null|null|  ← as expected
+-------------------+---+-------------------+-------------------+----+----+
{noformat}
When running the same code snippet with Spark 3.0.0, the result is non-deterministically one of these two:
{noformat}
+-------------------+---+-------------+-------------------+----+----+
|eventTime          |id |comment      |eventTime          |id  |name|
+-------------------+---+-------------+-------------------+----+----+
|2020-01-02 01:00:00|abc|joined with B|2020-01-02 00:59:59|abc |B   |
|2020-01-02 00:00:00|abc|joined with A|2020-01-02 00:00:10|abc |A   |
|2020-01-02 00:00:00|abc|joined with A|null               |null|null|  ← this entry was already joined with ""A"" above,
+-------------------+---+-------------+-------------------+----+----+    but is now here once more without it's right join side
{noformat}
{noformat}
+-------------------+---+-------------+-------------------+----+----+
|eventTime          |id |comment      |eventTime          |id  |name|
+-------------------+---+-------------+-------------------+----+----+
|2020-01-02 00:00:00|abc|joined with A|2020-01-02 00:00:10|abc |A   |
|2020-01-02 01:00:00|abc|joined with B|2020-01-02 00:59:59|abc |B   |
|2020-01-02 01:00:00|abc|joined with B|null               |null|null|  ← this entry was already joined with ""B"" above,
+-------------------+---+-------------+-------------------+----+----+    but is now here once more without it's right join side
{noformat}
... with {{""has no join partner""}} completely missing, and instead one of the actually joinable left-side items repeated without the right-side fields.
----
In case the input data is modified, so that the non-joinable event additionally has a different ID, then Spark 3.0 generates correct output:
{code:scala}
// [...]
val leftData = Vector(
  LeftEntry(Timestamp.valueOf(""2020-01-01 00:00:00""), ""ddd"", ""has no join partner""),
                                                    // ↑↑↑ changed
  LeftEntry(Timestamp.valueOf(""2020-01-02 00:00:00""), ""abc"", ""joined with A""),
  LeftEntry(Timestamp.valueOf(""2020-01-02 01:00:00""), ""abc"", ""joined with B"")
)
// [...]
{code}
{noformat}
+-------------------+---+-------------------+-------------------+----+----+
|eventTime          |id |comment            |eventTime          |id  |name|
+-------------------+---+-------------------+-------------------+----+----+
|2020-01-02 00:00:00|abc|joined with A      |2020-01-02 00:00:10|abc |A   |
|2020-01-02 01:00:00|abc|joined with B      |2020-01-02 00:59:59|abc |B   |
|2020-01-01 00:00:00|ddd|has no join partner|null               |null|null|
+-------------------+---+-------------------+-------------------+----+----+
{noformat}",,angerszhuuu,apachespark,cloud_fan,hiddenbit,kabhwan,medb,warrenzhu25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 07:39:55 UTC 2020,,,,,,,,,,"0|z0gd80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/20 00:20;kabhwan;Looking into it. Looks like a problem indeed (if then this should be a blocker) but want to make it sure.;;;","02/Jul/20 02:29;cloud_fan;I assume this is for streaming join? I'm marking it as a blocker since it's correctness bug;;;","02/Jul/20 05:55;hiddenbit;[~cloud_fan], yes the issue is with stream-stream joins. Stream-batch joins and pure batch joins seem not to be affected.;;;","02/Jul/20 06:03;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/28975;;;","02/Jul/20 06:03;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/28975;;;","02/Jul/20 06:10;kabhwan;Just submitted a fix. Unfortunately existing UTs don't seem to be affected by the problematic part.

[~hiddenbit] Do you have any other kinds of queries broken by Spark 3.0.0? Your reproducer is very helpful to trace down the issue - we'd be more confident if we have more kinds of queries to verify.;;;","02/Jul/20 06:22;hiddenbit;[~kabhwan] thanks a lot for taking care of this so fast!

After the upgrade we only had one unit-test failing, from which I condensed the reported minimal example. Our other queries seem to work as before.;;;","09/Jul/20 07:39;cloud_fan;Issue resolved by pull request 28975
[https://github.com/apache/spark/pull/28975];;;",,,,,,,,,,,,,,,,,,
