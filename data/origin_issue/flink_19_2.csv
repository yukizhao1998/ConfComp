Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Cloners),Inward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Required),Outward issue link (Required),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Counter metrics are incorrectly reported as total counts to DataDog,FLINK-15438,13276886,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,joern,joern,joern,30/Dec/19 13:41,26/Mar/20 09:54,13/Jul/23 08:10,26/Mar/20 09:54,1.9.1,,,,,,,,,1.11.0,,,,Runtime / Metrics,,,,,0,pull-request-available,,,,"The Flink semantics of a counter are not matching with the counters in DataDog.

In Flink a counter counts the total of increment and decrement calls.
In DataDog a counter is a rate over the reporting interval. 

The Flink implementation of the DataDog reporter seems to send the Flink counter value each time the metrics are reported. Correct would be to send the delta of the counter since the last report.

There are some features in DataDog which are easier to use if this could be fixed, e.g. alerts based on counters.",,joern,x1q1j1,,,,,,,,,,,,"kottmann commented on pull request #11483: [FLINK-15438][metrics][datadog] Report counter deltas and not totals
URL: https://github.com/apache/flink/pull/11483
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The Flink semantics of a counter are not matching with the counters
   in DataDog. In Flink a counter counts the total of increment and
   decrement calls. In DataDog a counter is the number of increment
   and decrement calls during the reporting interval.
   
   ## Brief change log
   
   - Changed DCounter to report a delta since the last report and not the total value
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   There is an existing unit test for the datadog metrics component. No change was done to it.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Mar/20 18:16;githubbot;600","zentol commented on pull request #11483: [FLINK-15438][metrics][datadog] Report counter deltas and not totals
URL: https://github.com/apache/flink/pull/11483
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 09:08;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 09:38:41 UTC 2020,,,,,,,,,,"0|z0a2yg:",9223372036854775807,"The DataDog metrics reporter now reports counts as the number of events over the reporting interval, instead of the total count. This aligns the count semantics with the DataDog documentation.",,,,,,,,,,,,,,,,,,,"30/Dec/19 15:57;x1q1j1;hi [~joern] can you give an example of a specific counter? This can be easily checked and resolved.;;;","30/Dec/19 17:28;joern;Yes, one of the counters I need is numRecordsIn for a specific operator. The current implementation sends always the counter value for the total runtime of the job, and not the counter value of for the reporting interval (the delta since the last report).

I am happy to send a patch to resolve this issue.;;;","30/Dec/19 18:56;joern;I would like to propose to change DCounter to always return the delta in the counter since the last call. To be able to compute it an instance variable with the last count has to be added to DCounter.;;;","02/Jan/20 11:39;chesnay;[~joern] I've assigned the issue to you. Your proposed solution sounds good to me.;;;","06/Jan/20 09:15;joern;I am still working on the issue.

I extended DCounter with instance variables to track the last reported reading to be able to compute the delta for the interval, but the reported metrics in Datadog are still quite a bit different from the values which are displayed in Flink. I need to understand this first before I can send the fix.

;;;","31/Jan/20 11:06;chesnay;[~joern] Were you able to figure out what the problem is?;;;","01/Feb/20 15:49;joern;No, didn't mange to find the exact cause.

The current implementation is sending counts that are really wrong, with my changes this is much better. 

Should I send the PR for it and you could give it a critical review? 

;;;","13/Mar/20 12:06;chesnay;Yes, a PR would be highly appreciated.;;;","26/Mar/20 09:38;chesnay;Delta reporting merged to master:
69f20e929b9f6c58b3220dd41b41b21ad5f4bcdd
2db31ff808da1217ba04438af9530b08b3d33cb6;;;",,,,,,,,,,,,,,,,,,,,,,,
"Start session with property of ""-Dtaskmanager.memory.process.size"" not work",FLINK-15437,13276850,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xtsong,xiaojin.wy,xiaojin.wy,30/Dec/19 10:06,06/Jan/20 16:06,13/Jul/23 08:10,03/Jan/20 03:10,1.10.0,,,,,,,,,1.10.0,,,,Command Line Client,Deployment / YARN,,,,0,pull-request-available,,,,"*The environment:*
Yarn session cmd is as below, and the flink-conf.yaml has not the property of ""taskmanager.memory.process.size"":

export HADOOP_CLASSPATH=`hadoop classpath`;export HADOOP_CONF_DIR=/dump/1/jenkins/workspace/Stream-Spark-3.4/env/hadoop_conf_dirs/blinktest2; export BLINK_HOME=/dump/1/jenkins/workspace/test/blink_daily; $BLINK_HOME/bin/yarn-session.sh -d -qu root.default -nm 'Session Cluster of daily_regression_stream_spark_1.10' -jm 1024 -n 20 -s 10 -Dtaskmanager.memory.process.size=1024m


*After execute the cmd above, there is a exception like this:*
2019-12-30 17:54:57,992 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c07224.sqa.zth.tbsite.net/11.163.188.36:8050
2019-12-30 17:54:58,182 ERROR org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Error while running the Flink session.
org.apache.flink.configuration.IllegalConfigurationException: Either Task Heap Memory size (taskmanager.memory.task.heap.size) and Managed Memory size (taskmanager.memory.managed.size), or Total Flink Memory size (taskmanager.memory.flink.size), or Total Process Memory size (taskmanager.memory.process.size) need to be configured explicitly.
	at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.resourceSpecFromConfig(TaskExecutorResourceUtils.java:145)
	at org.apache.flink.client.deployment.AbstractClusterClientFactory.getClusterSpecification(AbstractClusterClientFactory.java:44)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:557)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.lambda$main$5(FlinkYarnSessionCli.java:803)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1804)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:803)

------------------------------------------------------------
 The program finished with the following exception:

org.apache.flink.configuration.IllegalConfigurationException: Either Task Heap Memory size (taskmanager.memory.task.heap.size) and Managed Memory size (taskmanager.memory.managed.size), or Total Flink Memory size (taskmanager.memory.flink.size), or Total Process Memory size (taskmanager.memory.process.size) need to be configured explicitly.
	at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.resourceSpecFromConfig(TaskExecutorResourceUtils.java:145)
	at org.apache.flink.client.deployment.AbstractClusterClientFactory.getClusterSpecification(AbstractClusterClientFactory.java:44)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:557)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.lambda$main$5(FlinkYarnSessionCli.java:803)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1804)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:803)

*The flink-conf.yaml is :*
jobmanager.rpc.address: localhost
jobmanager.rpc.port: 6123
jobmanager.heap.size: 1024m
taskmanager.memory.total-process.size: 1024m
taskmanager.numberOfTaskSlots: 1
parallelism.default: 1
jobmanager.execution.failover-strategy: region



",,liyu,tison,wangyang0918,xiaojin.wy,xtsong,,,,,,,,,"xintongsong commented on pull request #10728: [FLINK-15437][yarn] Apply dynamic properties early on client side.
URL: https://github.com/apache/flink/pull/10728
 
 
   ## What is the purpose of the change
   
   This PR applies dynamic properties early on client side, to make sure the client uses the correct configurations set via dynamic properties.
   
   ## Brief change log
   
   - 475b6421a1d94a5073e177d217311d9f195307cd: Write yarn properties file without `YarnClusterDescriptor`.
   - 322a65907fc3ee936a1ceca930e5427b235e6dc4: Apply dynamic properties early on client side.
   
   ## Verifying this change
   
   - Updated `FlinkYarnSessionCliTest#testDynamicProperties`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/19 05:16;githubbot;600","TisonKun commented on pull request #10728: [FLINK-15437][yarn] Apply dynamic properties early on client side.
URL: https://github.com/apache/flink/pull/10728
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 03:04;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-15470,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 06 03:12:10 UTC 2020,,,,,,,,,,"0|z0a2qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Dec/19 10:08;xiaojin.wy;[~wuzang];;;","30/Dec/19 10:35;xtsong;[~xiaojin.wy]
I think the problem is that you have '-n 20' in your command, which does not exist anymore.
Once the client see an unknown option, it will stop parsing the remaining options. Thus the '-D' memory option is not applied.
Could you verify whether removing '-n 20' fixes the problem?;;;","30/Dec/19 11:39;xiaojin.wy;[~xintongsong] I delete '-n 20', the same exception appears.;;;","31/Dec/19 02:57;xtsong;I've checked the codes. The problem is that '-D' properties are applied only to the cluster (JM) configuration but not the to client configuration, while the client is using the memory size configuration before deploying the cluster.

I'll try to provide a fix soon.;;;","31/Dec/19 03:12;wangyang0918;I think [~xintongsong] is right. The dynamic properties need to be applied earlier in {{FlinkYarnSessionCli#applyCommandLineOptionsToConfiguration}}. Hence, we will not need to pass the {{$internal.yarn.dynamic-properties}} to {{YarnClusterDescriptor}}. Also the clusterSpecification will be calculated correctly.

For the magic yarn properties file, we should keep the same behavior currently. And it will be removed in the next release. I will create a ticket to track this.;;;","03/Jan/20 03:10;tison;master via 
367765be025a325da52256bb05c773383975114e
267f59017bf14a30bf5f70a94d85f800def92109
d82828d8c8fffcc5afd3a7c79bfdb7e018908546
671f18ba165e60a0f1da0639c51446ef838f18cf

1.10 via
0a89b9a40ffd13595702851909c6bcdfbc8affc1
30717aa04ced1be2024543f8907290ff9bf4b488
ed4e2de24847d3b84168afaa35db7e3ba4102654
02873cba8b2e2cbc29365a063192a8adf2182a6f;;;","03/Jan/20 06:21;tison;[~fly_in_gis] is there a ticket tracking the removal of magic yarn properties file? If so, please link it to this ticket.;;;","03/Jan/20 11:19;wangyang0918;Hi, [~tison] 

I have created a ticket to tracking removing magic properties file. Let's keep discussion there.;;;","05/Jan/20 01:01;tison;[~fly_in_gis] I mean if you have created one, link it here(you can click ""more"" bottom above and add a related issue) and discuss there. So far you said ""I have created one"" but where it is :P;;;","05/Jan/20 01:03;tison;OK I found FLINK-15470 and will link it here.;;;","06/Jan/20 03:12;wangyang0918;Aha, i forget to link it here. 

[~tison] You have found it.(y);;;",,,,,,,,,,,,,,,,,,,,,
ExecutionConfigTests.test_equals_and_hash in pyFlink fails when cpu core numbers is 6,FLINK-15435,13276755,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,libenchao,libenchao,29/Dec/19 10:32,29/Dec/19 15:18,13/Jul/23 08:10,29/Dec/19 15:18,1.10.0,,,,,,,,,1.10.0,1.9.2,,,API / Python,,,,,0,pull-request-available,,,,"My laptop's cpu core number is 6, so the default parallelism of the ExecutionEnvironment is 12. And the test will fail.",,hequn8128,libenchao,,,,,,,,,,,,"libenchao commented on pull request #10715: [FLINK-15435][python] Fix ExecutionConfigTests.test_equals_and_hash when cpu core number is 6
URL: https://github.com/apache/flink/pull/10715
 
 
   ## Brief change log 
   
   Fix ExecutionConfigTests.test_equals_and_hash. It fails when default parallelism is 12.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Dec/19 10:37;githubbot;600","hequn8128 commented on pull request #10715: [FLINK-15435][python] Fix ExecutionConfigTests.test_equals_and_hash when cpu core number is 6
URL: https://github.com/apache/flink/pull/10715
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Dec/19 15:13;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 29 15:17:26 UTC 2019,,,,,,,,,,"0|z0a25c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/19 15:17;hequn8128;Fixed in
1.11.0 via 539e6a09979c2fdc835bb8eac95989a2f57325e3
1.10.0 via 4a0da4c6afd4a1895f95a617ff4fc7723e9124ce
1.9.2 via 2079f8d20d90afaf2da8195d647b0e9e2c005b0e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testResourceManagerConnectionAfterRegainingLeadership test fail when run  azure,FLINK-15434,13276721,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hailong wang,hailong wang,hailong wang,28/Dec/19 23:48,15/Jan/20 09:13,13/Jul/23 08:10,15/Jan/20 09:13,1.9.1,,,,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,,,,"Error message
Expected: <b65797325db6323dd5f8bdfeeaa18467>
 but: was <57f975298d116a9d7623f5a844ce6502>

 
Stack trace
java.lang.AssertionError: Expected: <b65797325db6323dd5f8bdfeeaa18467> but: was <57f975298d116a9d7623f5a844ce6502> at org.apache.flink.runtime.jobmaster.JobMasterTest.testResourceManagerConnectionAfterRegainingLeadership(JobMasterTest.java:1033)",,gjy,hailong wang,liyu,,,,,,,,,,,"wangxlong commented on pull request #10814: [FLINK-15434][Tests]Fix unstable tests in JobMasterTest
URL: https://github.com/apache/flink/pull/10814
 
 
   ## What is the purpose of the change
   
   Fix unstable tests in JobMasterTest
   
   ## Brief change log
   
   Create the jobmaster with heartbeatServices instead of fastHeartbeatServices for some tests.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 12:36;githubbot;600","GJL commented on pull request #10814: [FLINK-15434][Tests]Fix unstable tests in JobMasterTest
URL: https://github.com/apache/flink/pull/10814
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jan/20 09:09;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 15 09:13:33 UTC 2020,,,,,,,,,,"0|z0a1xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Dec/19 23:54;hailong wang;I reproduced this problem on my laptop, and the debug log as follow:
{code:java}
 [flink-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger  - Slf4jLogger started0    [flink-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger  - Slf4jLogger started72   [flink-akka.actor.default-dispatcher-4] DEBUG akka.event.EventStream  - logger log1-Slf4jLogger started79   [flink-akka.actor.default-dispatcher-4] DEBUG akka.event.EventStream  - Default Loggers started1165 [main] INFO  org.apache.flink.runtime.jobmaster.JobMasterTest  - ================================================================================Test testResourceManagerConnectionAfterRegainingLeadership(org.apache.flink.runtime.jobmaster.JobMasterTest) is running.--------------------------------------------------------------------------------4106 [main] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService  - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/jobmanager_0 .4171 [main] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Initializing job (unnamed job) (f781d92dbe44f52505eff1b144e45bb6).4268 [main] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Using restart strategy NoRestartStrategy for (unnamed job) (f781d92dbe44f52505eff1b144e45bb6).4362 [main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Job recovers via failover strategy: full graph restart4433 [main] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Running initialization on master for job (unnamed job) (f781d92dbe44f52505eff1b144e45bb6).4433 [main] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Successfully ran initialization on master in 0 ms.4439 [main] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Adding 0 vertices from job graph (unnamed job) (f781d92dbe44f52505eff1b144e45bb6).4439 [main] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph  - Attaching 0 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.4484 [main] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Successfully created execution graph from job graph (unnamed job) (f781d92dbe44f52505eff1b144e45bb6).4518 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Starting execution of job (unnamed job) (f781d92dbe44f52505eff1b144e45bb6) under job master id 5b6d296cfdedc261c4bbd525aea971b3.4521 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Job (unnamed job) (f781d92dbe44f52505eff1b144e45bb6) switched from state CREATED to RUNNING.4533 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.4537 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.4543 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Connecting to ResourceManager localhost/cc67ffdf-cb14-4f8a-bf85-ddd6f11327b2(5421d4878888c36920c6ee4eb0136801)4556 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Resolved ResourceManager address, beginning registration4557 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Registration at ResourceManager attempt 1 (timeout=100ms)4565 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.4594 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.into 5b6d296cfdedc261c4bbd525aea971b3, f781d92dbe44f52505eff1b144e45bb64619 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - JobManager successfully registered at ResourceManager, leader id: 5421d4878888c36920c6ee4eb0136801.4629 [flink-akka.actor.default-dispatcher-3] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.4656 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.4657 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - The heartbeat of ResourceManager with id 4cce21c1eac0cad2675e364a1d0f41c1 timed out.4668 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Close ResourceManager connection 4cce21c1eac0cad2675e364a1d0f41c1.org.apache.flink.runtime.jobmaster.JobMasterException: The heartbeat of ResourceManager with id 4cce21c1eac0cad2675e364a1d0f41c1 timed out. at org.apache.flink.runtime.jobmaster.JobMaster$ResourceManagerHeartbeatListener.notifyHeartbeatTimeout(JobMaster.java:1179) at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl$HeartbeatMonitor.run(HeartbeatManagerImpl.java:318) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)4679 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Connecting to ResourceManager localhost/cc67ffdf-cb14-4f8a-bf85-ddd6f11327b2(5421d4878888c36920c6ee4eb0136801)4682 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Resolved ResourceManager address, beginning registration4683 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Registration at ResourceManager attempt 1 (timeout=100ms)into 5b6d296cfdedc261c4bbd525aea971b3, f781d92dbe44f52505eff1b144e45bb64682 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.4685 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - JobManager successfully registered at ResourceManager, leader id: 5421d4878888c36920c6ee4eb0136801.4686 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.4713 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Job (unnamed job) (f781d92dbe44f52505eff1b144e45bb6) switched from state RUNNING to SUSPENDED.org.apache.flink.util.FlinkException: Test exception. at org.apache.flink.runtime.jobmaster.JobMasterTest.testResourceManagerConnectionAfterRegainingLeadership(JobMasterTest.java:1025) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)4730 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Job f781d92dbe44f52505eff1b144e45bb6 has been suspended.4731 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl  - Suspending SlotPool.4731 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Close ResourceManager connection 4cce21c1eac0cad2675e364a1d0f41c1.org.apache.flink.util.FlinkException: Test exception. at org.apache.flink.runtime.jobmaster.JobMasterTest.testResourceManagerConnectionAfterRegainingLeadership(JobMasterTest.java:1025) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)4733 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor  - Fencing token not set: Ignoring message LocalFencedMessage(5b6d296cfdedc261c4bbd525aea971b3, org.apache.flink.runtime.rpc.messages.RunAsync@4826bb87) because the fencing token is null.4739 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor  - Fencing token not set: Ignoring message LocalFencedMessage(5b6d296cfdedc261c4bbd525aea971b3, org.apache.flink.runtime.rpc.messages.RunAsync@5ebd3007) because the fencing token is null.4740 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor  - Fencing token not set: Ignoring message LocalFencedMessage(null, org.apache.flink.runtime.rpc.messages.RunAsync@53eb8b7a) because the fencing token is null.4744 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Starting execution of job (unnamed job) (f781d92dbe44f52505eff1b144e45bb6) under job master id 8db9e94add813aacf2794eb6881a1573.4747 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Using restart strategy NoRestartStrategy for (unnamed job) (f781d92dbe44f52505eff1b144e45bb6).4749 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Job recovers via failover strategy: full graph restart4749 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Running initialization on master for job (unnamed job) (f781d92dbe44f52505eff1b144e45bb6).4749 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Successfully ran initialization on master in 0 ms.4749 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Adding 0 vertices from job graph (unnamed job) (f781d92dbe44f52505eff1b144e45bb6).4750 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph  - Attaching 0 topologically sorted vertices to existing job graph with 0 vertices and 0 intermediate results.4750 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Successfully created execution graph from job graph (unnamed job) (f781d92dbe44f52505eff1b144e45bb6).4755 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Job (unnamed job) (f781d92dbe44f52505eff1b144e45bb6) switched from state CREATED to RUNNING.4756 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.5b6d296cfdedc261c4bbd525aea971b34758 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Connecting to ResourceManager localhost/cc67ffdf-cb14-4f8a-bf85-ddd6f11327b2(5421d4878888c36920c6ee4eb0136801)4758 [flink-akka.actor.default-dispatcher-4] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.4758 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Resolved ResourceManager address, beginning registration4758 [flink-akka.actor.default-dispatcher-2] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Registration at ResourceManager attempt 1 (timeout=100ms)5b6d296cfdedc261c4bbd525aea971b3into 8db9e94add813aacf2794eb6881a1573, f781d92dbe44f52505eff1b144e45bb64760 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - JobManager successfully registered at ResourceManager, leader id: 5421d4878888c36920c6ee4eb0136801.4787 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - The heartbeat of ResourceManager with id 4cce21c1eac0cad2675e364a1d0f41c1 timed out.4787 [flink-akka.actor.default-dispatcher-5] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Close ResourceManager connection 4cce21c1eac0cad2675e364a1d0f41c1.org.apache.flink.runtime.jobmaster.JobMasterException: The heartbeat of ResourceManager with id 4cce21c1eac0cad2675e364a1d0f41c1 timed out. at org.apache.flink.runtime.jobmaster.JobMaster$ResourceManagerHeartbeatListener.notifyHeartbeatTimeout(JobMaster.java:1179) at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl$HeartbeatMonitor.run(HeartbeatManagerImpl.java:318) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)4788 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Connecting to ResourceManager localhost/cc67ffdf-cb14-4f8a-bf85-ddd6f11327b2(5421d4878888c36920c6ee4eb0136801)4790 [flink-akka.actor.default-dispatcher-5] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Trigger heartbeat request.4790 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Resolved ResourceManager address, beginning registration4793 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Registration at ResourceManager attempt 1 (timeout=100ms)into 8db9e94add813aacf2794eb6881a1573, f781d92dbe44f52505eff1b144e45bb64796 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - JobManager successfully registered at ResourceManager, leader id: 5421d4878888c36920c6ee4eb0136801.4796 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster  - Stopping the JobMaster for job (unnamed job)(f781d92dbe44f52505eff1b144e45bb6).4798 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Job (unnamed job) (f781d92dbe44f52505eff1b144e45bb6) switched from state RUNNING to SUSPENDED.org.apache.flink.util.FlinkException: JobManager is shutting down. at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:347) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:509) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:175) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)4803 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Job f781d92dbe44f52505eff1b144e45bb6 has been suspended.4803 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl  - Suspending SlotPool.4804 [flink-akka.actor.default-dispatcher-5] DEBUG org.apache.flink.runtime.jobmaster.JobMaster  - Close ResourceManager connection 4cce21c1eac0cad2675e364a1d0f41c1.org.apache.flink.util.FlinkException: JobManager is shutting down. at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:347) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:509) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:175) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)4805 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl  - Stopping SlotPool.4916 [main] ERROR org.apache.flink.runtime.jobmaster.JobMasterTest  - --------------------------------------------------------------------------------Test testResourceManagerConnectionAfterRegainingLeadership(org.apache.flink.runtime.jobmaster.JobMasterTest) failed with:java.lang.AssertionError: Expected: <8db9e94add813aacf2794eb6881a1573>     but: was <5b6d296cfdedc261c4bbd525aea971b3> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8) at org.apache.flink.runtime.jobmaster.JobMasterTest.testResourceManagerConnectionAfterRegainingLeadership(JobMasterTest.java:1034) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
================================================================================4942 [main] INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService  - Stopping Akka RPC service.
java.lang.AssertionError: Expected: <8db9e94add813aacf2794eb6881a1573>     but: was <5b6d296cfdedc261c4bbd525aea971b3>Expected :<8db9e94add813aacf2794eb6881a1573>Actual   :<5b6d296cfdedc261c4bbd525aea971b3><Click to see difference> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8) at org.apache.flink.runtime.jobmaster.JobMasterTest.testResourceManagerConnectionAfterRegainingLeadership(JobMasterTest.java:1034) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)Process finished with exit code 255{code}
According to the log, for the lost heartbeat of ResourceManager, It will reconnect. So the registrationQueue's element will be old JobMasterId.

So I think we can look if registrationQueue is empty, if so, we delete  it and offer new value then.
{code:java}
testingResourceManagerGateway.setRegisterJobManagerConsumer(
   jobMasterIdResourceIDStringJobIDTuple4 -> {
      if (!registrationQueue.isEmpty()) {
         registrationQueue.poll();
      }
      registrationQueue.offer(jobMasterIdResourceIDStringJobIDTuple4.f0);
   });
{code}
 ;;;","02/Jan/20 01:51;hailong wang;Hi [~chesnay], Could you help me to have a look? Thank you.;;;","07/Jan/20 16:42;gjy;[~hailong wang] Can you explain how you reproduced the issue locally? I ran this test more than 10000 times without failure.

If I understand correctly, this is only a problem with the test. Your solution would work but it could also mask out other bugs. Moreover, I think that an AtomicReference would be more suited. Do we know why the heartbeat times out? From what I can see, the timeout is set to only 10 ms, which seems quite low.

cc: [~trohrmann];;;","07/Jan/20 17:00;gjy;[~hailong wang] After talking to [~trohrmann], we think it's best to create the jobmaster with {{heartbeatServices}} instead of {{fastHeartbeatServices}}. One can use JobMasterBuilder for that. Do you want to work on this issue?;;;","09/Jan/20 07:18;hailong wang;Hi [~gjy] , Thank you for assigning this ticket to me. I will change 
{code:java}
final JobMaster jobMaster = createJobMaster(
   configuration,
   jobGraph,
   haServices,
   new TestingJobManagerSharedServicesBuilder().build());
{code}
to 
{code:java}
final JobMaster jobMaster = createJobMaster(
   configuration,
   jobGraph,
   haServices,
   new TestingJobManagerSharedServicesBuilder().build(),
   heartbeatServices);
{code}
BTW, should we do the same  in testReconnectionAfterDisconnect() method?

 ;;;","09/Jan/20 09:35;gjy;[~hailong wang] I assigned you to the issue. We should fix all misusages of the fast heartbeat timeout in JobMasterTest.;;;","09/Jan/20 12:38;hailong wang;[~gjy] OK, I have open a PR.;;;","15/Jan/20 09:13;gjy;1.10: efe266fa9143446a33072a3cdece52c64b83f185
master: e1ad65b3a511fb81425ab5a9364db5661eac1a84;;;",,,,,,,,,,,,,,,,,,,,,,,,
HiveObjectConversion implementations need to handle null values,FLINK-15429,13276613,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,Terry1897,Terry1897,27/Dec/19 12:20,06/Jan/20 16:07,13/Jul/23 08:10,03/Jan/20 04:15,1.10.0,,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"When there is null value of timestamp type in hive table, will have exception like following:


Caused by: org.apache.flink.table.api.TableException: Exception in writeRecord
	at org.apache.flink.table.filesystem.FileSystemOutputFormat.writeRecord(FileSystemOutputFormat.java:122)
	at org.apache.flink.streaming.api.functions.sink.OutputFormatSinkFunction.invoke(OutputFormatSinkFunction.java:87)
	at org.apache.flink.streaming.api.functions.sink.SinkFunction.invoke(SinkFunction.java:52)
	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.pushToOperator(OperatorChain.java:550)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.collect(OperatorChain.java:527)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.collect(OperatorChain.java:487)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:730)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:708)
	at SinkConversion$1.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.pushToOperator(OperatorChain.java:550)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.collect(OperatorChain.java:527)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$ChainingOutput.collect(OperatorChain.java:487)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:730)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:708)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:93)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:196)
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.catalog.hive.client.HiveShimV100.ensureSupportedFlinkTimestamp(HiveShimV100.java:386)
	at org.apache.flink.table.catalog.hive.client.HiveShimV100.toHiveTimestamp(HiveShimV100.java:357)
	at org.apache.flink.table.functions.hive.conversion.HiveInspectors.lambda$getConversion$b054b59b$1(HiveInspectors.java:216)
	at org.apache.flink.table.functions.hive.conversion.HiveInspectors.lambda$getConversion$7f882244$1(HiveInspectors.java:172)
	at org.apache.flink.connectors.hive.HiveOutputFormatFactory$HiveOutputFormat.getConvertedRow(HiveOutputFormatFactory.java:190)
	at org.apache.flink.connectors.hive.HiveOutputFormatFactory$HiveOutputFormat.writeRecord(HiveOutputFormatFactory.java:206)
	at org.apache.flink.connectors.hive.HiveOutputFormatFactory$HiveOutputFormat.writeRecord(HiveOutputFormatFactory.java:178)
	at org.apache.flink.table.filesystem.SingleDirectoryWriter.write(SingleDirectoryWriter.java:52)
	at org.apache.flink.table.filesystem.FileSystemOutputFormat.writeRecord(FileSystemOutputFormat.java:120)
	... 19 more



We should add null check in HiveShim100#ensureSupportedFlinkTimestamp and return a prper value.",,lirui,liyu,lzljs3620320,phoenixjiangnan,Terry1897,,,,,,,,,"lirui-apache commented on pull request #10721: [FLINK-15429][hive] HiveObjectConversion implementations need to hand…
URL: https://github.com/apache/flink/pull/10721
 
 
   …le null values
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix NPEs when writing nulls to a Hive table
   
   
   ## Brief change log
   
     - Add null check to HiveObjectConversion implementations
     - Add test
   
   
   ## Verifying this change
   
   New test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): yes
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Dec/19 08:43;githubbot;600","bowenli86 commented on pull request #10721: [FLINK-15429][hive] HiveObjectConversion implementations need to hand…
URL: https://github.com/apache/flink/pull/10721
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 04:14;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 04:15:56 UTC 2020,,,,,,,,,,"0|z0a19s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 12:21;Terry1897;cc [~lirui];;;","30/Dec/19 07:20;lzljs3620320;[~jark] Can you assign this ticket to [~lirui] ?;;;","30/Dec/19 07:23;lirui;Actually I believe the NPE happens during writing to Hive tables. Need to handle nulls when converting DATE/TIMESTAMP to Hive objects.;;;","30/Dec/19 07:27;lzljs3620320;It seems so many ""HiveObjectConversion"" lack null handling.

But only Date/Timestamp related codes are useful, others are dead codes? Otherwise, there will be NPE too.;;;","30/Dec/19 07:33;lirui;[~lzljs3620320] No it's not dead code. I guess it's simply because we don't have tests to expose the NPEs. I'll see if I can add more tests for that.;;;","30/Dec/19 07:37;lzljs3620320;I have tested UDFs, will not go through these codes.

Good to know where these code will be came from. We can add comments to explain.;;;","30/Dec/19 08:27;lirui;[~lzljs3620320] Writing to Hive table will need to convert Flink objects to Hive objects. That's the most common use case of {{HiveObjectConversion}}. I'll add a test to write null values for all Hive data types.;;;","03/Jan/20 04:15;phoenixjiangnan;master: 1c5806ae443626ce80f2de3bb85100abe9ba8e0c
1.10: 386d5067e723ebfc1f1d13a4aacb277734d36b74;;;",,,,,,,,,,,,,,,,,,,,,,,,
Avro Confluent Schema Registry nightly end-to-end test fails on travis,FLINK-15428,13276604,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,guoyangze,liyu,liyu,27/Dec/19 11:53,31/Dec/19 07:51,13/Jul/23 08:10,31/Dec/19 07:51,1.10.0,,,,,,,,,1.10.0,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,"Avro Confluent Schema Registry nightly end-to-end test fails with below error:
{code}
Could not start confluent schema registry
/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/kafka-common.sh: line 78: ./bin/kafka-server-stop: No such file or directory
No zookeeper server to stop
Tried to kill 1549 but never saw it die
[FAIL] Test script contains errors.
{code}

https://api.travis-ci.org/v3/job/629699437/log.txt",,becket_qin,guoyangze,liyu,,,,,,,,,,,"KarmaGYZ commented on pull request #10720: [FLINK-15428][e2e] Fix the error command for stopping kafka cluster
URL: https://github.com/apache/flink/pull/10720
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This PR fix the error command for stopping kafka cluster. This issue cause the failure of retrying execute given command in `retry_times_with_backoff_and_cleanup`.
   
   ## Brief change log
   
   - Fix the error command for stopping kafka cluster
   
   ## Verifying this change
   
   Now, the `retry_times_with_backoff_and_cleanup` function should work correctly. I'll give the travis link when I reproduce such scenario.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Dec/19 07:42;githubbot;600","becketqin commented on pull request #10720: [FLINK-15428][e2e] Fix the error command for stopping kafka cluster and exclude kafka 1.10 related test under JDK profile
URL: https://github.com/apache/flink/pull/10720
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/19 07:49;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-15442,,,FLINK-13567,FLINK-13528,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 31 07:51:50 UTC 2019,,,,,,,,,,"0|z0a17s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 12:35;liyu;Not sure whether it reproduces FLINK-13567 or is some new issue, link the two together.;;;","30/Dec/19 02:50;guoyangze;I found that if the cleanup command return 1, the retry_times_with_backoff_and_cleanup function will not retry the given command as expected. Thus, we should either wrap the error of cleanup command or make sure it will not fail. 
I'd like to work on it.;;;","30/Dec/19 03:35;guoyangze;BTW, the error message shows that there is missing "".sh"" for ""./bin/kafka-server-stop"";;;","30/Dec/19 06:52;liyu;Another two instances:
https://api.travis-ci.org/v3/job/630372597/log.txt
https://api.travis-ci.org/v3/job/630652321/log.txt;;;","30/Dec/19 07:01;guoyangze;Since the root cause is the missing of "".sh"" for ""./bin/kafka-server-stop"", I'd like to fix that problem in this issue.

Regarding the failure of cleanup command, I prefer to discuss with [~chesnay] and [~trohrmann] about the exact contract of `retry_times_with_backoff_and_cleanup` before bringing any change. After discussion, I'll open another ticket to track it.;;;","30/Dec/19 08:10;guoyangze;Another instance after fix the wrong command:
https://api.travis-ci.org/v3/job/630846725/log.txt

Thus, we need to change the way to execute cleanup command in ""retry_times_with_backoff_and_cleanup"" function.;;;","30/Dec/19 09:27;guoyangze;FYI, I found it only produced under JDK 11 profile. As discussed in https://issues.apache.org/jira/browse/FLINK-13528 , I'll disable this test under JDK 11 profile.;;;","31/Dec/19 05:07;guoyangze;For not blocking the release progress, I open [FLINK-15442|https://issues.apache.org/jira/browse/FLINK-15442] to address some lower level issue related to this test. 

I'll only fix the root cause of the failure under this ticket.;;;","31/Dec/19 07:51;becket_qin;Merged.

Master: 0c0dc79548fb4414e8515517a03158a416808705

release-1.10: ed56d66c0597064bd77d1d2183cb221ff01c2da9;;;",,,,,,,,,,,,,,,,,,,,,,,
State TTL RocksDb backend end-to-end test stalls on travis,FLINK-15427,13276603,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,klion26,liyu,liyu,27/Dec/19 11:48,03/Jan/20 10:49,13/Jul/23 08:10,03/Jan/20 10:49,1.10.0,,,,,,,,,1.10.0,,,,Runtime / State Backends,,,,,0,pull-request-available,test-stability,,,"The 'State TTL RocksDb backend end-to-end test' case stalls and finally timedout with error message:
{noformat}
The job exceeded the maximum log length, and has been terminated.
{noformat}

https://api.travis-ci.org/v3/job/629699416/log.txt",,klion26,liyu,trohrmann,,,,,,,,,,,"tillrohrmann commented on pull request #10726: [FLINK-15427][Statebackend][test] Check TTL test in test_stream_statettl.sh and skip the exception check
URL: https://github.com/apache/flink/pull/10726
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 10:25;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15403,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 10:49:51 UTC 2020,,,,,,,,,,"0|z0a17k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 11:49;klion26;I'll  take a look at this issue;;;","28/Dec/19 11:23;klion26;This test itself passed, but failed when {{checking exceptions}}
{code:java}
Checking for errors...^M
Found error in log files:^M
{code}
after executing the command to find the exception we'll get
{code:java}
2019-12-27 05:18:34,743 ERROR org.apache.flink.streaming.runtime.tasks.StreamTask           - Received CancelTaskException while we are not canceled. This is a bug and should be reported^M
org.apache.flink.runtime.execution.CancelTaskException: Consumed partition PipelinedSubpartitionView(index: 2) of ResultPartition 745fd76b3c0327b1b0732bb14045de1c@2e06db5ab07dfc5dabc32576a9a40a0f has been released.^M
        at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.getNextBuffer(LocalInputChannel.java:190)^M
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:509)^M
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:487)^M
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:475)^M
        at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:75)^M
        at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:125)^M
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)^M
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)^M
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)^M
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)^M
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:488)^M
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)^M
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)^M

{code}
This {{CancelException}} was introduced by [~AHeise] in FLINK-15317, and I think FLINK-15403 has already been tracking it.

For this issue, it wants to verify state ttl, if the verify failed, it will print something in the stdout. we used the {{check exceptions(which will check whether the `.out` file is empty or not)}} to do this thing.

So, I propose to check in the {{test_stream_state__ttl}}.sh other than {{delegating to the exceptions check}}, the reason is something like FLINK-15105.

we'll add some checking logic in the end of {{test_stream_state_ttl.sh}}  such as below and skip exception check for this test. [~azagrebin]
{code:java}
if grep ""TtlVerificationContext{"" $FLINK_DIR/log/*.out > /dev/null; then
   exit 1; # contains the output
fi
{code};;;","30/Dec/19 06:50;liyu;Another instance: https://api.travis-ci.org/v3/job/630372588/log.txt;;;","03/Jan/20 10:49;trohrmann;Fixed via

master: 
552a07677470d2df620aeb4e681e829f31d1526f
897e13793aa05a07ebb946e2198ee7b5d0c74e79

1.10.0: 
239c1640d5d09d89152980fe92f13a9e0d19186c
a7eec5cfffd4d22aaf31efd976540338bb95c7a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPC-DS end-to-end test (Blink planner) fails on travis,FLINK-15426,13276600,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,liyu,liyu,27/Dec/19 11:43,28/Dec/19 05:16,13/Jul/23 08:10,28/Dec/19 05:16,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,"TPC-DS end-to-end test (Blink planner) fails on travis with below error:
{code}
The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Field types of query result and registered TableSink default_catalog.default_database.query2_sinkTable do not match.
...
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
{code}

https://api.travis-ci.org/v3/job/629699422/log.txt",,jark,leonard,liyu,,,,,,,,,,,"wuchong commented on pull request #10707: [FLINK-15426][e2e] Fix TPC-DS end-to-end test (Blink planner) fails on travis
URL: https://github.com/apache/flink/pull/10707
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   FLINK-15313 forbids to use TypeInformation defined in blink planner
   internally as the output type of TableSink (physical type and logical
   type not equal). However, the TPC-DS frameworks breaks this. The fixing
   is to use DataTypes which is the standard way.
   
   ## Brief change log
   
   Update TPC-DS framework to use DataTypes to create `CsvTableSink`. 
   
   ## Verifying this change
   
   Verified the TPC-DS end to end test in my local machine.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 14:44;githubbot;600","wuchong commented on pull request #10707: [FLINK-15426][e2e] Fix TPC-DS end-to-end test (Blink planner) fails on travis
URL: https://github.com/apache/flink/pull/10707
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Dec/19 03:54;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 28 05:16:16 UTC 2019,,,,,,,,,,"0|z0a16w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 11:44;liyu;[~ykt836] [~jark] [~lzljs3620320] FYI.;;;","27/Dec/19 14:46;jark;The root cause is:


{code}
Caused by: org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink default_catalog.default_database.query2_sinkTable do not match.
Query schema: [d_week_seq1: INT, EXPR$1: DECIMAL(35, 2), EXPR$2: DECIMAL(35, 2), EXPR$3: DECIMAL(35, 2), EXPR$4: DECIMAL(35, 2), EXPR$5: DECIMAL(35, 2), EXPR$6: DECIMAL(35, 2), EXPR$7: DECIMAL(35, 2)]
Sink schema: [d_week_seq1: INT, EXPR$1: LEGACY('RAW', 'ANY<java.math.BigDecimal, rO0ABXNyADtvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkJpZ0RlY2ltYWxUeXBlSW5mbwAAAAAAAAABAgACSQAJcHJlY2lzaW9uSQAFc2NhbGV4cgAyb3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGVpbmZvLkJhc2ljVHlwZUluZm_6BPCCpWndBgIABEwABWNsYXp6dAARTGphdmEvbGFuZy9DbGFzcztMAA9jb21wYXJhdG9yQ2xhc3NxAH4AAlsAF3Bvc3NpYmxlQ2FzdFRhcmdldFR5cGVzdAASW0xqYXZhL2xhbmcvQ2xhc3M7TAAKc2VyaWFsaXplcnQANkxvcmcvYXBhY2hlL2ZsaW5rL2FwaS9jb21tb24vdHlwZXV0aWxzL1R5cGVTZXJpYWxpemVyO3hyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZWluZm8uVHlwZUluZm9ybWF0aW9ulI3ISLqzeusCAAB4cHZyABRqYXZhLm1hdGguQmlnRGVjaW1hbFTHFVf5gShPAwACSQAFc2NhbGVMAAZpbnRWYWx0ABZMamF2YS9tYXRoL0JpZ0ludGVnZXI7eHIAEGphdmEubGFuZy5OdW1iZXKGrJUdC5TgiwIAAHhwdnIAO29yZy5hcGFjaGUuZmxpbmsuYXBpLmNvbW1vbi50eXBldXRpbHMuYmFzZS5CaWdEZWNDb21wYXJhdG9yAAAAAAAAAAECAAB4cgA-b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJhc2ljVHlwZUNvbXBhcmF0b3IAAAAAAAAAAQIAAloAE2FzY2VuZGluZ0NvbXBhcmlzb25bAAtjb21wYXJhdG9yc3QAN1tMb3JnL2FwYWNoZS9mbGluay9hcGkvY29tbW9uL3R5cGV1dGlscy9UeXBlQ29tcGFyYXRvcjt4cgA0b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5UeXBlQ29tcGFyYXRvcgAAAAAAAAABAgAAeHB1cgASW0xqYXZhLmxhbmcuQ2xhc3M7qxbXrsvNWpkCAAB4cAAAAABzcgA7b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJpZ0RlY1NlcmlhbGl6ZXIAAAAAAAAAAQIAAHhyAEJvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLmJhc2UuVHlwZVNlcmlhbGl6ZXJTaW5nbGV0b255qYeqxy53RQIAAHhyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLlR5cGVTZXJpYWxpemVyAAAAAAAAAAECAAB4cAAAACMAAAAC>'), EXPR$2: LEGACY('RAW', 'ANY<java.math.BigDecimal, rO0ABXNyADtvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkJpZ0RlY2ltYWxUeXBlSW5mbwAAAAAAAAABAgACSQAJcHJlY2lzaW9uSQAFc2NhbGV4cgAyb3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGVpbmZvLkJhc2ljVHlwZUluZm_6BPCCpWndBgIABEwABWNsYXp6dAARTGphdmEvbGFuZy9DbGFzcztMAA9jb21wYXJhdG9yQ2xhc3NxAH4AAlsAF3Bvc3NpYmxlQ2FzdFRhcmdldFR5cGVzdAASW0xqYXZhL2xhbmcvQ2xhc3M7TAAKc2VyaWFsaXplcnQANkxvcmcvYXBhY2hlL2ZsaW5rL2FwaS9jb21tb24vdHlwZXV0aWxzL1R5cGVTZXJpYWxpemVyO3hyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZWluZm8uVHlwZUluZm9ybWF0aW9ulI3ISLqzeusCAAB4cHZyABRqYXZhLm1hdGguQmlnRGVjaW1hbFTHFVf5gShPAwACSQAFc2NhbGVMAAZpbnRWYWx0ABZMamF2YS9tYXRoL0JpZ0ludGVnZXI7eHIAEGphdmEubGFuZy5OdW1iZXKGrJUdC5TgiwIAAHhwdnIAO29yZy5hcGFjaGUuZmxpbmsuYXBpLmNvbW1vbi50eXBldXRpbHMuYmFzZS5CaWdEZWNDb21wYXJhdG9yAAAAAAAAAAECAAB4cgA-b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJhc2ljVHlwZUNvbXBhcmF0b3IAAAAAAAAAAQIAAloAE2FzY2VuZGluZ0NvbXBhcmlzb25bAAtjb21wYXJhdG9yc3QAN1tMb3JnL2FwYWNoZS9mbGluay9hcGkvY29tbW9uL3R5cGV1dGlscy9UeXBlQ29tcGFyYXRvcjt4cgA0b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5UeXBlQ29tcGFyYXRvcgAAAAAAAAABAgAAeHB1cgASW0xqYXZhLmxhbmcuQ2xhc3M7qxbXrsvNWpkCAAB4cAAAAABzcgA7b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJpZ0RlY1NlcmlhbGl6ZXIAAAAAAAAAAQIAAHhyAEJvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLmJhc2UuVHlwZVNlcmlhbGl6ZXJTaW5nbGV0b255qYeqxy53RQIAAHhyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLlR5cGVTZXJpYWxpemVyAAAAAAAAAAECAAB4cAAAACMAAAAC>'), EXPR$3: LEGACY('RAW', 'ANY<java.math.BigDecimal, rO0ABXNyADtvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkJpZ0RlY2ltYWxUeXBlSW5mbwAAAAAAAAABAgACSQAJcHJlY2lzaW9uSQAFc2NhbGV4cgAyb3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGVpbmZvLkJhc2ljVHlwZUluZm_6BPCCpWndBgIABEwABWNsYXp6dAARTGphdmEvbGFuZy9DbGFzcztMAA9jb21wYXJhdG9yQ2xhc3NxAH4AAlsAF3Bvc3NpYmxlQ2FzdFRhcmdldFR5cGVzdAASW0xqYXZhL2xhbmcvQ2xhc3M7TAAKc2VyaWFsaXplcnQANkxvcmcvYXBhY2hlL2ZsaW5rL2FwaS9jb21tb24vdHlwZXV0aWxzL1R5cGVTZXJpYWxpemVyO3hyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZWluZm8uVHlwZUluZm9ybWF0aW9ulI3ISLqzeusCAAB4cHZyABRqYXZhLm1hdGguQmlnRGVjaW1hbFTHFVf5gShPAwACSQAFc2NhbGVMAAZpbnRWYWx0ABZMamF2YS9tYXRoL0JpZ0ludGVnZXI7eHIAEGphdmEubGFuZy5OdW1iZXKGrJUdC5TgiwIAAHhwdnIAO29yZy5hcGFjaGUuZmxpbmsuYXBpLmNvbW1vbi50eXBldXRpbHMuYmFzZS5CaWdEZWNDb21wYXJhdG9yAAAAAAAAAAECAAB4cgA-b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJhc2ljVHlwZUNvbXBhcmF0b3IAAAAAAAAAAQIAAloAE2FzY2VuZGluZ0NvbXBhcmlzb25bAAtjb21wYXJhdG9yc3QAN1tMb3JnL2FwYWNoZS9mbGluay9hcGkvY29tbW9uL3R5cGV1dGlscy9UeXBlQ29tcGFyYXRvcjt4cgA0b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5UeXBlQ29tcGFyYXRvcgAAAAAAAAABAgAAeHB1cgASW0xqYXZhLmxhbmcuQ2xhc3M7qxbXrsvNWpkCAAB4cAAAAABzcgA7b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJpZ0RlY1NlcmlhbGl6ZXIAAAAAAAAAAQIAAHhyAEJvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLmJhc2UuVHlwZVNlcmlhbGl6ZXJTaW5nbGV0b255qYeqxy53RQIAAHhyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLlR5cGVTZXJpYWxpemVyAAAAAAAAAAECAAB4cAAAACMAAAAC>'), EXPR$4: LEGACY('RAW', 'ANY<java.math.BigDecimal, rO0ABXNyADtvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkJpZ0RlY2ltYWxUeXBlSW5mbwAAAAAAAAABAgACSQAJcHJlY2lzaW9uSQAFc2NhbGV4cgAyb3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGVpbmZvLkJhc2ljVHlwZUluZm_6BPCCpWndBgIABEwABWNsYXp6dAARTGphdmEvbGFuZy9DbGFzcztMAA9jb21wYXJhdG9yQ2xhc3NxAH4AAlsAF3Bvc3NpYmxlQ2FzdFRhcmdldFR5cGVzdAASW0xqYXZhL2xhbmcvQ2xhc3M7TAAKc2VyaWFsaXplcnQANkxvcmcvYXBhY2hlL2ZsaW5rL2FwaS9jb21tb24vdHlwZXV0aWxzL1R5cGVTZXJpYWxpemVyO3hyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZWluZm8uVHlwZUluZm9ybWF0aW9ulI3ISLqzeusCAAB4cHZyABRqYXZhLm1hdGguQmlnRGVjaW1hbFTHFVf5gShPAwACSQAFc2NhbGVMAAZpbnRWYWx0ABZMamF2YS9tYXRoL0JpZ0ludGVnZXI7eHIAEGphdmEubGFuZy5OdW1iZXKGrJUdC5TgiwIAAHhwdnIAO29yZy5hcGFjaGUuZmxpbmsuYXBpLmNvbW1vbi50eXBldXRpbHMuYmFzZS5CaWdEZWNDb21wYXJhdG9yAAAAAAAAAAECAAB4cgA-b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJhc2ljVHlwZUNvbXBhcmF0b3IAAAAAAAAAAQIAAloAE2FzY2VuZGluZ0NvbXBhcmlzb25bAAtjb21wYXJhdG9yc3QAN1tMb3JnL2FwYWNoZS9mbGluay9hcGkvY29tbW9uL3R5cGV1dGlscy9UeXBlQ29tcGFyYXRvcjt4cgA0b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5UeXBlQ29tcGFyYXRvcgAAAAAAAAABAgAAeHB1cgASW0xqYXZhLmxhbmcuQ2xhc3M7qxbXrsvNWpkCAAB4cAAAAABzcgA7b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJpZ0RlY1NlcmlhbGl6ZXIAAAAAAAAAAQIAAHhyAEJvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLmJhc2UuVHlwZVNlcmlhbGl6ZXJTaW5nbGV0b255qYeqxy53RQIAAHhyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLlR5cGVTZXJpYWxpemVyAAAAAAAAAAECAAB4cAAAACMAAAAC>'), EXPR$5: LEGACY('RAW', 'ANY<java.math.BigDecimal, rO0ABXNyADtvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkJpZ0RlY2ltYWxUeXBlSW5mbwAAAAAAAAABAgACSQAJcHJlY2lzaW9uSQAFc2NhbGV4cgAyb3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGVpbmZvLkJhc2ljVHlwZUluZm_6BPCCpWndBgIABEwABWNsYXp6dAARTGphdmEvbGFuZy9DbGFzcztMAA9jb21wYXJhdG9yQ2xhc3NxAH4AAlsAF3Bvc3NpYmxlQ2FzdFRhcmdldFR5cGVzdAASW0xqYXZhL2xhbmcvQ2xhc3M7TAAKc2VyaWFsaXplcnQANkxvcmcvYXBhY2hlL2ZsaW5rL2FwaS9jb21tb24vdHlwZXV0aWxzL1R5cGVTZXJpYWxpemVyO3hyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZWluZm8uVHlwZUluZm9ybWF0aW9ulI3ISLqzeusCAAB4cHZyABRqYXZhLm1hdGguQmlnRGVjaW1hbFTHFVf5gShPAwACSQAFc2NhbGVMAAZpbnRWYWx0ABZMamF2YS9tYXRoL0JpZ0ludGVnZXI7eHIAEGphdmEubGFuZy5OdW1iZXKGrJUdC5TgiwIAAHhwdnIAO29yZy5hcGFjaGUuZmxpbmsuYXBpLmNvbW1vbi50eXBldXRpbHMuYmFzZS5CaWdEZWNDb21wYXJhdG9yAAAAAAAAAAECAAB4cgA-b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJhc2ljVHlwZUNvbXBhcmF0b3IAAAAAAAAAAQIAAloAE2FzY2VuZGluZ0NvbXBhcmlzb25bAAtjb21wYXJhdG9yc3QAN1tMb3JnL2FwYWNoZS9mbGluay9hcGkvY29tbW9uL3R5cGV1dGlscy9UeXBlQ29tcGFyYXRvcjt4cgA0b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5UeXBlQ29tcGFyYXRvcgAAAAAAAAABAgAAeHB1cgASW0xqYXZhLmxhbmcuQ2xhc3M7qxbXrsvNWpkCAAB4cAAAAABzcgA7b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJpZ0RlY1NlcmlhbGl6ZXIAAAAAAAAAAQIAAHhyAEJvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLmJhc2UuVHlwZVNlcmlhbGl6ZXJTaW5nbGV0b255qYeqxy53RQIAAHhyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLlR5cGVTZXJpYWxpemVyAAAAAAAAAAECAAB4cAAAACMAAAAC>'), EXPR$6: LEGACY('RAW', 'ANY<java.math.BigDecimal, rO0ABXNyADtvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkJpZ0RlY2ltYWxUeXBlSW5mbwAAAAAAAAABAgACSQAJcHJlY2lzaW9uSQAFc2NhbGV4cgAyb3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGVpbmZvLkJhc2ljVHlwZUluZm_6BPCCpWndBgIABEwABWNsYXp6dAARTGphdmEvbGFuZy9DbGFzcztMAA9jb21wYXJhdG9yQ2xhc3NxAH4AAlsAF3Bvc3NpYmxlQ2FzdFRhcmdldFR5cGVzdAASW0xqYXZhL2xhbmcvQ2xhc3M7TAAKc2VyaWFsaXplcnQANkxvcmcvYXBhY2hlL2ZsaW5rL2FwaS9jb21tb24vdHlwZXV0aWxzL1R5cGVTZXJpYWxpemVyO3hyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZWluZm8uVHlwZUluZm9ybWF0aW9ulI3ISLqzeusCAAB4cHZyABRqYXZhLm1hdGguQmlnRGVjaW1hbFTHFVf5gShPAwACSQAFc2NhbGVMAAZpbnRWYWx0ABZMamF2YS9tYXRoL0JpZ0ludGVnZXI7eHIAEGphdmEubGFuZy5OdW1iZXKGrJUdC5TgiwIAAHhwdnIAO29yZy5hcGFjaGUuZmxpbmsuYXBpLmNvbW1vbi50eXBldXRpbHMuYmFzZS5CaWdEZWNDb21wYXJhdG9yAAAAAAAAAAECAAB4cgA-b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJhc2ljVHlwZUNvbXBhcmF0b3IAAAAAAAAAAQIAAloAE2FzY2VuZGluZ0NvbXBhcmlzb25bAAtjb21wYXJhdG9yc3QAN1tMb3JnL2FwYWNoZS9mbGluay9hcGkvY29tbW9uL3R5cGV1dGlscy9UeXBlQ29tcGFyYXRvcjt4cgA0b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5UeXBlQ29tcGFyYXRvcgAAAAAAAAABAgAAeHB1cgASW0xqYXZhLmxhbmcuQ2xhc3M7qxbXrsvNWpkCAAB4cAAAAABzcgA7b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJpZ0RlY1NlcmlhbGl6ZXIAAAAAAAAAAQIAAHhyAEJvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLmJhc2UuVHlwZVNlcmlhbGl6ZXJTaW5nbGV0b255qYeqxy53RQIAAHhyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLlR5cGVTZXJpYWxpemVyAAAAAAAAAAECAAB4cAAAACMAAAAC>'), EXPR$7: LEGACY('RAW', 'ANY<java.math.BigDecimal, rO0ABXNyADtvcmcuYXBhY2hlLmZsaW5rLnRhYmxlLnJ1bnRpbWUudHlwZXV0aWxzLkJpZ0RlY2ltYWxUeXBlSW5mbwAAAAAAAAABAgACSQAJcHJlY2lzaW9uSQAFc2NhbGV4cgAyb3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGVpbmZvLkJhc2ljVHlwZUluZm_6BPCCpWndBgIABEwABWNsYXp6dAARTGphdmEvbGFuZy9DbGFzcztMAA9jb21wYXJhdG9yQ2xhc3NxAH4AAlsAF3Bvc3NpYmxlQ2FzdFRhcmdldFR5cGVzdAASW0xqYXZhL2xhbmcvQ2xhc3M7TAAKc2VyaWFsaXplcnQANkxvcmcvYXBhY2hlL2ZsaW5rL2FwaS9jb21tb24vdHlwZXV0aWxzL1R5cGVTZXJpYWxpemVyO3hyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZWluZm8uVHlwZUluZm9ybWF0aW9ulI3ISLqzeusCAAB4cHZyABRqYXZhLm1hdGguQmlnRGVjaW1hbFTHFVf5gShPAwACSQAFc2NhbGVMAAZpbnRWYWx0ABZMamF2YS9tYXRoL0JpZ0ludGVnZXI7eHIAEGphdmEubGFuZy5OdW1iZXKGrJUdC5TgiwIAAHhwdnIAO29yZy5hcGFjaGUuZmxpbmsuYXBpLmNvbW1vbi50eXBldXRpbHMuYmFzZS5CaWdEZWNDb21wYXJhdG9yAAAAAAAAAAECAAB4cgA-b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJhc2ljVHlwZUNvbXBhcmF0b3IAAAAAAAAAAQIAAloAE2FzY2VuZGluZ0NvbXBhcmlzb25bAAtjb21wYXJhdG9yc3QAN1tMb3JnL2FwYWNoZS9mbGluay9hcGkvY29tbW9uL3R5cGV1dGlscy9UeXBlQ29tcGFyYXRvcjt4cgA0b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5UeXBlQ29tcGFyYXRvcgAAAAAAAAABAgAAeHB1cgASW0xqYXZhLmxhbmcuQ2xhc3M7qxbXrsvNWpkCAAB4cAAAAABzcgA7b3JnLmFwYWNoZS5mbGluay5hcGkuY29tbW9uLnR5cGV1dGlscy5iYXNlLkJpZ0RlY1NlcmlhbGl6ZXIAAAAAAAAAAQIAAHhyAEJvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLmJhc2UuVHlwZVNlcmlhbGl6ZXJTaW5nbGV0b255qYeqxy53RQIAAHhyADRvcmcuYXBhY2hlLmZsaW5rLmFwaS5jb21tb24udHlwZXV0aWxzLlR5cGVTZXJpYWxpemVyAAAAAAAAAAECAAB4cAAAACMAAAAC>')]
{code}
;;;","28/Dec/19 05:16;jark;Fixed in 1.11.0: 450795ecbfd40d1f39df0769440bbfcb5abbcc93
1.10.0: 3d88f711bfadb9e48d5a70d0797ce6652ef1e7a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GroupAggsHandler throws java.time.LocalDateTime cannot be cast to java.sql.Timestamp,FLINK-15421,13276559,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,docete,libenchao,libenchao,27/Dec/19 07:41,31/Dec/19 06:29,13/Jul/23 08:10,31/Dec/19 06:29,1.10.0,1.9.1,,,,,,,,1.10.0,1.9.2,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"`TimestmapType` has two types of physical representation: `Timestamp` and `LocalDateTime`. When we use following SQL, it will conflict each other:
{code:java}
SELECT 
  SUM(cnt) as s, 
  MAX(ts)
FROM 
  SELECT 
    `string`,
    `int`,
    COUNT(*) AS cnt,
    MAX(rowtime) as ts
  FROM T1
  GROUP BY `string`, `int`, TUMBLE(rowtime, INTERVAL '10' SECOND)
GROUP BY `string`
{code}
with 'table.exec.emit.early-fire.enabled' = true.

The exceptions is below:
{quote}Caused by: java.lang.ClassCastException: java.time.LocalDateTime cannot be cast to java.sql.Timestamp
 at GroupAggsHandler$83.getValue(GroupAggsHandler$83.java:529)
 at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:164)
 at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
 at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85)
 at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)
 at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)
 at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)
 at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:488)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
 at java.lang.Thread.run(Thread.java:748)
{quote}
I also create a UT to quickly reproduce this bug in `WindowAggregateITCase`:
{code:java}
@Test
def testEarlyFireWithTumblingWindow(): Unit = {
  val stream = failingDataSource(data)
    .assignTimestampsAndWatermarks(
      new TimestampAndWatermarkWithOffset
        [(Long, Int, Double, Float, BigDecimal, String, String)](10L))
  val table = stream.toTable(tEnv,
    'rowtime.rowtime, 'int, 'double, 'float, 'bigdec, 'string, 'name)
  tEnv.registerTable(""T1"", table)
  tEnv.getConfig.getConfiguration.setBoolean(""table.exec.emit.early-fire.enabled"", true)
  tEnv.getConfig.getConfiguration.setString(""table.exec.emit.early-fire.delay"", ""1000 ms"")

  val sql =
    """"""
      |SELECT
      |  SUM(cnt) as s,
      |  MAX(ts)
      |FROM
      |  (SELECT
      |    `string`,
      |    `int`,
      |    COUNT(*) AS cnt,
      |    MAX(rowtime) as ts
      |  FROM T1
      |  GROUP BY `string`, `int`, TUMBLE(rowtime, INTERVAL '10' SECOND))
      |GROUP BY `string`
      |"""""".stripMargin

  tEnv.sqlQuery(sql).toRetractStream[Row].print()
  env.execute()
}
{code}
 

 ",,jark,libenchao,lzljs3620320,,,,,,,,,,,"docete commented on pull request #10722: [FLINK-15421][table-planner-blink] Fix TimestampMaxAggFunction/Timest…
URL: https://github.com/apache/flink/pull/10722
 
 
   …ampMinAggFunction to accept SqlTimestamp values
   
   ## What is the purpose of the change
   
   The `MaxWithRetractAggFunction`/`MinWithRetractAggFunction` in blink planner not support ""accumulate(acc, Object value)"" generic object with third-part data formats. We only support internal/external format in this usage. In this PR, we use SqlTimestamp(the internal format of TimestampType) in `TimestampMaxAggFunction`/`TimestampMinAggFunction` to fix the `ClassCastException`
   
   ## Brief change log
   - 5a8613d use SqlTimestamp in `TimestampMaxAggFunction` and `TimestampMinAggFunction`
   
   ## Verifying this change
   
   This change added tests 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**yes** / no)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Dec/19 09:36;githubbot;600","docete commented on pull request #10723: [FLINK-15421][table-planner-blink] Fix TimestampMaxAggFunction/TimestampMinAggFunction to accept LocalDateTime values
URL: https://github.com/apache/flink/pull/10723
 
 
   ## What is the purpose of the change
   
   The MaxWithRetractAggFunction/MinWithRetractAggFunction in blink planner not support ""accumulate(acc, Object value)"" generic object with third-part data formats. We only support internal/external format in this usage. In this PR, we use LocalDateTime(the default external format of TimestampType) in TimestampMaxAggFunction/TimestampMinAggFunction to fix the ClassCastException. This PR **only fix release 1.9**. For release 1.10 or above, see https://github.com/apache/flink/pull/10722
   
   ## Brief change log
   
   - b0bd11c use LocalDateTime in `TimestampMaxAggFunction` and `TimestampMinAggFunction`
   
   ## Verifying this change
   
   This change added tests 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**yes** / no)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Dec/19 12:03;githubbot;600","wuchong commented on pull request #10722: [FLINK-15421][table-planner-blink] Fix TimestampMaxAggFunction/Timest…
URL: https://github.com/apache/flink/pull/10722
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/19 06:26;githubbot;600","wuchong commented on pull request #10723: [FLINK-15421][table-planner-blink] Fix TimestampMaxAggFunction/TimestampMinAggFunction to accept LocalDateTime values
URL: https://github.com/apache/flink/pull/10723
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/19 06:27;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 31 06:29:19 UTC 2019,,,,,,,,,,"0|z0a0xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 07:45;lzljs3620320;CC: [~docete];;;","27/Dec/19 07:51;lzljs3620320;Thanks [~libenchao] for reporting. Can you format the code in description?;;;","27/Dec/19 07:53;libenchao;[~lzljs3620320] Sorry about the format of the code. Could you tell me how to write a code block correctly in issue description ?;;;","27/Dec/19 08:02;lzljs3620320;[~libenchao] Maybe you can try + in the upper right corner and create code block.;;;","30/Dec/19 02:50;docete;[~lzljs3620320] I investigate this case and find the cause: 

1) the inner MAX(rowtime) would translate to *TimestampMaxAggFunction* and returns `DataTypes.TIMESTAMP(3)` which by default bridged to java.time.LocalDateTime

2) the outer MAX(ts) would translate to *TimestampMaxWithRetractAggFunction* since the two-level GROUP BY, and assumes the input objects are java.sql.Timestamp

Then exceptions above are thrown. We can hotfix it by change the value type of *TimestampMaxWithRetractAggFunction* to java.time.LocalDateTime, since we have make the default conversion of TimestampType as java.time.LocalDateTime. 

What do you think?;;;","30/Dec/19 03:12;lzljs3620320;Thanks [~docete], make sense, we can just return ""Types.LOCAL_DATE_TIME"".;;;","30/Dec/19 03:13;lzljs3620320;NOTE: ""MinWithRetractAggFunction"" need modify too.;;;","30/Dec/19 05:49;jark;Hi [~lzljs3620320], [~docete], I'm sorry, I don't think so. 

IMO, it is a bug of agg code generation, not a user side problem. 
I looked the implementation of TimestampMaxWithRetractAggFunction, the definition is totally right: declare the result 
value type and input type are both {{Timestamp}}. Actually, there are many user-defined aggregates are the same.
I even think it is a backward compatible problem.;;;","30/Dec/19 06:41;lzljs3620320;Hi [~jark], Thanks for your reminding. After taking a deep look to aggregate code generation, it is a limitation of blink planner. But I don't think it could be a backward compatible bug or planner bug. It should be fixed in max aggregate function.

Hi [~docete], I was misunderstood by your ""two-level GROUP BY"". It is not about ""two-level"", consider use TimestampMaxWithRetractAggFunction directly: ""select my_user_max(timestamp '2019-09-09') from T group by b"". It will fail too.

The real reason is that blink planner not support ""accumulate(acc, Object value)"" generic object with third-part data formats. Note, we only support internal/external format in this usage. 

[~jark] UDAF only defines ""getResultType"" and ""getAccumulatorType"", if UDAF define ""accumulate(acc, Object input)"", planner never know the real format of input, I think we only can support this after UDF type refactoring.;;;","30/Dec/19 06:52;jark;Thanks for the explanation [~lzljs3620320], so it is a by-desgin backward compatible problem. 
Regarding to the {{TimestampMaxWithRetractAggFunction}}, I'm fine with the fixing way. 
Assigned to you [~docete].;;;","30/Dec/19 12:05;docete;Opened two PRs to fix this issue:

[https://github.com/apache/flink/pull/10722] for release-1.10 and master

[https://github.com/apache/flink/pull/10723] for release-1.9

 ;;;","31/Dec/19 06:29;jark;1.11.0: ba4433540561ef942062c70eb6bce64c02d8a54a
1.10.0: f58a2ecf2c6a60c0c81f9ece13d58797407232fa 
1.9.2: ecd4e42d4980928655ec3ba2f1517d12c29a1d94;;;",,,,,,,,,,,,,,,,,,,,
Cast string to timestamp will loose precision,FLINK-15420,13276558,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,docete,lzljs3620320,lzljs3620320,27/Dec/19 07:41,02/Jan/20 12:18,13/Jul/23 08:10,02/Jan/20 12:18,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"{code:java}
cast('2010-10-14 12:22:22.123456' as timestamp(9))
{code}
Will produce ""2010-10-14 12:22:22.123"" in blink planner, this should not happen.",,jark,lirui,lzljs3620320,,,,,,,,,,,"docete commented on pull request #10727: [FLINK-15420][table-planner-blink] Cast string to timestamp will loos…
URL: https://github.com/apache/flink/pull/10727
 
 
   …e precision
   
   
   ## What is the purpose of the change
   
   Now Casting string to timestamp in blink planner only support millisecond's precision. For example: 
   `cast('2010-10-14 12:22:22.123456' as timestamp(9))` returns '2010-10-14 12:22:22.123'. This is a bug and this PR fix it.
   
   ## Brief change log
   
   - 9bbb283 let casting string to timestamp support nanosecond's precision.
   
   ## Verifying this change
   
   This change added tests 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**yes** / no)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/19 03:46;githubbot;600","wuchong commented on pull request #10727: [FLINK-15420][table-planner-blink] Cast string to timestamp will loos…
URL: https://github.com/apache/flink/pull/10727
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 12:17;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 02 12:18:59 UTC 2020,,,,,,,,,,"0|z0a0xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 07:45;lzljs3620320;CC: [~docete];;;","31/Dec/19 03:18;docete;[~jark] pls assign this to me.;;;","31/Dec/19 03:30;jark;Already assigned to you. ;;;","02/Jan/20 12:18;jark;1.11.0: 24c970b76880e51cb6c2ee2b78290380586f14fc
1.10.0: da2ea82e7021e58aa6e2af87bbc0e1cd8cbaa0a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamExecMatchRule not set FlinkRelDistribution,FLINK-15418,13276553,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,libenchao,libenchao,27/Dec/19 06:59,12/Mar/20 13:33,13/Jul/23 08:10,31/Dec/19 01:41,1.10.0,1.9.1,,,,,,,,1.10.0,1.9.2,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"StreamExecMatchRule forgets to set FlinkRelDistribution. When match clause with `partition by`, and parallelism > 1, will result in following exception:

```
Caused by: java.lang.NullPointerException
	at org.apache.flink.runtime.state.heap.StateTable.put(StateTable.java:336)
	at org.apache.flink.runtime.state.heap.StateTable.put(StateTable.java:159)
	at org.apache.flink.runtime.state.heap.HeapMapState.put(HeapMapState.java:100)
	at org.apache.flink.runtime.state.UserFacingMapState.put(UserFacingMapState.java:52)
	at org.apache.flink.cep.nfa.sharedbuffer.SharedBuffer.registerEvent(SharedBuffer.java:141)
	at org.apache.flink.cep.nfa.sharedbuffer.SharedBufferAccessor.registerEvent(SharedBufferAccessor.java:74)
	at org.apache.flink.cep.nfa.NFA$EventWrapper.getEventId(NFA.java:483)
	at org.apache.flink.cep.nfa.NFA.computeNextStates(NFA.java:605)
	at org.apache.flink.cep.nfa.NFA.doProcess(NFA.java:292)
	at org.apache.flink.cep.nfa.NFA.process(NFA.java:228)
	at org.apache.flink.cep.operator.CepOperator.processEvent(CepOperator.java:420)
	at org.apache.flink.cep.operator.CepOperator.processElement(CepOperator.java:242)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:488)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
```",,0x26dres,jark,libenchao,,,,,,,,,,,"libenchao commented on pull request #10706: [FLINK-15418][table-planner-blink] Set FlinkRelDistribution in StreamExecMatchRule
URL: https://github.com/apache/flink/pull/10706
 
 
   ## What is the purpose of the change
   
   This pr fix that StreamExecMatchRule not sets FlinkRelDistribution trait.
   
   ## Brief change log
   
   1, Set FlinkRelDistribution in StreamExecMatchRule.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   MatchRecognizeITCase.testPartitionByWithParallelSource
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 12:43;githubbot;600","wuchong commented on pull request #10706: [FLINK-15418][table-planner-blink] Set FlinkRelDistribution in StreamExecMatchRule
URL: https://github.com/apache/flink/pull/10706
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Dec/19 12:57;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 13:33:46 UTC 2020,,,,,,,,,,"0|z0a0wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 07:57;libenchao;[~jark] I've fixed this bug, could you help to verify and assign this issue to me? I'd like to contribute it to the community.;;;","27/Dec/19 13:01;jark;[~libenchao], assigned this issue to you.;;;","30/Dec/19 12:59;jark;1.11.0: f3108bf4808318125accfc5a250795601ca76605
1.10.0: 7ae8b1125a664832e77ab8b1774aeba50f577008
1.9.2: fd76c99434d59240f03eaa9fda7b1e42c0a81730;;;","11/Mar/20 15:19;0x26dres;I'm still running into this issue, in a standalone cluster, with Flink 1.9.2 when I turn *cluster.evenly-spread-out-slots* on.
It's working fine  *cluster.evenly-spread-out-slots* turned off.

I'm trying to write a simpler version of my job with which I can reproduce the issue.;;;","11/Mar/20 16:12;libenchao;[~0x26dres] Thanks for reporting. Could you give us more information like your sql, and the complete exception, which we can reproduce the issue?;;;","12/Mar/20 11:46;0x26dres;[~libenchao], it took me a while to figure it out, here's a jira for it: https://issues.apache.org/jira/browse/FLINK-16571;;;","12/Mar/20 13:33;libenchao;[~0x26dres] Your case is very different from this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Remove the docker volume or mount when starting Mesos e2e cluster,FLINK-15417,13276548,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,27/Dec/19 06:34,04/Nov/20 14:00,13/Jul/23 08:10,10/Feb/20 10:15,1.10.0,,,,,,,,,1.10.1,1.11.0,,,Deployment / Mesos,Tests,,,,0,pull-request-available,,,,"As discussed [here|https://github.com/apache/flink/pull/10695#discussion_r361574394], there  is a potential risk of permission problems when cleanup logs and output. We could found another way to let containers get the input and output file.",,gjy,guoyangze,,,,,,,,,,,,"KarmaGYZ commented on pull request #10746: [FLINK-15417] Remove the docker volume or mount when starting Mesos e…
URL: https://github.com/apache/flink/pull/10746
 
 
   …2e cluster
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   There is a potential risk of permission problems when cleanup logs and output if we mount a directory to docker container. This PR fix that issue by remove the docker volume from docker-compose file.
   
   ## Brief change log
   
   - Remove the docker volume or mount when starting Mesos e2e cluster
   
   ## Verifying this change
   
   Trigger the mesos e2e test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 10:15;githubbot;600","GJL commented on pull request #10746: [FLINK-15417] Remove the docker volume or mount when starting Mesos e…
URL: https://github.com/apache/flink/pull/10746
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Feb/20 09:56;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 10 10:15:22 UTC 2020,,,,,,,,,,"0|z0a0vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Dec/19 08:27;guoyangze;Could someone assign this to me?;;;","02/Jan/20 09:21;gjy;I will review this.;;;","10/Feb/20 10:15;gjy;1.10: e237f809ae04723502aea617421b748318867aae
master: ceefbe74c3f2166587a48ad4c3fe6a46b76f7771;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaITCase#prepare failed in travis,FLINK-15414,13276529,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,dian.fu,dian.fu,27/Dec/19 02:03,08/Jul/20 12:23,13/Jul/23 08:10,08/Jul/20 12:23,1.11.0,1.12.0,,,,,,,,1.11.1,1.12.0,,,Connectors / Kafka,Tests,,,,0,pull-request-available,test-stability,,,"The travis for release-1.9 failed with the following error:
{code}
org.apache.kafka.common.KafkaException: Socket server failed to bind to 0.0.0.0:44867: Address already in use.
	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.prepare(KafkaITCase.java:58)
Caused by: java.net.BindException: Address already in use
	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.prepare(KafkaITCase.java:58)
{code}
instance: [https://api.travis-ci.org/v3/job/629636116/log.txt]",,dian.fu,dwysakowicz,pnowojski,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 12:23:48 UTC 2020,,,,,,,,,,"0|z0a0r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/20 21:10;rmetzger;Another instance in Flink 1.11: [https://travis-ci.org/apache/flink/jobs/660152150?utm_medium=notification&utm_source=slack];;;","27/May/20 20:46;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2295&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","10/Jun/20 06:07;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3081&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20;;;","15/Jun/20 05:59;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3477&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20

{code}
org.apache.kafka.common.KafkaException: Socket server failed to bind to 0.0.0.0:46169: Address already in use.
	at kafka.network.Acceptor.openServerSocket(SocketServer.scala:573)
	at kafka.network.Acceptor.<init>(SocketServer.scala:451)
	at kafka.network.SocketServer.kafka$network$SocketServer$$createAcceptor(SocketServer.scala:245)
	at kafka.network.SocketServer$$anonfun$createDataPlaneAcceptorsAndProcessors$1.apply(SocketServer.scala:215)
	at kafka.network.SocketServer$$anonfun$createDataPlaneAcceptorsAndProcessors$1.apply(SocketServer.scala:214)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at kafka.network.SocketServer.createDataPlaneAcceptorsAndProcessors(SocketServer.scala:214)
	at kafka.network.SocketServer.startup(SocketServer.scala:114)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:253)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.getKafkaServer(KafkaTestEnvironmentImpl.java:425)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.prepare(KafkaTestEnvironmentImpl.java:132)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.startClusters(KafkaTestBase.java:142)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.startClusters(KafkaTestBase.java:131)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.prepare(KafkaTestBase.java:100)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.prepare(KafkaTestBase.java:92)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:220)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:78)
	at kafka.network.Acceptor.openServerSocket(SocketServer.scala:569)
	... 35 more


[ERROR] Errors: 
[ERROR]   FlinkKafkaProducerITCase>KafkaTestBase.prepare:92->KafkaTestBase.prepare:100->KafkaTestBase.startClusters:131->KafkaTestBase.startClusters:142 » Kafka
[ERROR]   KafkaProducerExactlyOnceITCase.prepare:31->KafkaTestBase.prepare:92->KafkaTestBase.prepare:100->KafkaTestBase.startClusters:131->KafkaTestBase.startClusters:142 » Kafka
[INFO] 
[ERROR] Tests run: 64, Failures: 0, Errors: 2, Skipped: 0

{code};;;","16/Jun/20 15:30;pnowojski;another instance
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3584&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20;;;","08/Jul/20 12:16;dwysakowicz;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4324&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5;;;","08/Jul/20 12:23;dwysakowicz;Merged a fix that should trigger the retry logic in:
* master:
** c23787093a9da0e12561fbe22dd6da6164ffe951
* 1.11.1:
** fcd4c58c966b3df4150ed9f761273ae608055475;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ScalarOperatorsTest failed in travis,FLINK-15413,13276527,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,dian.fu,dian.fu,27/Dec/19 01:59,28/Dec/19 03:50,13/Jul/23 08:10,28/Dec/19 03:50,,,,,,,,,,1.9.2,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"The travis of release-1.9 failed with the following error:
{code:java}
14:50:19.796 [ERROR] ScalarOperatorsTest>ExpressionTestBase.evaluateExprs:161 Wrong result for: [CASE WHEN (CASE WHEN f2 = 1 THEN CAST('' as INT) ELSE 0 END) is null THEN 'null' ELSE 'not null' END] optimized to: [_UTF-16LE'not null':VARCHAR(8) CHARACTER SET ""UTF-16LE""] expected:<n[]ull> but was:<n[ot n]ull>
{code}
instance: [https://api.travis-ci.org/v3/job/629636107/log.txt]",,dian.fu,jark,,,,,,,,,,,,"wuchong commented on pull request #10708: [FLINK-15413][table-planner-blink] Fix ScalarOperatorsTest failed in travis in 1.9 branch
URL: https://github.com/apache/flink/pull/10708
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   `ScalarOperatorsTest` is failed because we use `CASE WHEN xxx is null ...` to check the null result, however, this will optimized into a not-null constant in some cases, e.g. `cast('' as int)`. We shouldn't use `testSqlNullable` to check null result for now, which is already removed in 1.10/master branch. 
   
   ## Brief change log
   
   Remove `testSqlNullable`.
   
   ## Verifying this change
   
   This change is covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 15:00;githubbot;600","wuchong commented on pull request #10708: [FLINK-15413][table-planner-blink] Fix ScalarOperatorsTest failed in travis in 1.9 branch
URL: https://github.com/apache/flink/pull/10708
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Dec/19 03:50;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 28 03:50:39 UTC 2019,,,,,,,,,,"0|z0a0qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/19 03:50;jark;Fixed in 1.9.2: 025aca82a541d838101d14de5190111597c6db83;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalExecutorITCase#testParameterizedTypes failed in travis,FLINK-15412,13276525,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,dian.fu,dian.fu,27/Dec/19 01:33,27/Dec/19 18:05,13/Jul/23 08:10,27/Dec/19 18:05,,,,,,,,,,1.9.2,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"The travis of release-1.9 failed with the following error:
{code:java}
14:43:17.916 [INFO] Running org.apache.flink.table.client.gateway.local.LocalExecutorITCase
14:44:47.388 [ERROR] Tests run: 34, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 89.468 s <<< FAILURE! - in org.apache.flink.table.client.gateway.local.LocalExecutorITCase
14:44:47.388 [ERROR] testParameterizedTypes[Planner: blink](org.apache.flink.table.client.gateway.local.LocalExecutorITCase) Time elapsed: 7.88 s <<< ERROR!
org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL statement at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testParameterizedTypes(LocalExecutorITCase.java:557)
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. findAndCreateTableSource failed
 at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testParameterizedTypes(LocalExecutorITCase.java:557)
Caused by: org.apache.flink.table.api.TableException: findAndCreateTableSource failed
 at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testParameterizedTypes(LocalExecutorITCase.java:557)
Caused by: org.apache.flink.table.api.NoMatchingTableFactoryException: 
Could not find a suitable table factory for 'org.apache.flink.table.factories.TableSourceFactory' in
the classpath.
Reason: No context matches.
{code}
instance: [https://api.travis-ci.org/v3/job/629636106/log.txt]",,dian.fu,jark,lirui,phoenixjiangnan,,,,,,,,,,"lirui-apache commented on pull request #10702: [FLINK-15412][hive] LocalExecutorITCase#testParameterizedTypes failed…
URL: https://github.com/apache/flink/pull/10702
 
 
   … in travis
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix LocalExecutorITCase#testParameterizedTypes in release-1.9
   
   
   ## Brief change log
   
     - Back port the changes in FLINK-15240.
   
   
   ## Verifying this change
   
   Covered by existing tests
   
   ## Does this pull request potentially affect one of the following parts:
   
   NA
   
   ## Documentation
   
   NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 07:52;githubbot;600","bowenli86 commented on pull request #10702: [FLINK-15412][hive] LocalExecutorITCase#testParameterizedTypes failed…
URL: https://github.com/apache/flink/pull/10702
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 18:05;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15240,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 27 18:05:50 UTC 2019,,,,,,,,,,"0|z0a0q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 06:14;ykt836;cc [~lirui];;;","27/Dec/19 07:28;lirui;The failure was introduced since FLINK-15240 was not properly ported to release-1.9. I'll submit a fix for it.;;;","27/Dec/19 18:05;phoenixjiangnan;release-1.9: baeb9b38bfef4b39f12bc7fe3934e140b3926698;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Planner can't prune partition on DATE/TIMESTAMP columns,FLINK-15411,13276497,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,phoenixjiangnan,phoenixjiangnan,26/Dec/19 18:33,02/Jan/20 07:39,13/Jul/23 08:10,02/Jan/20 07:39,1.10.0,,,,,,,,,1.10.0,,,,Connectors / Hive,Table SQL / Planner,,,,0,pull-request-available,,,,Hive should work after planner fixed due to: [https://github.com/apache/flink/pull/10690#issuecomment-569021089],,jark,lirui,lzljs3620320,phoenixjiangnan,,,,,,,,,,"JingsongLi commented on pull request #10704: [FLINK-15411][table-planner-blink] Fix prune partition on DATE/TIME/TIMESTAMP columns
URL: https://github.com/apache/flink/pull/10704
 
 
   
   ## What is the purpose of the change
   
   When use DATE/TIME/TIMESTAMP partition columns, planner will not do partition pruning, that lead to read all partitions.
   
   ## Brief change log
   
   - Fix comparison of timestamp with local time zone
   - Fix prune partition on DATE/TIME/TIMESTAMP columns
   - Fix listPartitions in HiveCatalog & Add time prune partition tests
   
   ## Verifying this change
   
   - HiveTableSourceTest.testPartitionFilterDateTimestamp
   - PartitionPruneTest.testTimePrunePartitions
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 09:50;githubbot;600","wuchong commented on pull request #10704: [FLINK-15411][table-planner-blink] Fix prune partition on DATE/TIME/TIMESTAMP columns
URL: https://github.com/apache/flink/pull/10704
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 07:36;githubbot;600",,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 02 07:39:44 UTC 2020,,,,,,,,,,"0|z0a0k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 03:15;lzljs3620320;Consider this is not hive bug, it is a planner bug, I change its name.;;;","27/Dec/19 03:17;lzljs3620320;[~phoenixjiangnan] What do you think about version? Is this so important to must be fixed in 1.10?;;;","27/Dec/19 06:09;ykt836;IMO partition with DATE field maybe the most widely used in hive data warehouse. ;;;","27/Dec/19 06:21;lzljs3620320;[~ykt836] Can you assign this to me? ;;;","27/Dec/19 06:27;lzljs3620320;We did a simple survey. Most users use string to represent date to be a partition field.

But we also need fix this ticket.;;;","27/Dec/19 06:27;lirui;I find a lot of users using string partition columns to represent the date. So IMO while it's good if we can implement this in 1.10, it doesn't have to be a blocker.;;;","27/Dec/19 06:32;ykt836;Yeah, that might also be true. Even if users want to partition the data by date, they could still choose STRING as field type. ;;;","27/Dec/19 22:39;phoenixjiangnan;yes, date is probably the most popular partition type. We'd better fix this in 1.10;;;","02/Jan/20 07:39;jark;[FLINK-15411][hive] Fix listPartitions in HiveCatalog & Add time prune partition tests
 - 1.11.0: dc0cf3be354019f9f017d73eda08e4129a962209
 - 1.10.0: 3ee517e262c51865d07a0cd8ed651c2ffb1a428b

[FLINK-15411][table-planner-blink] Fix prune partition on DATE/TIME/TIMESTAMP columns
 - 1.11.0: eadbd4c4c289497de6b38672d095dd1d0819e1b2
 - 1.10.0: f3c829c6e36aeda8e35ff3f54302d87d87f3361b

[hotfix][table-planner-blink] Fix comparison of timestamp with local time zone
 - 1.11.0: e1e040df9df2735dfdf22711760c7fa426dbdb77
 - 1.10.0: 01587e90ee16ff0829d93597ca950ae90d28fbef
;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix code generation in windowed join function,FLINK-15409,13276482,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hailong wang,hailong wang,hailong wang,26/Dec/19 14:49,19/Nov/21 15:12,13/Jul/23 08:10,01/Jan/20 02:32,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"In WindowJoinUtil#generateJoinFunction, When otherCondition is none, it will go into  statement:
{code:java}
case None =>
  s""""""
     |$buildJoinedRow
     |$collectorTerm.collect($joinedRow)
     |"""""".stripMargin
{code}
And it miss a semicolon after collet($joinedRow). This will cause compile fail:
{code:java}
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue. at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81) at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:65) at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78) at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:52) ... 26 moreCaused by: org.codehaus.commons.compiler.CompileException: Line 28, Column 21: Expression ""c.collect(joinedRow)"" is not a type
{code}
 ",,hailong wang,jark,,,,,,,,,,,,"wangxlong commented on pull request #10714: [FLINK-15409]Add semicolon after WindowJoinUtil#generateJoinFunction '$collectorTerm.collect($joinedRow)' statement
URL: https://github.com/apache/flink/pull/10714
 
 
   ## What is the purpose of the change
   
   Add semicolon after WindowJoinUtil#generateJoinFunction '$collectorTerm.collect($joinedRow)' statement
   
   ## Brief change log
   
   Add semicolon after WindowJoinUtil#generateJoinFunction '$collectorTerm.collect($joinedRow)' statement.
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - WindowJoinTest#testJoinFunctionGenerate
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Dec/19 14:23;githubbot;600","wuchong commented on pull request #10714: [FLINK-15409]Add semicolon after WindowJoinUtil#generateJoinFunction '$collectorTerm.collect($joinedRow)' statement
URL: https://github.com/apache/flink/pull/10714
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/19 11:13;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15441,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 01 02:32:36 UTC 2020,,,,,,,,,,"0|z0a0go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 06:12;ykt836;cc [~jark] [~qingru.zhang];;;","27/Dec/19 13:05;jark;[~hailong wang] Good catch! Are you willing to contribute the fix? ;;;","27/Dec/19 14:01;hailong wang;Hi [~jark], It is my pleasure to take it. Thank you for assigning to me.;;;","01/Jan/20 02:32;jark;1.11.0: e1340a006e3f76b71323dc87f61063fedfdc7f3b
1.10.0: d41f75ec749703b778c0e2f0cb56359667fa87cd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB savepoints with heap timers cannot be restored by non-process functions,FLINK-15406,13276467,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,lintingbin,lintingbin,26/Dec/19 12:23,08/Jan/20 10:26,13/Jul/23 08:10,07/Jan/20 15:46,1.10.0,1.11.0,,,,,,,,1.10.0,,,,API / State Processor,,,,,0,pull-request-available,,,,"The savepoint is writted by ""State Processor API"" can't be restore by map or flatmap. But it can be retored by KeyedProcessFunction.  
 Following is the error message:
{code:java}
java.lang.Exception: Could not write timer service of Flat Map -> Map -> Sink: device_first_user_create (1/8) to checkpoint state stream.java.lang.Exception: Could not write timer service of Flat Map -> Map -> Sink: device_first_user_create (1/8) to checkpoint state stream. at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:466) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:89) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:399) at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1282) at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1216) at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:872) at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:777) at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:708) at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:88) at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177) at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:102) at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:47) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:135) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:279) at org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:301) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:406) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NullPointerException at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:58) at org.apache.flink.streaming.api.operators.InternalTimersSnapshot.<init>(InternalTimersSnapshot.java:52) at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.snapshotTimersForKeyGroup(InternalTimerServiceImpl.java:291) at org.apache.flink.streaming.api.operators.InternalTimerServiceSerializationProxy.write(InternalTimerServiceSerializationProxy.java:98) at org.apache.flink.streaming.api.operators.InternalTimeServiceManager.snapshotStateForKeyGroup(InternalTimeServiceManager.java:139) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:462) ... 19 more{code}

  ",,aljoscha,klion26,lintingbin,mzuehlke,sjwiesman,,,,,,,,,"sjwiesman commented on pull request #10780: [FLINK-15406][state-processor-api] RocksDB savepoints with heap timers cannot be restored by non-process functions
URL: https://github.com/apache/flink/pull/10780
 
 
   ## What is the purpose of the change
   
   The keyed bootstrap operator creates an internal timer service regardless of whether or not timers are used by the bootstrap function. The AbstractStreamOperator has a special case for handling heap-based timers during the snapshot. This case will fail if the operator 1) does not support timers such as Map and FlatMap and 2) the operator contains a timer service. This change makes the creation of the timer service lazy so that keyed Map and FlatMap operators can be bootstrapped by the state proc api. 
   
   ## Brief change log
   
   - Introduce a LazyTimerService
   
   ## Verifying this change
   
   UT
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 14:42;githubbot;600","aljoscha commented on pull request #10780: [FLINK-15406][state-processor-api] RocksDB savepoints with heap timers cannot be restored by non-process functions
URL: https://github.com/apache/flink/pull/10780
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 15:46;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jan/20 09:55;lintingbin;CountWord.java;https://issues.apache.org/jira/secure/attachment/12989766/CountWord.java",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 15:46:17 UTC 2020,,,,,,,,,,"0|z0a0dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 02:10;klion26;Hi, [~lintingbin]  From the description, seems it throw exception when snapshotting instead of restoring.  and the root cause is {{keySerializer}} used to constructing {{InternalTimersSnapshot}} is NULL, maybe this operator did not call InternalTimerServiceIMple#{{startTimerSerivice}} ?

Could you please share how do you reproduce this problem? (a minimal demo can reproduce is better) thanks.;;;","01/Jan/20 10:09;lintingbin;Hi, [~klion26] 

[^CountWord.java] This is the demo I written.
{code:java}
flink run CountWord.jar --init
{code}
if you run above command, you can generate a savepoint located ""file:///tmp/flink/savepoint"".Then you need run following commands to reprocude this problem.
{code:java}
nc -lk 12345 // listen in port 12345
flink run -s file:///tmp/flink/savepoint CountWord.jar --stream   // will checkpoint fail
flink run -s file:///tmp/flink/savepoint CountWord.jar --stream1   // will checkpoint success
{code}
 ;;;","06/Jan/20 10:10;sjwiesman;The issue is the `KeyedStateBootstrapOperator` creates a timer service whether or not it is used[1]. This means as it stands it can only create operators that also use a timer service. We can either update the operator to create the timer service lazily or all a different operator / api for non-process functions. 

[1] https://github.com/apache/flink/blob/a50d9ff6db93d961805c0e8426921efc00d42385/flink-libraries/flink-state-processing-api/src/main/java/org/apache/flink/state/api/output/operators/KeyedStateBootstrapOperator.java#L64-L74 ;;;","06/Jan/20 11:14;sjwiesman;[~lintingbin] The issue only surfaces when using rocksdb statebackend and on heap timers. If you set your flink conf to use rocksdb based timers instead that should provide a quick fix. 

[1] https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html#state-backend-rocksdb-timer-service-factory;;;","06/Jan/20 11:45;lintingbin;[~sjwiesman] When I set state.backend.rocksdb.timer-service.factory: ""ROCKSDB"" in conf/flink-conf.yaml, the following exception is throwed by flink.
{code:java}
 The program finished with the following exception:org.apache.flink.client.program.ProgramInvocationException: Could not retrieve the execution result. (JobID: 6dbd0dff30710e0fea0394e71457d594)
        at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:255)
        at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:338)
        at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:60)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1507)
        at org.lilith.stream.attr.AttrStream.buildStream(AttrStream.java:56)
        at org.lilith.stream.attr.AttrStream.main(AttrStream.java:38)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:576)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:438)
        at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:274)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:746)
        at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:273)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:205)
        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1010)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1083)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
        at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1083)
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$8(RestClusterClient.java:382)
        at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)
        at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:263)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$2(Dispatcher.java:333)
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
        at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
        ... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
        at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:152)
        at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:83)
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:375)
        at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)
        ... 7 more
Caused by: java.lang.IllegalArgumentException: No enum constant org.apache.flink.contrib.streaming.state.RocksDBStateBackend.PriorityQueueStateType.""ROCKSDB""
        at java.lang.Enum.valueOf(Enum.java:238)
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend$PriorityQueueStateType.valueOf(RocksDBStateBackend.java:97)
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.<init>(RocksDBStateBackend.java:320)
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.configure(RocksDBStateBackend.java:375)
        at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.configure(RocksDBStateBackend.java:92)
        at org.apache.flink.runtime.state.StateBackendLoader.fromApplicationOrConfigOrDefault(StateBackendLoader.java:210)
        at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:299)
        at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:106)
        at org.apache.flink.runtime.scheduler.LegacyScheduler.createExecutionGraph(LegacyScheduler.java:207)
        at org.apache.flink.runtime.scheduler.LegacyScheduler.createAndRestoreExecutionGraph(LegacyScheduler.java:184)
        at org.apache.flink.runtime.scheduler.LegacyScheduler.<init>(LegacyScheduler.java:176)
        at org.apache.flink.runtime.scheduler.LegacySchedulerFactory.createInstance(LegacySchedulerFactory.java:70)
        at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:275)
        at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:265)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)
        at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:146)
        ... 10 moreEnd of exception on server side>]
        at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:389)
        at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:373)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
        ... 4 more

{code};;;","06/Jan/20 14:46;sjwiesman;[~lintingbin] You are correct, I've opened a PR that fixes the underlying issue. ;;;","07/Jan/20 15:46;aljoscha;Fixed on master in f21f1c7dcdc81aef2fbbac8204b245f8a2dc3426
Fixed on release-1.10 in 92f7f1b5567d4dbef2222b6945c50597a9e79cae;;;",,,,,,,,,,,,,,,,,,,,,,,,,
'State Migration end-to-end test from 1.6' is unstable on travis.,FLINK-15403,13276447,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,xtsong,xtsong,26/Dec/19 10:18,03/Jan/20 09:14,13/Jul/23 08:10,03/Jan/20 09:14,1.10.0,,,,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,test-stability,,,"-api.travis-ci.org/v3/job/629576631/log.txt-
https://api.travis-ci.org/v3/job/631346939/log.txt

The test case fails because the log contains the following error message.
{code}
2019-12-26 09:19:35,537 ERROR org.apache.flink.streaming.runtime.tasks.StreamTask           - Received CancelTaskException while we are not canceled. This is a bug and should be reported
org.apache.flink.runtime.execution.CancelTaskException: Consumed partition PipelinedSubpartitionView(index: 0) of ResultPartition 3886657fb8cc980139fac67e32d6e380@8cfcbe851fe3bb3fa00e9afc370bd963 has been released.
	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.getNextBuffer(LocalInputChannel.java:190)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:509)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:487)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:475)
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:75)
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:125)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:488)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
{code}
",,guoyangze,klion26,liyu,pnowojski,xtsong,yunta,,,,,,,,"pnowojski commented on pull request #10749: [FLINK-15403][task] Allow the CancelTaskException to be thrown from mailbox thread
URL: https://github.com/apache/flink/pull/10749
 
 
   CancelTaskException can be thrown for example from LocalInputChannel#getNextBuffer even if the current (down stream) task hasn't YET been canceled. This restores the behaviour to before [FLINK-15317].
       
   StreamTask is able to correctly process/handle CancelTaskException.
   
   ## Verifying this change
   
   This adds a new test and is also covered by some end to end tests that were able to detect the issue in the first place.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 13:37;githubbot;600","pnowojski commented on pull request #10749: [FLINK-15403][task] Allow the CancelTaskException to be thrown from mailbox thread
URL: https://github.com/apache/flink/pull/10749
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 09:12;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-15317,,,,FLINK-15427,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 09:14:31 UTC 2020,,,,,,,,,,"0|z0a08w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/19 11:21;yunta;[~xintongsong], did you paste the wrong url of the instance? Cannot search the target message in the instance log.;;;","27/Dec/19 01:45;xtsong;My bad. I restarted the failed travis stage, and it seems the restarted stage has overwritten the log with the same url.;;;","27/Dec/19 02:21;klion26;The exception in the description is an error log, I'm not sure whether the test failed by it (maybe by excpetionCheck?) This log was introduced by [~AHeise] in FLINK-15317, maybe he can share more about it.;;;","01/Jan/20 10:59;liyu;Another instance: https://api.travis-ci.org/v3/job/631346939/log.txt;;;","02/Jan/20 12:05;pnowojski;Yes, indeed this is caused by FLINK-15317. I will prepare a fix for that.

The issue is that here, {{CancelTaskException}} is originating from a {{LocalInputChannel}}, from upstream task task, that was already cancelled, while the downstream task has been (yet?) cancelled.

In FLINK-15317 we made an assumption that {{CancelTaskException}} can not occur in the stack trace of the task, before task was cancelled. Clearly that was wrong assumption.

Code fix will be trivial, it might be more difficult to add a test coverage.;;;","03/Jan/20 09:14;pnowojski;merged commit 11be93a into apache:master and c538f0fe5cf1f197a3eb54d5ac413d47857270d5 into apache:release-1.10;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Join with a LookupableTableSource：java.lang.RuntimeException: while converting  XXXX Caused by: java.lang.AssertionError: Field ordinal 26 is invalid for  type,FLINK-15399,13276426,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Rockey Cui,Rockey Cui,26/Dec/19 08:31,20/May/20 08:51,13/Jul/23 08:10,20/May/20 08:51,1.9.1,,,,,,,,,1.11.0,,,,Table SQL / API,,,,,0,,,,," 
{code:java}
//代码占位符
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);
    env.setParallelism(1);

    DataStreamSource<String> stringDataStreamSource1 = env.fromElements(
            ""HA""
    );
    String[] fields1 = new String[]{""ORD_ID"", ""PS_PARTKEY"", ""PS_SUPPKEY"", ""PS_AVAILQTY"", ""PS_SUPPLYCOST"", ""PS_COMMENT""
            // key
            , ""PS_INT"", ""PS_LONG""
            , ""PS_DOUBLE8"", ""PS_DOUBLE14"", ""PS_DOUBLE15""
            , ""PS_NUMBER1"", ""PS_NUMBER2"", ""PS_NUMBER3"", ""PS_NUMBER4""
            , ""PS_DATE"", ""PS_TIMESTAMP"", ""PS_DATE_EVENT"", ""PS_TIMESTAMP_EVENT""};
    TypeInformation<?>[] types1 = new TypeInformation[]{Types.STRING, Types.INT, Types.LONG, Types.LONG, Types.DOUBLE, Types.STRING
            // key
            , Types.INT, Types.LONG
            , Types.DOUBLE, Types.DOUBLE, Types.DOUBLE
            , Types.LONG, Types.LONG, Types.DOUBLE, Types.DOUBLE
            , Types.SQL_DATE, Types.SQL_TIMESTAMP, Types.SQL_DATE, Types.SQL_TIMESTAMP};
    RowTypeInfo typeInformation1 = new RowTypeInfo(types1, fields1);
    DataStream<Row> stream1 = stringDataStreamSource1.map(new MapFunction<String, Row>() {
        private static final long serialVersionUID = 2349572544179673356L;

        @Override
        public Row map(String s) {
            return new Row(typeInformation1.getArity());
        }
    }).returns(typeInformation1);
    tableEnv.registerDataStream(""FUN_1"", stream1, String.join("","", typeInformation1.getFieldNames()) + "",PROCTIME.proctime"");

    DataStreamSource<String> stringDataStreamSource2 = env.fromElements(
            ""HA""
    );
    String[] fields2 = new String[]{""C_NAME"", ""C_ADDRESS"", ""C_NATIONKEY""
            // key
            , ""C_INT"", ""C_LONG""
            , ""C_DOUBLE8"", ""C_DOUBLE14""
            , ""C_DATE_EVENT"", ""C_TIMESTAMP_EVENT""};
    TypeInformation<?>[] types2 = new TypeInformation[]{Types.STRING, Types.STRING, Types.LONG
            // key
            , Types.INT, Types.LONG
            , Types.DOUBLE, Types.DOUBLE
            , Types.SQL_DATE, Types.SQL_TIMESTAMP};
    RowTypeInfo typeInformation2 = new RowTypeInfo(types2, fields2);

    DataStream<Row> stream2 = stringDataStreamSource2.map(new MapFunction<String, Row>() {
        private static final long serialVersionUID = 2349572544179673349L;

        @Override
        public Row map(String s) {
            return new Row(typeInformation2.getArity());
        }
    }).returns(typeInformation2);
    tableEnv.registerDataStream(""FUN_2"", stream2, String.join("","", typeInformation2.getFieldNames()) + "",PROCTIME.proctime"");

    MyLookupTableSource tableSource = MyLookupTableSource.newBuilder()
            .withFieldNames(new String[]{
                    ""S_NAME"", ""S_ADDRESS"", ""S_PHONE""
                    , ""S_ACCTBAL"", ""S_COMMENT""
                    // key
                    , ""S_INT"", ""S_LONG""
                    , ""S_DOUBLE8"", ""S_DOUBLE14""
                    , ""S_DOUBLE15"", ""S_DATE_EVENT"", ""S_TIMESTAMP_EVENT""})
            .withFieldTypes(new TypeInformation[]{
                    Types.STRING, Types.STRING, Types.STRING
                    , Types.DOUBLE, Types.STRING
                    // key
                    , Types.INT, Types.LONG
                    , Types.DOUBLE, Types.DOUBLE
                    , Types.DOUBLE, Types.SQL_DATE, Types.SQL_TIMESTAMP})
            .build();

    tableEnv.registerTableSource(""INFO"", tableSource);

    String sql = ""SELECT LN(F.PS_INT),LOG(F2.C_INT,1)\n"" +
            ""  FROM (SELECT *\n"" +
            ""          FROM FUN_1 F1\n"" +
            ""      JOIN INFO FOR SYSTEM_TIME AS OF F1.PROCTIME D1\n"" +
            "" ON F1.PS_INT = D1.S_INT AND F1.PS_LONG - 5700000 = D1.S_LONG \n"" +
            "") F\n"" +
            ""JOIN FUN_2 F2 ON F.PS_INT = F2.C_INT AND F.PS_LONG - 1500000 = F2.C_LONG\n"" +
            "" WHERE 1=1\n"" +
            "" AND F.PS_INT BETWEEN 1000 AND 5000\n"" +
            "" AND F.S_LONG < 2147792600\n"" + // I find this cause the Exception
            "" AND F.PS_COMMENT LIKE '%FILY%'\n"" +
            "" AND F2.C_INT IS NOT NULL\n"" +
            "" AND LN(F.PS_INT)<8"";

    Table table = tableEnv.sqlQuery(sql);

    DataStream<Row> result = tableEnv.toAppendStream(table, Row.class);

    result.print().setParallelism(1);

    tableEnv.execute(""LookUpTest"");
}
{code}
 

 

Exception
{code:java}
//代码占位符
Exception in thread ""main"" java.lang.RuntimeException: while converting 1 = 1 AND `F`.`PS_INT` BETWEEN ASYMMETRIC 1000 AND 5000 AND `F`.`S_LONG` < 2147792600 AND `F`.`PS_COMMENT` LIKE '%FILY%' AND `F2`.`C_INT` IS NOT NULL AND LN(`F`.`PS_INT`) < 8Exception in thread ""main"" java.lang.RuntimeException: while converting 1 = 1 AND `F`.`PS_INT` BETWEEN ASYMMETRIC 1000 AND 5000 AND `F`.`S_LONG` < 2147792600 AND `F`.`PS_COMMENT` LIKE '%FILY%' AND `F2`.`C_INT` IS NOT NULL AND LN(`F`.`PS_INT`) < 8 at org.apache.calcite.sql2rel.ReflectiveConvertletTable.lambda$registerNodeTypeMethod$0(ReflectiveConvertletTable.java:86) at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:63) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4772) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4077) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4641) at org.apache.calcite.sql2rel.SqlToRelConverter.convertWhere(SqlToRelConverter.java:981) at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:649) at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:627) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3166) at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:563) at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:139) at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:212) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:161) at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:91) at org.apache.flink.table.planner.delegation.PlannerBase.parse(PlannerBase.scala:132) at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:298) at com.rock.test.LookUpTest.main(LookUpTest.java:99)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.calcite.sql2rel.ReflectiveConvertletTable.lambda$registerNodeTypeMethod$0(ReflectiveConvertletTable.java:83) ... 17 moreCaused by: java.lang.RuntimeException: while converting 1 = 1 AND `F`.`PS_INT` BETWEEN ASYMMETRIC 1000 AND 5000 AND `F`.`S_LONG` < 2147792600 AND `F`.`PS_COMMENT` LIKE '%FILY%' AND `F2`.`C_INT` IS NOT NULL at org.apache.calcite.sql2rel.ReflectiveConvertletTable.lambda$registerNodeTypeMethod$0(ReflectiveConvertletTable.java:86) at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:63) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4772) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4077) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4641) at org.apache.calcite.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:787) at org.apache.calcite.sql2rel.StandardConvertletTable.convertCall(StandardConvertletTable.java:763) at org.apache.calcite.sql2rel.StandardConvertletTable.convertCall(StandardConvertletTable.java:750) ... 22 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.calcite.sql2rel.ReflectiveConvertletTable.lambda$registerNodeTypeMethod$0(ReflectiveConvertletTable.java:83) ... 30 moreCaused by: java.lang.RuntimeException: while converting 1 = 1 AND `F`.`PS_INT` BETWEEN ASYMMETRIC 1000 AND 5000 AND `F`.`S_LONG` < 2147792600 AND `F`.`PS_COMMENT` LIKE '%FILY%' at org.apache.calcite.sql2rel.ReflectiveConvertletTable.lambda$registerNodeTypeMethod$0(ReflectiveConvertletTable.java:86) at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:63) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4772) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4077) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4641) at org.apache.calcite.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:787) at org.apache.calcite.sql2rel.StandardConvertletTable.convertCall(StandardConvertletTable.java:763) at org.apache.calcite.sql2rel.StandardConvertletTable.convertCall(StandardConvertletTable.java:750) ... 35 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.calcite.sql2rel.ReflectiveConvertletTable.lambda$registerNodeTypeMethod$0(ReflectiveConvertletTable.java:83) ... 43 moreCaused by: java.lang.RuntimeException: while converting 1 = 1 AND `F`.`PS_INT` BETWEEN ASYMMETRIC 1000 AND 5000 AND `F`.`S_LONG` < 2147792600 at org.apache.calcite.sql2rel.ReflectiveConvertletTable.lambda$registerNodeTypeMethod$0(ReflectiveConvertletTable.java:86) at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:63) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4772) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4077) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4641) at org.apache.calcite.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:787) at org.apache.calcite.sql2rel.StandardConvertletTable.convertCall(StandardConvertletTable.java:763) at org.apache.calcite.sql2rel.StandardConvertletTable.convertCall(StandardConvertletTable.java:750) ... 48 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.calcite.sql2rel.ReflectiveConvertletTable.lambda$registerNodeTypeMethod$0(ReflectiveConvertletTable.java:83) ... 56 moreCaused by: java.lang.RuntimeException: while converting `F`.`S_LONG` < 2147792600 at org.apache.calcite.sql2rel.ReflectiveConvertletTable.lambda$registerNodeTypeMethod$0(ReflectiveConvertletTable.java:86) at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:63) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4772) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4077) at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4641) at org.apache.calcite.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:787) at org.apache.calcite.sql2rel.StandardConvertletTable.convertCall(StandardConvertletTable.java:763) at org.apache.calcite.sql2rel.StandardConvertletTable.convertCall(StandardConvertletTable.java:750) ... 61 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.calcite.sql2rel.ReflectiveConvertletTable.lambda$registerNodeTypeMethod$0(ReflectiveConvertletTable.java:83) ... 69 moreCaused by: java.lang.AssertionError: Field ordinal 26 is invalid for  type 'RecordType(VARCHAR(2147483647) ORD_ID, INTEGER PS_PARTKEY, BIGINT PS_SUPPKEY, BIGINT PS_AVAILQTY, DOUBLE PS_SUPPLYCOST, VARCHAR(2147483647) PS_COMMENT, INTEGER PS_INT, BIGINT PS_LONG, DOUBLE PS_DOUBLE8, DOUBLE PS_DOUBLE14, DOUBLE PS_DOUBLE15, BIGINT PS_NUMBER1, BIGINT PS_NUMBER2, DOUBLE PS_NUMBER3, DOUBLE PS_NUMBER4, DATE PS_DATE, TIMESTAMP(3) PS_TIMESTAMP, DATE PS_DATE_EVENT, TIMESTAMP(3) PS_TIMESTAMP_EVENT, TIME ATTRIBUTE(PROCTIME) PROCTIME)' at org.apache.calcite.rex.RexBuilder.makeFieldAccess(RexBuilder.java:197) at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:3710) at org.apache.calcite.sql2rel.SqlToRelConverter.access$2200(SqlToRelConverter.java:217) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4781) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4077) at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:317) at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4641) at org.apache.calcite.sql2rel.StandardConvertletTable.convertExpressionList(StandardConvertletTable.java:787) at org.apache.calcite.sql2rel.StandardConvertletTable.convertCall(StandardConvertletTable.java:763) at org.apache.calcite.sql2rel.StandardConvertletTable.convertCall(StandardConvertletTable.java:750) ... 74 more
{code}
I have uploaded a jar file， Did I use the wrong way ?",jdk1.8.0_211,danny0405,jark,Rockey Cui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15613,,,,,,,FLINK-14338,,,,,,,,,,,"26/Dec/19 08:31;Rockey Cui;JoinTest-1.0-SNAPSHOT.jar;https://issues.apache.org/jira/secure/attachment/12989481/JoinTest-1.0-SNAPSHOT.jar",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 13 07:35:02 UTC 2020,,,,,,,,,,"0|z0a048:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/19 13:43;jark;Thanks for reporting this [~Rockey Cui], I reproduced this problem in my local machine. 
Will dig into it in the next days. ;;;","03/Jan/20 03:39;jark;I converted the SQL query into 3 level stream-stream join, and get the same exception. It seems that it is not related to temporal join, but maybe a bug in Calcite. 
[~danny0405] will debug it from Calcite side. ;;;","06/Jan/20 09:44;danny0405;The bug has been fixed in CALCITE-3575, the root cause is that when converting Join in sql-to-rel conversion, Calcite would try to push down the join conditions, but the leaves nodes are not updated, so when trying to flatten the inputs when translating the field references, the right leaf node aren't there so the conversion fails.

see SqlToRelConverter.LookUpContext.flatten:

{code:java}
    public void flatten(
        List<RelNode> rels,
        int systemFieldCount,
        int[] start,
        List<Pair<RelNode, Integer>> relOffsetList) {
      for (RelNode rel : rels) {
        if (leaves.contains(rel) || rel instanceof LogicalMatch) {
          relOffsetList.add(
              Pair.of(rel, start[0]));
          start[0] += rel.getRowType().getFieldCount();
        } else {
          if (rel instanceof LogicalJoin
              || rel instanceof LogicalAggregate) {
            start[0] += systemFieldCount;
          }
          flatten(
              rel.getInputs(),
              systemFieldCount,
              start,
              relOffsetList);
        }
      }
    }
{code}
;;;","06/Jan/20 09:54;danny0405;I have link this issue with FLINK-14338.;;;","13/Jan/20 07:35;jark;Moved it to 1.11.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix incorrect code example in ""Catalogs"" page",FLINK-15398,13276419,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Terry1897,Terry1897,Terry1897,26/Dec/19 07:30,07/Jan/20 10:29,13/Jul/23 08:10,07/Jan/20 10:28,1.10.0,,,,,,,,,1.10.0,,,,Documentation,,,,,0,pull-request-available,,,,"https://ci.apache.org/projects/flink/flink-docs-master/dev/table/catalogs.html#how-to-create-and-register-flink-tables-to-catalog
Now we don't support `show tables` through TableEnvironemt.sqlQuery() method, 
we should correct it .",,jark,lzljs3620320,Terry1897,,,,,,,,,,,"zjuwangg commented on pull request #10786: [FLINK-15398][hotfix][doc]Correct catalog doc example mistake
URL: https://github.com/apache/flink/pull/10786
 
 
   ## What is the purpose of the change
   
   *Correct catalog doc example mistake*
   
   
   ## Brief change log
     - 84b3810 Correct catalog doc example mistake
   
   
   ## Verifying this change
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no )
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: ( no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable )
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 07:54;githubbot;600","wuchong commented on pull request #10786: [FLINK-15398][hotfix][doc]Correct catalog doc example mistake
URL: https://github.com/apache/flink/pull/10786
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 10:26;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 10:28:59 UTC 2020,,,,,,,,,,"0|z0a02o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/19 07:45;lzljs3620320;Thanks [~Terry1897] , It is good to verify ""How to Create and Register Flink Tables to Catalog"" too. Maybe document not work...;;;","07/Jan/20 10:28;jark;1.11.0: 61edf26a5073b5bf463d7554e6ea672981dabc5e
1.10.0: 576ef0d1ffd871f9257253103f53357adb17d10a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DATE and TIMESTAMP partition columns don't work,FLINK-15391,13276348,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,lirui,lirui,25/Dec/19 09:18,27/Dec/19 04:34,13/Jul/23 08:10,26/Dec/19 18:31,,,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,jark,Jinlong An,lirui,lzljs3620320,phoenixjiangnan,,,,,,,,,"lirui-apache commented on pull request #10690: [FLINK-15391][hive] DATE and TIMESTAMP partition columns don't work
URL: https://github.com/apache/flink/pull/10690
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix issues of read/write DATE and TIMESTAMP partition columns.
   
   
   ## Brief change log
   
     - When reading DATE/TIMESTAMP partition columns, make sure the value objects are converted to corresponding Flink objects.
     - When writing DATE/TIMESTAMP partition columns, make sure we convert to Hive objects before extracting the string values.
     - Add new tests
   
   
   ## Verifying this change
   
   New test cases
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 13:03;githubbot;600","bowenli86 commented on pull request #10690: [FLINK-15391][hive] DATE and TIMESTAMP partition columns don't work
URL: https://github.com/apache/flink/pull/10690
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 18:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 26 18:31:44 UTC 2019,,,,,,,,,,"0|z09zmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/19 10:06;Jinlong An;Can you provide some details?;;;","25/Dec/19 10:30;lzljs3620320;The title should be ""Can not select partition date or timestamp fields from hive table"".;;;","25/Dec/19 12:44;lirui;[~Jinlong An] The problem is we don't convert the partition values to corresponding Flink objects, which is expected by the reader later.
[~lzljs3620320] Writing to timestamp partition columns also doesn't work. We need to convert an object to Hive object before we call {{toString}} to get the string value.;;;","26/Dec/19 03:57;lzljs3620320;[~jark] Can you assign this tickect to [~lirui]?;;;","26/Dec/19 18:31;phoenixjiangnan;master: 2e9a7f38fa9971958ed3e02cdaa45528e5e657d4
1.10: 694c5883ed54328c182d1a359f778dbae63b4c01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SingleJobSubmittedJobGraphStore.putJobGraph has a logic error,FLINK-15386,13276243,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ethanli,ethanli,ethanli,24/Dec/19 16:05,04/Mar/20 14:52,13/Jul/23 08:10,04/Mar/20 14:52,1.9.0,,,,,,,,,1.9.3,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"https://github.com/apache/flink/blob/release-1.9/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/SingleJobSubmittedJobGraphStore.java#L61-L66

{code:java}
	@Override
	public void putJobGraph(SubmittedJobGraph jobGraph) throws Exception {
		if (!jobGraph.getJobId().equals(jobGraph.getJobId())) { //this always returns false.
			throw new FlinkException(""Cannot put additional jobs into this submitted job graph store."");
		}
	}
{code}



The code is there since 1.5 but fixed in the master branch (1.10). It's also better to add unit test for this.",,ethanli,liyu,,,,,,,,,,,,"Ethanlm commented on pull request #10734: [FLINK-15386] Fix logic error in SingleJobSubmittedJobGraphStore.putJobGraph and add unit tests
URL: https://github.com/apache/flink/pull/10734
 
 
   ## What is the purpose of the change
   
   This pull requests fixes a minor error in  SingleJobSubmittedJobGraphStore `putJobGraph` function.
   
   ## Brief change log
   
     - *Fix SingleJobSubmittedJobGraphStore putJobGraph function*
     - *Add unit tests for SingleJobSubmittedJobGraphStore*
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added SingleJobSubmittedJobGraphStoreTest that validates `putJobGraph` throws `FlinkException` when the input is a different jobGraph*
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/19 18:47;githubbot;600","zentol commented on pull request #10734: [FLINK-15386] Fix logic error in SingleJobSubmittedJobGraphStore.putJobGraph and add unit tests
URL: https://github.com/apache/flink/pull/10734
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Mar/20 14:51;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 14:52:09 UTC 2020,,,,,,,,,,"0|z09yzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Dec/19 16:05;ethanli;I'd like to work on this if anyone can assign it to me. Thanks;;;","04/Mar/20 14:52;chesnay;1.9: bd6d330902920116c43c5794797cf4560ec14319

Fixed on 1.10/master as part of FLINK-13573.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using sink Schema field name instead of query Schema field name in UpsertStreamTableSink,FLINK-15383,13276226,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,leonard,leonard,24/Dec/19 12:38,02/Jan/20 12:24,13/Jul/23 08:10,02/Jan/20 12:24,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Legacy Planner,Table SQL / Planner,,,,0,pull-request-available,,,,"when user  define a upsert table with primary key underlying(eg. mysql) 
{code:java}
// primary key (log_per_min, item, currency_time)
String sinkTableDDL =  ""CREATE TABLE gmv (\n"" +
        ""  log_per_min STRING,\n"" +
        ""  item STRING,\n"" +
        ""  order_cnt BIGINT,\n"" +
        ""  currency_time TIMESTAMP(3),\n"" +
        ""  gmv DECIMAL(38, 18),"" +
        ""  timestamp9 TIMESTAMP(6),\n"" +
        ""  time9 TIME(6),\n"" +
        ""  gdp DECIMAL(8, 4)\n"" +
         "") WITH (\n"" +
        ""   'connector.type' = 'jdbc',\n"" +
        ""   'connector.url' = 'jdbc:mysql://localhost:3306/test',\n"" +
        ""   'connector.username' = 'root',"" +
        ""   'connector.table' = 'gmv',\n"" +
        ""   'connector.driver' = 'com.mysql.jdbc.Driver',\n"" +
        ""   'connector.write.flush.max-rows' = '5000', \n"" +
        ""   'connector.write.flush.interval' = '2s', \n"" +
        ""   'connector.write.max-retries' = '3'"" +
        "")"";

{code}
If user‘s query field name is different with sinktable's field name. For example, user defined a field `log_ts` which not equals `log_per_min` as following:
{code:java}
// 
insert into gmv \n"" +
        ""select log_ts,\n"" +
        "" item, COUNT(order_id) as order_cnt, currency_time, cast(sum(amount_kg) * max(rate) as DECIMAL(38, 4))  as gmv,\n"" +
        "" max(timestamp9), max(time9), max(gdp) \n"" +
        "" from ( \n"" +
        "" select cast(o.ts as VARCHAR) as log_ts, o.item as item, o.order_id as order_id, c.currency_time as currency_time,\n"" +
        "" o.amount_kg as amount_kg, c.rate as rate, c.timestamp9 as timestamp9, c.time9 as time9, c.gdp as gdp \n"" +
        "" from orders as o \n"" +
        "" join currency FOR SYSTEM_TIME AS OF o.proc_time c \n"" +
        "" on o.currency = c.currency_name \n"" +
        "" ) a group by log_ts, item, currency_time

{code}
 The query will execute fail:
{code:java}
// 
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1Caused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter.lambda$create$0(UpsertWriter.java:59) at java.util.stream.IntPipeline$3$1.accept(IntPipeline.java:233) at java.util.Spliterators$IntArraySpliterator.forEachRemaining(Spliterators.java:1032) at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545) at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260) at java.util.stream.IntPipeline.toArray(IntPipeline.java:502) at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter.create(UpsertWriter.java:59) at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.open(JDBCUpsertOutputFormat.java:104) at org.apache.flink.api.java.io.jdbc.JDBCUpsertSinkFunction.open(JDBCUpsertSinkFunction.java:42) at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102) at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:48) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1023) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527) at java.lang.Thread.run(Thread.java:748)
{code}
The root cause is that we should use sink Schema field name rather than query Schema field name in StreamExecSink",,jark,leonard,lzljs3620320,,,,,,,,,,,"leonardBang commented on pull request #10700: [FLINK-15383][table sql / planner & legacy planner] Using sink schema field name instead of query schema field name for UpsertStreamTableSink.
URL: https://github.com/apache/flink/pull/10700
 
 
    Using sink schema field name instead of query schema field name for UpsertStreamTableSink.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request  fix sink schema field name was used in  UpsertStreamTableSink instead of query schema field name in blink planner and legacy planner.*
   
   
   ## Brief change log
   
     - *legacy planner: fix StreamPlanner and UpdatingPlanChecker*
     - *blink planner: fix StreamExecSink and UpdatingPlanChecker*
   
   
   ## Verifying this change
     - * add IT case in  TableSinkITCase for legacy planner and blink planner* 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 12:21;githubbot;600","leonardBang commented on pull request #10705: [FLINK-15383][connectors / filesystem] Support treat empty column as null option in CsvTableSourceFactory
URL: https://github.com/apache/flink/pull/10705
 
 
   Support treat empty column as null option in CsvTableSourceFactory
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *We have supported emptyColumnAsNull feature in  is supported CsvTableSource,  but don't surpport in CsvTableSourceFactory which will be called when user use YAML or DDL to define a CsvSoure from now. This pull request improve it.*
   
   
   ## Brief change log
   
     - *Add supported properties `emptyColumnAsNull` in CsvTableSourceFactoryBase.java*
     - *Add `emptyColumnAsNull` implement  in OldCsv.java and OldCsvValidator.java*
     - *Support `emptyColumnAsNull` in descriptors.py for Python API*
   
   ## Verifying this change
   
   Add unit test in OldCsvTest.scala
   Add python unit test in test_descriptor.py
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   * update docs in connect.md and connect.zh.md* 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 10:15;githubbot;600","leonardBang commented on pull request #10705: [FLINK-15383][connectors / filesystem] Support treat empty column as null option in CsvTableSourceFactory
URL: https://github.com/apache/flink/pull/10705
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 11:39;githubbot;600","leonardBang commented on pull request #10705: [FLINK-15383][connectors / filesystem] Support treat empty column as null option in CsvTableSourceFactory
URL: https://github.com/apache/flink/pull/10705
 
 
   Support treat empty column as null option in CsvTableSourceFactory
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *We have supported emptyColumnAsNull feature in  is supported CsvTableSource,  but don't surpport in CsvTableSourceFactory which will be called when user use YAML or DDL to define a CsvSoure from now. This pull request improve it.*
   
   
   ## Brief change log
   
     - *Add supported properties `emptyColumnAsNull` in CsvTableSourceFactoryBase.java*
     - *Add `emptyColumnAsNull` implement  in OldCsv.java and OldCsvValidator.java*
     - *Support `emptyColumnAsNull` in descriptors.py for Python API*
   
   ## Verifying this change
   
   Add unit test in OldCsvTest.scala
   Add python unit test in test_descriptor.py
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   * update docs in connect.md and connect.zh.md* 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 11:43;githubbot;600","wuchong commented on pull request #10700: [FLINK-15383][table sql / planner & legacy planner] Using sink schema field name instead of query schema field name for UpsertStreamTableSink.
URL: https://github.com/apache/flink/pull/10700
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 12:21;githubbot;600",,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,FLINK-15401,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 02 12:24:42 UTC 2020,,,,,,,,,,"0|z09yvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/19 12:28;lzljs3620320;Consider it is unfriendly behavior since UpsertSink was introduced. So I set it critical instead of blocker.;;;","02/Jan/20 12:24;jark;1.11.0: 3843e167a9bd47d891e82d12e8ae609185ebfbfe
1.10.0: 4a63854bc054c859b94d054d82371d100e3e5ab8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink failed generating python config docs ,FLINK-15382,13276213,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,xtsong,xtsong,24/Dec/19 10:15,30/Nov/21 20:38,13/Jul/23 08:10,26/Dec/19 08:40,,,,,,,,,,1.10.0,,,,API / Python,Runtime / Configuration,,,,0,pull-request-available,,,,"When generating config option docs with the command suggested by {{flink-docs/README.md}}, the generated {{docs/_includes/generated/python_configuration.html}} does not contain any config options despite that there are 4 options in {{PythonOptions}}. 

I encountered this problem at the commit {{545534e43ed37f518fe59b6ddd8ed56ae82a234b}} on master branch.

Command used to generate doc:
{code:bash}mvn package -Dgenerate-config-docs -pl flink-docs -am -nsu -DskipTests{code}
",,dian.fu,guoyangze,hequn8128,xtsong,,,,,,,,,,"dianfu commented on pull request #10688: [FLINK-15382][docs] Exclude PythonConfig from configuration docs generator
URL: https://github.com/apache/flink/pull/10688
 
 
   
   ## What is the purpose of the change
   
   *This pull request excludes PythonConfig from configuration docs generator.*
   
   ## Brief change log
   
     - *Add PythonConfig to the EXCLUSIONS list in ConfigOptionsDocGenerator*
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 10:22;githubbot;600","hequn8128 commented on pull request #10688: [FLINK-15382][docs] Exclude PythonConfig from configuration docs generator
URL: https://github.com/apache/flink/pull/10688
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 08:38;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 26 08:39:59 UTC 2019,,,,,,,,,,"0|z09ysw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Dec/19 10:21;xtsong;cc [~sunjincheng121] 
Since other components do not have the same problem, I guess this is probably some python related problem. Could you take a look?
Also, this issue doesn't seems to be a 1.10 blocker to me, but I'm not completely sure about this. Please update the priority and fix version if you feel it's necessary.;;;","25/Dec/19 03:51;guoyangze;Another instance in my environment.;;;","25/Dec/19 05:56;dian.fu;Hi [~xintongsong] [~guoyangze], thanks for reporting this issue. Could you help to check if the jar of flink-python is available(and correct) in your local maven repository? I have executed the above command and it seems everything works well in my local machine.;;;","25/Dec/19 06:28;xtsong;[~dian.fu]
There's only flink-python jar of version 1.9-SNAPSHOT in my local repository.
Do we need any special argument to build the flink-python module when building flink?;;;","25/Dec/19 06:47;dian.fu;The jar of flink-python should be available for the doc generation. This also holds for the config options of other components. You could try to remove the option ""-nsu"" to allow it to download the dependencies which are not available in the local repository. The flink-python module should be built by default when building flink and there is no special arguments required.;;;","25/Dec/19 07:15;xtsong;[~dian.fu]
I do not have flink-python of 1.11-SNAPSHOT in my local repository, but I do find flink-python_2.11 of 1.11-SNAPSHOT in my local repository.
Also, if the dependency jar file is missing, the document building should fail instead of success with empty generated table.;;;","25/Dec/19 09:52;dian.fu;Have investigated this problem with [~xintongsong] offline and found that the problem is caused by the newly introduced class PythonConfig. It will also be searched for doc generation which may override the doc generated for PythonOptions. We need to exclude it for doc generation. I will provide a PR ASAP. ;;;","25/Dec/19 10:09;xtsong;I think this problem should also be fixed for release 1.10.0.
Despite this should not be a release blocker since it only affects developers' but not users' experience, the problem does exist in the 1.10 branch and it's quite easy to fix.
I'm setting the fix version to 1.10.0 and leaving the priority to major.;;;","26/Dec/19 08:39;hequn8128;Fixed 
in 1.11.0 via ed73b5e1e147223607ca2be8e60c65c1a64a7cdd
in 1.10.0 via 8cc50f1ee8667e8a54988d2ac838d88fe7575067;;;",,,,,,,,,,,,,,,,,,,,,,,
INSERT INTO VALUES statement fails if a cast project is applied,FLINK-15381,13276207,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,jark,jark,24/Dec/19 09:20,02/Jan/20 12:56,13/Jul/23 08:10,02/Jan/20 12:56,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"The following query will fail:


{code:scala}
  @Test
  def test(): Unit = {
    val sinkDDL =
      """"""
        |create table t2(
        |  a int,
        |  b string
        |) with (
        |  'connector' = 'COLLECTION'
        |)
      """""".stripMargin
    val query =
      """"""
        |insert into t2 select cast(a as int), cast(b as varchar) from (values (3, 'c')) T(a,b)
      """""".stripMargin
    tableEnv.sqlUpdate(sinkDDL)
    tableEnv.sqlUpdate(query)
    execJob(""testJob"")
  }
{code}


exception:


{code}
org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: 

LogicalSink(name=[`default_catalog`.`default_database`.`t2`], fields=[a, b])
+- LogicalProject(EXPR$0=[$0], EXPR$1=[CAST($1):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL])
   +- LogicalValues(type=[RecordType(INTEGER a, CHAR(1) b)], tuples=[[{ 3, _UTF-16LE'c' }]])

This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.
{code}


",,godfreyhe,jark,,,,,,,,,,,,"godfreyhe commented on pull request #10694: [FLINK-15381] [table-planner-blink] correct collation derive logic on RelSubset in RelMdCollation
URL: https://github.com/apache/flink/pull/10694
 
 
   
   
   ## What is the purpose of the change
   
   *sql: `select cast(a as int), cast(b as varchar) from (values (3, 'c')) T(a,b)` will fail, 
   the reason is: the original LogicalProject has collation trait (see the picture in [FLINK-15381](https://issues.apache.org/jira/browse/FLINK-15381): [1] which means the second field is ordered and its direction is ascending), but when LogicalProject converts to LogicalCalc in ProjectToCalcRule, the collation info of new Calc is empty. The root cause is the collation derive logic on RelSubset in RelMdCollation in not collect.
   This PR aim to fix the bug*
   
   
   ## Brief change log
   
     - *Implement Collation handler in Flink (most logic except the part about RelSubset is same with RelMdCollation)*
   
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
     - *Added FlinkRelMdCollationTest to verify the collation derive logic on each RelNode *
     - *Added test that validates the plan of `select cast(a as int), cast(b as varchar) from (values (3, 'c')) T(a,b)`*
     
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no)**
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no)**
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 07:13;githubbot;600","wuchong commented on pull request #10694: [FLINK-15381] [table-planner-blink] correct collation derive logic on RelSubset in RelMdCollation
URL: https://github.com/apache/flink/pull/10694
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 12:53;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Dec/19 06:56;godfreyhe;image-2019-12-26-14-56-00-634.png;https://issues.apache.org/jira/secure/attachment/12989479/image-2019-12-26-14-56-00-634.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 02 12:56:14 UTC 2020,,,,,,,,,,"0|z09yrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/19 06:54;godfreyhe; !image-2019-12-26-14-56-00-634.png! 
the reason is: the original {{LogicalProject}} has collation trait (see the above picture: {{[1]}} which means the second field is ordered and its direction is ascending), but when {{LogicalProject}} converts to {{LogicalCalc}} in {{ProjectToCalcRule}}, the collation info of new Calc is empty. The root cause is the collation derive logic on {{RelSubset}} in {{RelMdCollation}} in not collect.


;;;","26/Dec/19 07:02;ykt836;Just curious, who generated collation trait in the plan? It seems no operator involved has such information. ;;;","02/Jan/20 02:55;godfreyhe;The collection trait is generated when creating {{LogicalProject}} in {{SqlToRelConverter}}. code is 
{code:java}
  // LogicalProject.java
  public static LogicalProject create(final RelNode input,
      final List<? extends RexNode> projects, RelDataType rowType) {
    final RelOptCluster cluster = input.getCluster();
    final RelMetadataQuery mq = cluster.getMetadataQuery();
    final RelTraitSet traitSet =
        cluster.traitSet().replace(Convention.NONE)
            .replaceIfs(RelCollationTraitDef.INSTANCE,
                () -> RelMdCollation.project(mq, input, projects));
    return new LogicalProject(cluster, traitSet, input, projects, rowType);
  }
{code}
;;;","02/Jan/20 04:21;godfreyhe;{{RelMdCollation}} will derive collation for `Values`. The derivation result of {{Values ((1, 9, true, 2), (2, 6, false, 3), (3, 3, true, 4))}} is {{(0, 1, 2, 3)}} and {{(3)}};;;","02/Jan/20 12:56;jark;[FLINK-15381] [table-planner-blink] Fix collation derive logic on RelSubset in RelMdCollation
 - 1.11.0: 6b6dfd07950a22e5de421f9712b826fe6f756ae1
 - 1.10.0: 49ddaa1f5bbc12570155e24ca766228089c76016;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to set number of TM and number of Slot for MiniCluster in Scala shell,FLINK-15380,13276198,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjffdu,zjffdu,zjffdu,24/Dec/19 08:31,06/Jan/20 16:07,13/Jul/23 08:10,03/Jan/20 09:33,1.10.0,,,,,,,,,1.10.0,,,,Scala Shell,,,,,0,pull-request-available,,,,"It is possible to set that for MiniCluster in other places, but unable to do that in Scala shell. ",,tison,zjffdu,,,,,,,,,,,,"zjffdu commented on pull request #10676: [FLINK-15380][scala-shell]. Unable to set number of TM and number of Slot for MiniCluster in Scala shell
URL: https://github.com/apache/flink/pull/10676
 
 
   # What is the purpose of the change
   
   User often do testing in local mode(especially in flink scala shell), but for now in scala shell it is uanble to set number of TM and number of Slot, that means we always get 1 TM with 1 slot which is usually doesn't work for streaming job.  This PR is a straightforward fix for this issue. 
   
   ## Brief change log
   
   This PR just set number of TM and slot per TM in MiniClusterConfiguration before creating MiniCluster.
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   Verify it manually in scala shell.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Dec/19 08:38;githubbot;600","TisonKun commented on pull request #10676: [FLINK-15380][scala-shell]. Unable to set number of TM and number of Slot for MiniCluster in Scala shell
URL: https://github.com/apache/flink/pull/10676
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 09:32;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 09:33:25 UTC 2020,,,,,,,,,,"0|z09ypk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/20 09:33;tison;master via a9839f3b974a9b62468495491a1d1e84c616e3e7
1.10 via e44f2d653da246807b4bf5070c4027e3afdf0a54;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos WordCount test fails on travis,FLINK-15377,13276186,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,guoyangze,liyu,liyu,24/Dec/19 06:47,30/Nov/21 20:38,13/Jul/23 08:10,02/Jan/20 15:50,1.10.0,,,,,,,,,1.10.0,,,,Deployment / Mesos,,,,,0,pull-request-available,test-stability,,,"The ""Run Mesos WordCount test"" fails nightly run on travis with below error:
{code}
rm: cannot remove '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test-data/log/mesos-sl/mesos-slave.INFO': Permission denied
rm: cannot remove '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test-data/log/mesos-sl/mesos-fetcher.INFO': Permission denied
rm: cannot remove '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test-data/log/mesos-sl/mesos-slave.4a4fda410c57.invalid-user.log.INFO.20191224-031307.1': Permission denied
...
[FAIL] 'Run Mesos WordCount test' failed after 5 minutes and 26 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
{code}

https://api.travis-ci.org/v3/job/628795106/log.txt",,gjy,guoyangze,liyu,zhuzh,zjwang,,,,,,,,,"KarmaGYZ commented on pull request #10675: [FLINK-15377][e2e] Mesos WordCount test fails on travis
URL: https://github.com/apache/flink/pull/10675
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The Mesos WordCount test fails on Travis because of permission relevant problem. This PR fix it.
   
   ## Brief change log
   
    - Using sudo privilege to clean-up files in Mesos test
    - Skip the exception check for Mesos e2e test as the correctness of the test has been verified by `check_result_hash` function.
   
   ## Verifying this change
   
   Trigger Mesos e2e test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? no
   
   cc @GJL @tillrohrmann @zhuzhurk 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Dec/19 08:27;githubbot;600","zhuzhurk commented on pull request #10675: [FLINK-15377][e2e] Mesos WordCount test fails on travis
URL: https://github.com/apache/flink/pull/10675
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 05:37;githubbot;600","GJL commented on pull request #10695: [FLINK-15377] Remove the useless stage in Mesos dockerfile
URL: https://github.com/apache/flink/pull/10695
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 15:46;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 02 15:50:21 UTC 2020,,,,,,,,,,"0|z09ymw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Dec/19 06:47;liyu;[~guoyangze] FYI.;;;","24/Dec/19 07:14;guoyangze;[~gjy] That's why I add _sudo_ to the clean-up function in the beginning. However, it's an accidental error. I could not reproduce it locally; Thus, I could not figure out what wrong with the permission. I prefer to add sudo privilege. WDYT?

cc [~trohrmann];;;","24/Dec/19 07:22;guoyangze;BTW, as the correctness of this test case has been verified by the *check_result_hash* function. I think we could also skip the exception check.

At least, it could be a quick fix without breaking the test. WDYT?;;;","25/Dec/19 02:07;guoyangze;After re-read the error log, I found error 
{code:java}
Exception occurred in REST handler: Cannot connect to ResourceManager right now. Please try to refresh.
{code}

It's because we wait the rest endpoint up by periodically requesting the TaskManagerInfo. However, at the very beginning, the resource manager has not been set up, which is the root cause of the above Exception and this issue.

I think we could add this exception to the whitelist. WDYT?

Regarding the permission issue, we could unmount the log and data dir of Mesos, Docker will clean-up the files itself.;;;","25/Dec/19 05:40;zhuzh;Fixed via

master:
3036dd97eef4d1451baf6984885ad3784333a617

1.10.0:
d3eb451c2551ffa928bb04c8445c0967af69da60;;;","26/Dec/19 06:10;liyu;Another instance after the merged commits:
{code}
ERROR: Could not build mesos image. Aborting...
Removing network docker-mesos-cluster-network
Network docker-mesos-cluster-network not found.
[FAIL] Test script contains errors.
{code}

https://api.travis-ci.org/v3/job/629426135/log.txt

build link: https://travis-ci.org/apache/flink/jobs/629426135
commit of build: 02ea912;;;","26/Dec/19 07:00;guoyangze;Thanks for reporting it [~liyu], I think it's a network environment issue while installing docker. Maybe we could found another way to enhance the stability of image building. But so far, I'd like to downgrade the priority of this issue to critical. WDYT?;;;","26/Dec/19 08:36;liyu;I suggest to wait for another nightly build to see whether the issue reproduces, and downgrade it if not. What do you think? [~guoyangze];;;","27/Dec/19 09:50;guoyangze;Since the issue not reproduce in https://travis-ci.org/apache/flink/builds/629699386?utm_medium=notification&utm_source=github_status and https://travis-ci.org/apache/flink/builds/629644154?utm_medium=notification&utm_source=github_status. Downgrade this issue to critical.;;;","02/Jan/20 09:22;gjy;I will review this.;;;","02/Jan/20 15:50;gjy;1.10: cd5cda1d34bfebeac2354207fdb97087d4381f90
master: 8830ef0eba258a52761e164d98b20ceef99b13e3;;;",,,,,,,,,,,,,,,,,,,,,
Configured write buffer manager actually not take effect in RocksDB's DBOptions,FLINK-15370,13276160,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyu,yunta,yunta,24/Dec/19 03:24,07/Jan/20 19:02,13/Jul/23 08:10,07/Jan/20 19:02,1.10.0,1.11.0,,,,,,,,1.10.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"Currently, we call {{DBOptions#setWriteBufferManager}} after we extract the {{DBOptions}} from {{RocksDBResourceContainer}}, however, we would extract a new {{DBOptions}}  when creating the RocksDB instance. In other words, the configured write buffer manager would not take effect in the {{DBOptions}} which finally used in target RocksDB instance.",,liyu,sewen,wind_ljy,yunta,,,,,,,,,,"carp84 commented on pull request #10670: [FLINK-15370][state backends] Make sure sharedResources takes effect in RocksDBResourceContainer
URL: https://github.com/apache/flink/pull/10670
 
 
   
   ## What is the purpose of the change
   
   Currently we never uses the `sharedResources` in `RocksDBResourceContainer`, and in `RocksDBStateBackend.createKeyedStateBackend` we set the shared write buffer manager to the `dbOptions` but never pass/use it in `RocksDBKeyedStateBackendBuilder`, which leads to the result that different RocksDB backends actually are using separate write buffer managers thus the total memory control is invalid.
   
   
   ## Brief change log
   
   Always set write buffer manager to the one from `sharedResources` if it's non-null, in `RocksDBResourceContainer.getDbOptions`.
   
   
   ## Verifying this change
   
   This change added a new `testSharedResources` test case to cover the problematic case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Dec/19 04:43;githubbot;600","asfgit commented on pull request #10670: [FLINK-15370][state backends] Make sure sharedResources takes effect in RocksDBResourceContainer
URL: https://github.com/apache/flink/pull/10670
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 17:00;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-15368,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 19:02:34 UTC 2020,,,,,,,,,,"0|z09yh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/20 19:02;sewen;Fixed in
  - 1.10.0 via {{6184922158b40d25ad42ae0ccb4abf579c9e7ff2}} and {{666c4562909c4512d1a0df51a0067ca7e577ab76}}
  - 1.11.0 via {{0b7d496f3ddb178dc2ad1333ffb692055006a210}} and {{6ff392842f9dc4d3c9c808e7912558d477826379}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hbase connector do not support datatypes with precision like TIMESTAMP(9) and DECIMAL(10,4)",FLINK-15363,13276040,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,23/Dec/19 12:44,02/Jun/20 15:29,13/Jul/23 08:10,02/Jun/20 15:28,1.10.0,,,,,,,,,1.11.0,,,,Connectors / HBase,Table SQL / Ecosystem,,,,0,usability,,,,"{code:java}
// exception msg
rowtype of new rel:rowtype of new rel:RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" item, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency, DECIMAL(10, 4) amount, TIMESTAMP(3) order_time, TIME ATTRIBUTE(PROCTIME) NOT NULL proc_time, DECIMAL(20, 4) amount_kg, TIME ATTRIBUTE(ROWTIME) ts, BIGINT currency_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency_name, DECIMAL(38, 4) rate, TIMESTAMP(3) currency_time, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" rowkey, RecordType:peek_no_expand(INTEGER country_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country_name, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country_name_cn, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" region_name) f1, RecordType:peek_no_expand(TIMESTAMP(3) record_timestamp3, TIMESTAMP(3) record_timestamp9, TIME(0) time3, TIME(0) time9, DECIMAL(38, 18) gdp) f2) NOT NULLrowtype of set:RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" item, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency, DECIMAL(10, 4) amount, TIMESTAMP(3) order_time, TIME ATTRIBUTE(PROCTIME) NOT NULL proc_time, DECIMAL(20, 4) amount_kg, TIME ATTRIBUTE(ROWTIME) ts, BIGINT currency_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency_name, DECIMAL(38, 4) rate, TIMESTAMP(3) currency_time, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" rowkey, RecordType:peek_no_expand(INTEGER country_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country_name, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country_name_cn, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" region_name) f1, RecordType:peek_no_expand(TIMESTAMP(3) record_timestamp3, TIMESTAMP(9) record_timestamp9, TIME(0) time3, TIME(0) time9, DECIMAL(10, 4) gdp) f2) NOT NULL at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:84) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:167) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:89) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:223) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:150) at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:351) at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.java:259) at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.java:250) at job.KafkaJoinHbase2Hbase.testJoinHbaseWithPrecision(KafkaJoinHbase2Hbase.java:109) at job.KafkaJoinHbase2Hbase.main(KafkaJoinHbase2Hbase.java:24)Caused by: java.lang.AssertionError: Type mismatch:rowtype of new rel:RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" item, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency, DECIMAL(10, 4) amount, TIMESTAMP(3) order_time, TIME ATTRIBUTE(PROCTIME) NOT NULL proc_time, DECIMAL(20, 4) amount_kg, TIME ATTRIBUTE(ROWTIME) ts, BIGINT currency_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency_name, DECIMAL(38, 4) rate, TIMESTAMP(3) currency_time, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" rowkey, RecordType:peek_no_expand(INTEGER country_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country_name, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country_name_cn, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" region_name) f1, RecordType:peek_no_expand(TIMESTAMP(3) record_timestamp3, TIMESTAMP(3) record_timestamp9, TIME(0) time3, TIME(0) time9, DECIMAL(38, 18) gdp) f2) NOT NULLrowtype of set:RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" item, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency, DECIMAL(10, 4) amount, TIMESTAMP(3) order_time, TIME ATTRIBUTE(PROCTIME) NOT NULL proc_time, DECIMAL(20, 4) amount_kg, TIME ATTRIBUTE(ROWTIME) ts, BIGINT currency_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency_name, DECIMAL(38, 4) rate, TIMESTAMP(3) currency_time, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" rowkey, RecordType:peek_no_expand(INTEGER country_id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country_name, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" country_name_cn, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" region_name) f1, RecordType:peek_no_expand(TIMESTAMP(3) record_timestamp3, TIMESTAMP(9) record_timestamp9, TIME(0) time3, TIME(0) time9, DECIMAL(10, 4) gdp) f2) NOT NULL at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31) at org.apache.calcite.plan.RelOptUtil.equal(RelOptUtil.java:2026) at org.apache.calcite.plan.volcano.RelSubset.add(RelSubset.java:284) at org.apache.calcite.plan.volcano.RelSet.add(RelSet.java:148) at org.apache.calcite.plan.volcano.VolcanoPlanner.addRelToSet(VolcanoPlanner.java:1818) at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1764) at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:846) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:868) at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1939) at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:129) at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:236) at org.apache.flink.table.planner.plan.rules.physical.common.BaseSnapshotOnTableScanRule.onMatch(CommonLookupJoinRule.scala:142) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:208) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:631) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:327) at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64) ... 21 more
Process finished with exit code 1
{code}",,jark,leonard,lzljs3620320,openinx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16146,,,,,FLINK-15525,,,,,,,FLINK-17028,FLINK-15469,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 15:22:11 UTC 2020,,,,,,,,,,"0|z09xqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/20 02:46;docete;Currently *HBaseTableSchema* use TypeInformation to specify an HBase table's schema, which would cause precision/scale loss for several data types. Meanwhile *HBaseTypeUtils* serialize a java.sql.Timestamp to long in bytes, which would cause precision loss for TIMESTAMP types.

the hbase connector should use new type system to fix this.;;;","09/Jan/20 06:17;leonard;[~docete][~lzljs3620320]
Do we plan to fix this in 1.10? if not we can set fix version to 1.11;;;","10/Jan/20 02:51;lzljs3620320;Looks like it is not a minor fix, I am OK to set it to 1.11.;;;","03/Feb/20 11:21;leonard;[~jark] could you assign this to me？I would like to work for this.;;;","03/Feb/20 14:00;jark;Assigned to you [~Leonard Xu]. Thanks for picking up it.;;;","06/Feb/20 08:49;leonard;HBaseUpsertTableSink implements UpsertStreamTableSink and uses new type system,  FLINK-15469 will enable UpsertStreamTableSink to support new type system, after that, we can support dataType in source and sink at the same time.;;;","02/Jun/20 15:22;leonard;After we replaced `TypeInformation` with `DataType`  in HBaseDynamicTableSource/HBaseDynamicTableSink`(FLINK-17028), the decimal precision bug has disappeared in new table Source/Sink, for timestamp precision the new Source/Sink support TIMESTAMP(3) well and have a TODO to fix later.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ParquetTableSource should pass predicate in projectFields,FLINK-15361,13275995,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,23/Dec/19 07:30,06/Jan/20 16:08,13/Jul/23 08:10,23/Dec/19 17:49,,,,,,,,,,1.10.0,1.9.2,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,"After projectFields, ParquetTableSource will loose predicates.

Since this is only performance related, the test did not fail.",,lzljs3620320,phoenixjiangnan,,,,,,,,,,,,"JingsongLi commented on pull request #10660: [FLINK-15361][parquet] ParquetTableSource should pass predicate in projectFields
URL: https://github.com/apache/flink/pull/10660
 
 
   
   ## What is the purpose of the change
   
   After projectFields, ParquetTableSource will loose predicates.
   Since this is only performance related, the test did not fail.
   
   ## Brief change log
   
   pass  predicate in `ParquetTableSource.projectFields`.
   
   ## Verifying this change
   
   Since volcano, it is hard to verify it.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 07:32;githubbot;600","bowenli86 commented on pull request #10660: [FLINK-15361][parquet] ParquetTableSource should pass predicate in projectFields
URL: https://github.com/apache/flink/pull/10660
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 17:47;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 23 17:49:17 UTC 2019,,,,,,,,,,"0|z09xgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/19 17:49;phoenixjiangnan;master: 0f88c392b6ad6f91aed33e157bdc9df6f613f09d
1.10: 627a47289c350d6ef0e6cd00cf62f021f2ead614
1.9: 85e0f1534eeb0fc5218007ee09a90d9b3f8bad0e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn e2e test is broken with building docker image,FLINK-15360,13275923,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,guoyangze,wangyang0918,wangyang0918,22/Dec/19 10:33,24/Dec/19 02:18,13/Jul/23 08:10,23/Dec/19 14:31,,,,,,,,,,1.10.0,,,,,,,,,0,pull-request-available,,,,"Yarn e2e test is broken with building docker image. This is because this change [https://github.com/apache/flink/commit/cce1cef50d993aba5060ea5ac597174525ae895e].

 

Shell function \{{retry_times}} do not support passing a command as multiple parts. For example, \{{retry_times 5 0 docker build image}} could not work.

 

cc [~karmagyz]",,guoyangze,liyu,wangyang0918,zjwang,,,,,,,,,,"KarmaGYZ commented on pull request #10657: [FLINK-15360][e2e] Fix the parameter issue in retry_times
URL: https://github.com/apache/flink/pull/10657
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This PR fix the parameter issue in `retry_times` function of common.sh.
   
   ## Brief change log
   
   Fix the parameter issue in `retry_times` function of common.sh.
   
   ## Verifying this change
   
   Trigger container relevant e2e tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
   cc: @wangyang0918 @tillrohrmann 
   Since it's a blocker issue for e2e test, I open this PR without being assigned first.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Dec/19 11:57;githubbot;600","zhijiangW commented on pull request #10657: [FLINK-15360][e2e] Fix the parameter issue in retry_times
URL: https://github.com/apache/flink/pull/10657
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 09:12;githubbot;600","KarmaGYZ commented on pull request #10663: [FLINK-15360][e2e] Fix the parameter issue in retry_times
URL: https://github.com/apache/flink/pull/10663
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This PR fix the parameter issue in `retry_times` function of common.sh.
   
   ## Brief change log
   
   Fix the parameter issue in `retry_times` function of common.sh.
   
   ## Verifying this change
   
   Trigger container relevant e2e tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
   cc @zhijiangW 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 09:20;githubbot;600","KarmaGYZ commented on pull request #10663: [FLINK-15360][e2e] Fix the parameter issue in retry_times
URL: https://github.com/apache/flink/pull/10663
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 09:21;githubbot;600","KarmaGYZ commented on pull request #10664: [FLINK-15360][e2e] Fix the parameter issue in retry_times
URL: https://github.com/apache/flink/pull/10664
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This PR fix the parameter issue in `retry_times` function of common.sh.
   
   ## Brief change log
   
   Fix the parameter issue in `retry_times` function of common.sh.
   
   ## Verifying this change
   
   Trigger container relevant e2e tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
   cc @zhijiangW 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 09:25;githubbot;600","zhijiangW commented on pull request #10664: [FLINK-15360][e2e] Fix the parameter issue in retry_times
URL: https://github.com/apache/flink/pull/10664
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 14:23;githubbot;600",,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 23 14:31:40 UTC 2019,,,,,,,,,,"0|z09x0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/19 11:31;guoyangze;[~fly_in_gis] Could you give the Travis link here?;;;","22/Dec/19 11:36;guoyangze;I think it because of the bug of  https://github.com/apache/flink/commit/bdd432a09a776fec88f465771101893a8151dc1d. The local variable `command` does not catch the command correctly.
 I'd like to fix it. [~trohrmann] Could you assign this to me?;;;","23/Dec/19 06:02;liyu;Another instance: https://api.travis-ci.org/v3/job/628412456/log.txt;;;","23/Dec/19 14:31;zjwang;Merged in master: e30bcfd9c8cbe56c1072fe9895f1e6d03389c31e

Merged in release-1.10: 5ebab4fa2e51791fc04e04e3ab6fbbfc9f243fce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nightly streaming file sink fails with unshaded hadoop,FLINK-15355,13275851,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,arvid,arvid heise,21/Dec/19 07:16,22/Jun/21 14:07,13/Jul/23 08:10,21/Jan/20 12:16,1.10.0,1.11.0,,,,,,,,1.10.0,,,,FileSystems,,,,,0,pull-request-available,,,,"{code:java}
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
 at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
 at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
 at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138)
 at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)
 at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
 at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)
 at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
 at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
 at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
 at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:199)
 at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1751)
 at org.apache.flink.streaming.api.environment.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:94)
 at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:63)
 at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1628)
 at StreamingFileSinkProgram.main(StreamingFileSinkProgram.java:77)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
 ... 11 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
 at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
 at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1746)
 ... 20 more
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
 at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$7(RestClusterClient.java:326)
 at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)
 at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852)
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
 at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:274)
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
 at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
 at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.
 at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:336)
 at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)
 at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
 at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)
 at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
 ... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
 at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:152)
 at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84)
 at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:379)
 at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)
 ... 7 more
Caused by: java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.getTimeDuration(Ljava/lang/String;Ljava/lang/String;Ljava/util/concurrent/TimeUnit;)J
 at org.apache.hadoop.fs.s3a.S3ARetryPolicy.<init>(S3ARetryPolicy.java:113)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:257)
 at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:126)
 at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:61)
 at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:441)
 at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:362)
 at org.apache.flink.core.fs.Path.getFileSystem(Path.java:298)
 at org.apache.flink.runtime.state.memory.MemoryBackendCheckpointStorage.<init>(MemoryBackendCheckpointStorage.java:85)
 at org.apache.flink.runtime.state.memory.MemoryStateBackend.createCheckpointStorage(MemoryStateBackend.java:295)
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.<init>(CheckpointCoordinator.java:279)
 at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.<init>(CheckpointCoordinator.java:205)
 at org.apache.flink.runtime.executiongraph.ExecutionGraph.enableCheckpointing(ExecutionGraph.java:486)
 at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:338)
 at org.apache.flink.runtime.scheduler.SchedulerBase.createExecutionGraph(SchedulerBase.java:245)
 at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:217)
 at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:205)
 at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:119)
 at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:105)
 at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:278)
 at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:266)
 at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)
 at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)
 at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.<init>(JobManagerRunnerImpl.java:146)
 ... 10 more
{code}",,arvid heise,banmoy,chenyuyun-emc,klion26,liyu,pnowojski,trohrmann,wangyang0918,zhuzh,zjwang,,,,"AHeise commented on pull request #10678: [FLINK-15355][plugin] Remove parent first from plugin classloader.
URL: https://github.com/apache/flink/pull/10678
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Parent first classes are mostly used to guard against shaded flink classes in user code. Since plugins are currently provided by us or will be provided by expert users, the chance of wrongly shaded flink classes are minuscule.
   
   ## Brief change log
   
   Remove parent first from plugin classloader.
   
   ## Verifying this change
   
   - Existing e2e tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (**yes** / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Dec/19 20:30;githubbot;600","carp84 commented on pull request #10696: [hotfix][e2e] Disable Streaming File Sink s3 e2e test until FLINK-15355 has been fixed
URL: https://github.com/apache/flink/pull/10696
 
 
   
   ## What is the purpose of the change
   
   Disable Streaming File Sink s3 e2e test temporarily until FLINK-15355 has been fixed, since it's blocking the nightly build and may undercover some other problems.
   
   
   ## Brief change log
   
   Disable Streaming File Sink s3 e2e test temporarily in relative travis scripts.
   
   
   ## Verifying this change
   
   This change is a trivial work as described.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 09:46;githubbot;600","carp84 commented on pull request #10699: [hotfix][e2e] Disable Streaming File Sink s3 e2e test until FLINK-15355 has been fixed
URL: https://github.com/apache/flink/pull/10699
 
 
   
   ## What is the purpose of the change
   
   Disable Streaming File Sink s3 e2e test temporarily until FLINK-15355 has been fixed, since it's blocking the nightly build and may undercover some other problems.
   
   ## Brief change log
   
   Disable Streaming File Sink s3 e2e test temporarily in relative travis scripts.
   
   ## Verifying this change
   
   This change is a trivial work as described.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 11:29;githubbot;600","carp84 commented on pull request #10696: [hotfix][e2e] Disable Streaming File Sink s3 e2e test until FLINK-15355 has been fixed
URL: https://github.com/apache/flink/pull/10696
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 11:31;githubbot;600","zhijiangW commented on pull request #10699: [hotfix][e2e] Disable Streaming File Sink s3 e2e test until FLINK-15355 has been fixed
URL: https://github.com/apache/flink/pull/10699
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 11:37;githubbot;600","carp84 commented on pull request #10719: [hotfix][e2e] Disable Shaded Hadoop S3A with credentials provider e2e test until FLINK-15355 has been fixed
URL: https://github.com/apache/flink/pull/10719
 
 
   
   ## What is the purpose of the change
   
   Further disable 'Shaded Hadoop S3A with credentials provider' e2e test to survive from the issue of FLINK-15355, as indicated by [this comment](https://issues.apache.org/jira/browse/FLINK-15355?focusedCommentId=17005159&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17005159)
   
   
   ## Brief change log
   
   Disable 'Shaded Hadoop S3A with credentials provider' in `split_misc.sh`.
   
   ## Verifying this change
   
   This change is a trivial work without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Dec/19 07:07;githubbot;600","zhijiangW commented on pull request #10719: [hotfix][e2e] Disable Shaded Hadoop S3A with credentials provider e2e test until FLINK-15355 has been fixed
URL: https://github.com/apache/flink/pull/10719
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Dec/19 10:34;githubbot;600","pnowojski commented on pull request #10678: [FLINK-15355][plugin] Remove parent first from plugin classloader.
URL: https://github.com/apache/flink/pull/10678
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 09:08;githubbot;600","AHeise commented on pull request #10798: [FLINK-15355][plugins] Added parent first patterns for plugins.
URL: https://github.com/apache/flink/pull/10798
 
 
   Forwards port of https://github.com/apache/flink/pull/10778 . Main commit untouched and only the commit that reenables tests is dropped as they were never disabled on master.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/20 08:51;githubbot;600","pnowojski commented on pull request #10798: [FLINK-15355][plugins] Added parent first patterns for plugins.
URL: https://github.com/apache/flink/pull/10798
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/20 09:25;githubbot;600","AHeise commented on pull request #10826: [FLINK-15355][plugins] Fixed parsing of plugin parent-first patterns.
URL: https://github.com/apache/flink/pull/10826
 
 
   Unchanged forward port of https://github.com/apache/flink/pull/10816 .
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 08:57;githubbot;600","zhijiangW commented on pull request #10826: [FLINK-15355][plugins] Fixed parsing of plugin parent-first patterns.
URL: https://github.com/apache/flink/pull/10826
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 11:28;githubbot;600","AHeise commented on pull request #10845: [FLINK-15355][plugins] Classloader avoids loading unrelated services.
URL: https://github.com/apache/flink/pull/10845
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Extends child-first class loader to a child-only classloader for service definitions.
   
   Plugins are considered self-contained such that all required services are contained within it. Thus, it would even be possible to use different versions of security providers for different classes.
   
   This behavior also avoids accidental loading of services outside of the plugin, especially for hadoop distributions in /lib. A service loader from the plugin would detect such services but cannot load them successfully because parts of the service' class hierarchy would have been loaded with the system class loader.
   
   ## Brief change log
   
   - Plugin classloader does not look into parent classloader for service definitions.
   
   ## Verifying this change
   
   - `Shaded Hadoop S3A with credentials provider end-to-end test`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (**yes** / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 15:43;githubbot;600","pnowojski commented on pull request #10845: [FLINK-15355][plugins]Classloader restricts access to parent to whitelist.
URL: https://github.com/apache/flink/pull/10845
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 11:54;githubbot;600",,0,8400,,,0,8400,,,,,,,,,,,,,FLINK-15551,,,,,,,,,,,,,FLINK-11956,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 21 12:16:40 UTC 2020,,,,,,,,,,"0|i3jpe5:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/19 07:20;arvid;Most likely the Hadoop `Configuration` from flink-fs-hadoop-shaded has already been loaded, such that it's not loaded through child class loader.

A quick-fix could be to relocate everything in the hadoop-s3 and presto-s3 that is also contained in flink-fs-hadoop-shaded (3 files at the time of writing).

Long-term solution is to get rid of flink-fs-hadoop-shaded entirely and in particular moving it out of flink-dist (which is currently not feasible because of mapr).;;;","22/Dec/19 13:36;arvid;Initial suspicion could not be verified: `Configuration` from flink-fs-hadoop-shaded has this method. So somehow another Configuration is coming into flink-dist or hadoop-s3.;;;","23/Dec/19 05:59;liyu;Another 2 instances:
https://api.travis-ci.org/v3/job/628137535/log.txt
https://api.travis-ci.org/v3/job/628412438/log.txt;;;","24/Dec/19 07:32;banmoy;This test begins to fail after FLINK-11956 with commit 8ec545d56f007645ca8f2a2374386882132ffc7a. The name of this test is ""e2e - misc - hadoop 2.8"" whose profile contains ""-Dinclude-hadoop -Dhadoop.version=2.8.3"", and flink-shaded-hadoop-2-uber-2.8.3-9.0.jar will be put into lib directory.  After enable jvm option ""-verbose:class"" in jobmanager.sh, we can find that ""org.apache.hadoop.conf.Configuration"" is loaded from flink-shaded-hadoop-2-uber-2.8.3-9.0.jar rather than hadoop-s3. ""Configuration#getTimeDuration(String name, String defaultValue, TimeUnit unit)"" only exists in hadoop-3.1.0 which hadoop-s3 depends on, so NoSuchMethodError is throwed.   ;;;","24/Dec/19 08:36;banmoy;The root cause is that default value of ""classloader.parent-first-patterns.default"" contains ""org.apache.hadoop."", and parent class loader includes hadoop, so ChildFirstClassLoader will not load Configuration from plugin jar. Actually if job runs on yarn cluster with no bundled hadoop, there may be also conflict between hadoop-s3 and yarn. My colleagues and I make some discussions about the solutions
 # remove ""org.apache.hadoop."" from parent-first-patterns globally, but it can change other component's behavior, and need to be cautious.
 # provide a plugin-level option for parent-first-patterns, and each plugin can decide what dependency to inherit from parent class loader. 
 # make hadoop used by Flink pluggable, and parent class loader will not contain hadoop. It maybe a long term goal.

At the moment, solution 2 may be a better choice. What do you think? [~arvid heise] [~pnowojski];;;","24/Dec/19 19:07;liyu;Does it make sense if we revert FLINK-11956 and retarget it at 1.11.0 since it introduces some problem that a quick fix cannot resolve but requires long term solution with quite some impact, especially at this phase of the release? What's your opinion [~maguowei] [~zjwang] [~zhuzh]? Thanks.;;;","24/Dec/19 20:37;arvid;Hi [~banmoy] , thank you for the throughout investigation. Your provided options are plausible, but it's hard for me to decide which way to go.

I drafted a 4. option as a PR that is inspired by your 1. and 2. options: remove parent-first-patterns from plugins altogether. I can see it being super useful in user code, especially for the often happening shading of flink jars inside the user code, but I currently don't see why it's necessary in the plugins that we provide. If that doesn't work out, option 2 should be best choice, albeit more complex.

I added all e2e tests to the normal execution of said PR to hopefully gain some insides. [https://github.com/apache/flink/pull/10678]

[~liyu], reverting FLINK-11956 would certainly be an option. Removing the shading was pushed by [~sewen] to avoid a whole lot of other issues with the current shaded plugins (namely the inability to use credential providers from AWS sufficiently, see FLINK-15215). Before reverting, I'd definitively consult him as well.;;;","25/Dec/19 02:45;wangyang0918;I think removing {{alwaysParentFirstPatterns}} in plugin mechanism is a valid fix. Since we will delegate the always-parent-first-pattern configuration to each plugin implementation. Every plugin could decide which packages need to always load from parent. We need to create a new ticket to track this. However, it is not a blocker. Users could exclude these parent-first packages from the plugin jar to work around.;;;","26/Dec/19 06:18;liyu;Another instance: https://api.travis-ci.org/v3/job/629426131/log.txt;;;","26/Dec/19 06:21;liyu;[~arvid heise] thanks for the clarification and good to know the context. I'm watching this issue since it's failing the release-1.10 nightly build stably, and hopefully we could figure out a way to unblock it quickly (but of course no rush or hack and finally resolve it through a thorough solution). Thanks.;;;","26/Dec/19 09:43;liyu;Planning to temporarily disable the ""Streaming File Sink s3 end-to-end test"" until the issue here is fixed to unblock nightly build, FYI.;;;","26/Dec/19 12:08;liyu;Temporarily disabled ""Streaming File Sink s3 end-to-end test"" in release-1.10 via: de45fdbb59;;;","26/Dec/19 14:17;arvid;I reran [https://github.com/apache/flink/pull/10678] three times and haven't observed the issues anymore. However, none of the builds succeeded for other known e2e bugs.;;;","27/Dec/19 11:56;liyu;True, 3 more e2e blocker issues emerge after disable the test here...: FLINK-15426, FLINK-15427, FLINK-15428;;;","30/Dec/19 06:44;liyu;'Shaded Hadoop S3A with credentials provider end-to-end test' failed with the same issue, need to disable more tests to prevent the impact:

https://api.travis-ci.org/v3/job/630372569/log.txt
https://api.travis-ci.org/v3/job/630652293/log.txt
https://api.travis-ci.org/v3/job/630652300/log.txt

Update: temporarily disabled ""Shaded Hadoop S3A with credentials provider"" in release-1.10 via 7e2690489d;;;","02/Jan/20 14:30;pnowojski;Adapting [my comment from the pull request|https://github.com/apache/flink/pull/10678#pullrequestreview-336577210]

I would be against 4th option proposed by [~arvid heise]. I think it’s really important to all SPI classes be parent loaded first, otherwise very bad things can happen. For example if user accidentally embeds a different version (1.9.2 vs 1.9.1) of org.apache.flink classes. How I understand the issue:

If you have a SPI class (like Foo), if you implement it's method (like Bar Foo#bar() \{ return new Bar(); }), it's important that every object/class that crosses the boundary between plugin's implementation and the core system (like Bar) must originate from the core's system class loader. If not, if they are incompatible, you risk all kind of the dependency convergence errors (deadlocks, livelocks, method not found) if the versions of Bar are not binary compatible. Additionally if this Bar is passed around, it can crash at any place/point of time, very far away from the place it was created. Also using older plugin versions can just result in having an unfixed bugs, for example if Bar in 1.9.2 has some bug fixed.

Disclaimer: I don't have that much of an experience with class loading in Java and especially loading user classes, however all of the systems that I have seen, that were doing similar things (loading user classes, especially in a separate class loader), strongly emphasise ""parent first"" pattern for all of the dependencies that are on the boundary of the SPI.

https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/server/PluginClassLoader.java#L41
https://github.com/apache/nifi/blob/master/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-nar-utils/src/main/java/org/apache/nifi/nar/NarClassLoader.java#L42

I would be more inclined towards something like [~banmoy]'s option 1. or 2.

However probably we can choose 1. for the sake of backward compatibility with non plugin based class loading ({{classloader.parent-first-patterns.default}} is used in more places, not only in plugins). 

*What is the reason, why {{org.apache.hadoop}} is added to this list in the first place?* For plugins I think we should be able to drop it. Unless I'm missing something, I do not see a reason why {{org.apache.hadoop}} should be loaded parent first for any plugin - that would kind of defeat the purpose of any plugin using hadoop. As I wrote above, for Plugins, only SPI classes should be parent first loaded and {{org.apache.hadoop}} should never be a part of any SPI that we expose (unless I'm missing something...). 

If I'm not missing anything, I would vote for duplicating {{classloader.parent-first-patterns.additional}} and {{classloader.parent-first-patterns.default}} to something like {{classloader.plugin-parent-first-patterns.*}} and dropping maybe not only {{org.apache.hadoop}}, but maybe almost everything from there.;;;","06/Jan/20 08:05;arvid;Like I said before, option 4 assumes that no one bundles Flink classes into plugins. If we cannot guarantee that, it's not valid.

What I'd like to avoid is to have some magic list of patterns to avoid exactly these questions ""*why {{org.apache.hadoop}} is added to this list in the first place*"".;;;","06/Jan/20 08:17;arvid;Btw concerning why *{{org.apache.hadoop}}* is in the list: my guess is that the main reason is that we provide exactly some overridden implementation of Hadoop configuration etc. where parent first is absolutely necessary to properly support relocations.

Now, we could argue that this becomes unnecessary when we remove the relocations from plugins. However, so far we have only removed it partly (s3 plugins) and not for all. Thus, we must support such a parent-first list for some plugins and adjust it for others.

I'd propose a fifth option which expands on the option 2 of [~banmoy] . Allow plugins to specify a custom list (code is mostly there already). If such a custom list is supplied, it completely replaces Flink's default list. If no custom list is supplied, we fall back to Flink's default list.;;;","06/Jan/20 14:08;chesnay;Hadoop is in that list because some users ran into ClassCastException when using Hadoop and having Hadoop classes bundled in their jar. Source: https://github.com/apache/flink/pull/5313;;;","08/Jan/20 09:23;pnowojski;We have introduced a separate config options for parent first pattern in plugins.

Merged to master as d46b07bc6ef44dde058f4a15710938d29cdc1798
Merged to release-1.10 as 4c7562bf84c865859710dc79f05ae09541fa3335..14c26e489d1e8f7ae02080b3b69810b59e3ebe32;;;","09/Jan/20 06:52;liyu;Reopen since the same issue reoccurred in the latest release-1.10 nightly build: https://api.travis-ci.org/v3/job/634330760/log.txt

SHA of the build is f1ee817 thus 4c7562b is already included: https://travis-ci.org/apache/flink/builds/634330720;;;","09/Jan/20 07:43;liyu;Checked the travis run of [PR#10778|https://github.com/apache/flink/pull/10778] and confirmed it didn't really run the e2e case and gave a misleading result...
{noformat}
==============================================================================
Running 'Streaming File Sink s3 end-to-end test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/flink-ci/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-51024728284
Flink dist directory: /home/travis/build/flink-ci/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT
Did not find AWS environment variables, NOT running the e2e test.
Checking of logs skipped.

[PASS] 'Streaming File Sink s3 end-to-end test' passed after 0 minutes and 0 seconds! Test exited with exit code 0.
{noformat}

https://api.travis-ci.com/v3/job/272987373/log.txt;;;","09/Jan/20 10:36;chesnay;In {{FileSystem#getUnguardedFileSystem}} we use the classloader returned by the factory when calling {{FileSystem#create:}}
{code:java}
   ClassLoader classLoader = factory.getClassLoader();
   try (TemporaryClassLoaderContext classLoaderContext = new TemporaryClassLoaderContext(classLoader)) {
      fs = factory.create(uri);
   }

{code}
Supposedly this is the plugin classloader, but since the factory package is {{org.apache.flink...}}, isn't it loaded by the ParentClassLoader?

 ;;;","10/Jan/20 01:58;zjwang;Merged in release-1.10: aa37c0cd89053e68e72a19d51715b3a31b74163c

Merged in master: f7833aff7d50af5a3a3a671d9b6a44bd5dc17a67;;;","12/Jan/20 08:35;liyu;'Shaded Hadoop S3A with credentials provider end-to-end test' still fails in latest release-1.10 nightly builds, with below warning:
{code}
2020-01-12 01:32:13,095 WARN  org.apache.hadoop.fs.FileSystem                               - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.DistributedFileSystem not a subtype
2020-01-12 01:32:13,096 WARN  org.apache.hadoop.fs.FileSystem                               - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.WebHdfsFileSystem not a subtype
2020-01-12 01:32:13,097 WARN  org.apache.hadoop.fs.FileSystem                               - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.SWebHdfsFileSystem not a subtype
2020-01-12 01:32:13,097 WARN  org.apache.hadoop.fs.FileSystem                               - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.HftpFileSystem not a subtype
2020-01-12 01:32:13,098 WARN  org.apache.hadoop.fs.FileSystem                               - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.HsftpFileSystem not a subtype
{code}

https://api.travis-ci.org/v3/job/635358189/log.txt
https://api.travis-ci.org/v3/job/635722691/log.txt

It seems to me the root cause is still relative to the issue here, but please correct me if I'm wrong and feel free to close this one and open another issue to track the new problem (if it's new). Thanks.;;;","13/Jan/20 10:39;trohrmann;Another instance of the problem: https://api.travis-ci.org/v3/job/635957245/log.txt

Full build matrix: https://travis-ci.org/apache/flink/builds/635957218;;;","13/Jan/20 16:15;arvid;[~liyu], you are right this issue is related to the unshading. Analysis of the issue:
 * In the system classloader, there is flink-dist and hadoop-uber.
 * The plugin classloader contains our s3 filesystem and the s3 hadoop implementation.
 * During initialization, the s3 hadoop implementation uses the `ServiceLoader` to load all hadoop FileSystems.
 * Classloader finds not only the expected service definitions but also those of hadoop-uber.
 * `ServiceLoader` tries to load all file systems but ofc cannot load those of hadoop-uber because of inconsistent class hierarchies (mixed plugin and system classloader)
 * `ServiceLoader` issues warning that trigger the `check_error` script to fail.

Firstly, I verified if this is a fundamental issue in the interplay of plugins and libraries or a cosmetic. I successfully mixed plugins and libraries by reading from a local HDFS and writing to S3 minio and vice versa.

I implemented a solution [https://github.com/apache/flink/pull/10845] that limits the scope of services to plugins from the plugin perspective. That gives the desired isolation but may require users to add more jars to the plugin folder in the worst case (security provider?). On the other hand, that use case would allow users to use different versions of security providers for different plugins.

We also need to investigate if the sketchy reinitialization of `FileSystem`s in `BucketingSink` is working with plugins.;;;","16/Jan/20 11:56;pnowojski;Yet another fix merged:
merged commit d96456ebe32ebcf41a6bbf5dfaa76a4f4a8a1edb into apache:release-1.10
merged commit f2b6da5780742db82b428c9dbeabe02e3b080c48 into master;;;","17/Jan/20 10:15;arvid;Current PluginClassLoader fails to load module classes in Java 9+.
{noformat}
Caused by: java.lang.ClassNotFoundException: org.ietf.jgss.GSSException
 at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)
 at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)
 at org.apache.flink.core.plugin.PluginLoader$PluginClassLoader.loadClass(PluginLoader.java:147)
 at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
 ... 34 more{noformat};;;","21/Jan/20 08:14;liyu;More instances in latest release-1.10 nightly build complaining {{GSSException}} class not found:
https://api.travis-ci.org/v3/job/639573115/log.txt
https://api.travis-ci.org/v3/job/639573142/log.txt;;;","21/Jan/20 12:16;pnowojski;Merged to release-1.10 as 2035b2ba3fc9cf39feeaa2427d10807527d8be20^..2035b2ba3fc9cf39feeaa2427d10807527d8be20
Merged to master as 9af50565fa4bfc676a6fb2d9d0ff2aec23779fe4^..9af50565fa4bfc676a6fb2d9d0ff2aec23779fe4

Please do not re-open this issue unless the problem is exactly the same as before (the same exception, the same cause etc...), but create a new issue.;;;",
Start and stop minikube only in kubernetes related e2e tests,FLINK-15354,13275843,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,wangyang0918,wangyang0918,21/Dec/19 04:38,14/Jan/20 09:51,13/Jul/23 08:10,14/Jan/20 09:51,,,,,,,,,,1.10.0,,,,,,,,,0,pull-request-available,,,,"Currently, we start minikube in {{nightly.sh for every e2e test}}, and it will never be stopped. It is unnecessary and will occupy some resources on travis. I think the minikube should only be started in the kubernetes related test(\{{test_kubernetes_embedded_job.sh}}) and need to be stopped once the test finished.",,trohrmann,wangyang0918,zhuzh,,,,,,,,,,,"wangyang0918 commented on pull request #10665: [FLINK-15354] Start and stop minikube only in kubernetes related e2e tests
URL: https://github.com/apache/flink/pull/10665
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently, we start minikube in `nightly.sh` for every e2e test, and it will never be stopped. It is unnecessary and will occupy some resources on travis. I think the minikube should only be started in the kubernetes related test(`test_kubernetes_embedded_job.sh` and `test_kubernetes_session.sh`) and need to be stopped once the test finished.
   
   
   ## Brief change log
   
   * Use stable api apps/v1 instead of extensions/v1beta1
   * Remove starting minikube from`nightly.sh` and move to `common_kubernetes.sh`
   
   
   ## Verifying this change
   * The changes are covered by e2e tests `test_kubernetes_embedded_job.sh` and `test_kubernetes_session.sh`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 10:22;githubbot;600","tillrohrmann commented on pull request #10665: [FLINK-15354] Start and stop minikube only in kubernetes related e2e tests
URL: https://github.com/apache/flink/pull/10665
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jan/20 09:49;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,FLINK-15531,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 14 09:51:19 UTC 2020,,,,,,,,,,"0|z09wio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/19 08:24;wangyang0918;[~trohrmann] Please assign this ticket to me. After [FLINK-15360|https://issues.apache.org/jira/browse/FLINK-15360] fixed, i will attached a PR to move the minikube start/stop only in kubernetes related tests.;;;","24/Dec/19 02:35;zhuzh;Thanks [~fly_in_gis] for reporting this issue. I have assigned the ticket to you.;;;","14/Jan/20 09:51;trohrmann;Fixed via

master: 6cf5940480aec6c0e0d5ba22e1c3d2397229349a
1.10.0: 9c0bd30218bf4cd7f823ff40161ee315f9084b0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix orc optimization for version less than 2.3 by introducing orc shim,FLINK-15348,13275789,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,phoenixjiangnan,phoenixjiangnan,20/Dec/19 19:08,06/Jan/20 16:08,13/Jul/23 08:10,23/Dec/19 17:56,1.10.0,1.11.0,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Reading ORC table from Hive 2.0.1 fails with:
{noformat}
Caused by: java.lang.NoSuchMethodError: org.apache.orc.OrcFile.createReader(Lorg/apache/hadoop/fs/Path;Lorg/apache/orc/OrcFile$ReaderOptions;)Lorg/apache/orc/Reader;
	at org.apache.flink.orc.OrcSplitReader.<init>(OrcSplitReader.java:78)
	at org.apache.flink.orc.OrcColumnarRowSplitReader.<init>(OrcColumnarRowSplitReader.java:53)
	at org.apache.flink.orc.OrcSplitReaderUtil.genPartColumnarRowReader(OrcSplitReaderUtil.java:93)
	at org.apache.flink.connectors.hive.read.HiveVectorizedOrcSplitReader.<init>(HiveVectorizedOrcSplitReader.java:64)
	at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:117)
	at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:57)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:85)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:196)
{noformat}",,lzljs3620320,phoenixjiangnan,,,,,,,,,,,,"bowenli86 commented on pull request #10618: [FLINK-15348][hive]  Fix orc optimization for version less than 2.3 by introducing orc shim
URL: https://github.com/apache/flink/pull/10618
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 17:55;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 23 17:56:30 UTC 2019,,,,,,,,,,"0|z09w6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/19 02:25;lzljs3620320;PR: [https://github.com/apache/flink/pull/10618];;;","23/Dec/19 17:56;phoenixjiangnan;master: 790a07787e3dd6f5795b4fb5cb84df953363d64a
1.10: 37fbbd4ba57d356b1877a9b7d600254550b21321;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperDefaultDispatcherRunnerTest.testResourceCleanupUnderLeadershipChange failed on Travis,FLINK-15347,13275751,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,20/Dec/19 15:49,11/May/20 19:13,13/Jul/23 08:10,20/Apr/20 16:22,1.11.0,,,,,,,,,1.11.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"The test {{ZooKeeperDefaultDispatcherRunnerTest.testResourceCleanupUnderLeadershipChange}} failed on Travis because it got stuck.

https://api.travis-ci.org/v3/job/627661879/log.txt",,aitozi,dian.fu,jingzhang,rmetzger,roman,trohrmann,,,,,,,,"tillrohrmann commented on pull request #11683: [FLINK-15347] Add SupervisorActor which monitors the proper termination of AkkaRpcActors
URL: https://github.com/apache/flink/pull/11683
 
 
   
   ## What is the purpose of the change
   
   In order to properly complete the termination future of an RpcEndpoint, we need to monitor
   when the underlying AkkaRpcActor has been removed from the ActorSystem. If this is not done,
   then it can happen that another RpcEndpoint using the same name cannot be started because
   the old RpcEndpoint is still registered.
   
   The way we achieve this monitoring is to introduce a helper actor which is responsible for
   starting the AkkaRpcActors for all RpcEndpoints. Since the SupervisorActor is the parent
   of all RpcEndpoints, it can tell when they are being removed from the ActorSystem through
   the SupervisorStrategy.
   
   A consequence of the new actor is that the akka urls change from akka://flink@actorsystem:port/user/xyz
   to akka://flink@actorsystem:port/user/rpc/xyz. The respective method AkkaRpcServiceUtils.getRpcUrl
   has been updated to reflect this change. This hierarchy change also warrants the bump of the
   AkkaRpcService.VERSION.
   
   The failure behaviour of the underlying ActorSystem has been changed so that it terminates
   all running actors if an exception is thrown from the SupervisorActor. The assumption is that
   such an exception always indicates a programming error and is unrecoverable.
   
   The same applies to failure originating from an AkkaRpcActor (children of the SupervisorActor).
   If such an exception is thrown, then we assume that the system is in an illegal state and shut it
   down. The way it works is by recording the failure cause for the respective AkkaRpcActor and
   then terminating the ActorSystem.
   
   ## Verifying this change
   
   - Added `AkkaActorSystemTest`, `SupervisorActorTest` and `AkkaRpcActorTest.canReuseEndpointNameAfterTermination`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/20 08:38;githubbot;600","GJL commented on pull request #11761: [FLINK-15347][test] Enable DEBUG level logging in Jepsen tests
URL: https://github.com/apache/flink/pull/11761
 
 
   ## What is the purpose of the change
   
   *Re-enable DEBUG level logging in Jepsen tests.*
   
   
   ## Brief change log
   
     - *See commit*
   
   ## Verifying this change
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 19:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 19:13:24 UTC 2020,,,,,,,,,,"0|z09vy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/20 11:29;trohrmann;Another instance: https://api.travis-ci.org/v3/job/646928906/log.txt;;;","01/Mar/20 14:01;roman;Another instance: [https://api.travis-ci.org/v3/job/656892059/log.txt]

 ;;;","07/Apr/20 15:37;trohrmann;The problem seems to be a race condition between two {{Dispatchers}} between two leader sessions. Before the second leader instance can be created, the former needs to be unregistered from the underlying {{AkkaRpcService}} because both share the same endpoint name {{dispatcher}}. If the old leader is not completely unregistered, then one sees the following exception

{code}
java.util.concurrent.CompletionException: org.apache.flink.util.FlinkRuntimeException: Could not create the Dispatcher rpc endpoint.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:659)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:632)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1595)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkRuntimeException: Could not create the Dispatcher rpc endpoint.
	at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherGatewayServiceFactory.create(DefaultDispatcherGatewayServiceFactory.java:66)
	at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.createDispatcher(SessionDispatcherLeaderProcess.java:100)
	at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.lambda$createDispatcherIfRunning$0(SessionDispatcherLeaderProcess.java:95)
	at org.apache.flink.runtime.dispatcher.runner.AbstractDispatcherLeaderProcess.runIfState(AbstractDispatcherLeaderProcess.java:210)
	at org.apache.flink.runtime.dispatcher.runner.AbstractDispatcherLeaderProcess.runIfStateIs(AbstractDispatcherLeaderProcess.java:198)
	at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.createDispatcherIfRunning(SessionDispatcherLeaderProcess.java:95)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:656)
	... 10 more
Caused by: akka.actor.InvalidActorNameException: actor name [dispatcher] is not unique!
	at akka.actor.dungeon.ChildrenContainer$NormalChildrenContainer.reserve(ChildrenContainer.scala:129)
	at akka.actor.dungeon.Children$class.reserveChild(Children.scala:135)
	at akka.actor.ActorCell.reserveChild(ActorCell.scala:429)
	at akka.actor.dungeon.Children$class.makeChild(Children.scala:275)
	at akka.actor.dungeon.Children$class.attachChild(Children.scala:49)
	at akka.actor.ActorCell.attachChild(ActorCell.scala:429)
	at akka.actor.ActorSystemImpl.actorOf(ActorSystem.scala:753)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcService.startServer(AkkaRpcService.java:219)
	at org.apache.flink.runtime.rpc.RpcEndpoint.<init>(RpcEndpoint.java:129)
	at org.apache.flink.runtime.rpc.FencedRpcEndpoint.<init>(FencedRpcEndpoint.java:48)
	at org.apache.flink.runtime.rpc.PermanentlyFencedRpcEndpoint.<init>(PermanentlyFencedRpcEndpoint.java:36)
	at org.apache.flink.runtime.dispatcher.Dispatcher.<init>(Dispatcher.java:137)
	at org.apache.flink.runtime.dispatcher.StandaloneDispatcher.<init>(StandaloneDispatcher.java:39)
	at org.apache.flink.runtime.dispatcher.SessionDispatcherFactory.createDispatcher(SessionDispatcherFactory.java:44)
	at org.apache.flink.runtime.dispatcher.SessionDispatcherFactory.createDispatcher(SessionDispatcherFactory.java:29)
	at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherGatewayServiceFactory.create(DefaultDispatcherGatewayServiceFactory.java:60)
	... 16 more

{code};;;","11/Apr/20 03:07;dian.fu;Another instance: [https://api.travis-ci.org/v3/job/673433680/log.txt];;;","20/Apr/20 16:22;trohrmann;Fixed via

db375bb2b4e8ff5de0960558c250e511c1bbc05c
4c63f212e2d98bb8f6a5b813ed594e78a6b5b420
f0529de9a35009c190863060c8bda1328b3bfd83
2cf79554971e6d0f9b8a4dcb2508f3968ce75b8e
422aa8956c246cad379f1e72a3aeb8b9b858abba;;;","11/May/20 19:13;rmetzger;https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7975&view=logs&j=908dabb4-1f6d-59d0-2f4e-c1109f3ac087&t=07cfb3fc-99de-5eb2-3b56-268615095175&l=7096
Java 11, private Azure account
{code}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 11.596 s <<< FAILURE! - in org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest
[ERROR] testResourceCleanupUnderLeadershipChange(org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest)  Time elapsed: 11.559 s  <<< ERROR!
java.util.concurrent.TimeoutException: Condition was not met in given timeout.
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:129)
	at org.apache.flink.runtime.dispatcher.runner.ZooKeeperDefaultDispatcherRunnerTest.testResourceCleanupUnderLeadershipChange(ZooKeeperDefaultDispatcherRunnerTest.java:195)
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Reset context classload in PackagedProgramUtils#createJobGraph,FLINK-15341,13275669,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,20/Dec/19 07:39,03/Jan/20 10:48,13/Jul/23 08:10,03/Jan/20 10:47,1.10.0,,,,,,,,,1.10.0,,,,Client / Job Submission,,,,,0,pull-request-available,,,,,,aljoscha,tison,,,,,,,,,,,,"TisonKun commented on pull request #10641: [FLINK-15341][client] Reset context classload in PackagedProgramUtils#createJobGraph
URL: https://github.com/apache/flink/pull/10641
 
 
   It happens we don't reset context classload in PackagedProgramUtils#createJobGraph. For fixing this, we can just reuse PackagedProgramUtils#getPipelineFromProgram.
   
   cc @aljoscha @kl0u 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 07:43;githubbot;600","aljoscha commented on pull request #10641: [FLINK-15341][client] Reset context classload in PackagedProgramUtils#createJobGraph
URL: https://github.com/apache/flink/pull/10641
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 10:48;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 10:47:14 UTC 2020,,,,,,,,,,"0|z09vg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/20 10:47;aljoscha;Fixed on master in 4920ac89fa34d20e06674da67bf55a594162a29a
Fixed on release-1.10 in 26172d3136a6165631112a2dc503f57c8d42f47d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TM Metaspace memory leak when submitting PyFlink UDF jobs multiple times,FLINK-15338,13275642,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,weizhong,sunjincheng121,sunjincheng121,20/Dec/19 03:13,09/Jan/20 06:01,13/Jul/23 08:10,09/Jan/20 06:01,1.10.0,,,,,,,,,1.10.0,,,,API / Python,,,,,0,pull-request-available,,,,"Start a standalone cluster and after submit PyFlink UDF jobs multiple times, the TM will fail with the following exception:

 
{code:java}
Caused by: java.lang.OutOfMemoryError: Metaspace
  at java.lang.ClassLoader.defineClass1(Native Method)
  at java.lang.ClassLoader.defineClass(ClassLoader.java:788)
  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
  at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
  at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:448)
  at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:380)
  at org.apache.flink.api.python.shaded.com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:628)
  at org.apache.flink.api.python.shaded.com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:531)
  at org.apache.beam.sdk.options.PipelineOptionsFactory.<clinit>(PipelineOptionsFactory.java:469)
  at org.apache.flink.python.AbstractPythonFunctionRunner.open(AbstractPythonFunctionRunner.java:173)
  at org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator$ProjectUdfInputPythonScalarFunctionRunner.open(AbstractPythonScalarFunctionOperator.java:193)
  at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:139)
  at org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:143)
  at org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator.open(BaseRowPythonScalarFunctionOperator.java:86)
  at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1018)
  at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)
  at org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$125/800044563.run(Unknown Source)
  at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
  at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)
  at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)
  at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
  at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
  at java.lang.Thread.run(Thread.java:834)
{code}",,dian.fu,hequn8128,liyu,sunjincheng121,xtsong,zhongwei,,,,,,,,"WeiZhong94 commented on pull request #10772: [FLINK-15338][python] Cherry-pick NETTY#8955 to fix the TM Metaspace memory leak problem in shaded netty when submitting PyFlink UDF jobs multiple times.
URL: https://github.com/apache/flink/pull/10772
 
 
   ## What is the purpose of the change
   
   *This pull request cherry-picks [NETTY#8955](https://github.com/netty/netty/pull/8955) to fix the TM Metaspace memory leak problem in shaded netty when submitting PyFlink UDF jobs multiple times.*
   
   
   ## Brief change log
   
     - *cherry-picks [NETTY#8955](https://github.com/netty/netty/pull/8955).*
     - *ignore netty files when checking code style.*
   
   ## Verifying this change
   
   This change can be verified as follows:
   
     - *Start a local cluster with default configuration.*
     - *Submit several jobs which contains PyFlink UDF.*
     - *Dump the heap of the TaskManager process.*
     - *Ensure that the ThreadLocal Map of the Finalizer thread do not contain netty objects.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 07:04;githubbot;600","hequn8128 commented on pull request #10772: [FLINK-15338][python] Cherry-pick NETTY#8955 and BEAM-9006(#10462) to fix the TM Metaspace memory leak problem when submitting PyFlink UDF jobs multiple times.
URL: https://github.com/apache/flink/pull/10772
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jan/20 05:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,BEAM-9006,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 09 06:01:41 UTC 2020,,,,,,,,,,"0|z09va0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/19 03:17;sunjincheng121;We dumped the heap and found that there are two places which caused the meta space memory leak:

1) The class `ProcessManager` from Beam creates a [shutdown hook|https://github.com/apache/beam/blob/06454044b699f6b1bbc5becee5c9b752923beb3b/runners/java-fn-execution/src/main/java/org/apache/beam/runners/fnexecution/environment/ProcessManager.java#L52] which will not be removed before JVM exits. The shutdown hook holds a reference to the user class loader and it means that the user class loader could not be GC even after job finished.

2) The class `ThreadPoolCache` from Netty (dependent on by Beam) defines a finalize method which will create an object InternalThreadLocalMap. The object InternalThreadLocalMap will be added to the ThreadLocal map of the Finalizer thread. As InternalThreadLocalMap holds a reference to the user class loader and this means that the user class loader could not be GC even after job finished. This issue has been fixed in [NETTY#8955|https://github.com/netty/netty/pull/8955].

So, there are two options to fix this issues:
 1) Fix the above two problems case by case
 2) Move the jar of flink-python to lib (this means that the classes of flink-python will loaded with system classloader)

What do you think?;;;","20/Dec/19 03:18;xtsong;Is this the same issue as FLINK-15239?;;;","20/Dec/19 03:25;sunjincheng121;The causes are similar, i.e. the user classloader could not be garbage collected after job finished. However, the reasons to the problems are different and so we have to fix them case by case.;;;","26/Dec/19 08:00;sunjincheng121;The PR to solve `ProcessManager` issue is opened, mode detail can be found in [https://github.com/apache/beam/pull/10462];;;","26/Dec/19 08:12;sunjincheng121;The detail of discussion and the PR for NETTY issue can be found in [1] and [2].
[1] https://lists.apache.org/thread.html/ef5b24766d94d3d389bee9c03e59003b9cf417c81cde50ede5856ad7%40%3Cdev.beam.apache.org%3E
[2] https://github.com/apache/beam/pull/10463;;;","06/Jan/20 02:43;zhongwei;[~sunjincheng121] It seems the PR to solve `ProcessManager` issue is merged in Beam. I think we can cp the fixes from Beam and Netty now. I'm willing to create the PR if you can assign this Jira to me. :);;;","06/Jan/20 05:58;sunjincheng121;Thanks for the reminder [~zhongwei]! and it's would be great that you can help fix this issue :);;;","09/Jan/20 06:01;hequn8128;Fixed
in 1.11.0 via f30dd982f7c54c76ef67c6be9ffd5c9c27d900f9..74dc89514c4de01b9419bcaf584c51ccc8a40962
in 1.10.0 via
dfd66a4296e9237ef287c4539977a70ccb963bc3..a084e37e7df241ac858f359e7dae7ec4d76d806a
;;;",,,,,,,,,,,,,,,,,,,,,,,,
add-dependencies-for-IDEA not working anymore and dangerous in general,FLINK-15335,13275533,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,nkruber,nkruber,19/Dec/19 15:26,07/Nov/21 11:33,13/Jul/23 08:10,05/Feb/20 08:53,1.10.0,1.8.3,1.9.1,,,,,,,1.10.0,,,,Documentation,Quickstarts,,,,0,pull-request-available,usability,,,"The quickstart's {{add-dependencies-for-IDEA}} profile (for including {{flink-runtime}} and further dependencies that are usually {{provided}}) is not automatically enabled with IntelliJ anymore since the {{idea.version}} property is not set anymore (since a couple of versions of IntelliJ). My IntelliJ, for example, sets {{idea.version2019.3}} instead but even if the profile activation is changed to that, it is not enabled by default by IntelliJ.

There are two workarounds:
 * Tick {{Include dependencies with ""Provided"" scope}} in the run configuration (available in any newer IntelliJ version, probably since 2018) or
 * enable the profile manually - downside: if you create a jar inside IntelliJ via its own maven targets, the jar would contain the provided dependencies and make it unsuitable for submission into a Flink cluster.

I propose to remove the {{add-dependencies-for-IDEA}} profile for good (from the quickstarts) and adapt the documentation accordingly, e.g. [https://ci.apache.org/projects/flink/flink-docs-stable/dev/projectsetup/dependencies.html#setting-up-a-project-basic-dependencies]",,aljoscha,liyu,nkruber,sharp-pixel,,,,,,,,,,"zentol commented on pull request #10959: [FLINK-15335] Replace `add-dependencies-for-IDEA` profile
URL: https://github.com/apache/flink/pull/10959
 
 
   Removes the `add-dependencies-for-IDEA` profile and documents how to tell IntelliJ to put provided dependencies on the classpath, with an additional fallback should things not work as expected.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jan/20 12:49;githubbot;600","zentol commented on pull request #10959: [FLINK-15335] Replace `add-dependencies-for-IDEA` profile
URL: https://github.com/apache/flink/pull/10959
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/20 08:52;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 07 11:33:33 UTC 2021,,,,,,,,,,"0|z09uls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/20 09:44;aljoscha;I agree! [~NicoK] do you maybe want to open a PR for that?;;;","08/Jan/20 12:34;liyu;I believe it would be nice to fix it in 1.10.0, thanks.;;;","05/Feb/20 08:53;chesnay;master: 73017a82d9c13a16b42e5025d5922eb746dd458d
1.10: 4df788ae81e6ed18caa07f318362ad57510ec316 ;;;","07/Nov/21 11:33;sharp-pixel;This is just plain wrong. Not only is the {{idea.version}} still set up to the 2021.2 version of IntelliJ, but it is now a pain to quickstart a new project because the profile was removed. I need to manually add the profile back to the generated {{pom.xml}} and am considering to fork the archetype which would be a shame. This a clear regression.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix physcial schema mapping in TableFormatFactoryBase to support define orderless computed column ,FLINK-15334,13275530,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,19/Dec/19 15:04,02/Jan/20 12:31,13/Jul/23 08:10,02/Jan/20 12:31,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"If user defined a DDL like:
""CREATE TABLE orders (\n"" +
            ""  order_id STRING,\n"" +
            ""  item    STRING,\n"" +
            ""  currency STRING,\n"" +
            ""  amount INT,\n"" +
            ""  amount_kg as amount * 1000,\n"" +
            ""  order_time TIMESTAMP(3),\n"" +
            ""  ts as order_time + INTERVAL '1' SECOND,\n"" +
            ""  WATERMARK FOR ts AS ts\n"" +
            "") WITH (\n"" +
which generated columns(eg:amount_kg) defined before custom columns(eg:order_time) will get wrong properties. 
in this case, we still use index 5 for column order_time and the new index should be 4  after prune generated column.


",,jark,leonard,,,,,,,,,,,,"leonardBang commented on pull request #10693: [FLINK-15334][table sql / api] Fix physical schema mapping in TableFormatFactoryBase to support define orderless computed column
URL: https://github.com/apache/flink/pull/10693
 
 
   Fix physical schema mapping in TableFormatFactoryBase to support define orderless computed column
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request Fix physical schema mapping in schemaValidator which used in TableSoure and tableSourceFactory, we can support  define orderless computed column after fix it.*
   
   
   ## Brief change log
   
     - *fix mapping in org/apache/flink/table/factories/TableFormatFactoryBase.java*
     - *Add unit test in  file org/apache/flink/table/descriptors/SchemaValidatorTest.scala*
     - *Add IT case org/apache/flink/streaming/connectors/kafka/KafkaTableTestBase.java*
   
   ## Verifying this change
   
     - *Add unit test cover path that define table by DescriptorProperties*
     - *Add IT case cover path that define table by DDL*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 03:56;githubbot;600","wuchong commented on pull request #10693: [FLINK-15334][table sql / api] Fix physical schema mapping in TableFormatFactoryBase to support define orderless computed column
URL: https://github.com/apache/flink/pull/10693
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 12:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 02 12:31:00 UTC 2020,,,,,,,,,,"0|z09ul4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/19 05:50;jark;Hi [~Leonard Xu], could you list the wrong properties? ;;;","20/Dec/19 06:24;leonard;[~jark] I update the description make it more clear, Could you help assgin this issue to me ?;;;","02/Jan/20 12:31;jark;1.11.0: 898064f4a743b3e10af1c6d7a0d4d73c432815f4
1.10.0: a1fe7adfd1f7b1807d611614ae88838bb178023f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jepsen tests are broken due to copying un-relocated flink-s3-fs-hadoop* into lib,FLINK-15332,13275505,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,trohrmann,trohrmann,19/Dec/19 13:42,20/Dec/19 10:45,13/Jul/23 08:10,20/Dec/19 10:45,1.10.0,,,,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,test-stability,,,The Jepsen tests are currently broken because we copy {{flink-s3-fs-hadoop*}} into {{lib}}. With FLINK-11956 we removed the relocations from these classes and hence they need to reside in {{plugins}}.,,liyu,trohrmann,,,,,,,,,,,,"tillrohrmann commented on pull request #10634: [FLINK-15332][jepsen] Copy flink-s3-fs-hadoop* into /plugins directory instead of lib
URL: https://github.com/apache/flink/pull/10634
 
 
   ## What is the purpose of the change
   
   We can no longer copy the flink-s3-fs-hadoop* jars into lib as they contain un-relocated
   Hadoop 3.1.0 dependencies.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Dec/19 13:53;githubbot;600","tillrohrmann commented on pull request #10634: [FLINK-15332][jepsen] Copy flink-s3-fs-hadoop* into /plugins directory instead of lib
URL: https://github.com/apache/flink/pull/10634
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 10:44;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 10:45:55 UTC 2019,,,,,,,,,,"0|z09ufk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/19 10:45;trohrmann;Fixed via

master: 47f2efe3c07a65a0af75e6e6d6fec78975e55bf7
1.10.0: 3b3299aefe06822261f8c9382620e4a8d4e0912e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Walkthrough DataStream Scala nightly' sometimes fails with InterruptedException,FLINK-15327,13275464,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,rmetzger,rmetzger,19/Dec/19 10:34,22/Jun/21 14:05,13/Jul/23 08:10,15/Jan/20 12:29,1.10.0,,,,,,,,,1.10.0,,,,Examples,Runtime / Task,Tests,,,0,pull-request-available,,,,"The ""Walkthrough DataStream Scala nightly end-to-end test"" (test_datastream_walkthroughs.sh) failed with the following error found in the log 

 
  
{code:java}
2019-12-19 12:57:03,131 WARN  org.apache.flink.streaming.runtime.tasks.StreamTask           - Error while canceling task.
java.lang.RuntimeException: java.lang.InterruptedException: sleep interrupted
	at org.apache.flink.walkthrough.common.source.TransactionSource$RateLimitedIterator.next(TransactionSource.java:60)
	at org.apache.flink.streaming.api.functions.source.FromIteratorFunction.run(FromIteratorFunction.java:43)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:196)
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.walkthrough.common.source.TransactionSource$RateLimitedIterator.next(TransactionSource.java:58)
	... 4 more

{code}
 

Run: [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=3742&view=logs&j=b1623ac9-0979-5b0d-2e5e-1377d695c991&t=48867695-c47f-5af3-2f21-7845611247b9]

I assume this error happens only rarely.
  ",,aljoscha,arvid heise,pnowojski,rmetzger,,,,,,,,,,"rmetzger commented on pull request #10628: [FLINK-15327][walkthroughs] Replace InterruptedException by running flag
URL: https://github.com/apache/flink/pull/10628
 
 
   ## What is the purpose of the change
   
   The end to end tests sometimes fail because a RuntimeException is logged when cancelling the TransactionSource.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Dec/19 10:52;githubbot;600","AHeise commented on pull request #10779: [FLINK-15327][runtime] No warning of InterruptedException during cancel.
URL: https://github.com/apache/flink/pull/10779
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   InterruptedException are previously only handled when wrapped in
   WrappingRuntimeException. This patch looks through the whole exception
   chain.
   
   ## Brief change log
   
    - Look for InterruptedException in whole exception in StreamTask.
    - Reenable previously flaky nightly tests.
   
   
   ## Verifying this change
   
   - Reenabled flaky nightly tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no**)**
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 14:15;githubbot;600","pnowojski commented on pull request #10779: [FLINK-15327][runtime] No warning of InterruptedException during cancel.
URL: https://github.com/apache/flink/pull/10779
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jan/20 12:23;githubbot;600","pnowojski commented on pull request #10628: [FLINK-15327][walkthroughs] Replace InterruptedException by running flag
URL: https://github.com/apache/flink/pull/10628
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jan/20 12:29;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 07:34:28 UTC 2020,,,,,,,,,,"0|z09u6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/19 14:11;rmetzger;[~pnowojski]: I had an offline chat with [~sewen] about this. He mentioned to me that in the past, we were not printing exceptions during cancellation, to avoid polluting the log. It seems that the behavior of this has changed with the {{LegacySourceFunctionThread}}. Should we change it back? 

 ;;;","06/Jan/20 14:34;arvid;Hi [~rmetzger] , this is independent of legacy source and more related to the recent work of Piotr on better exception handling for mailbox.

We are trying to improve that handling further by being more lenient on processing specific exceptions and I added a PR that replaces your fix with another fix. I'd like to keep the tests as they are as we really want to surpress these kind of exceptions while still avoid swallowing other exceptions during cancellation.;;;","15/Jan/20 12:29;pnowojski;merged commit 4852002 into apache:master
merged commit 8acd60b661a7c92a5922ae566d547dbfad799c8c into release-1.10;;;","07/Apr/20 07:26;rmetzger;The test failed again: https://travis-ci.org/github/apache/flink/jobs/671755595?utm_medium=notification&utm_source=slack
This time with this error (on 1.10)

{code}
2020-04-07 03:31:09,800 WARN  org.apache.flink.streaming.runtime.tasks.StreamTask           - Error while canceling task.
org.apache.flink.runtime.execution.CancelTaskException: Input gate is already closed.
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:484)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:475)
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:75)
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:125)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
	at java.lang.Thread.run(Thread.java:748)
{code}

I guess this exception is something we don't want to log during cancellation?;;;","08/Apr/20 07:34;pnowojski;Yes [~rmetzger] and it was fixed on master as part of https://issues.apache.org/jira/browse/FLINK-15751 .  You can check my last comment there for an explanation why is this issue still in the release-1.10. At that point of time we didn't know that other tests can also be failing. If you think we should backport the proper fix to release-1.10, please respond in the other ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager crashes in the standalone model when cancelling job which subtask' status is scheduled,FLINK-15320,13275390,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhuzh,lining,lining,19/Dec/19 02:28,02/Jan/20 14:04,13/Jul/23 08:10,23/Dec/19 13:01,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Use start-cluster.sh to start a standalone cluster, and then submit a job from the streaming's example which name is TopSpeedWindowing, parallelism is 20. Wait for one minute, cancel the job, jobmanager will crash. The exception stack is:

{noformat}
2019-12-19 10:12:11,060 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler       - FATAL: Thread 'flink-akka.actor.default-dispatcher-2' produced an uncaught exception. Stopping the process...2019-12-19 10:12:11,060 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler       - FATAL: Thread 'flink-akka.actor.default-dispatcher-2' produced an uncaught exception. Stopping the process...java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Could not assign resource org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@583585c6 to current execution Attempt #0 (Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (19/20)) @ (unassigned) - [CANCELED]. at org.apache.flink.runtime.scheduler.DefaultScheduler.propagateIfNonNull(DefaultScheduler.java:387) at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$deployAll$4(DefaultScheduler.java:372) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:705) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:170) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.tryFulfillSlotRequestOrMakeAvailable(SlotPoolImpl.java:534) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseSingleSlot(SlotPoolImpl.java:479) at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseSlot(SlotPoolImpl.java:390) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:557) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.releaseChild(SlotSharingManager.java:607) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.access$700(SlotSharingManager.java:352) at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:716) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.releaseSharedSlot(SchedulerImpl.java:552) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.cancelSlotRequest(SchedulerImpl.java:184) at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.returnLogicalSlot(SchedulerImpl.java:195) at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.lambda$returnSlotToOwner$0(SingleLogicalSlot.java:181) at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:778) at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2140) at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.returnSlotToOwner(SingleLogicalSlot.java:178) at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.releaseSlot(SingleLogicalSlot.java:125) at org.apache.flink.runtime.executiongraph.Execution.releaseAssignedResource(Execution.java:1451) at org.apache.flink.runtime.executiongraph.Execution.finishCancellation(Execution.java:1170) at org.apache.flink.runtime.executiongraph.Execution.completeCancelling(Execution.java:1150) at org.apache.flink.runtime.executiongraph.Execution.completeCancelling(Execution.java:1129) at org.apache.flink.runtime.executiongraph.Execution.cancelAtomically(Execution.java:1111) at org.apache.flink.runtime.executiongraph.Execution.cancel(Execution.java:804) at org.apache.flink.runtime.executiongraph.ExecutionVertex.cancel(ExecutionVertex.java:729) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.mapExecutionVertices(ExecutionJobVertex.java:505) at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.cancelWithFuture(ExecutionJobVertex.java:494) at org.apache.flink.runtime.executiongraph.ExecutionGraph.cancelVerticesAsync(ExecutionGraph.java:952) at org.apache.flink.runtime.executiongraph.ExecutionGraph.cancel(ExecutionGraph.java:903) at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:432) at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:364) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Could not assign resource org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@583585c6 to current execution Attempt #0 (Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (19/20)) @ (unassigned) - [CANCELED]. at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:824) at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ... 69 moreCaused by: java.lang.IllegalStateException: Could not assign resource org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@583585c6 to current execution Attempt #0 (Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (19/20)) @ (unassigned) - [CANCELED]. at org.apache.flink.runtime.executiongraph.ExecutionVertex.tryAssignResource(ExecutionVertex.java:701) at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$5(DefaultScheduler.java:409) at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ... 70 more2019-12-19 10:12:11,066 INFO  org.apache.flink.runtime.blob.BlobServer                      - Stopped BLOB server at 0.0.0.0:54944
{noformat}",,lining,liyu,trohrmann,zhuzh,,,,,,,,,,"zhuzhurk commented on pull request #10646: [FLINK-15320][runtime] Increment versions of all vertices when failing/canceling/suspending a job
URL: https://github.com/apache/flink/pull/10646
 
 
   ## What is the purpose of the change
    
   JM crashes may happen when an external job cancel request comes when the job is still allocating slots.
   The root cause is that, the version of the canceled vertices are not incremented in the case of external job cancel request, and the pending slot requests are also not canceled in this case, so that the returned slot can be used to fulfill an outdated deployment, which finally triggers the fatal error.
   
   ## Brief change log
   
     - *increment all vertex versions in #failJob(), #cancel(), #suspend() in SchedulerBase*
   
   ## Verifying this change
   
     - *Added unit tests for job failing/canceling/suspending cases*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**yes** / no / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 08:25;githubbot;600","tillrohrmann commented on pull request #10646: [FLINK-15320][runtime] Increment versions of all vertices when failing/canceling/suspending a job
URL: https://github.com/apache/flink/pull/10646
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 13:00;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 23 13:01:52 UTC 2019,,,,,,,,,,"0|z09tq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/19 07:31;zhuzh;To summarize, this issue happens when an external job cancel request comes when the job is still allocating slots.
The root cause is that, the version of the canceled vertices are not incremented in the case of external job cancel request, and the pending slot requests are also not canceled in this case, so that the returned slot can be used to fulfill an outdated deployment, which finally triggers the fatal error.
To fix it, I think we should always increment the version of a vertex before canceling/failing it. Besides {{JobMaster#cancel}}, there are several other cases including {{JobMaster#suspend}} and {{ExecutionGraph#failJob}}.

cc [~gjy];;;","19/Dec/19 09:25;trohrmann;This is a very good find and is indeed a blocker. Gary is already on vacation. I try to find some time to look into your proposed solution.;;;","19/Dec/19 09:54;zhuzh;Thanks [~trohrmann]. And also thanks [~lining] for reporting this issue.;;;","20/Dec/19 08:32;zhuzh;The PR is opened. [~trohrmann]
I checked the code and found the only possible entry points which cancel vertices out of {{DefaultScheduler#restartTasksWithDelay()}} (the method would increment vertex versions) are 3 methods in SchedulerBase: {{#failJob()}}, {{#cancel()}} and {{#suspend()}}.
So I added `SchedulerBase#incrementVersionsOfAllVertices()` and use it in those 3 cases above. In this way, outdated deployments can always be identified by checking the vertex versions and this unexpected issue would not happen.;;;","23/Dec/19 13:01;trohrmann;Fixed via

master:
90b0feb64dbc33b1f50792e06e3f99ff934827a1
94edb0ca4eddfb245da79c52b2bc087a8e8a71d3

1.10.0:
71eef7985a888c14a51308d464b9f0db1f6b7ddc
2e2665068b857bec558635aa7eb7943c5bc25d07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDBWriteBatchPerformanceTest.benchMark fails on ppc64le,FLINK-15318,13275237,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,siddheshghadi,siddheshghadi,18/Dec/19 11:41,05/Feb/21 02:40,13/Jul/23 08:10,05/Feb/21 02:39,,,,,,,,,,1.12.0,,,,Benchmarks,Runtime / State Backends,,,,0,,,,,"RocksDBWriteBatchPerformanceTest.benchMark fails due to TestTimedOut, however when test-timeout is increased from 2s to 5s in org/apache/flink/contrib/streaming/state/benchmark/RocksDBWriteBatchPerformanceTest.java:75, it passes. Is this acceptable solution?

Note: Tests are ran inside a container.","arch: ppc64le
os: rhel7.6, ubuntu 18.04
jdk: 8, 11
mvn: 3.3.9, 3.6.2",klion26,liyu,maguowei,Ming Li,redmark-ibm,rmetzger,sewen,siddheshghadi,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18373,,,,,,,,"18/Dec/19 11:32;siddheshghadi;surefire-report.txt;https://issues.apache.org/jira/secure/attachment/12989088/surefire-report.txt",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 05 02:39:31 UTC 2021,,,,,,,,,,"0|z09ss0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/19 12:35;yunta;Which version of Flink did you verify? Did you observe a stable performance behavior before and then suddenly fail due to sync with new commits or you just come across this error for the first time when you just want to try it on ppc64le platform.

Actually, Flink community lacks of benchmark on ppc64le environment and I noticed that RocksDB on ppc64le behaves not as good as those on linux64.;;;","20/Dec/19 04:08;siddheshghadi;I verified it with master, release-1.10, release-1.9 & release-1.8 branches, RocksDBWriteBatchPerformanceTest.benchMark fails on all these branches. Also I came across this error for the first time when I tried it on ppc64le.;;;","24/Dec/19 08:11;yunta;[~siddheshghadi] I noticed that you also come across that in release-1.8 which is an older version of FRocksDB. From my previous experience, I have noticed that FRocksDB on ppc64le platform behaves worse than other platforms and I actually have not met some guys using Flink in production with ppc64le environment.

 

In a nut shell, the timeout for FRocksDB is not enough on ppc64le platform. Did you use Flink in production on ppc64le platform? I am afraid Flink community lacks of rich experience on ppc64le especially for FRocksDB performance. By the way, can you try to use RocksDB instead of FRocksDB to run the tests (Remember to remove all the usage of {{org.rocksdb.FlinkCompactionFilter}} so that you could build with official RocksDB).;;;","07/Jan/20 12:44;redmark-ibm; I'm hitting the same problem in Flink 1.8.3. Did anyone find a fix for this issue?  I have a ppc64le environment to help debug the issue.

 

Red Hat 7.6 Linux ppc64le

Java 1.8.0.232

Maven 3.2.5;;;","08/Jan/20 18:07;yunta;[~redmark-ibm] can you retry the official RocksDB to see whether could meet this problem:
 * Edit {{flink-state-backends/flink-statebackend-rocksdb/pom.xml}} to use rocksDB instead of FrocksDB

{code:java}
		<dependency>
			<groupId>org.rocksdb</groupId>
			<artifactId>frocksdbjni</artifactId>
			<version>5.17.2</version>
		</dependency>
{code}
 * Edit {{flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/ttl/RocksDbTtlCompactFiltersManager.java}} to drop all usage of {{org.rocksdb.FlinkCompactionFilter}} and {{org.rocksdb.FlinkCompactionFilter.FlinkCompactionFilterFactory}}. Remove them would not affect you to run that test.

By doing so, you could verify whether the problem still existed for official RocksDB.;;;","09/Jan/20 22:48;redmark-ibm;Yum,

I would like to point out that the tests just fails, changing the number from 2 seconds to 3 seconds works.  Most of the time the failure are just over 2 seconds.  i.e. *Time elapsed: 2.095*

 

I made these changes to  the pom.  Current Flink 1.8.3 has

<!-- https://mvnrepository.com/artifact/com.data-artisans/frocksdbjni -->
   <dependency>
   <groupId>com.data-artisans</groupId>
   <artifactId>*frocksdbjni*</artifactId>
   <version>*5.17.2-artisans-1.0*</version>
</dependency>

Changed to 

<!-- https://mvnrepository.com/artifact/org.rocksdb/rocksdbjni -->
<dependency>
   <groupId>org.rocksdb</groupId>
   <artifactId>*rocksdbjni*</artifactId>
   <version>*5.17.2*</version>
</dependency>

 

I worked on removing *org.rocksdb.FlinkCompactionFilter* and *org.rocksdb.FlinkCompactionFilter.FlinkCompactionFilterFactory* but I was hitting issues getting it cleanly removed, if you can provide a modified *RocksDbTtlCompactFiltersManager.java* version that will help. Otherwise I'll work on it tomorrow when I have more time.

 

Thanks for helping,

Ron

 ;;;","13/Jan/20 03:31;yunta;[~redmark-ibm] you can try this [commit|https://github.com/Myasuka/flink/commit/484fffb08620ab177175405c53d64faaeb585d01].;;;","13/Jan/20 13:21;redmark-ibm;Yun thank you for the commit changes.

I've modified the `pom` file, applied the commit changes and ran a clean test build of RocksDB but we still see the failure. Changing the time from 2 to 3 seconds does work-around the problem.

 

Is 3 sec an acceptable timeout?

 

-Ron

```

[redmark@p006vm23 flink]$ mvn clean test -rf :flink-statebackend-rocksdb_2.11

[INFO] Scanning for projects...

..........................

[INFO] ------------------------------------------------------------------------

[INFO] Building flink-statebackend-rocksdb 1.8.3

[INFO] ------------------------------------------------------------------------

[INFO]

[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ flink-statebackend-rocksdb_2.11 --- [INFO] Deleting /root/flink/flink-state-backends/flink-statebackend-rocksdb/target

..........................

[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.473 s <<< FAILURE! - in org.apache.flink.contrib.streaming.state.benchmark.RocksDBWriteBatchPerformanceTest [ERROR] benchMark(org.apache.flink.contrib.streaming.state.benchmark.RocksDBWriteBatchPerformanceTest) Time elapsed: 2.073 s <<<

ERROR! org.junit.runners.model.TestTimedOutException: test timed out after 2000 milliseconds at org.apache.flink.contrib.streaming.state.benchmark.RocksDBWriteBatchPerformanceTest.benchMarkHelper(RocksDBWriteBatchPerformanceTest.java:118) at org.apache.flink.contrib.streaming.state.benchmark.RocksDBWriteBatchPerformanceTest.benchMark(RocksDBWriteBatchPerformanceTest.java:96)

```

 ;;;","13/Jan/20 14:20;yunta;[~redmark-ibm] thanks for your feedback. 

What's your hardware information?

It seems no one has ever compared the performance of RocksDB with the same hardware on amd64 V.S ppc64le, we could open an issue in RocksDB community. For this question in Flink, we could increase the timeout if RocksDB has some performance issue on ppc64le confirmed.;;;","13/Jan/20 14:40;redmark-ibm;[~yunta] thanks again for helping.

 

This is a Red Hat 7.6 KVM hosted virtual machine, ppc64le with 16 GB of memory, 8 GB swap, 4 cpus.

 

Currently our only solution is to change the performance write time-out from 2 seconds to 3 seconds in 

```
./flink-state-backends/flink-statebackend-rocksdb/src/test/java/org/apache/flink/contrib/streaming/state/benchmark/RocksDBWriteBatchPerformanceTest.java

[*@Test*|https://jazz06.rchland.ibm.com:12443/jazz/users/Test]*(timeout = 2000) change to [@Test|https://jazz06.rchland.ibm.com:12443/jazz/users/Test](timeout = 3000)*

*```*

 ;;;","05/May/20 13:39;rmetzger;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=610&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=4ed44b66-cdd6-5dcf-5f6a-88b07dda665d;;;","03/Jun/20 05:39;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2587&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=39a61cac-5c62-532f-d2c1-dea450a66708;;;","08/Jun/20 06:53;yunta;Since we already have the repo: [https://github.com/apache/flink-benchmarks], I wonder the significance of those \{{RocksDB*PerformanceTest}}s and unit test performance would easily be affected by the status of running host.

I prefer to remove them all: ({{RocksDBListStatePerformanceTest}}, {{RocksDBWriteBatchPerformanceTest}} and {{RocksDBPerformanceTest}}), and we could also add cases in flink-benchmarks if we think some field is only covered by those tests. 

What do you think of this [~sewen], [~rmetzger]?;;;","09/Jun/20 06:32;rmetzger;I don't really have an opinion here, because I'm not very familiar with the RocksDB code. Let's wait for Stephan's response.

Another case in {{[ERROR]   RocksDBPerformanceTest.testRocksDbRangeGetPerformance:146 » TestTimedOut test ...}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2973&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034;;;","16/Jun/20 14:22;sewen;The purpose of this test is to guard against the ""quadratic concatenation complexity bug"" that RocksDB had a few versions ago.
In that case, the benchmark took 50s or so. We can probably increase this to 5s without a problem.

How about this?
  - we add it to the benchmarks suite to monitor regressions more precisely
  - we keep it in the codebase with a timeout of 5 seconds or so, as a rough guard;;;","18/Jun/20 09:04;yunta;[~sewen] After digging into those tests, I think all of them could be dropped.

{{RocksDBListStatePerformanceTest}} targets for performance of ""stringappendtest"" merge operator, which has been covered by [ListStateBenchmark#add|https://github.com/apache/flink-benchmarks/blob/8b449865cf733dbb3c01e997fe44b1a5b6f82cdc/src/main/java/org/apache/flink/state/benchmark/ListStateBenchmark.java#L118].

{{RocksDBWriteBatchPerformanceTest}} targets for performance of WriteBatch which should be covered by [MapStateBenchmark#mapPutAll|https://github.com/apache/flink-benchmarks/blob/8b449865cf733dbb3c01e997fe44b1a5b6f82cdc/src/main/java/org/apache/flink/state/benchmark/MapStateBenchmark.java#L160]

{{RocksDBPerformanceTest}} targets for performance of merge and iterator seek and next, which have been covered by [ListStateBenchmark#add|https://github.com/apache/flink-benchmarks/blob/8b449865cf733dbb3c01e997fe44b1a5b6f82cdc/src/main/java/org/apache/flink/state/benchmark/ListStateBenchmark.java#L118] and [MapStateBenchmark#mapIterator|https://github.com/apache/flink-benchmarks/blob/8b449865cf733dbb3c01e997fe44b1a5b6f82cdc/src/main/java/org/apache/flink/state/benchmark/MapStateBenchmark.java#L143]

And the most important thing is unit test cannot watch the performance issues clearly. If the execution time expands from 2 seconds to 2.5 seconds, which means the performance regression is about 25%. However, a timeout limit of 3 seconds cannot detect such performance regression. Thus, I do not see real benefits to still keep them in unit tests, not to mention sometimes break the CI.;;;","18/Jun/20 11:47;sewen;Fair enough, I agree with your conclusion, Yun Tang.

+1 to drop these benchmark unit tests.;;;","04/Feb/21 05:34;maguowei;another case

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12891&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=39a61cac-5c62-532f-d2c1-dea450a66708;;;","05/Feb/21 02:39;yunta;[~maguowei] These tests are dropped in FLINK-18373 and I will close this ticket as it fixed from 1.12.0;;;",,,,,,,,,,,,,
State TTL Heap backend end-to-end test fails on Travis,FLINK-15317,13275230,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,gjy,gjy,18/Dec/19 11:02,22/Jun/21 14:04,13/Jul/23 08:10,19/Dec/19 13:20,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Network,Tests,,,,0,pull-request-available,,,,"https://api.travis-ci.org/v3/job/626286529/log.txt

{noformat}
Checking for errors...
Found error in log files:
...
java.lang.IllegalStateException: Released
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:483)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:474)
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:75)
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:125)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:488)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
{noformat}",,gjy,liyu,pnowojski,,,,,,,,,,,"AHeise commented on pull request #10621: [FLINK-15317][runtime] Handle closed input gates more gracefully during
URL: https://github.com/apache/flink/pull/10621
 
 
   cancellation.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   During task cancellation, input gates can be closed in the task cancellation thread, before the main invokable has been interrupted, which resulted in reads from a closed gate.
   
   This commit will handle this case in a more graceful manner such that the end-user is not seeing any additional exception.
   
   ## Brief change log
   
   - Use `TaskCancelException` instead of `IllegalStateException` when input gates are closed in a similar fashion as this exception has been used in `LocalInputChannel`.
   - Add a special handling of `TaskCancelException` to `StreamTask#runMainLoop()`, to make sure the exception is not appearing in the log.
   
   ## Verifying this change
   
   This fix is indirectly covered by many e2e tests that rely on this exception being swallowed.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/19 14:31;githubbot;600","AHeise commented on pull request #10629: [FLINK-15317][runtime] Handle closed input gates more gracefully during
URL: https://github.com/apache/flink/pull/10629
 
 
   Backport of https://github.com/apache/flink/pull/10621 with no changes.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Dec/19 11:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-15285,,,FLINK-15076,,FLINK-15403,,FLINK-15280,FLINK-15285,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 03:29:18 UTC 2019,,,,,,,,,,"0|z09sqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/19 11:25;gjy;Same root cause as FLINK-15285 ;;;","18/Dec/19 16:20;liyu;Thanks for logging the issue [~gjy] and the quick fix [~arvid heise].

Correct me if I'm wrong, is this a regression of FLINK-15076? If so, let's add some link to the issue. Thanks.;;;","19/Dec/19 09:50;pnowojski;[~liyu] no, it's not a regression from FLINK-15076. That one was about cancelling source tasks. This one is happening with non-source (network) tasks (as you can see, there is a {{SingleInputGate}} in the stack trace). It looks more like a completely independent issue that was there for quite some time. I don't know why has it popped up just right now, but I think this doesn't matter - it has to be fixed anyway.;;;","19/Dec/19 10:03;liyu;Got it and agreed it needs to be fixed, thanks for the clarification [~pnowojski]!;;;","19/Dec/19 10:54;pnowojski;Correction [~liyu]. After some discussion with [~arvid heise] we realised that this bug/race condition was indeed caused by FLINK-15076.;;;","19/Dec/19 13:20;pnowojski;merged commit 36aff23 to master

merged commit 08d63a7 to release-1.10;;;","20/Dec/19 03:29;liyu;Thanks for the further confirmation and quick fix [~pnowojski]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Can not insert decimal with precision into sink using TypeInformation,FLINK-15313,13275213,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,lzljs3620320,lzljs3620320,18/Dec/19 09:43,26/Dec/19 04:33,13/Jul/23 08:10,26/Dec/19 04:32,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Sink DDL:

 
{code:java}
val sinkDDL =
  s""""""
    |CREATE TABLE T2 (
    |  d DECIMAL(10, 2),
    |  cnt INT
    |) with (
    |  'connector.type' = 'filesystem',
    |  'connector.path' = '$sinkFilePath',
    |  'format.type' = 'csv',
    |  'format.field-delimiter' = ','
    |)
  """""".stripMargin
{code}
Using blink with batch mode. (ensure insert BinaryRow into sink table)

 

In FLINK-15124 , but we still use wrong precision to construct DataFormatConverter. This will lead to exception when inserting BinaryRow. (BinaryRow need correct precision to get)

 ",,dwysakowicz,jark,leonard,lzljs3620320,,,,,,,,,,"wuchong commented on pull request #10667: [FLINK-15313][table] Fix can't insert decimal data into sink using TypeInformation
URL: https://github.com/apache/flink/pull/10667
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix can't insert decimal data into sink using TypeInformation
   
   The root problem is we use TypeInformation for sink code generation which
   loses the precision of decimal type. This result in the illegal accessing to
   the internal Decimal value. 
   
   
   ## Brief change log
   
   The fixing including:
   
   1. Add validation for query and sink field types should be compatible (including precision)
   2. If not compatible, an implicit cast is added (this makes sure that it is still correct
     to access internal decimal data using sink field type, e.g. genToExternal)
   3. Checks logical schema (from DDL) and physical schema (from TableSink#getConsumedDataType)
     are compatible.
   4. Use new type system for sink code generation
   
   ## Verifying this change
   
   This change is covered by existing test, but also added some other tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 14:53;githubbot;600","wuchong commented on pull request #10667: [FLINK-15313][table] Fix can't insert decimal data into sink using TypeInformation
URL: https://github.com/apache/flink/pull/10667
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 04:27;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-15309,FLINK-15310,FLINK-15326,,,,,FLINK-15310,FLINK-15326,FLINK-15309,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 26 04:32:37 UTC 2019,,,,,,,,,,"0|z09smo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/19 09:44;lzljs3620320;CC: [~jark];;;","26/Dec/19 04:32;jark;[FLINK-15313][table-blink] Fix can't insert decimal data into sink using TypeInformation
 - 1.11.0: fb0cd0ee3cc06de078b60bd56fb89d951877e67d
 - 1.10.0: 7eb80ff3745aa9619af4835da24f4a7062b8e229

[hotfix][hive] Use TableUtils.collectToList(Table) to verify query result
 - 1.11.0: 32ff63e7fe23e69d6a4421e51e4b6d00578dfcb1
 - 1.10.0: 019a4e75fdd8aff9f038a1d8b80a4128e1de5950

[hotfix][table-api] Use new type system for CsvTableSink
 - 1.11.0: f7ac7b9bc2a1fb90e6f06ca26d18b131e034a70d
 - 1.10.0: 3817455eb79c14a1574c9a6da11b7363b2c7b805

[hotfix][table-common] Add toNullable() and legacyDecimalToDefaultDecimal() TypeTransformations
 - 1.11.0: 18717431590f1f2768add8dbb3a876df70e6f7cd
 - 1.10.0: 4deb170fca6f1c9caf59109582323ed7d8a7d854
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job failed when enable pipelined-shuffle.compression and numberOfTaskSlots > 1,FLINK-15308,13275177,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,fengjiajie,fengjiajie,18/Dec/19 07:42,22/Jan/20 13:40,13/Jul/23 08:10,20/Dec/19 15:59,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"Job worked well with default flink-conf.yaml with pipelined-shuffle.compression:
{code:java}
taskmanager.numberOfTaskSlots: 1
taskmanager.network.pipelined-shuffle.compression.enabled: true
{code}
But when I set taskmanager.numberOfTaskSlots to 4 or 6:
{code:java}
taskmanager.numberOfTaskSlots: 6
taskmanager.network.pipelined-shuffle.compression.enabled: true
{code}
job failed:
{code:java}
$ bin/flink run -m yarn-cluster -p 16 -yjm 1024m -ytm 12288m ~/flink-example-1.0-SNAPSHOT.jar
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/data/build/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/lib/slf4j-log4j12-1.7.15.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/data/sa_cluster/cloudera/parcels/CDH-5.14.4-1.cdh5.14.4.p0.3/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2019-12-18 15:04:40,514 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory ('/data/build/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.
2019-12-18 15:04:40,514 WARN  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The configuration directory ('/data/build/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.
2019-12-18 15:04:40,907 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2019-12-18 15:04:41,084 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Cluster specification: ClusterSpecification{masterMemoryMB=1024, taskManagerMemoryMB=12288, numberTaskManagers=1, slotsPerTaskManager=6}
2019-12-18 15:04:42,344 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Submitting application master application_1576573857638_0026
2019-12-18 15:04:42,370 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1576573857638_0026
2019-12-18 15:04:42,371 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Waiting for the cluster to be allocated
2019-12-18 15:04:42,372 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Deploying cluster, current state ACCEPTED
2019-12-18 15:04:45,388 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - YARN application has been deployed successfully.
2019-12-18 15:04:45,390 INFO  org.apache.flink.yarn.YarnClusterDescriptor                   - Found Web Interface debugboxcreate431x3.sa:36162 of application 'application_1576573857638_0026'.
Job has been submitted with JobID 9140c70769f4271cc22ea8becaa26272

------------------------------------------------------------
 The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 9140c70769f4271cc22ea8becaa26272)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 9140c70769f4271cc22ea8becaa26272)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:83)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1628)
	at cn.kbyte.StreamingJob.main(StreamingJob.java:247)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
	... 11 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 9140c70769f4271cc22ea8becaa26272)
	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:112)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$21(RestClusterClient.java:532)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:291)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:110)
	... 19 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:188)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:183)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:177)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:456)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380)
	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.io.compression.DataCorruptionException: Input is corrupted
	at org.apache.flink.runtime.io.compression.Lz4BlockDecompressor.decompress(Lz4BlockDecompressor.java:80)
	at org.apache.flink.runtime.io.network.buffer.BufferDecompressor.decompress(BufferDecompressor.java:97)
	at org.apache.flink.runtime.io.network.buffer.BufferDecompressor.decompressToIntermediateBuffer(BufferDecompressor.java:60)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.decompressBufferIfNeeded(SingleInputGate.java:581)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.transformBuffer(SingleInputGate.java:543)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.transformToBufferOrEvent(SingleInputGate.java:536)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:492)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:474)
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:75)
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:125)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:488)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.shaded.net.jpountz.lz4.LZ4Exception: Malformed input at 1875
	at org.apache.flink.runtime.shaded.net.jpountz.lz4.LZ4JavaUnsafeFastDecompressor.decompress(LZ4JavaUnsafeFastDecompressor.java:172)
	at org.apache.flink.runtime.io.compression.Lz4BlockDecompressor.decompress(Lz4BlockDecompressor.java:65)
	... 18 more
{code}
 ","$ git log
commit 4b54da2c67692b1c9d43e1184c00899b0151b3ae
Author: bowen.li <bowenli86@gmail.com>
Date: Tue Dec 17 17:37:03 2019 -0800",fengjiajie,gjy,kevin.cyj,zjwang,,,,,,,,,,"wsry commented on pull request #10627: [FLINK-15308][runtime] Remove data compression for pipelined shuffle.
URL: https://github.com/apache/flink/pull/10627
 
 
   ## What is the purpose of the change
   This PR removes data compression for pipelined shuffle mode. The feature may be added in the future version of Flink if there is a better solution.
   
   
   ## Brief change log
   
     - The code and document relevant to data compression for pipeline mode is removed.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Dec/19 10:23;githubbot;600","wsry commented on pull request #10638: [FLINK-15308][runtime] Remove data compression for pipelined shuffle.
URL: https://github.com/apache/flink/pull/10638
 
 
   ## What is the purpose of the change
   This PR removes data compression for pipelined shuffle mode. The feature may be added in the future version of Flink if there is a better solution.
   
   
   ## Brief change log
   
     - The code and document relevant to data compression for pipeline mode is removed.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 03:28;githubbot;600","zhijiangW commented on pull request #10638: [FLINK-15308][runtime] Remove data compression for pipelined shuffle.
URL: https://github.com/apache/flink/pull/10638
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 15:40;githubbot;600","zhijiangW commented on pull request #10627: [FLINK-15308][runtime] Remove data compression for pipelined shuffle.
URL: https://github.com/apache/flink/pull/10627
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 15:48;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,FLINK-14845,,,,,,,,,,,"19/Dec/19 02:55;kevin.cyj;image-2019-12-19-10-55-30-644.png;https://issues.apache.org/jira/secure/attachment/12989155/image-2019-12-19-10-55-30-644.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 22 13:39:28 UTC 2020,,,,,,,,,,"0|z09seo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/19 07:49;kevin.cyj;[~fengjiajie] Could you share your code? I would like to see if I can reproduce the problem locally.;;;","18/Dec/19 08:29;fengjiajie;Here is my test code:

[https://github.com/fengjiajie/my-flink-test|https://github.com/fengjiajie/my-flink-test/tree/master/src/main]

run cmd:
{code:java}
bin/flink run -m yarn-cluster -p 16 -yjm 1024m -ytm 8192m ~/laputa-flink-example-1.0-SNAPSHOT.jar
{code}
and
{code:java}
nc -l 31212
{code}
on the host debugboxcreate431x1 `cn/kbyte/StreamingJob.java:88` 
{code:java}
new SocketClientSink<>(""debugboxcreate431x1"", 31212, new SimpleStringSchema()))
{code}
[~kevin.cyj]

 ;;;","18/Dec/19 08:34;kevin.cyj;[~fengjiajie] Thanks for reporting the issue and sharing the code. I'll try to reproduce the problem.;;;","19/Dec/19 03:17;fengjiajie;Hi [~kevin.cyj] ,

I can reproduce the problem every time.

YARN cluster:  3 node ( 8 core 32GB )
{code:java}
$ cat flink-conf.yaml | grep -v '^#' | grep -v '^$'
jobmanager.rpc.address: localhost
jobmanager.rpc.port: 6123
jobmanager.heap.size: 1024m
taskmanager.memory.total-process.size: 1024m
taskmanager.numberOfTaskSlots: 6
parallelism.default: 1
taskmanager.network.pipelined-shuffle.compression.enabled: true
jobmanager.execution.failover-strategy: region
{code};;;","19/Dec/19 03:19;kevin.cyj;[~fengjiajie] I also reproduced it.;;;","19/Dec/19 09:13;kevin.cyj;The problem is cause by race of multi netty threads. The simplest way of fix the problem may be make the BufferCompressor/BufferDecompressor util thread safe, however it can complicate the network stack. After an offline discussion, we finally decide to disable data compression for pipeline mode in version release-1.10 and we may add the feature back if there a better solution in the future.;;;","19/Dec/19 09:50;fengjiajie;Really looking forward to it.;;;","20/Dec/19 15:56;kevin.cyj;Fix via 8525c378b91c16245d2e0456d423ed39f5c9b330 on master.

Fix via b87fc76ace24c69423037e68220091cb2965ac3e on release-1.10.;;;","22/Jan/20 13:39;gjy;If affectsVersion is 1.10.0 and fixVersion is 1.10.0, I think we can remove the release note since there is no behavioral change compared to 1.9.;;;",,,,,,,,,,,,,,,,,,,,,,,
MemoryMappedBoundedDataTest fails with IOException on ppc64le,FLINK-15305,13275154,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kevin.cyj,siddheshghadi,siddheshghadi,18/Dec/19 05:10,03/Apr/20 08:38,13/Jul/23 08:10,03/Apr/20 08:38,1.10.0,,,,,,,,,1.11.0,,,,Runtime / Network,Tests,,,,0,pull-request-available,,,,"By reducing the buffer size from 76_687 to 60_787 in flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/BoundedDataTestBase.java:164, test passes. Any thoughts on this approach?","arch: ppc64le
os: rhel 7.6
jdk: 8
mvn: 3.3.9",kevin.cyj,pnowojski,siddheshghadi,,,,,,,,,,,"wsry commented on pull request #11591: [FLINK-15305][tests] Make BoundedDataTestBase#testGetSizeMultipleRegions respect system page size to avoid potential failure
URL: https://github.com/apache/flink/pull/11591
 
 
   ## What is the purpose of the change
   
   Make BoundedDataTestBase#testGetSizeMultipleRegions respect system page size to avoid potential failure.
   
   
   ## Brief change log
   
     - Make BoundedDataTestBase#testGetSizeMultipleRegions respect system page size to avoid potential failure
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 07:56;githubbot;600","pnowojski commented on pull request #11591: [FLINK-15305][tests] Make BoundedDataTestBase#testGetSizeMultipleRegions respect system page size to avoid potential failure
URL: https://github.com/apache/flink/pull/11591
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Apr/20 08:38;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/19 05:01;siddheshghadi;surefire-report.txt;https://issues.apache.org/jira/secure/attachment/12989067/surefire-report.txt",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 08:38:28 UTC 2020,,,,,,,,,,"0|z09s9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/19 07:41;kevin.cyj;I guess this is because that the page size on your platform is 65536 bytes. Flink round down the input mmap region size (10_000 for the test case) by the following method:
{code:java}
private static int alignSize(int maxRegionSize) {
   checkArgument(maxRegionSize >= PAGE_SIZE);
   return maxRegionSize - (maxRegionSize % PAGE_SIZE);
}{code}
Except for decreasing the buffer size, increasing the mmap region can also fix the problem.

You can check whether the page size on your platform is 65536 bytes or not.;;;","21/Jan/20 10:40;pnowojski;[~kevin.cyj] what do we need to do, to close this ticket? Maybe document it? In some generic page with ""good to know things"" when running on non x86 architectures?;;;","04/Feb/20 06:28;kevin.cyj;Sorry for the late response, we were on Chinese Spring Festival vacation last week.

The reported exception happens when the data buffer to be written plus the header length (8 bytes) is larger than the region size. Flink always uses Integer.MAX_VALUE (2G) as region size which is not configurable and for data buffer size, Flink uses 32k by default and this option is configurable. Theoretically, it is possible to trigger the exception if a very large buffer size is configured (especially when huge pages are used).

To fix the problem, we can:
 # Remind the user to not using too large buffer size in the document of MEMORY_SEGMENT_SIZE;
 # Modify the test case to respect the page size, that is, calculating a proper data buffer size and region size based on the page size.

IMO, it is not a critical problem, after all, there is hardly anyone who sets buffer size to a very large value.

[~pnowojski] What do you think? Should we give it a fix?;;;","04/Feb/20 08:23;pnowojski;Ok, thanks for the explanation, it's just a test issue. I think this should be a simple fix (one liner?), if you want, feel free to open a PR for this. If not, I could fix it at some occasion. ;;;","04/Feb/20 09:11;kevin.cyj;OK, I will open a PR to fix it soon.;;;","03/Apr/20 08:38;pnowojski; merged commit 13ef823 into apache:master

Thanks for the fix [~kevin.cyj] and [~siddheshghadi] for reporting the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kinesis AsyncRecordEmitter needs to handle unchecked exception gracefully,FLINK-15301,13275066,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,yxu-apache,yxu-apache,17/Dec/19 17:41,31/Dec/19 01:24,13/Jul/23 08:10,20/Dec/19 16:19,,,,,,,,,,1.10.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,"Currently, any runTime exception encountered inside the `AsyncRecordEmitter.emitRecordAndUpdateState()` function could cause the thread to exit silently. Flink job would continue to run, but the stopped record emitter would subsequently cause Kinesis data consumption to stall. 

 

The AsyncRecordEmitter need to catch unchecked exception, log errors, and perhaps trigger job restart subsequently. ",,yxu-apache,,,,,,,,,,,,,"tweise commented on pull request #10611: [FLINK-15301] [kinesis] Exception propagation from record emitter to source thread
URL: https://github.com/apache/flink/pull/10611
 
 
   
   
   ## What is the purpose of the change
   
   * Exceptions encountered in the asynchronous record emitter need to be propagated to the source thread. This will allow the source to fail with the error instead of just the record emitter thread terminating, causing shard consumption to stop silently.
   
   ## Brief change log
   
   *(for example:)*
     - *Handover exception to source thread*
     - *Test coverage*
   
   ## Verifying this change
   
   This change added unit tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / *no*)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / *no*)
     - The serializers: (yes / *no* / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / *no* / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / *no* / don't know)
     - The S3 file system connector: (yes / *no* / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / *no*)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 23:05;githubbot;600","tweise commented on pull request #10611: [FLINK-15301] [kinesis] Exception propagation from record emitter to source thread
URL: https://github.com/apache/flink/pull/10611
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 16:18;githubbot;600","tweise commented on pull request #10725: [FLINK-15301] [kinesis] Exception propagation from record emitter to source thread
URL: https://github.com/apache/flink/pull/10725
 
 
   backport of https://github.com/apache/flink/pull/10611
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Dec/19 22:00;githubbot;600","tweise commented on pull request #10725: [FLINK-15301] [kinesis] Exception propagation from record emitter to source thread
URL: https://github.com/apache/flink/pull/10725
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/19 01:23;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 17:44:53 UTC 2019,,,,,,,,,,"0|z09rq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/19 17:44;yxu-apache;Example stacktrace
{code:java}
org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
 {{ at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:596)}}
 {{ at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:554)}}
 {{ at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:534)}}
 {{ at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:718)}}
 {{ at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:696)}}
 {{ at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollectWithTimestamp(StreamSourceContexts.java:310)}}
 {{ at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collectWithTimestamp(StreamSourceContexts.java:409)}}
 {{ at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.emitRecordAndUpdateState(KinesisDataFetcher.java:772)}}
 {{ at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.access$000(KinesisDataFetcher.java:91)}}
 {{ at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$AsyncKinesisRecordEmitter.emit(KinesisDataFetcher.java:272)}}
 {{ at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher$AsyncKinesisRecordEmitter.emit(KinesisDataFetcher.java:260)}}
 {{ at org.apache.flink.streaming.connectors.kinesis.util.RecordEmitter.run(RecordEmitter.java:230)}}
 {{ at java.lang.Thread.run(Thread.java:748)
...{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle memory fraction sanity check does not account for its min/max limit,FLINK-15300,13275022,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,azagrebin,azagrebin,17/Dec/19 14:28,08/Jan/20 09:03,13/Jul/23 08:10,08/Jan/20 09:03,,,,,,,,,,1.10.0,,,,Runtime / Configuration,,,,,0,pull-request-available,,,,"If we have a configuration which results in setting shuffle memory size to its min or max, not fraction during TM startup then starting TM parses generated dynamic properties and while doing the sanity check (TaskExecutorResourceUtils#sanityCheckShuffleMemory) it fails because it checks the exact fraction for min/max value.

Example, start TM with the following Flink config:
{code:java}
taskmanager.memory.total-flink.size: 350m
taskmanager.memory.framework.heap.size: 16m
taskmanager.memory.shuffle.fraction: 0.1{code}
The calculation will happen for total Flink memory and will result in the following extra program args:
{code:java}
taskmanager.memory.shuffle.max: 67108864b
taskmanager.memory.framework.off-heap.size: 134217728b
taskmanager.memory.managed.size: 146800642b
taskmanager.cpu.cores: 1.0
taskmanager.memory.task.heap.size: 2097150b
taskmanager.memory.task.off-heap.size: 0b
taskmanager.memory.shuffle.min: 67108864b{code}
where the derived fraction is less than shuffle memory min size (64mb), so it was set to the min value: 64mb.

While TM starts, the calculation happens now for the explicit task heap and managed memory but also with the explicit total Flink memory and TaskExecutorResourceUtils#sanityCheckShuffleMemory throws the following exception:
{code:java}
org.apache.flink.configuration.IllegalConfigurationException:
Derived Shuffle Memory size(64 Mb (67108864 bytes)) does not match configured Shuffle Memory fraction (0.10000000149011612).
at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.sanityCheckShuffleMemory(TaskExecutorResourceUtils.java:552)
at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.deriveResourceSpecWithExplicitTaskAndManagedMemory(TaskExecutorResourceUtils.java:183)
at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.resourceSpecFromConfig(TaskExecutorResourceUtils.java:135)
{code}
This can be fixed by checking whether the fraction to assert is within the min/max range.",,azagrebin,wind_ljy,xtsong,,,,,,,,,,,"azagrebin commented on pull request #10608: [FLINK-15300][Runtime] Fix sanity check to not fail if shuffle memory fraction is out of min/max range
URL: https://github.com/apache/flink/pull/10608
 
 
   ## What is the purpose of the change
   
   If we have a configuration which sets shuffle memory min/max value then the sanity check (TaskExecutorResourceUtils#sanityCheckShuffleMemory) can fail if the fraction is out of min/max range because it checks that the fraction exactly matches the derived min/max value.
   
   ## Brief change log
   
     - Fix TaskExecutorResourceUtils#sanityCheckShuffleMemory
     - add unit tests
   
   ## Verifying this change
   
   unit tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 15:42;githubbot;600","azagrebin commented on pull request #10608: [FLINK-15300][Runtime] Fix sanity check to not fail if shuffle memory fraction is out of min/max range
URL: https://github.com/apache/flink/pull/10608
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/20 09:00;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13980,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 08 09:03:02 UTC 2020,,,,,,,,,,"0|z09rg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/20 09:03;azagrebin;merged into master by d22fdc39a86496ebfc74914a72916d8a0ea7ab89
merged into 1.10 by a342e418a2d8df52645dd75588f8b9f74a07ad63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not throw exception if YARN Application switched to FINISHED immediately after deployed in YarnClusterDescriptor#startAppMaster,FLINK-15297,13274983,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,17/Dec/19 12:30,03/Jan/20 12:22,13/Jul/23 08:10,03/Jan/20 12:22,,,,,,,,,,1.10.0,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"Currently we throw an exception in {{YarnClusterDescriptor#startAppMaster}} if we first detect {{FINISHED}} before {{RUNNING}}. However, it is possible a legal state that the application finished normally immediately.

Right now we always try to connect the Dispatcher so it may be fine to throw the exception a bit earlier(otherwise when connect to a closed cluster an exception thrown also), but it is semantically wrong. Internally we have a code path that only required to report the ApplicationReport and it causes trouble.

cc [~trohrmann] what do you think?",,aljoscha,tison,trohrmann,wangyang0918,,,,,,,,,,"TisonKun commented on pull request #10615: [FLINK-15297][yarn] Do not throw exception if YARN Application switched to FINISHED immediately after deployed in YarnClusterDescriptor#startAppMaster
URL: https://github.com/apache/flink/pull/10615
 
 
   ## What is the purpose of the change
   
   Currently we throw an exception in YarnClusterDescriptor#startAppMaster if we first detect FINISHED before RUNNING. However, it is possible a legal state that the application finished normally immediately.
   
   ## Verifying this change
   
   straightforward & it hardly stably tests a job finished ""immediately"".
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yarn deployment, bug fix)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/19 02:45;githubbot;600","aljoscha commented on pull request #10615: [FLINK-15297][yarn] Do not throw exception if YARN Application switched to FINISHED immediately after deployed in YarnClusterDescriptor#startAppMaster
URL: https://github.com/apache/flink/pull/10615
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 12:22;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 12:22:32 UTC 2020,,,,,,,,,,"0|z09r7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/19 16:51;trohrmann;This sounds like a bug to me. In particular in detached mode it could happen that the deployed job terminates and does not wait for the client to connect. Hence, I think we should correct this.;;;","18/Dec/19 03:24;wangyang0918;[~tison] I'm not sure how this could happen? Does the flink application on Yarn finished very quickly that when `YarnClusterDescriptor` first get the application report, the state is ""FINISHED"". Also i think it could not happen in 250ms that application state changes from ""ACCEPTED"" to ""FINISHED"".;;;","18/Dec/19 04:24;tison;Internally we have a new entrypoint that finished job very quickly so it unstably encounter this issue. Anyway, no matter 250ms or 1ms the issue is conceptually existing so that the fix is valid.;;;","03/Jan/20 12:22;aljoscha;Fixed on master in 23c0bb0f4f73412ac594819b720337e8ab4acd2f
Fixed on release-1.10 in 36a3333263875f39795b065017b6ef5679478904;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need a way to turn off vectorized orc reader for SQL CLI,FLINK-15290,13274924,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,17/Dec/19 08:04,12/Mar/20 05:42,13/Jul/23 08:10,02/Jan/20 04:40,,,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,aljoscha,godfreyhe,jark,lirui,lzljs3620320,phoenixjiangnan,Terry1897,TsReaper,,,,,,"lirui-apache commented on pull request #10632: [FLINK-15290][streaming] Configure user defined function with global …
URL: https://github.com/apache/flink/pull/10632
 
 
   …configuration
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Make the global configurations available to user defined functions.
   
   
   ## Brief change log
   
     - Use `GlobalConfiguration.loadConfiguration` when opening the user function
   
   
   ## Verifying this change
   
   TBD
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
   NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Dec/19 12:31;githubbot;600","bowenli86 commented on pull request #10632: [FLINK-15290][hive] Need a way to turn off vectorized orc reader for SQL CLI
URL: https://github.com/apache/flink/pull/10632
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 04:38;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-16179,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 05:42:14 UTC 2020,,,,,,,,,,"0|z09qug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/19 08:32;ykt836;We already have this in `HiveOptions`, the config name is 
{noformat}
table.exec.hive.fallback-mapred-reader = true
{noformat}
Could you check whether this can work?;;;","17/Dec/19 10:24;lirui;[~ykt836] Yeah we have the configuration but currently it won't be passed to {{HiveTableInputFormat}};;;","17/Dec/19 13:08;lirui;I set {{table.exec.hive.fallback-mapred-reader}} in flink-conf.yaml. However, in {{AbstractUdfStreamOperator::open}}, we open the UDF with [an empty configuration|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/AbstractUdfStreamOperator.java#L102]. So what I put in flink-conf.yaml is not visible to HiveTableInputFormat. Replacing the empty configuration with {{GlobalConfiguration.loadConfiguration()}} solves the problem.
[~ykt836] [~lzljs3620320] Do you think that's the right way to go?;;;","19/Dec/19 10:06;lzljs3620320;I think yes, this is a temporary way.

In the future, I think it is better to provide a way to get TableConfig.;;;","30/Dec/19 09:25;jark;I'm not sure about this. 

1) Does it work in distributed environment (e.g. YARN session)?
2) I'm not sure whether it is intended to create a new instance of Configuration in {{AbstractUdfStreamOperator#open()}}, cc [~aljoscha]
3) Could you {{GlobalConfiguration.loadConfiguration()}} in {{HiveTableInputFormat}}?
;;;","30/Dec/19 09:26;jark;I added {{DataStream}} component, as the pull request changes code of DataStream part. ;;;","30/Dec/19 09:39;lzljs3620320;Yes [~jark], Hi [~lirui], we should modify ""HiveInputFormat"" to ""GlobalConfiguration.loadConfiguration"".

We should not modify ""AbstractUdfStreamOperator"", it should be a big impact on most of users.;;;","30/Dec/19 09:57;lzljs3620320;Hi [~jark]: 

> I'm not sure whether it is intended to create a new instance of Configuration in {{AbstractUdfStreamOperator#open()}}

I found the comments in ""AbstractUdfStreamOperator.getUserFunctionParameters"":

Since the streaming API does not implement any parametrization of functions via a configuration, the config returned here is actually empty.;;;","30/Dec/19 12:13;lirui;[~lzljs3620320] [~jark] Yes I'll update the PR to load the configurations in {{HiveTableInputFormat}};;;","02/Jan/20 04:40;phoenixjiangnan;master: f205d754ad513a8e16dd2b9d5fa4586149ac7013
1.10: 694ae9c9c943a8f8cc0aff5aa54ce7207eb3da21;;;","06/Jan/20 10:51;aljoscha;I don't think {{GlobalConfiguration.loadConfiguration()}} is the way to go here, as [~jark] mentioned, this could not work in some environments.

In general {{GlobalConfiguration}} is a ""hack"" that I think we should get rid of in the long-term because it needs to much global state to be correct, i.e. the config must be there and a certain environment variable needs to be set. If we can, we should always pass through options explicitly.;;;","06/Jan/20 11:17;lirui;Hi [~aljoscha], thanks for the inputs. The merged PR only loads global configurations on client side, assuming the env variable and conf file are always available on client side. I agree with you the solution seems a little hacky, but I don't know what's the recommended way to get configurations user puts in {{flink-conf.yaml}} or {{sql-client-defaults.yaml}}. Could you please elaborate on what you mean by ""pass through options explicitly""?;;;","07/Jan/20 01:15;lzljs3620320;Hi [~aljoscha] , actually we can not pass through options explicitly because of our current Catalog/TableFactory api, we have no chance to get options from configuration. In 1.11, we should solve this problem by passing TableConfig to Catalog/TableFactory.

So as [~lirui] said, we get option from the GlobalConfiguration.loadConfiguration() of client side. What do you think about the GlobalConfiguration of client side? Should always work?;;;","12/Mar/20 05:04;TsReaper;Hi [~lzljs3620320],

I also agree that hive source configs should go into table config, as HiveTableSource is a table source and will be created and optimized during optimization. How's the progress of moving these options into table config, is it planned to be done in 1.11?;;;","12/Mar/20 05:42;lzljs3620320;Hi [~TsReaper], it is done in FLINK-16179;;;",,,,,,,,,,,,,,,,,
ack flink-hadoop-compatibility and flink-orc into flink-hive,FLINK-15287,13274910,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lirui,lirui,17/Dec/19 06:28,06/Jan/20 16:08,13/Jul/23 08:10,20/Dec/19 19:01,1.10.0,,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"flink-connector-hive should contain flink-hadoop-compatibility to reduce users' efforts in figuring out dependency jars, and flink-orc for adding orc dependency which is missing in hive 2.2.x.",,lirui,phoenixjiangnan,,,,,,,,,,,,"JingsongLi commented on pull request #10618: [FLINK-15287][hive] Disable hive orc optimization for version less than 2.3
URL: https://github.com/apache/flink/pull/10618
 
 
   
   ## What is the purpose of the change
   
   Without shim support, it is hard to support hive orc version less than 2.3
   
   ## Brief change log
   
   Remove hive orc optimization for version less than 2.3
   
   ## Verifying this change
   
   Manually verify.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/19 04:16;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 19:01:19 UTC 2019,,,,,,,,,,"0|z09qrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/19 19:01;phoenixjiangnan;master: 57be6edb7da0e32a9e4a35172e2d5ed56065a6df
1.10: 9026a5ae35d566f8bf755a3e053fffb4fe9abb35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Managed Memory Option for RocksDB not picked up from config,FLINK-15286,13274839,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,16/Dec/19 20:55,17/Dec/19 08:49,13/Jul/23 08:10,17/Dec/19 08:49,1.10.0,1.11.0,,,,,,,,1.10.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"This is a missing lookup in the config in the State Backend's configure() method. Or, more precisely, in the State Backend's MemoryConfiguration's configure() method.",,sewen,,,,,,,,,,,,,"StephanEwen commented on pull request #10600: [FLINK-15286][state-backend-rocksdb] Read option whether to use managed memory from configuration
URL: https://github.com/apache/flink/pull/10600
 
 
   ## What is the purpose of the change
   
   Fixes the bug that RocksDB did not pick up the value for `state.backend.rocksdb.memory.managed` from the configuration.
   
   ## Verifying this change
   
     - Reviewers can set the `state.backend.rocksdb.memory.managed: true` option manually on a standalone cluster and check the log that RocksDB uses managed memory.
     - This PR also adds a unit test tat covers this case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 21:03;githubbot;600","asfgit commented on pull request #10600: [FLINK-15286][state-backend-rocksdb] Read option whether to use managed memory from configuration
URL: https://github.com/apache/flink/pull/10600
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 08:41;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 08:49:27 UTC 2019,,,,,,,,,,"0|z09qbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/19 08:49;sewen;Fixed in 
  - 1.10.0 via 38b2b969322ad1b846206dadb4d55ff1bd86bd74
  - 1.11.0 via 685c443a7230dc914f83e793ead83ccd0e46e773;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better error message when insert partition with values,FLINK-15272,13274700,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,16/Dec/19 08:33,23/Dec/19 15:06,13/Jul/23 08:10,23/Dec/19 15:06,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Now, we not support insert partition with values like:

Insert into mytable partition (date='2019-08-08') values ('jason', 25)

Will throw a exception:

schema not match.

We should improve error message to tell user we not support this pattern.",,jark,lzljs3620320,,,,,,,,,,,,"JingsongLi commented on pull request #10591: [FLINK-15272][table-planner-blink] Better error message when insert partition with values
URL: https://github.com/apache/flink/pull/10591
 
 
   
   ## What is the purpose of the change
   
   Now, we not support insert partition with values like:
   ```
   Insert into mytable partition (date='2019-08-08') values ('jason', 25)
   ```
   Will throw a exception:
   schema not match.
   
   We should improve error message to tell user we not support this pattern.
   
   ## Brief change log
   
   Check insert partition with select, others will throw validate exception.
   
   ## Verifying this change
   
   PartitionableSinkTest.testStaticWithValues
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 09:05;githubbot;600","wuchong commented on pull request #10591: [FLINK-15272][table-planner-blink] Better error message when insert partition with values
URL: https://github.com/apache/flink/pull/10591
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 14:55;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 23 15:06:40 UTC 2019,,,,,,,,,,"0|z09pgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Dec/19 15:06;jark;1.11.0: dcf486922ab773d2ffc479f4818711babb0faf8b
1.10.0: 4ac3551f013a60c76ad2e6f764ce297392eb9c49;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix hive dialect limitation to overwrite and partition syntax,FLINK-15269,13274682,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,16/Dec/19 05:54,20/Dec/19 06:36,13/Jul/23 08:10,20/Dec/19 06:36,,,,,,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"As [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Overwrite-and-partition-inserting-support-in-1-10-td35829.html#a35885] discussed.

We should:
 * Remove hive dialect limitation for supported ""INSERT OVERWRITE"" and ""INSERT ... PARTITION(...)"".
 * Limit ""CREATE TABLE ... PARTITIONED BY"" to hive dialect.",,jark,lirui,lzljs3620320,,,,,,,,,,,"JingsongLi commented on pull request #10587: [FLINK-15269][table] Fix hive dialect limitation to overwrite and partition syntax
URL: https://github.com/apache/flink/pull/10587
 
 
   
   ## What is the purpose of the change
   
   We should support ""INSERT OVERWRITE"" ""INSERT ... PARTITION()"" without dialect limitation.
   As http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Overwrite-and-partition-inserting-support-in-1-10-td35829.html#a35885 discussed.
   
   ## Brief change log
   
   We should:
   - Remove hive dialect limitation for supported ""INSERT OVERWRITE"" and ""INSERT ... PARTITION(...)"".
   - Limit ""CREATE TABLE ... PARTITIONED BY"" to hive dialect.
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 06:04;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 06:36:11 UTC 2019,,,,,,,,,,"0|z09pco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/19 06:36;jark;1.11.0: b692d101e086ee2596d3c53a5ceec0a26c74ca3d
1.10.0: a898daeddb7764b4a79a71e5b1a7025dc66518b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shaded Hadoop S3A end-to-end test fails on travis,FLINK-15268,13274681,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,liyu,liyu,16/Dec/19 05:46,22/Jun/21 14:05,13/Jul/23 08:10,20/Dec/19 13:06,1.10.0,,,,,,,,,1.10.0,,,,Connectors / FileSystem,Tests,,,,0,pull-request-available,,,,"As titled, the 'Shaded Hadoop S3A end-to-end test' case failed with below error:
{code}
java.io.IOException: regular upload failed: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AUtils.extractException(S3AUtils.java:291)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3ABlockOutputStream.putObject(S3ABlockOutputStream.java:448)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:360)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.flink.fs.s3.common.hadoop.HadoopDataOutputStream.close(HadoopDataOutputStream.java:52)
	at org.apache.flink.core.fs.ClosingFSDataOutputStream.close(ClosingFSDataOutputStream.java:64)
	at java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)
	at java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:341)
	at java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:161)
	at java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:258)
	at org.apache.flink.api.java.io.CsvOutputFormat.close(CsvOutputFormat.java:170)
	at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:227)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException
	at org.apache.flink.fs.s3base.shaded.com.amazonaws.util.Md5Utils.md5AsBase64(Md5Utils.java:104)
	at org.apache.flink.fs.s3base.shaded.com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1647)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.putObjectDirect(S3AFileSystem.java:1531)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$putObject$5(WriteOperationHelper.java:426)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)
{code}

https://api.travis-ci.org/v3/job/625037121/log.txt",,arvid heise,klion26,leonard,liyu,pnowojski,trohrmann,,,,,,,,"AHeise commented on pull request #10649: [FLINK-15268][build] Correctly set Multi-Release in manifest.
URL: https://github.com/apache/flink/pull/10649
 
 
   
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   FLINK-14905 introduced a multi-release approach to bundle libraries
   for different JDK versions. However, the multi-release flag was not
   correctly set, which effectively disabled the mechanism and lead to
   incomplete dependencies on Java 11. This commit fixes the setup, such
   that the Java 11 libraries are correctly loaded.
   
   ## Brief change log
   
   - Fixes the Multi-Release flag in manifest creation of flink-dist.
   
   
   ## Verifying this change
   
   - Existing hadoop-s3 e2e test already cover it.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 09:14;githubbot;600","pnowojski commented on pull request #10649: [FLINK-15268][build] Correctly set Multi-Release in manifest.
URL: https://github.com/apache/flink/pull/10649
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 13:04;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14905,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 13:06:41 UTC 2019,,,,,,,,,,"0|z09pcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/19 11:59;chesnay;hmm, this is pre-commit test and has passed on latest master/release-1.10. Not sure how classloading could render the test unstable.;;;","18/Dec/19 01:29;liyu;Another instance: https://api.travis-ci.org/v3/job/625770580/log.txt;;;","18/Dec/19 12:01;arvid;Is this related to Java 11 not having jaxb anymore?;;;","19/Dec/19 14:06;trohrmann;Another instance: https://api.travis-ci.org/v3/job/626676926/log.txt;;;","19/Dec/19 14:50;trohrmann;Arvid will take a look at the problem.;;;","20/Dec/19 13:06;pnowojski;Merged to master as 96f4c79
Merged to release-1.10 as 2478e8ae0d121c65953f1ed9d320e9620c6d4fcf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NoSuchElementException if rowtime field is remapped in TableSource,FLINK-15267,13274680,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,liyu,liyu,16/Dec/19 05:43,17/Dec/19 06:38,13/Jul/23 08:10,17/Dec/19 06:38,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Planner,Tests,,,,0,pull-request-available,test-stability,,,"As titled, the 'Streaming SQL end-to-end test (Blink planner)' case failed with below error:
{code}
The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: key not found: ts
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:146)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:671)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:216)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:989)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:989)
Caused by: java.util.NoSuchElementException: key not found: ts
	at scala.collection.MapLike$class.default(MapLike.scala:228)
	at scala.collection.AbstractMap.default(Map.scala:59)
	at scala.collection.MapLike$class.apply(MapLike.scala:141)
	at scala.collection.AbstractMap.apply(Map.scala:59)
	at org.apache.flink.table.planner.sources.TableSourceUtil$$anonfun$6.apply(TableSourceUtil.scala:164)
	at org.apache.flink.table.planner.sources.TableSourceUtil$$anonfun$6.apply(TableSourceUtil.scala:163)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.sources.TableSourceUtil$.fixPrecisionForProducedDataType(TableSourceUtil.scala:163)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.translateToPlanInternal(StreamExecTableSourceScan.scala:143)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.translateToPlanInternal(StreamExecTableSourceScan.scala:60)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecTableSourceScan.translateToPlan(StreamExecTableSourceScan.scala:60)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:54)
{code}

https://api.travis-ci.org/v3/job/625037124/log.txt",,jark,leonard,liyu,,,,,,,,,,,"wuchong commented on pull request #10596: [FLINK-15267][table-planner-blink] Fix NoSuchElementException if rowtime field is remapped
URL: https://github.com/apache/flink/pull/10596
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix NoSuchElementException if rowtime field is remapped. This fixed the E2E test failed in Travis. 
   
   ## Brief change log
   
   The commit messages describe the changes.
   
   ## Verifying this change
   
   This change added a test to cover this case and also verified the e2e test in my local machine.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 13:36;githubbot;600","wuchong commented on pull request #10596: [FLINK-15267][table-planner-blink] Fix NoSuchElementException if rowtime field is remapped
URL: https://github.com/apache/flink/pull/10596
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 06:35;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 06:38:38 UTC 2019,,,,,,,,,,"0|z09pc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/19 05:44;liyu;[~jark] [~ykt836] [~lzljs3620320] FYI.;;;","16/Dec/19 06:01;liyu;Another instance (and many more in different nightly stages): https://api.travis-ci.org/v3/job/625335885/log.txt;;;","17/Dec/19 06:38;jark;1.10.0: 4b4f81f29f40e82bed27a157b2b81bd06c5fc86b
1.11.0: 60098a707f7f7d114dcc24ee794c9afe8c814141;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in blink planner code gen,FLINK-15266,13274665,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,libenchao,libenchao,libenchao,16/Dec/19 02:17,05/May/23 18:46,13/Jul/23 08:10,20/Dec/19 06:50,1.9.1,,,,,,,,,1.10.0,1.9.2,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"`cast` function in blink planner and old planner are different:

in legacy planner:
cast('' as int)  ->  throw NumberFormatException
cast(null as int) -> throw NullPointerException
cast('abc' as int)  -> throw NumberFormatException

but in blink planner:
cast('' as int)  ->  return null
cast(null as int) -> return null
cast('abc' as int)  -> return null

A step forward:
```
create table source {
  age int,
  id varchar
};
select case when age < 20 then cast(id as bigint) else 0 end from source;
```
queries like above will throw NPE because we will try assign a `null` to a `long` field when the input satisfy `age < 20`.",,jark,leonard,libenchao,,,,,,,,,,,"libenchao commented on pull request #10594: [FLINK-15266] Fix NPE for case operator code gen in blink planner
URL: https://github.com/apache/flink/pull/10594
 
 
   ## What is the purpose of the change
   
   This pr fixes the NPE bug in blink case operator code gen as described in FLINK-15266. 
   
   ## Brief change log
   
   For case operator result type, generate primitive type only when trueAction and falseAction both return non-null types.
   
   ## Verifying this change
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (yes)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 12:59;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 06:50:46 UTC 2019,,,,,,,,,,"0|z09p8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/19 02:53;jark;Thanks for reporting this issue. I verified in master branch and reproduced this exception. I think it is a bug in code generation.;;;","16/Dec/19 03:18;libenchao;[~jark] I also reproduced it in current master branch.

What's your option for fixing this?
1. align the behavior between legacy and blink planner, which means throw Exception when casting illegal input,
2, or keep blink planner align with hive's behavior, and return null when casting illegal input ?
3, or 1&2, both return null when casting illegal input ?;;;","16/Dec/19 03:23;jark;Hi [~libenchao], the behavior of returning null is intended in blink planner. Because for a streaming job, it is similar to a online service, it shouldn't fail due to some dirty input data. 

So it is just a bug in code generation in blink planner. 

For legacy planner, we don't plan to change the behavior for now. Because it is major refactoring and lagacy planner will be dropped in the future. ;;;","16/Dec/19 03:29;libenchao;[~jark] Thanks for your comment.

Could you please help assign this ticket to me ? I'll prepare the pr very soon.;;;","16/Dec/19 04:23;jark;Thanks for taking this [~libenchao], I assigned this issue to you.;;;","20/Dec/19 06:50;jark;1.11.0: 5f86f2d1ae608a4d2259bfcf637d97b490dbd328
1.10.0: 319466af2a63ad28e18481243e3cbb9b39ecc1de
1.9.2: 73c82c3d4d7176b04cb4f8720ed64fdb1c208bf9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""-executor"" suffix from executor names",FLINK-15265,13274640,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,aljoscha,aljoscha,15/Dec/19 15:49,17/Dec/19 09:28,13/Jul/23 08:10,17/Dec/19 09:27,,,,,,,,,,1.10.0,,,,API / Core,,,,,0,pull-request-available,,,,"The executor names always have ""-executor"" as a suffix, this is reduntant. Currently, the executor name is also used to retrieve a {{ClusterClient}}, where it is unfortunate that the name has executor as a suffix. In the future we might provide something like a {{FlinkClient}} that offers a programmatic API for the functionality of {{bin/flink}}, here we would also use the same names.

In reality, the ""executor names"" are not names of executors but deployment targets. That's why the current naming seems a bit unnatural.

This is a simple search-and-replace job, no new functionality.",,aljoscha,kkl0u,,,,,,,,,,,,"aljoscha commented on pull request #10583: [FLINK-15265] Remove ""-executor"" suffix from executor names
URL: https://github.com/apache/flink/pull/10583
 
 
   The executor names always have ""-executor"" as a suffix, this is
   reduntant. Currently, the executor name is also used to retrieve a
   ClusterClient, where it is unfortunate that the name has executor as a
   suffix. In the future we might provide something like a FlinkClient that
   offers a programmatic API for the functionality of bin/flink, here we
   would also use the same names.
   
   In reality, the ""executor names"" are not names of executors but
   deployment targets. That's why the current naming seems a bit unnatural.
   
   This is a simple search-and-replace job, no new functionality. So no tests are needed.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Dec/19 15:58;githubbot;600","aljoscha commented on pull request #10583: [FLINK-15265] Remove ""-executor"" suffix from executor names
URL: https://github.com/apache/flink/pull/10583
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 09:28;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 09:27:42 UTC 2019,,,,,,,,,,"0|z09p3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/19 15:52;aljoscha;cc [~GJL] [~liyu] We would like to add this to Flink 1.10. It's a simple change of names that are introduced in Flink 1.10 and not introducing that renaming now will make the renaming impossible in future versions if we don't want to break things.;;;","15/Dec/19 15:55;kkl0u;Wouldn't it make sense to put it under the https://issues.apache.org/jira/browse/FLINK-14376 umbrella?;;;","17/Dec/19 09:27;aljoscha;Fixed on master in 972aac15eb1251727dd026e154ce53f61ab1d3fe
Fixed on release-1.10 in 2d67cba58abb24713e5e3213e13acc466c9284a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveInspector.toInspectors() should convert Flink constant to Hive constant ,FLINK-15259,13274512,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,phoenixjiangnan,phoenixjiangnan,13/Dec/19 22:44,06/Jan/20 16:07,13/Jul/23 08:10,02/Jan/20 04:47,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.2,,,Connectors / Hive,,,,,0,pull-request-available,,,,"repro test: 

{code:java}
public class HiveModuleITCase {
	@Test
	public void test() {
		TableEnvironment tEnv = HiveTestUtils.createTableEnvWithBlinkPlannerBatchMode();

		tEnv.unloadModule(""core"");
		tEnv.loadModule(""hive"", new HiveModule(""2.3.4""));

		tEnv.sqlQuery(""select concat('an', 'bn')"");
	}
}
{code}

seems that currently HiveInspector.toInspectors() didn't convert Flink constant to Hive constant before calling hiveShim.getObjectInspectorForConstant

I don't think it's a blocker",,lirui,lzljs3620320,phoenixjiangnan,Terry1897,,,,,,,,,,"lirui-apache commented on pull request #10625: [FLINK-15259][hive] HiveInspector.toInspectors() should convert Flink…
URL: https://github.com/apache/flink/pull/10625
 
 
   … constant to Hive constant
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Convert constant Flink object to Hive object before creating Hive's constant object inspectors.
   
   
   ## Brief change log
   
     - Convert the constant value to Hive object in `HiveInspectors::toInspectors`
     - Implement our own java constant objector inspectors for char, varchar and decimal
     - Add test case
   
   
   ## Verifying this change
   
   New test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Dec/19 07:06;githubbot;600","bowenli86 commented on pull request #10625: [FLINK-15259][hive] HiveInspector.toInspectors() should convert Flink…
URL: https://github.com/apache/flink/pull/10625
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 04:43;githubbot;600","lirui-apache commented on pull request #10747: [FLINK-15259][hive] HiveInspector.toInspectors() should convert Flink…
URL: https://github.com/apache/flink/pull/10747
 
 
   … constant to Hive constant
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Backport FLINK-15259 to release-1.9.
   
   
   ## Brief change log
   
     - More details in #10625 
   
   
   ## Verifying this change
   
   More details in #10625
   
   ## Does this pull request potentially affect one of the following parts:
   
   More details in #10625
   
   ## Documentation
   
   More details in #10625
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jan/20 12:22;githubbot;600","bowenli86 commented on pull request #10747: [FLINK-15259][hive] HiveInspector.toInspectors() should convert Flink…
URL: https://github.com/apache/flink/pull/10747
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Jan/20 22:55;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 05 22:50:49 UTC 2020,,,,,,,,,,"0|z09oaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/19 03:24;Terry1897;Why it is not a blocker? It'll cause hive function not work normally. cc [~lzljs3620320]too;;;","16/Dec/19 05:04;phoenixjiangnan;This bug is only effecting functions that take char/varchar constant. The reason Flnk's concat works is Flink casts 'a' and 'b' in  {{concat('a', 'b')}} as string, while Hive's concat function takes varchar. So if users don't intentionally use Hive's concat (I can't think of why they don't want to use Flink's concat :)), it should be fine.

I‘m not sure how many functions are impacted as this point, and no user seems to report this. So I didn't mark it as blocker
;;;","16/Dec/19 12:15;lzljs3620320;Some UDX will be affected, like hive ""GenericUDTFStack"", ""GenericUDFLeadLag""... Will throws exceptions.

It seems that this bug is easy to fix, so I think it can be marked as a blocker.;;;","18/Dec/19 03:29;lirui;[~phoenixjiangnan] I guess you meant to assign this to me, lol;;;","20/Dec/19 04:59;phoenixjiangnan;[~lirui] glad you added profile picture. It helps!;;;","02/Jan/20 04:47;phoenixjiangnan;master: 6f3a0778ba352e76c96160c3125e4e0ee88addba
1.10: f8b6bed54e21ce3a51f9d63389e5a037e89f9ca8;;;","02/Jan/20 04:48;phoenixjiangnan;[~lirui] there's conflict when merging in 1.9. if we want to backport to 1.9, can you open a separate hotfix pr?;;;","02/Jan/20 06:41;lirui;[~phoenixjiangnan] Sure I'll open a PR for it.;;;","05/Jan/20 22:50;phoenixjiangnan;1.9: bcf540016fadf0e1bed708467ddeb0ae30c0a18a;;;",,,,,,,,,,,,,,,,,,,,,,,
HiveModuleFactory should be able to distinguish hive-version as property,FLINK-15258,13274502,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,phoenixjiangnan,phoenixjiangnan,phoenixjiangnan,13/Dec/19 21:54,06/Jan/20 16:09,13/Jul/23 08:10,17/Dec/19 19:36,1.10.0,,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,,,,,HiveModuleFactory should be able to distinguish hive-version as supported property. it currently can't,,phoenixjiangnan,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 19 06:11:05 UTC 2019,,,,,,,,,,"0|z09o8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/19 08:48;trohrmann;Why do you think this is a blocker issue [~phoenixjiangnan]?;;;","17/Dec/19 19:32;phoenixjiangnan;[~trohrmann] the current behavior is different than our design and document;;;","17/Dec/19 19:36;phoenixjiangnan;master: 4e3b8cc3142da9694add4893d2713e43fa596a09
1.10： aeb00ab478d5d518bec6722378ae05e966c0ff8a;;;","18/Dec/19 08:53;trohrmann;In the future I would appreciate if you could add a bit more context to JIRA issue descriptions and if asked giving a bit more details why this exactly is a problem. Just because something is not exactly implemented as initially planned does not justify to make it a blocker [~phoenixjiangnan].;;;","18/Dec/19 08:58;trohrmann;Another comment, the commit you referenced here has a a JIRA tag FLINK-15256. FLINK-15256 seems to be about an unstable {{HiveCatalogITCase}}. I believe the JIRA tag should have been FLINK-15258. It would be great to pay attention to these details [~phoenixjiangnan].;;;","19/Dec/19 06:11;phoenixjiangnan;[~trohrmann] I thought the original title was pretty self explanatary, it probably can be rephrased as ""HiveModuleFactory should be able to distinguish hive-version as property"". Thanks for the reminder;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
modules in SQL CLI yaml should preserve order,FLINK-15254,13274491,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,phoenixjiangnan,phoenixjiangnan,phoenixjiangnan,13/Dec/19 20:05,18/Dec/19 11:32,13/Jul/23 08:10,16/Dec/19 18:46,,,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"there seems to be a problem when a hive module is named ""hive"" and the module cannot be loaded/used properly. reported by [~Terry1897]

update: the root cause is that modules in SQL CLI yaml aren't handled in order right now, which they should be",,gjy,phoenixjiangnan,Terry1897,,,,,,,,,,,"bowenli86 commented on pull request #10578: [FLINK-15254][sql cli][module] modules in SQL CLI yaml should preserve order
URL: https://github.com/apache/flink/pull/10578
 
 
   ## What is the purpose of the change
   
   currently the module map is a hash map in sql cli, which doesn't preserve module loading order from yaml.
   fix it by always using a linked hash map
   
   ## Brief change log
   
   - always using a linked hash map in sql cli to handle module order
   - added UT
   
   ## Verifying this change
   
   This change added tests and can be verified as follows: `EnvironmentTest.testModuleOrder`
   
   ## Does this pull request potentially affect one of the following parts:
   
   n/a
   
   ## Documentation
   
   n/a
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Dec/19 00:07;githubbot;600","bowenli86 commented on pull request #10578: [FLINK-15254][sql cli][module] modules in SQL CLI yaml should preserve order
URL: https://github.com/apache/flink/pull/10578
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 18:45;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 11:32:24 UTC 2019,,,,,,,,,,"0|z09o68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/19 18:46;phoenixjiangnan;master: 6a0570d6cd2e6da5159bec7e3a59144e91503ea9
1.10: 6dd510e6efb372f89f4b6c7cf82064d4fa21da0d;;;","18/Dec/19 11:32;gjy;At this point we should set fix version only to 1.10 because 1.10 has not been released yet. If we set fix version to both 1.10 and 1.11, the issue will also appear in the release notes of 1.11.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Update ""Connect to External Systems"" page to list the required formats in connectors definition section",FLINK-15250,13274418,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,13/Dec/19 13:19,17/Dec/19 11:44,13/Jul/23 08:10,17/Dec/19 11:44,1.10.0,,,,,,,,,1.10.0,,,,Documentation,,,,,0,pull-request-available,,,,"Some demo In docs
 [https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connect.html#kafka-connector] are easy to misunderstand.

For example, kafka connector DDL definition should declare {{format.type}} mandatory, however, the definition part of kafka misses this.
Some users will meet a {{NoMatchingTableFactoryException}} if they follow the documentation.
There is a statement at the end of this section describe the required format properties, but that is not obvious enought. ",,jark,leonard,,,,,,,,,,,,"leonardBang commented on pull request #10585: [FLINK-15250][doc] Docs of Table Connect to External Systems is outdate and need to fix
URL: https://github.com/apache/flink/pull/10585
 
 
   [FLINK-15250][doc] Docs of Table Connect to External Systems is outdate and need to fix.
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   * This pull request update DDL content in Docs of Table Connect to External Systems.*
   
   
   ## Brief change log
   
     - *update file `connect.md`, `connect_zh.md`*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 04:08;githubbot;600","wuchong commented on pull request #10585: [FLINK-15250][doc] Docs of Table Connect to External Systems is outdate and need to fix
URL: https://github.com/apache/flink/pull/10585
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 11:36;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-15229,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 11:44:55 UTC 2019,,,,,,,,,,"0|z09nq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/19 11:44;jark;1.11.0: 864668f0c430e4219905f134e6d534e6000f8494
1.10.0: 5391ac1dbad7df9bd7608a05dc479a511743ef98;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileUtils#compressDirectory behaves buggy when processing relative directory path,FLINK-15248,13274401,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,weizhong,zhongwei,zhongwei,13/Dec/19 11:44,24/Dec/19 09:14,13/Jul/23 08:10,24/Dec/19 09:14,1.10.0,,,,,,,,,1.10.0,,,,FileSystems,,,,,0,pull-request-available,,,,"_FileUtils#compressDirectory_ behaves buggy when processing relative directory path. If the path of target directory is a relative path, the relative path inside the target zip file can not be constructed correctly:

 
{code:java}
public static Path compressDirectory(Path directory, Path target) throws IOException {
   FileSystem sourceFs = directory.getFileSystem();
   FileSystem targetFs = target.getFileSystem();

   try (ZipOutputStream out = new ZipOutputStream(targetFs.create(target, FileSystem.WriteMode.NO_OVERWRITE))) {
      addToZip(directory, sourceFs, directory.getParent(), out);
   }
   return target;
}

private static void addToZip(Path fileOrDirectory, FileSystem fs, Path rootDir, ZipOutputStream out) throws IOException {
   String relativePath = fileOrDirectory.getPath().replace(rootDir.getPath() + '/', """");
   if (fs.getFileStatus(fileOrDirectory).isDir()) {
      out.putNextEntry(new ZipEntry(relativePath + '/'));
      
      // The containedFile.getPath() returns an absolute path but the rootDir
      // could be a relative path or an empty string (if user only specify the 
      // directory name as the relative path). In this case when calling this 
      // method recursively the string replacement at the beginning of it will
      // return a wrong result.
      for (FileStatus containedFile : fs.listStatus(fileOrDirectory)) {
         addToZip(containedFile.getPath(), fs, rootDir, out);
      }

   } else {
      ZipEntry entry = new ZipEntry(relativePath);
      out.putNextEntry(entry);

      try (FSDataInputStream in = fs.open(fileOrDirectory)) {
         IOUtils.copyBytes(in, out, false);
      }
      out.closeEntry();
   }
}{code}
 

Currently PyFlink allows users to upload python library directories and requirements cached directory, which will be compressed by _FileUtils#compressDirectory_ eventually. If users specify them via relative paths, this bug will be triggered and causes those features unavailable.

we can fix this bug by converting the directory path to absolute path in _FileUtils#compressDirectory_ before calling _addToZip_ method.",,dian.fu,hequn8128,wangyang0918,zhongwei,,,,,,,,,,"WeiZhong94 commented on pull request #10570: [FLINK-15248][FileSystems] Fix the bug in FileUtils#compressDirectory when processing relative directory path.
URL: https://github.com/apache/flink/pull/10570
 
 
   ## What is the purpose of the change
   
   *This pull request fixes the bug in FileUtils#compressDirectory when processing relative directory path.*
   
   
   ## Brief change log
   
     - *Convert the relative path to absolute path before compressing it.*
     - *Add test for the change.*
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *FileUtilsTest*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 12:25;githubbot;600","WeiZhong94 commented on pull request #10659: [FLINK-15248][FileSystems] Allow FileUtils#compressDirectory to process relative directories.
URL: https://github.com/apache/flink/pull/10659
 
 
   ## What is the purpose of the change
   
   *FileUtils#compressDirectory behaves buggy when processing relative directory path. If the path of target directory is a relative path, the relative path inside the target zip file can not be constructed correctly. Convert the relative path to absolute path before compressing it.*
   
   
   ## Brief change log
   
     - *Convert the relative path to absolute path before compressing it.*
     - *Add test for the change.*
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *FileUtilsTest*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/19 04:21;githubbot;600","zhijiangW commented on pull request #10570: [FLINK-15248][FileSystems] Allow FileUtils#compressDirectory to process relative directories.
URL: https://github.com/apache/flink/pull/10570
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Dec/19 08:58;githubbot;600","zhijiangW commented on pull request #10659: [FLINK-15248][FileSystems] Allow FileUtils#compressDirectory to process relative directories.
URL: https://github.com/apache/flink/pull/10659
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Dec/19 08:59;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 24 09:13:50 UTC 2019,,,,,,,,,,"0|z09nm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/19 13:18;wangyang0918;How about add some java document description that the target should be an absolute path? The current caller in \{{JobGraphGenerator#addUserArtifactEntries}} has done like this.;;;","16/Dec/19 06:44;zhongwei;[~fly_in_gis] FileUtils#compressDirectory is used by StreamExecutionEnvironment#registerCachedFile and this means that users can specify relative file path in StreamExecutionEnvironment#registerCachedFile, while can not specify relative directory path if we don't fix this issue. I personally think that it would make more sense to fix this directly than to add Java documentation to tell users not to do this. What's your thought?;;;","16/Dec/19 06:51;wangyang0918;If flink support to register a relative directory as distributed cache, then `FileUtils#compressDirectory` should deal with relative path correctly. 

Make sense to me. Go ahead.;;;","24/Dec/19 09:13;hequn8128;Resolved 
in 1.10.0 via f002780091752b97a47e1866b653dbaf4eef802f
in 1.11.0 via 545534e43ed37f518fe59b6ddd8ed56ae82a234b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Closing (Testing)MiniCluster may cause ConcurrentModificationException,FLINK-15247,13274381,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,azagrebin,gjy,gjy,13/Dec/19 10:55,24/Jan/20 12:53,13/Jul/23 08:10,24/Jan/20 12:53,1.10.0,,,,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,test-stability,,,"{noformat}
Test operatorsBecomeBackPressured(org.apache.flink.test.streaming.runtime.BackPressureITCase) failed with:
org.apache.flink.util.FlinkException: Could not close resource.
        at org.apache.flink.util.AutoCloseableAsync.close(AutoCloseableAsync.java:42)org.apache.flink.test.streaming.runtime.BackPressureITCase.tearDown(BackPressureITCase.java:165)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
        at org.junit.rules.RunRules.evaluate(RunRules.java:20)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.junit.runners.Suite.runChild(Suite.java:128)
        at org.junit.runners.Suite.runChild(Suite.java:27)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: org.apache.flink.util.FlinkException: Error while shutting the TaskExecutor down.
        at org.apache.flink.runtime.taskexecutor.TaskExecutor.handleOnStopException(TaskExecutor.java:397)
        at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$onStop$0(TaskExecutor.java:382)
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.concurrent.FutureUtils.lambda$runAfterwardsAsync$16(FutureUtils.java:518)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at org.apache.flink.runtime.concurrent.DirectExecutorService.execute(DirectExecutorService.java:211)
        at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:529)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:751)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
        at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:699)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:529)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not properly shut down the TaskManager services.
        at org.apache.flink.runtime.taskexecutor.TaskManagerServices.shutDown(TaskManagerServices.java:198)
        at org.apache.flink.runtime.taskexecutor.TaskExecutor.stopTaskExecutorServices(TaskExecutor.java:419)
        at org.apache.flink.runtime.concurrent.FutureUtils.lambda$runAfterwardsAsync$16(FutureUtils.java:512)
        ... 16 more
Caused by: java.util.ConcurrentModificationException
        at java.util.HashMap$Values.forEach(HashMap.java:984)
        at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable.stop(TaskSlotTable.java:155)
        at org.apache.flink.runtime.taskexecutor.TaskManagerServices.shutDown(TaskManagerServices.java:184)
        ... 18 more
{noformat}
https://api.travis-ci.com/v3/job/266467337/log.txt",,azagrebin,gjy,guoyangze,jark,klion26,liyu,SleePy,trohrmann,xtsong,yanghua,zhuzh,,,"azagrebin commented on pull request #10682: [FLINK-15247][Runtime] Wait for all slots to be free before task executor services shutdown upon stopping
URL: https://github.com/apache/flink/pull/10682
 
 
   ## What is the purpose of the change
   
   #10161 introduced canceling and waiting of all tasks in TM to release resources and exit before shutting down TM services on stop. #10034 assigned a separate memory manager per slot and #10330 introduced dynamic slots and state to `TaskSlotTable`. To achieve clean lifecycle of slot, table and TM, this PR introduces freeing of all slots before shutting down TM services and stopping slot table on TM stop. Freeing of all slots cancels the running tasks and releases slot resources, including memory manager.
    
   ## Brief change log
   
     - Introduce closing future to `TaskSlot`
     - Introduce `TaskSlotTable#freeAllSlots`
     - Adjust task executor tests
     - Add more test coverage for `TaskSlot` and `TaskSlotTable`
   
   ## Verifying this change
   
   CI unit tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 06:10;githubbot;600","tillrohrmann commented on pull request #10682: [FLINK-15247][Runtime] Wait for all slots to be free before task executor services shutdown upon stopping
URL: https://github.com/apache/flink/pull/10682
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jan/20 12:52;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 24 12:53:55 UTC 2020,,,,,,,,,,"0|z09nhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/19 11:01;gjy;Root cause is in TaskSlotTable. Do you know something, [~azagrebin] [~xintongsong] ?;;;","13/Dec/19 12:41;xtsong;[~gjy]
Not sure at the moment.
TaskSlotTable is not thread safe, and when working on this class [~azagrebin] and I were kind of expecting it to be always invoked on the PRC main thread. My gut feeling is that this assumption might be broken somewhere. I would need some time to look into it.;;;","16/Dec/19 06:38;xtsong;I could not reproduce this failure locally.

I tried to print out the thread information in all the places where {{TaskSlotTable#taskSlots}} (the hash map that is concurrently modified according to the error message) is accessed, and find that the only access that is not from the RPC main thread is {{TaskSlotTable#stop}}.

{{TaskSlotTable#stop}} can be invoked indirectly from {{TaskExecutor#onStop}}.
({{TaskExecutor#onStop}} -> {{TaskExecutor#stopTaskExecutorServices}} -> {{TaskManagerServices#shutDown}} -> {{TaskSlotTable#stop}})

While {{TaskExecutor#onStop}} is invoked on the RPC main thread, it uses  {{FutureUtils#runAfterwards}} to call {{TaskExecutor#stopTaskExecutorServices}}, which does not guarantee {{stopTaskExecutorServices}} to be always invoked on the RPC main thread.

I think we can fix this problem by replacing {{FutureUtils#runAfterwards}} with {{FutureUtils#runAfterwardsAsync}} and passing in the main thread executor.

[~gjy], [~azagrebin], if you agree with my findings and proposed fixing, you can assign this ticket to me and I'll fix it. Thanks.;;;","18/Dec/19 15:01;azagrebin;It seems the problem is a bit deeper.

We introduced the delayed TaskExecutor#stopTaskExecutorServices in case of stopping TM in FLINK-11630 to gracefully cancel the tasks and let them release the resources of services before the final shutdown of services. Previously, slots were static, there were no resources attached to slots and TaskSlotTable was stateless, that did not cause issues.

Now TaskSlotTable has dynamically allocated slots after [FLINK-14189|https://jira.apache.org/jira/browse/FLINK-14189] and memory manager is attached to the slot after [FLINK-14400|https://jira.apache.org/jira/browse/FLINK-14400]. It means that when we cancel now tasks it triggers stopping of the services (including TaskSlotTable) concurrently with freeing slots and their resources. Even we put stopping of the services into the main thread, freeing of slots can be after stopping. It would be cleaner to firstly cancel tasks, wait for the subsequent freeing of the slots and then shutdown services.

cc [~trohrmann@apache.org];;;","19/Dec/19 12:16;trohrmann;Another instance of the same issue I believe: https://api.travis-ci.com/v3/job/266467337/log.txt;;;","31/Dec/19 06:11;klion26;another instance [https://travis-ci.com/flink-ci/flink/jobs/271335452];;;","02/Jan/20 04:06;klion26;Seems another instance [https://travis-ci.com/flink-ci/flink/jobs/271415113];;;","06/Jan/20 03:37;guoyangze;Another instance of the same issue I believe: [https://api.travis-ci.com/v3/job/272019582/log.txt];;;","06/Jan/20 06:30;guoyangze;Another instance https://api.travis-ci.com/v3/job/272585417/log.txt;;;","06/Jan/20 13:18;gjy;Another instance https://api.travis-ci.org/v3/job/633240679/log.txt;;;","07/Jan/20 03:05;liyu;Another instance: https://api.travis-ci.com/v3/job/272801720/log.txt;;;","07/Jan/20 13:50;trohrmann;Another instance: https://api.travis-ci.com/v3/job/273003263/log.txt;;;","10/Jan/20 04:15;guoyangze;Another instance: https://travis-ci.com/flink-ci/flink/jobs/273870242;;;","10/Jan/20 07:00;liyu;Another instance: https://api.travis-ci.com/v3/job/272186736/log.txt;;;","16/Jan/20 02:38;yanghua;Another instance: https://api.travis-ci.com/v3/job/275753985/log.txt;;;","16/Jan/20 08:04;pnowojski;another instance: https://api.travis-ci.com/v3/job/276023174/log.txt;;;","19/Jan/20 06:41;jark;Maybe another instance: https://api.travis-ci.com/v3/job/277276310/log.txt

{code:java}
TaskExecutorITCase.teardown:86 » Flink Could not close resource.

[ERROR] testJobReExecutionAfterTaskExecutorTermination(org.apache.flink.runtime.taskexecutor.TaskExecutorITCase)  Time elapsed: 0.326 s  <<< ERROR!
org.apache.flink.util.FlinkException: Could not close resource.
	at org.apache.flink.runtime.taskexecutor.TaskExecutorITCase.teardown(TaskExecutorITCase.java:86)
Caused by: org.apache.flink.util.FlinkException: Error while shutting the TaskExecutor down.
Caused by: org.apache.flink.util.FlinkException: Could not properly shut down the TaskManager services.
Caused by: java.util.ConcurrentModificationException
{code}
;;;","20/Jan/20 09:51;klion26;Seems another instance [https://travis-ci.org/klion26/flink/jobs/639304855?utm_medium=notification&utm_source=github_status]
{code:java}
07:02:05.922 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.328 s <<< FAILURE! - in org.apache.flink.test.streaming.runtime.BackPressureITCase3910
07:02:05.922 [ERROR] operatorsBecomeBackPressured(org.apache.flink.test.streaming.runtime.BackPressureITCase)  Time elapsed: 7.319 s  <<< ERROR!3911org.apache.flink.util.FlinkException: Could not close resource.
3912	at org.apache.flink.test.streaming.runtime.BackPressureITCase.tearDown(BackPressureITCase.java:166)
3913           Caused by: org.apache.flink.util.FlinkException: Error while shutting the TaskExecutor down.
3914           Caused by: org.apache.flink.util.FlinkException: Could not properly shut down the TaskManager services.
3915           Caused by: java.util.ConcurrentModificationException
{code};;;","24/Jan/20 12:53;trohrmann;Fixed via

master: e68c6c5992d0f59c7f796a6afa58fecbee854445
1.10.0: 675822edc35d364b01de1a6ef927d15ce4504a7d;;;",,,,,,,,,,,,,
Query result schema: [EXPR$0: TIMESTAMP(6) NOT NULL]   not equal to TableSink schema:    [EXPR$0: TIMESTAMP(3)],FLINK-15246,13274370,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,xiaojin.wy,xiaojin.wy,13/Dec/19 10:08,16/Dec/19 12:21,13/Jul/23 08:10,16/Dec/19 12:21,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,1,,,,,"When I excute the sql below and check the result of it, the ""Query result schema"" is not equal to the ""TableSink schema"";

 

 

The sql is:

CREATE TABLE `t` (
 x INT
) WITH (
 'format.field-delimiter'=',',
 'connector.type'='filesystem',
 'format.derive-schema'='true',
 'connector.path'='/defender_test_data/daily_regression_batch_spark_1.10/test_case_when_coercion/sources/t.csv',
 'format.type'='csv'
);

SELECT CASE WHEN true THEN cast('2017-12-12 09:30:00.0' as timestamp) ELSE cast(2 as tinyint) END FROM t;

 

The exception is:

org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink `default_catalog`.`default_database`.`_tmp_table_443938765` do not match. Query result schema: [EXPR$0: TIMESTAMP(6) NOT NULL] TableSink schema: [EXPR$0: TIMESTAMP(3)]

 

The input data is:

1",,jark,xiaojin.wy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 16 12:20:58 UTC 2019,,,,,,,,,,"0|z09nfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/19 11:19;jark;Can you verify it using the latest code? I think it should be fixed in FLINK-15124. ;;;","16/Dec/19 12:20;xiaojin.wy;This issue has not appear any more when I user the newest code.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink running in one cluster cannot write data to Hive tables in another cluster,FLINK-15245,13274362,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,13/Dec/19 09:35,17/Dec/19 07:27,13/Jul/23 08:10,17/Dec/19 07:27,,,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,Launch Flink cluster and write some data to a Hive table in another cluster. The job finishes successfully but data is not really written.,,lirui,,,,,,,,,,,,,"lirui-apache commented on pull request #10568: [FLINK-15245][hive] Flink running in one cluster cannot write data to…
URL: https://github.com/apache/flink/pull/10568
 
 
   … Hive tables in another cluster
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix the issue that we can't run flink in one cluster and write to Hive tables in another cluster.
   
   
   ## Brief change log
   
     - When creating Hive record writer, use the passed out path URI instead of just its path.
   
   
   ## Verifying this change
   
   TBD
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 10:00;githubbot;600","KurtYoung commented on pull request #10568: [FLINK-15245][hive] Flink running in one cluster cannot write data to…
URL: https://github.com/apache/flink/pull/10568
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 07:22;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 07:27:12 UTC 2019,,,,,,,,,,"0|z09ndk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/19 07:27;ykt836;master: b0783b50cb1610611f9442d456501160322b5028

1.10.0: 34093227ed681f1b0c842c50eb39bc790ee10090;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileUtils#deleteDirectoryQuietly will delete files in the symbolic link which point to a directory,FLINK-15244,13274344,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,weizhong,zhongwei,zhongwei,13/Dec/19 08:56,18/Dec/19 14:20,13/Jul/23 08:10,18/Dec/19 14:20,1.10.0,,,,,,,,,1.10.0,,,,FileSystems,,,,,1,pull-request-available,,,,"_FileUtils.deleteDirectoryQuietly_ will delete files in symbolic link which point to a directory. Currently the PyFlink uses this method to delete temporary folders generated during the job submission and python UDF execution, which contains the symbolic links which may point to users' libraries and directories in distributed cache. 

To resolve this problem we need to check if the directory is symbolic link in _FileUtils.deleteDirectoryInternal:_
{code:java}
private static void deleteDirectoryInternal(File directory) throws IOException {
    // **We should check if the directory is symbolic link.**
    if (directory.isDirectory()) {
      // directory exists and is a directory

      // empty the directory first
      try {
         cleanDirectoryInternal(directory);
      }
      catch (FileNotFoundException ignored) {
         // someone concurrently deleted the directory, nothing to do for us
         return;
      }

      // delete the directory. this fails if the directory is not empty, meaning
      // if new files got concurrently created. we want to fail then.
      // if someone else deleted the empty directory concurrently, we don't mind
      // the result is the same for us, after all
      Files.deleteIfExists(directory.toPath());
   }
   else if (directory.exists()) {
      // exists but is file, not directory
      // either an error from the caller, or concurrently a file got created
      throw new IOException(directory + "" is not a directory"");
   }
   // else: does not exist, which is okay (as if deleted)
}

{code}",,dian.fu,gjy,zhongwei,,,,,,,,,,,"WeiZhong94 commented on pull request #10567: [FLINK-15244][FileSystems] Fix the bug that FileUtils#deleteDirectoryInternal will delete files in the symbolic link which point to a directory.
URL: https://github.com/apache/flink/pull/10567
 
 
   ## What is the purpose of the change
   
   *This pull request fixes the bug that FileUtils#deleteDirectoryInternal will delete files in the symbolic link which point to a directory.*
   
   
   ## Brief change log
     
     - *Check if the directory is symbolic link before walk into it.*
     - *Add test for the change.*
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *FileUtilsTest*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 09:25;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 14:20:13 UTC 2019,,,,,,,,,,"0|z09n9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/19 14:20;gjy;1.10: 337f1cc534aac7f2806465687eeefb4d4984e3f3
master: 3c8a3ac0e7c5784a125d2bce3d2058fd5d1066a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert the unexpected change of the configuration for Mesos CPU cores,FLINK-15241,13274336,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,guoyangze,guoyangze,guoyangze,13/Dec/19 08:30,30/Nov/21 20:38,13/Jul/23 08:10,18/Dec/19 16:49,,,,,,,,,,1.10.0,,,,Deployment / Mesos,,,,,0,pull-request-available,,,,"Before 1.10, the default config of cpu cores for Mesos Workers is equal to taskmanager.numberOfTaskSlots. However, it is now 0.0 by default. I feel sorry I've not catch it in [FLINK-15196|https://issues.apache.org/jira/browse/FLINK-15196]. I think this change is unexpected to user thus we need to fix it.

cc [~trohrmann] Could you assigned this ticket to me?",,guoyangze,trohrmann,,,,,,,,,,,,"KarmaGYZ commented on pull request #10571: [FLINK-15241] Revert the unexpected change of the configuration for M…
URL: https://github.com/apache/flink/pull/10571
 
 
   …esos CPU cores
   
   ## What is the purpose of the change
   
   This PR Revert the unexpected change of the configuration for Mesos CPU cores. Now, if not explicitly set ""mesos.resourcemanager.tasks.cpus"", the cpu cores will be setted to the ""taskmanager.numberOfTaskSlots""
   
   ## Brief change log
   
   Change the condition of using fallback cpu configuration in TaskExecutorResourceUtils#getCpuCores to ""fallback > 0.0""
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - MesosTaskManagerParametersTest#testConfigNoCpuCores
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
   cc @tillrohrmann 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 14:11;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 16:49:51 UTC 2019,,,,,,,,,,"0|z09n7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/19 13:18;trohrmann;Thanks for reporting this issue [~guoyangze]. I've assigned you to the ticket. It would be good to align how Mesos obtains the number of cpus with FLINK-14188 and to use {{TaskExecutorResourceUtils#getCpuCores}}. Let me know once you've opened a PR so that I can review it.;;;","18/Dec/19 16:49;trohrmann;Fixed via

master: 4fc55c747d001b75ba4652e867c98a5e2d62fd69
1.10.0: b360019433f25395a8fc151b23d94eb7bb81359d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
is_generic key is missing for Flink table stored in HiveCatalog,FLINK-15240,13274330,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,phoenixjiangnan,phoenixjiangnan,phoenixjiangnan,13/Dec/19 07:52,27/Dec/19 07:29,13/Jul/23 08:10,14/Dec/19 04:13,,,,,,,,,,1.10.0,1.9.2,,,Connectors / Hive,,,,,0,,,,,,,phoenixjiangnan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15412,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 01:52:37 UTC 2019,,,,,,,,,,"0|z09n6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/19 04:13;phoenixjiangnan;master: e4cecef07e73dcb7a2990b075b8fc12eaa02845f
1.10: fc0a19d2a12c3454759ad621093cfa3cd68aac15;;;","18/Dec/19 01:52;phoenixjiangnan;1.9.2: 4e07b8c371a9130a9316ff9d4a731caac06c8df9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompileUtils::COMPILED_CACHE leaks class loaders,FLINK-15239,13274322,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,lirui,lirui,13/Dec/19 07:25,27/Dec/19 02:23,13/Jul/23 08:10,27/Dec/19 02:23,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"Start a standalone cluster and then submit multiple queries for Hive tables via SQL CLI. Hive connector dependencies are specified via the {{library}} option. TM will fail eventually with:
{noformat}
2019-12-13 15:11:03,698 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Values(tuples=[[{ 4.3 }]], values=[EXPR$0]) -> SinkConversionToRow -> Sink: Unnamed (1/1) (b9f9667f686fd97c1c5af65b8b163c44) switched from RUNNING to FAILED.
java.lang.OutOfMemoryError: Metaspace
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:439)
        at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:324)
        at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:687)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:628)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:93)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2701)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2683)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:372)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
        at org.apache.flink.connectors.hive.HadoopFileSystemFactory.create(HadoopFileSystemFactory.java:46)
        at org.apache.flink.table.filesystem.PartitionTempFileManager.<init>(PartitionTempFileManager.java:73)
        at org.apache.flink.table.filesystem.FileSystemOutputFormat.open(FileSystemOutputFormat.java:104)
        at org.apache.flink.streaming.api.functions.sink.OutputFormatSinkFunction.open(OutputFormatSinkFunction.java:65)
        at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
        at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:48)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1018)
{noformat}

Even for the succeeded queries, TM prints the following errors:
{noformat}
Exception in thread ""LeaseRenewer:lirui@localhost:8020"" java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/LeaseRenewer$2
        at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:412)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:448)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:304)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdfs.LeaseRenewer$2
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 5 more
{noformat}",,dian.fu,jark,kevin.cyj,lirui,liyu,lzljs3620320,trohrmann,wind_ljy,xtsong,xuefuz,zhongwei,,,"lirui-apache commented on pull request #10620: [FLINK-15239][table-planner-blink] TM Metaspace memory leak
URL: https://github.com/apache/flink/pull/10620
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix the metaspace memory leak of TM.
   
   
   ## Brief change log
   
     - Use weak/soft references for keys and values of COMPILED_CACHE.
   
   
   ## Verifying this change
   
   Verified by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
   NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/19 08:18;githubbot;600","wuchong commented on pull request #10620: [FLINK-15239][table-planner-blink] CompileUtils::COMPILED_CACHE leaks class loaders
URL: https://github.com/apache/flink/pull/10620
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 02:07;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 27 02:23:24 UTC 2019,,,,,,,,,,"0|z09n4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/19 07:31;lzljs3620320;CC: [~kevin.cyj];;;","13/Dec/19 13:04;lirui;Did a thread dump and found we leak a {{StatisticsDataReferenceCleaner}} thread for each job we run. This thread is started by some static block in {{FileSystem.Statistics}}, therefore we launch a {{StatisticsDataReferenceCleaner}} when the Hadoop class was initialized. And since the Hadoop dependencies are loaded by user code class loader, we end up starting it each time we execute a job.;;;","16/Dec/19 09:13;kevin.cyj;Not only TM, JM also suffers from the problem.;;;","16/Dec/19 11:54;lirui;I thought we could find this {{StatisticsDataReferenceCleaner}} thread by name and interrupt it at the end of the job. However, in earlier Hadoop versions (like the one depended upon by Hive), this thread catches all throwables and does not respond to an interrupt exception :(;;;","18/Dec/19 08:20;lirui;I tried moving the Hadoop dependencies to parent class loader. While it solves the thread leak, TM still hits metaspace OOM after several (more) queries are executed. By checking the heap dump I find the child class loaders are retained by {{CompileUtils::COMPILED_CACHE}}. Although this cache has a maximum limit on its size, it can still take lot of space because each loader can hold lots of class instances. I tried making the cache use weak/soft references and verified it solves the OOM in my local env.;;;","27/Dec/19 02:23;jark;1.11.0: 52fdee1d0c7af24d25c51caa073e29f11b07210b
1.10.0: 6d10ce4caf69d80363ac5ec4be11bf092c716f63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Hive table created from flink catalog table shouldn't have null properties in parameters,FLINK-15234,13274304,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,phoenixjiangnan,phoenixjiangnan,phoenixjiangnan,13/Dec/19 05:33,24/Jan/20 01:13,13/Jul/23 08:10,14/Dec/19 04:13,,,,,,,,,,1.10.0,1.9.2,,,Connectors / Hive,,,,,0,pull-request-available,,,,"we store comment of a catalog table in Hive table's parameters. When it's null, we put a <""comment"", null> k-v in the parameters. Hive table doesn't take null in its params.",,phoenixjiangnan,twalthr,,,,,,,,,,,,"bowenli86 commented on pull request #10566: [FLINK-15234][hive] hive table created from flink catalog table cannot have null properties in parameters
URL: https://github.com/apache/flink/pull/10566
 
 
   ## What is the purpose of the change
   
   discovered a couple bugs in HiveCatalog
   
   FLINK-15234 - hive table created from flink catalog table cannot have null properties in parameters
   FLINK-15240 - is_generic key is missing for Flink table stored in HiveCatalog
   
   ## Brief change log
   
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   Added new UT and IT 
   
   ## Does this pull request potentially affect one of the following parts:
   
   N/A
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 08:50;githubbot;600","bowenli86 commented on pull request #10566: [FLINK-15234][hive] hive table created from flink catalog table cannot have null properties in parameters
URL: https://github.com/apache/flink/pull/10566
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 21:26;githubbot;600","bowenli86 commented on pull request #10575: [FLINK-15234] hive table created from flink catalog table shouldn't have null properties in parameters
URL: https://github.com/apache/flink/pull/10575
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Dec/19 04:11;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 01:53:00 UTC 2019,,,,,,,,,,"0|z09n0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/19 08:09;twalthr;Could you fill out the issue description? Why should there be \{{null}} in properties?;;;","13/Dec/19 17:42;phoenixjiangnan;[~twalthr] sorry, didn't have time since it's too late last night. description added. thanks;;;","14/Dec/19 04:13;phoenixjiangnan;master: 940bdfc41a2d78ba972541c537925d102ccdc87f  1.10: b6373cba04a9f45d6a851684f80785af1f94b489;;;","18/Dec/19 01:53;phoenixjiangnan;1.9: cca2387a48e42950f0e53f8d9f002a4e0acf0c94;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Message of NoMatchingTableFactoryException should tell users what's wrong,FLINK-15232,13274302,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Dec/19 05:08,18/Dec/19 07:48,13/Jul/23 08:10,18/Dec/19 07:48,,,,,,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Currently, all the required properties should exist and match, otherwise, {{NoMatchingTableFactoryException}} will be thrown.

User have no idea to know what is wrong.

We can pick a best candidate to print where is wrong,  print requiredContext and supportedProperties in different formats.

In this way, users can know which property in requiredContext is lack, and which property is not allow in supportedProperties.

 ",,jark,lzljs3620320,wind_ljy,,,,,,,,,,,"JingsongLi commented on pull request #10563: [FLINK-15232][table] Print match candidates to improve NoMatchingTableFactoryException
URL: https://github.com/apache/flink/pull/10563
 
 
   
   ## What is the purpose of the change
   
   Currently, all the required properties should exist and match, otherwise, NoMatchingTableFactoryException will be thrown.
   We can pick a best candidate to print where is wrong,  print requiredContext and supportedProperties in different formats.
   In this way, users can know which property in requiredContext is lack, and which property is not allow in supportedProperties.
   
   ## Brief change log
   
   - print candidates when required context properties mismatch. 
   - print candidates when No factory supports all properties.
   - following factories just print factory class
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 06:20;githubbot;600","wuchong commented on pull request #10563: [FLINK-15232][table] Message of NoMatchingTableFactoryException should tell users what's wrong
URL: https://github.com/apache/flink/pull/10563
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/19 07:46;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 07:48:22 UTC 2019,,,,,,,,,,"0|z09n08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/19 07:48;jark;1.11.0: 781a7c85e7b476762aa2b921276ab36f11c85312
1.10.0: a879535a5afaee1c63bbbbfb36e2b49eb73e2707;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong HeapVector in AbstractHeapVector.createHeapColumn,FLINK-15231,13274296,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,docete,docete,docete,13/Dec/19 04:06,31/Dec/19 03:59,13/Jul/23 08:10,31/Dec/19 03:58,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"For TIMESTAMP WITHOUT TIME ZONE/TIMESTAMP WITH LOCAL TIME ZONE/DECIMAL types, AbstractHeapVector.createHeapColumn generates wrong HeapVectors.",,lzljs3620320,,,,,,,,,,,,,"docete commented on pull request #10562: [FLINK-15231][table-planner-blink] Do not support TIMESTAMP/DECIMAL t…
URL: https://github.com/apache/flink/pull/10562
 
 
   …ypes for HeapVector
   
   ## What is the purpose of the change
   
   For TIMESTAMP WITHOUT TIME ZONE/TIMESTAMP WITH LOCAL TIME ZONE/DECIMAL types, AbstractHeapVector.createHeapColumn generates wrong HeapVectors. This PR removes TIMESTAMP/TIMESTAMP WITH LOCAL TIME ZONE/DECIMAL support since the HeapVectors is used for testing.
   
   ## Brief change log
   
   - dc4897 do not support TIMESTAMP/DECIMAL types for HeapVector
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**yes** / no)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 05:44;githubbot;600","KurtYoung commented on pull request #10562: [FLINK-15231][table-planner-blink] Do not support TIMESTAMP/DECIMAL t…
URL: https://github.com/apache/flink/pull/10562
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Dec/19 03:49;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 31 03:58:59 UTC 2019,,,,,,,,,,"0|z09myw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/19 04:13;docete;Since we have DecimalColumnVector/TimestampColumnVector interfaces, we should either provide a Heap-based implementation or DONOT support TIMESTAMP/DECIMAL type for HeapVector.

[~lzljs3620320] What do you think?
 ;;;","13/Dec/19 05:02;lzljs3620320;Modified version to 1.11, it is not a user bug. So let's fix it in 1.11.

DO NOT support TIMESTAMP/DECIMAL looks good to me.;;;","13/Dec/19 05:26;docete;[~lzljs3620320] OK, i will make a PR soon

[~jark] pls assign this to me 
 ;;;","31/Dec/19 03:58;ykt836;master: 9dc252849966dd21279572afff55dcbdd3f77f35

1.10.0: 62f0303f07120c51317aff171f105ecb0c65a2be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarks are not compiling,FLINK-15199,13273909,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,11/Dec/19 15:09,19/Dec/19 06:59,13/Jul/23 08:10,11/Dec/19 15:45,1.10.0,,,,,,,,,1.10.0,,,,Benchmarks,,,,,0,pull-request-available,,,,"Recent changes in FLINK-14926 caused:

{noformat}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/state/benchmark/BackendUtils.java:[61,57] cannot infer type arguments for org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder<>
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2.635 s
[INFO] Finished at: 2019-12-11T14:58:37+01:00
[INFO] Final Memory: 31M/751M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project flink-hackathon-benchmarks: Compilation failure
[ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/state/benchmark/BackendUtils.java:[61,57] cannot infer type arguments for org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder<>
{noformat}
",,liyu,pnowojski,,,,,,,,,,,,"pnowojski commented on pull request #10534: [FLINK-15199][rocksdb] Make RocksDBResourceContainer public for the benchmarks
URL: https://github.com/apache/flink/pull/10534
 
 
   flink-benchmarks need to access this class in order to use RocksDBKeyedStateBackendBuilder
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Dec/19 15:35;githubbot;600","pnowojski commented on pull request #10534: [FLINK-15199][rocksdb] Make RocksDBResourceContainer public for the benchmarks
URL: https://github.com/apache/flink/pull/10534
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Dec/19 15:39;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-14926,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 19 06:59:11 UTC 2019,,,,,,,,,,"0|z09kkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/19 15:45;pnowojski;Merged to flink as commit 9277e9e
Merged to flink-benchmarks  as commit 06574f9ace40f80f9c225853d01ee8092c26125a;;;","18/Dec/19 16:39;liyu;Waiting for this one to be merged also into release-1.10;;;","18/Dec/19 17:21;pnowojski;It's already merged to release-1.10 as 535845153ba45dc8e564f235e353e86aa1429fdd.

 

I probably forgot to specify it in my previous comment. ;;;","19/Dec/19 06:59;liyu;Great to know, thanks for the update! [~pnowojski];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
The mesos.resourcemanager.tasks.cpus configuration does not work as expectation,FLINK-15196,13273847,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,guoyangze,guoyangze,guoyangze,11/Dec/19 11:01,30/Nov/21 20:38,13/Jul/23 08:10,12/Dec/19 13:56,,,,,,,,,,1.10.0,,,,Deployment / Mesos,,,,,0,pull-request-available,,,,"When I tried to set the number of CPU of Mesos workers, I found the ""mesos.resourcemanager.tasks.cpus"" configuration does not work anymore. After passing over  recent commits, I guess the root cause is [FLINK-14188|https://github.com/apache/flink/pull/10146]. The configuration key in MesosTaskManagerParameters#getCpuCores is incorrect.

[~azagrebin] ",,azagrebin,guoyangze,trohrmann,twalthr,xtsong,,,,,,,,,"KarmaGYZ commented on pull request #10531: [FLINK-15196][Mesos] Using the mesos.resourcemanager.tasks.cpus to determine the cpus of Mesos worker
URL: https://github.com/apache/flink/pull/10531
 
 
   ##  What is the purpose of the change
   
   Using the correct option ""mesos.resourcemanager.tasks.cpus"" to determine the cpu cores of Mesos worker.
   
   ## Brief change log
   
   - Using the mesos.resourcemanager.tasks.cpus to determine the cpus of Mesos worker
   - Adding relevant test case.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   - Extends MesosTaskManagerParametersTest#testLegacyConfigCpuCores
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
   cc @xintongsong @azagrebin 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Dec/19 12:38;githubbot;600","tillrohrmann commented on pull request #10531: [FLINK-15196][Mesos] Using the mesos.resourcemanager.tasks.cpus to determine the cpus of Mesos worker
URL: https://github.com/apache/flink/pull/10531
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 13:55;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 13:56:13 UTC 2019,,,,,,,,,,"0|z09k74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/19 11:09;guoyangze;Hi, [~twalthr], I'm now working on building Mesos e2e tests. This issue is related to my work. If you do not have enough capacity, I'd like to work on this ticket.;;;","11/Dec/19 12:02;twalthr;Sorry that was a mistake.;;;","11/Dec/19 12:10;azagrebin;[~guoyangze] right, we overlooked that, thanks for noticing it
we should use MESOS_RM_TASKS_CPUS instead of MESOS_RM_TASKS_SLOTS in MesosTaskManagerParameters#getCpuCores;;;","12/Dec/19 13:56;trohrmann;Fixed via

master: 7bc79a17241959b5504861ca4082665d10a7932a
1.10.0: 6bd06a649833bb41d62a8420f3a174605c6a35e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Directories in distributed caches are not extracted in Yarn Per Job Cluster Mode,FLINK-15194,13273799,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,zhongwei,zhongwei,11/Dec/19 08:40,20/Dec/19 10:54,13/Jul/23 08:10,20/Dec/19 10:54,1.10.0,,,,,,,,,1.10.0,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"If we insert such code into the word count batch examples:
{code:java}
File testDirectory = new File(""test_directory"");
testDirectory.mkdirs();
env.registerCachedFile(testDirectory.getAbsolutePath(), ""test_directory"");

text = text.map(new RichMapFunction<String, String>() {
   @Override
   public String map(String value) throws Exception {
      File testDirectory = getRuntimeContext().getDistributedCache().getFile(""test_directory"");
      if (!testDirectory.isDirectory()) {
         throw new RuntimeException(
            String.format(""the directory %s is not a directory!"", testDirectory.getAbsolutePath()));
      }
      return value;
   }
});
{code}
It works well in standalone mode but fails in Yarn Per Job Cluster Mode, the exception is:
{code:java}
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: da572c60eb63b13b7a90892f1958a7b7)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:146)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:671)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:216)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:933)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1006)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1006)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: da572c60eb63b13b7a90892f1958a7b7)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:93)
	at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:804)
	at org.apache.flink.api.java.DataSet.collect(DataSet.java:413)
	at org.apache.flink.api.java.DataSet.print(DataSet.java:1652)
	at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:115)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
	... 11 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: da572c60eb63b13b7a90892f1958a7b7)
	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:112)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$21(RestClusterClient.java:532)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:291)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:110)
	... 19 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:188)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:183)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:177)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:452)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:380)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.RuntimeException: the directory /tmp/hadoop-zhongwei/nm-local-dir/usercache/zhongwei/appcache/application_1576030059607_0008/flink-dist-cache-bb275987-90cf-406a-9890-caed34983a04/da572c60eb63b13b7a90892f1958a7b7/test_directory.zip is not a directory!
	at org.apache.flink.examples.java.wordcount.WordCount$1.map(WordCount.java:95)
	at org.apache.flink.examples.java.wordcount.WordCount$1.map(WordCount.java:89)
	at org.apache.flink.runtime.operators.chaining.ChainedMapDriver.collect(ChainedMapDriver.java:79)
	at org.apache.flink.runtime.operators.util.metrics.CountingCollector.collect(CountingCollector.java:35)
	at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:196)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
{code}
It seems the zip file is not extracted in yarn per job mode.

Here is the complete code of the example:
{code:java}
public class WordCount {

   public static void main(String[] args) throws Exception {

      final MultipleParameterTool params = MultipleParameterTool.fromArgs(args);

      final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

      env.getConfig().setGlobalJobParameters(params);

      DataSet<String> text = null;
      if (params.has(""input"")) {
         for (String input : params.getMultiParameterRequired(""input"")) {
            if (text == null) {
               text = env.readTextFile(input);
            } else {
               text = text.union(env.readTextFile(input));
            }
         }
         Preconditions.checkNotNull(text, ""Input DataSet should not be null."");
      } else {
         System.out.println(""Executing WordCount example with default input data set."");
         System.out.println(""Use --input to specify file input."");
         text = WordCountData.getDefaultTextLineDataSet(env);
      }

      File testDirectory = new File(""test_directory"");
      testDirectory.mkdirs();
      env.registerCachedFile(testDirectory.getAbsolutePath(), ""test_directory"");

      text = text.map(new RichMapFunction<String, String>() {
         @Override
         public String map(String value) throws Exception {
            File testDirectory = getRuntimeContext().getDistributedCache().getFile(""test_directory"");
            if (!testDirectory.isDirectory()) {
               throw new RuntimeException(
                  String.format(""the directory %s is not a directory!"", testDirectory.getAbsolutePath()));
            }
            return value;
         }
      });

      DataSet<Tuple2<String, Integer>> counts =
            text.flatMap(new Tokenizer())
            .groupBy(0)
            .sum(1);

      if (params.has(""output"")) {
         counts.writeAsCsv(params.get(""output""), ""\n"", "" "");
         env.execute(""WordCount Example"");
      } else {
         System.out.println(""Printing result to stdout. Use --output to specify output path."");
         counts.print();
      }

   }

   public static final class Tokenizer implements FlatMapFunction<String, Tuple2<String, Integer>> {

      @Override
      public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
         String[] tokens = value.toLowerCase().split(""\\W+"");

         for (String token : tokens) {
            if (token.length() > 0) {
               out.collect(new Tuple2<>(token, 1));
            }
         }
      }
   }

}
{code}",,dian.fu,trohrmann,wangyang0918,xtsong,zhongwei,,,,,,,,,"wangyang0918 commented on pull request #10550: [FLINK-15194] Support registering directory as cache file in Yarn per-job mode
URL: https://github.com/apache/flink/pull/10550
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Flink could not support registering a directory as a cached file in yarn per-job cluster. When we generate the job graph, if a cached file is directory, it will be zipped and then uploaded to the hdfs. Then the user artifact is updated to remote file. However, on the taskmanager side, it does not unzip the remote file. It is a bug and i will give a fix asap.
   
   For session cluster(Yarn session and standalone), since all the cached file are distributed via blob, so it works fine.
   
   
   ## Brief change log
   
   * Let `FileCache#CopyFromDFSProcess` could process zip file
   
   ## Verifying this change
   
   * Update the existing ITCase `testPerJobModeWithDistributedCache` to cover directory case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 12:57;githubbot;600","tillrohrmann commented on pull request #10550: [FLINK-15194] Support registering directory as cache file in Yarn per-job mode
URL: https://github.com/apache/flink/pull/10550
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 10:53;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 10:54:30 UTC 2019,,,,,,,,,,"0|z09jwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/19 08:43;zhongwei;cc [~ZhenqiuHuang] could you help to take a look at this issue as I guess this is a bug introduced in FLINK-14033?;;;","12/Dec/19 07:59;wangyang0918;[~zhongwei] Hi Wei, thanks for creating this ticket.

 

I have took a look the code and find that flink could not support registering a directory as a cached file in yarn per-job cluster. When we generate the job graph, if a cached file is directory, it will be zipped and then uploaded to the hdfs. Then the user artifact is updated to remote file. However, on the taskmanager side, it does not unzip the remote file. It is a bug and i will give a fix asap.

 

For session cluster(Yarn session and standalone), since all the cached file are distributed via blob, so it works fine.

 ;;;","12/Dec/19 13:01;wangyang0918;[~zhongwei] I have attached a PR to fix this issue. And i already tested it on Yarn MiniCluster and a real Yarn cluster. Could you help to double check whether it could solve your problem?;;;","13/Dec/19 06:33;zhongwei;[~fly_in_gis] I have tested in a real Yarn cluster and it works. Great job!;;;","13/Dec/19 06:56;wangyang0918;[~zhongwei] Many thanks for your quick response.

[~trohrmann] The attached PR has been verified. Could you help to review and merge.;;;","20/Dec/19 10:54;trohrmann;Fixed via

master: e47bbcc61437bd5987585a1fc526196d662bb979
1.10.0: 54e6c375017323818e45b46a179806c237a7f0df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Hive sink can not run in standalone mode,FLINK-15185,13273752,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,11/Dec/19 03:27,12/Dec/19 09:26,13/Jul/23 08:10,12/Dec/19 09:26,,,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Now in hive HadoopFileSystemFactory, we use org.apache.flink.runtime.fs.hdfs.HadoopFileSystem to get FileSystem.

But it should not work after we setting default child first class loader. Because in standalone mode, the cluster has no hadoop dependency. So the solution is:
 # Add `flink-hadoop-fs` dependency to hive module, not work, because classes with ""org.apache.flink"" prefix will always be loaded by parent class loader :(
 # User add hadoop dependency to standalone cluster, it breaks out-of-the-box.
 # Shade hadoop FileSystem in hive module, not complex, good.",,lirui,lzljs3620320,,,,,,,,,,,,"JingsongLi commented on pull request #10540: [FLINK-15185][hive] Shade flink-hadoop-fs to run hive in standalone mode
URL: https://github.com/apache/flink/pull/10540
 
 
   
   ## What is the purpose of the change
   
   Now in hive HadoopFileSystemFactory, we use org.apache.flink.runtime.fs.hdfs.HadoopFileSystem to get FileSystem.
   
   But it should not work after we setting default child first class loader. Because in standalone mode, the cluster has no hadoop dependency. So the solution is:
   - Add `flink-hadoop-fs` dependency to hive module, not work, because classes with ""org.apache.flink"" prefix will always be loaded by parent class loader 
   - User add hadoop dependency to standalone cluster, it breaks out-of-the-box.
   - Shade hadoop FileSystem in hive module, not complex, good.
   
   ## Brief change log
   
   Shade hadoop FileSystem in hive module
   
   ## Verifying this change
   
   Manually verified the change
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 06:20;githubbot;600","KurtYoung commented on pull request #10540: [FLINK-15185][hive] Shade flink-hadoop-fs to run hive in standalone mode
URL: https://github.com/apache/flink/pull/10540
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 09:23;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 09:26:33 UTC 2019,,,,,,,,,,"0|z09jm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/19 06:01;lzljs3620320;[~jark] Can you assign this to Me ?;;;","12/Dec/19 09:26;ykt836;master: 3a30e29043351e749df928b2bf86c578bb0f1bdb

1.10: cebe6e954917c0ec74a0310ef70bb0578bd8aa80;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
syntax  not supported in SQLClient for TPCDS queries,FLINK-15175,13273579,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liupengcheng,liupengcheng,liupengcheng,10/Dec/19 10:24,24/Dec/19 06:11,13/Jul/23 08:10,24/Dec/19 06:11,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"{code:java}
Flink SQL> WITH customer_total_return AS
> ( SELECT
>     sr_customer_sk AS ctr_customer_sk,
>     sr_store_sk AS ctr_store_sk,
>     sum(sr_return_amt) AS ctr_total_return
>   FROM store_returns, date_dim
>   WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000
>   GROUP BY sr_customer_sk, sr_store_sk)
> SELECT c_customer_id
> FROM customer_total_return ctr1, store, customer
> WHERE ctr1.ctr_total_return >
>   (SELECT avg(ctr_total_return) * 1.2
>   FROM customer_total_return ctr2
>   WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk)
>   AND s_store_sk = ctr1.ctr_store_sk
>   AND s_state = 'TN'
>   AND ctr1.ctr_customer_sk = c_customer_sk
> ORDER BY c_customer_id
> LIMIT 100;
[ERROR] Unknown or invalid SQL statement.
{code}
It seems that the newest branch already support all TPCDS queries, but currently the sql client parser has not supported yet. 

Anyone already working on this? If not I can try it.",,jark,liupengcheng,lzljs3620320,,,,,,,,,,,"KurtYoung commented on pull request #10619: [FLINK-15175]Fix CTES not supported in SQL CLI
URL: https://github.com/apache/flink/pull/10619
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Dec/19 06:10;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 24 06:11:50 UTC 2019,,,,,,,,,,"0|z09ijk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/19 12:10;ykt836;Hi [~liupengcheng], thanks for reporting this. We will look into this, and my gut feeling is there maybe some disconnection between SQL CLI and Flink SQL.;;;","10/Dec/19 12:19;lzljs3620320;Hi [~liupengcheng], thanks for reporting this ticket.

Now, SQL-CLI has bug to deal with SQL ""WITH tableName AS .."" grammar. It only consider the string start with ''SELECT"".;;;","11/Dec/19 03:04;liupengcheng;Thanks for reply, [~ykt836] [~lzljs3620320], Can you assign this Jira to me, I will create a pull request later.

I think the fast fix is easy, we can just update the regex expression for SELECT in SQL CLI command parser, but I'm a little worried that this kind of fix is not complete, there might be some sql still left unsupported.

How about extending calcite core parser in flink to support DDL and more sql syntax, thus can make the parser unified, the SQL CLI just manage it's own customized command(e.g. QUIT), and passing all other sql to the planner directly.;;;","11/Dec/19 03:36;lzljs3620320;> extending calcite core parser in flink to support DDL and more sql syntax

We have this plan too. CC: [~danny0405];;;","11/Dec/19 14:17;ykt836;The final plan would be FLINK-14671, we could do a temporary fix for this version. Feel free to open a PR [~liupengcheng];;;","18/Dec/19 07:27;liupengcheng;[~ykt836] [~lzljs3620320] I've create an tempory fix [10619|https://github.com/apache/flink/pull/10619] for this issue, Thanks!;;;","24/Dec/19 06:11;ykt836;master: fc4927e41989be75866218c2a60aa914e1eedcd3

1.10.0: 3af8e1ef31aa61cf08ed910df32a1a26dd26f892;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in serialisation benchmarks,FLINK-15171,13273558,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,10/Dec/19 08:57,05/Jan/20 08:50,13/Jul/23 08:10,05/Jan/20 08:50,1.10.0,,,,,,,,,1.10.0,,,,API / Type Serialization System,Benchmarks,,,,0,pull-request-available,,,,"There is quite significant performance regression in serialisation benchmarks in the commit range 2ecf7ca..9320f34 (which includes FLINK-14346).

http://codespeed.dak8s.net:8000/timeline/?ben=serializerTuple&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=serializerRow&env=2
http://codespeed.dak8s.net:8000/timeline/?ben=serializerPojo&env=2

it coincides with the performance improvement for heavy strings

http://codespeed.dak8s.net:8000/timeline/?ben=serializerHeavyString&env=2

it might be caused by some accidental change in the benchmarking code (changing parallelism in one benchmarks is carried on to the next one?) or in the code itself.

CC [~rgrebennikov] [~AHeise]",,liyu,pnowojski,rgrebennikov,roman,SleePy,,,,,,,,,"shuttie commented on pull request #10529: [FLINK-15171] [serialization] fix performance regression caused by too many buffer allocations on string serialization
URL: https://github.com/apache/flink/pull/10529
 
 
   ## What is the purpose of the change
   
   [FLINK-14346](https://issues.apache.org/jira/browse/FLINK-14346) Introduced a faster implementation for string [de]serialization. But while running the flink-benchmarks suite, there was a [performance regression found](https://issues.apache.org/jira/browse/FLINK-15171) for almost all serialization tests: a significant 10% drop-down for the total job throughput.
   
   Flame graph before the FLINK-14346 was applied:
   ![flink-gc-dec05](https://user-images.githubusercontent.com/999061/70614387-5cc2bd00-1c02-11ea-85b4-27ec8ece72db.png)
   
   Flame graph after the FLINK-14346 was applied:
   ![flink-gc-dec11](https://user-images.githubusercontent.com/999061/70614391-5f251700-1c02-11ea-9a8d-5c5d894692bf.png)
   
   From these almost identical graphs we may notice that the GC spends much more time cleaning up the heap with the FLINK-14346 applied.
   
   Running the new and old code with the allocation profiling proved the theory with higher allocation rate:
   
   Top allocations, with FLINK-14346:
   ```
          bytes  percent  samples  top
     ----------  -------  -------  ---
     8222540128   32.45%    40779  byte[]
     7509810768   29.64%    37258  char[]
     4320201040   17.05%    21491  java.lang.String
     1667513984    6.58%     8247  org.apache.flink.api.java.tuple.Tuple2
      749432744    2.96%     3711  org.apache.flink.api.java.tuple.Tuple8
      589192264    2.33%     2897  java.lang.String[]
      497193120    1.96%     2458  org.apache.flink.streaming.runtime.streamrecord.StreamRecord
      478790376    1.89%     2372  org.apache.flink.api.java.tuple.Tuple2[]
      404943784    1.60%     2007  java.lang.ThreadLocal$ThreadLocalMap
      156780240    0.62%      564  java.nio.DirectByteBuffer
   ```
   
   Top allocations, no FLINK-14346:
   ```
          bytes  percent  samples  top
     ----------  -------  -------  ---
     7591122240   29.43%     3271  char[]
     5360582240   20.78%     2243  java.lang.ThreadLocal$ThreadLocalMap
     5147640184   19.96%     2231  java.lang.String
     1758207472    6.82%      765  org.apache.flink.api.java.tuple.Tuple2
     1717572128    6.66%      758  java.util.concurrent.locks.AbstractQueuedSynchronizer$Node
      891013696    3.45%      380  org.apache.flink.api.java.tuple.Tuple8
      598698832    2.32%      266  java.lang.String[]
      440182240    1.71%      202  org.apache.flink.streaming.runtime.streamrecord.StreamRecord
      364959680    1.41%      141  org.apache.flink.api.java.tuple.Tuple2[]
   ```
   So almost third of all the allocations made were done for these intermediate array buffers.
   
   All the benchmarks posted in the [original PR](https://github.com/apache/flink/pull/10358) were done on Ryzen 7 2700 (8 physical cores), and the CPU used for the `flink-benchmarks` is i7 7700 (4 physical cores). Also note that almost all the flink-benchmarks use parallelism=4, so:
   * new code generated +30% more garbage.
   * as originally performance was measured on a 8 core CPU with only 4 threads, GC threads were scheduled on the idle cores and didn't interfere with the benchmark.
   * on the i7 7700 flink-benchmarks used 4 threads on 4 core CPU with additional active GC threads, heavily interfering with the main benchmark.
   * this is the reason why lowering the parallelism for [the stringHeavyBenchmark](src/main/java/org/apache/flink/benchmark/SerializationFrameworkMiniBenchmarks.java) improved the throughput: it added more space for the GC threads to run.
   
   With this PR we did the following:
   * added a static `ThreadLocal<byte[]>` buffer for short strings smaller than 1024 characters
   * when the string is short enough, we do not allocate the buffer, but reuse the static one, eliminating the allocation completely.
   * for this case we need to always preallocate a small buffer for each worker thread, even if there is almost no string serialization at all.
   * for long strings we do a regular allocation as before.
   
   ## Brief change log
     - Add ThreadLocal byte buffer for write and read path for short strings instead of allocating it on each invocation.
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as StringSerializationTest.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: ( **yes** / no / don't know)
     - The runtime per-record code paths (performance sensitive): (**yes** / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Dec/19 11:07;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,FLINK-14346,,,,,,,,,,,,,,,"11/Dec/19 09:46;rgrebennikov;dec05.svg;https://issues.apache.org/jira/secure/attachment/12988508/dec05.svg","11/Dec/19 09:46;rgrebennikov;dec11.svg;https://issues.apache.org/jira/secure/attachment/12988509/dec11.svg",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 14:12:02 UTC 2020,,,,,,,,,,"0|z09iew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/19 14:16;rgrebennikov;[~pnowojski] I've tried running these benchmarks on my hardware and got weird results:
{noformat}
1thread master SerializationFrameworkMiniBenchmarks.serializerTuple  thrpt  100  608.128 ± 8.701  ops/ms
1thread no-pr  SerializationFrameworkMiniBenchmarks.serializerTuple  thrpt  100  605.246 ± 9.584  ops/ms

4thread no-pr  SerializationFrameworkMiniBenchmarks.serializerTuple  thrpt  100  610.825 ± 10.945  ops/ms
4thread master SerializationFrameworkMiniBenchmarks.serializerTuple  thrpt  100  613.504 ± 8.557  ops/ms{noformat}
TLDR: no changes, as in original PR.

I may suspect that the difference may come out of different memory access patterns in the old and new version of serializers on different hardware: for example, my desktop having better memory throughput compared to the one used on dak8s.net.

Can you please describe a bit more the hardware on this Hetzner box used for benchmarking:
 * CPU model (cat /proc/cpuinfo)
 * memory details (cat /proc/meminfo)
 * hardware details (sudo dmidecode)

So I can try to reproduce the benchmark problem.;;;","10/Dec/19 15:23;pnowojski;{noformat}
model name	: Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz
{noformat}

{noformat}
MemTotal:       65765564 kB
MemFree:        11099068 kB
MemAvailable:   60741468 kB
Buffers:         2633220 kB
Cached:         36536872 kB
SwapCached:            4 kB
Active:         19142348 kB
Inactive:       20350104 kB
Active(anon):     653616 kB
Inactive(anon):   355320 kB
Active(file):   18488732 kB
Inactive(file): 19994784 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:      33521660 kB
SwapFree:       33521616 kB
Dirty:                44 kB
Writeback:             0 kB
AnonPages:        316104 kB
Mapped:            67060 kB
Shmem:            686580 kB
Slab:           15012660 kB
SReclaimable:   11884900 kB
SUnreclaim:      3127760 kB
KernelStack:        3376 kB
PageTables:         4192 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    66404440 kB
Committed_AS:    2185680 kB
VmallocTotal:   34359738367 kB
VmallocUsed:           0 kB
VmallocChunk:          0 kB
HardwareCorrupted:     0 kB
AnonHugePages:    247808 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
CmaTotal:              0 kB
CmaFree:               0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:      145096 kB
DirectMap2M:    29003776 kB
DirectMap1G:    37748736 kB
{noformat}
https://pastebin.com/9TmcDQQG

I wouldn't rule out some benchmarking issue like the one I mentioned: 
{quote}
(changing parallelism in one benchmarks is carried on to the next one?)
{quote}
(as your benchmarking PR changed parallelism of one of the benchmarks). I will try to take a look and help investigate this tomorrow.;;;","11/Dec/19 09:59;rgrebennikov;[~pnowojski] [~roman_khachatryan] ,while running before-after benchmarks with async-profiler, I noticed that the amount of GC produced by serializerTuple is really different: the new code has much higher GC (about +10%) pressure due to intermediate buffer allocations in StringValue.writeString. All the benchmarks I've did were done on a Ryzen 7 2700 with 8 physical cores (16 with HT), but the benchmarking Hetzner box with i7 7700 has only 4 (8 with HT).

Before PR: [^dec05.svg]
 After PR: [^dec11.svg]

As GC threads are concurrent, on 8 core box they didn't interfere with the main benchmark code (as most probably they were scheduled on the idle cores). But on 4 core box they started significantly interfere with the benchmark threads, as there is much less CPU resources available.

I did a yet another round of improvements in the serialization code to avoid additional allocations in StringValue.writeString by adding a static ThreadLocal buffer for short strings, the same trick was done for StringValue.readString (but it looks a bit ugly, as thread-locals are dangerous). But anyway this seems to fix the issue with the too heavy GC pressure. I will make a PR today when I will be able to finish all the benchmarks to validate the results.;;;","11/Dec/19 10:25;roman;Was able to reproduce locally running just a single `serializerRow` test:
{code:java}
HEAD detached at 2ecf7cacbe
    Result ""org.apache.flink.benchmark.SerializationFrameworkMiniBenchmarks.serializerRow"":
      386.236 ±(99.9%) 13.426 ops/ms [Average]
      (min, avg, max) = (349.271, 386.236, 432.025), stdev = 20.095
HEAD detached at be967fa1f7
    Result ""org.apache.flink.benchmark.SerializationFrameworkMiniBenchmarks.serializerRow"":
      360.470 ±(99.9%) 9.693 ops/ms [Average]
      (min, avg, max) = (323.661, 360.470, 398.283), stdev = 14.507
{code}
(flink-benchmarks at d30f2b3e0bf616e857f5e47be6d6b407a2a0921e)

The difference is ~7% which matches -7.66% at [http://codespeed.dak8s.net:8000/changes/?rev=9320f34&exe=1&env=2];;;","11/Dec/19 10:36;roman;{color:#172b4d}[~rgrebennikov]  g{color}{color:#172b4d}reat, go ahead! (y){color};;;","11/Dec/19 12:05;rgrebennikov;Results of the fix:
{noformat}
1  with-fix SerializationFrameworkMiniBenchmarks.serializerTuple  thrpt   50  628.377 ± 14.407  ops/ms
1    no-fix SerializationFrameworkMiniBenchmarks.serializerTuple  thrpt   50  628.152 ± 11.835  ops/ms
8  with-fix SerializationFrameworkMiniBenchmarks.serializerTuple  thrpt   50  451.422 ± 5.737  ops/ms
8    no-fix SerializationFrameworkMiniBenchmarks.serializerTuple  thrpt   50  450.485 ± 6.710  ops/ms
16 with-fix SerializationFrameworkMiniBenchmarks.serializerTuple  thrpt   50  260.823 ± 4.057  ops/ms
16   no-fix SerializationFrameworkMiniBenchmarks.serializerTuple  thrpt   50  190.611 ± 2.663  ops/ms
{noformat}
TLDR: so if you stress the GC hard enough, it will start interfering with the benchmark code.;;;","19/Dec/19 10:41;rgrebennikov;Current status: the regression seems to be fixed (hopefully).

String read-write benchmarks have ~ +30% improvement:
{noformat}
[info] Benchmark            (length)  (stringType)  Mode  Cnt     Score    Error  Units
[info] deserializeDefault          1         ascii  avgt   30    23.903 ±  0.266  ns/op
[info] deserializeDefault          4         ascii  avgt   30    26.371 ±  0.248  ns/op
[info] deserializeDefault         16         ascii  avgt   30    40.711 ±  1.187  ns/op
[info] deserializeDefault        128         ascii  avgt   30   289.613 ± 21.176  ns/op
[info] deserializeDefault        256         ascii  avgt   30   633.237 ± 47.604  ns/op
[info] deserializeDefault        512         ascii  avgt   30   820.571 ±  7.825  ns/op
[info] deserializeDefault       1024         ascii  avgt   30  1761.036 ± 25.948  ns/op
[info] deserializeImproved         1         ascii  avgt   30    18.546 ±  0.183  ns/op
[info] deserializeImproved         4         ascii  avgt   30    20.753 ±  0.517  ns/op
[info] deserializeImproved        16         ascii  avgt   30    31.796 ±  0.147  ns/op
[info] deserializeImproved       128         ascii  avgt   30   148.159 ±  2.655  ns/op
[info] deserializeImproved       256         ascii  avgt   30   286.721 ±  3.492  ns/op
[info] deserializeImproved       512         ascii  avgt   30   674.932 ±  2.495  ns/op
[info] deserializeImproved      1024         ascii  avgt   30  1361.801 ±  8.740  ns/op
[info] serializeDefault            1         ascii  avgt   30     7.113 ±  0.341  ns/op
[info] serializeDefault            4         ascii  avgt   30    15.779 ±  0.195  ns/op
[info] serializeDefault           16         ascii  avgt   30    60.260 ±  1.022  ns/op
[info] serializeDefault          128         ascii  avgt   30   364.671 ±  1.541  ns/op
[info] serializeDefault          256         ascii  avgt   30   732.862 ±  9.764  ns/op
[info] serializeDefault          512         ascii  avgt   30  1455.048 ± 19.815  ns/op
[info] serializeDefault         1024         ascii  avgt   30  2921.182 ± 37.154  ns/op
[info] serializeImproved           1         ascii  avgt   30     5.469 ±  0.059  ns/op
[info] serializeImproved           4         ascii  avgt   30    11.976 ±  0.720  ns/op
[info] serializeImproved          16         ascii  avgt   30    37.645 ±  0.540  ns/op
[info] serializeImproved         128         ascii  avgt   30   286.634 ±  1.193  ns/op
[info] serializeImproved         256         ascii  avgt   30   592.564 ± 37.882  ns/op
[info] serializeImproved         512         ascii  avgt   30  1227.392 ± 55.484  ns/op
[info] serializeImproved        1024         ascii  avgt   30  2608.061 ± 29.902  ns/op{noformat}
The flink-benchmarks suite before the original PR:
{noformat}
Benchmark                                                    Mode  Cnt    Score    Error   Units
SerializationFrameworkMiniBenchmarks.serializerAvro         thrpt   30  392.587 ± 12.009  ops/ms
SerializationFrameworkMiniBenchmarks.serializerHeavyString  thrpt   30   82.036 ±  0.420  ops/ms
SerializationFrameworkMiniBenchmarks.serializerKryo         thrpt   30  160.399 ± 10.793  ops/ms
SerializationFrameworkMiniBenchmarks.serializerPojo         thrpt   30  459.539 ±  5.958  ops/ms
SerializationFrameworkMiniBenchmarks.serializerRow          thrpt   30  595.623 ± 10.421  ops/ms
SerializationFrameworkMiniBenchmarks.serializerTuple        thrpt   30  661.703 ±  7.895  ops/ms{noformat}
After this fix:
{noformat}
Benchmark                                                    Mode  Cnt    Score    Error   Units
SerializationFrameworkMiniBenchmarks.serializerAvro         thrpt   30  379.987 ± 13.619  ops/ms
SerializationFrameworkMiniBenchmarks.serializerHeavyString  thrpt   30   87.521 ±  1.275  ops/ms
SerializationFrameworkMiniBenchmarks.serializerKryo         thrpt   30  160.332 ±  9.577  ops/ms
SerializationFrameworkMiniBenchmarks.serializerPojo         thrpt   30  465.664 ±  6.814  ops/ms
SerializationFrameworkMiniBenchmarks.serializerRow          thrpt   30  622.130 ± 19.682  ops/ms
SerializationFrameworkMiniBenchmarks.serializerTuple        thrpt   30  679.704 ± 14.360  ops/ms{noformat}
If anyone is interested in tech details, please go to the github PR page.;;;","19/Dec/19 11:47;pnowojski;Merged to master as 300e249d57fd24c3445a27f4f100f15e774cb297

Merged to release-1.10 as 0486ab00ef4f624e0d66505b72cf6ed22c574fe6

I'm not closing the ticket now, as we are waiting for a couple of benchmark runs on the master to confirm regression was solved.

 

Thanks for your work [~rgrebennikov] ! ;;;","02/Jan/20 14:01;chesnay;[~pnowojski] Looks like the regression got mostly fixed. For tuples/row/pojo we appear to be getting better numbers than before; although for heavy strings it looks like we took a small performance hit.;;;","03/Jan/20 14:04;pnowojski;Yes, you are right. It's hard to say what's going on here. I've lost a little bit of trust regarding that small differences/regressions after FLINK-15103. The small regression might be unrelated to this change, however it's a pitty that there is no visible performance improvement outside of small unit style benchmarks.

I would be inclined to ignore this issue and close this ticket. What do you think [~chesnay]? ;;;","03/Jan/20 14:12;chesnay;I'm fine with closing this ticket for the time being [~pnowojski].;;;",,,,,,,,,,,,,,,,,,,,,
WebFrontendITCase.testCancelYarn fails on travis,FLINK-15170,13273552,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,dwysakowicz,dwysakowicz,10/Dec/19 08:12,19/Nov/20 13:39,13/Jul/23 08:10,19/Nov/20 13:39,1.10.0,,,,,,,,,1.10.3,1.11.3,1.12.0,,Deployment / YARN,Runtime / Web Frontend,,,,0,pull-request-available,test-stability,,,"https://api.travis-ci.org/v3/job/622756846/log.txt

{code}
23:59:33.299 [INFO] Running org.apache.flink.runtime.webmonitor.WebFrontendITCase
23:59:33.365 [INFO] Running org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase
23:59:35.379 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.012 s - in org.apache.flink.runtime.webmonitor.handlers.JarSubmissionITCase
23:59:38.802 [ERROR] Tests run: 9, Failures: 1, Errors: 1, Skipped: 0, Time elapsed: 5.5 s <<< FAILURE! - in org.apache.flink.runtime.webmonitor.WebFrontendITCase
23:59:38.802 [ERROR] testCancelYarn(org.apache.flink.runtime.webmonitor.WebFrontendITCase)  Time elapsed: 0.436 s  <<< ERROR!
java.util.concurrent.TimeoutException: Connection failed
	at org.apache.flink.runtime.webmonitor.WebFrontendITCase.testCancelYarn(WebFrontendITCase.java:324)

23:59:38.802 [ERROR] testCancel(org.apache.flink.runtime.webmonitor.WebFrontendITCase)  Time elapsed: 0.003 s  <<< FAILURE!
java.lang.AssertionError
	at org.apache.flink.runtime.webmonitor.WebFrontendITCase.testCancel(WebFrontendITCase.java:240)

23:59:39.193 [INFO] 
23:59:39.193 [INFO] Results:
23:59:39.193 [INFO] 
{code}",,dian.fu,dwysakowicz,lining,liyu,rmetzger,trohrmann,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 19 13:39:18 UTC 2020,,,,,,,,,,"0|z09idk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/19 08:54;lining;[~dwysakowicz], has it happened again? I have run it in local for many times, and it didn't happen. Do you have another instance for it?;;;","12/Dec/19 09:16;dwysakowicz;That was the only occurrence that I've seen. Have you tried running it in travis? If you do not manage to reproduce it there, we can close it for now.;;;","12/Dec/19 09:52;gjy;How often did you run it?;;;","12/Dec/19 09:54;lining;I have run it in trails, it didn't happen, so we could close it first.;;;","12/Dec/19 10:05;lining;I kept running until there were exceptions, and then I ran more than 2,000 times without this problem.;;;","12/Dec/19 10:18;xtsong;[~lining] 
Maybe you can try checkout the very commit where this problem happen, see if it can be reproduced.
The commit id is 'bb6fd8281654be2a4e03a04c09e755525ec058ca', which can be found in the travis log attached in the issue description.
And if it still cannot be reduced, I would suggest to downgrade this issue to Critical instead of close it.;;;","12/Dec/19 10:48;lining;Ok, I'll do it.;;;","13/Dec/19 02:18;lining;I have checked out to the commit from the error log，this problem still didn't happen, so I update it from blocker to critical first.;;;","24/Dec/19 09:24;liyu;Further downgrade the priority to Major since no more occurrence in recent days. ;;;","30/Dec/19 06:30;liyu;Reproduced, the failure instance: https://api.travis-ci.org/v3/job/630652255/log.txt
(Note: this only reproduced in the {{core-hadoop 2.4.1}} stage: https://travis-ci.org/apache/flink/jobs/630652255)

[~lining] could you take a look? Thanks.;;;","30/Dec/19 08:53;lining;Discover testCancelYarn and testCancel all fail.

Method testCancel failed because java.lang.AssertionError is at WebFrontendITCase.java:240.
 Methoud testCancelYarn failed because java.util.concurrent.TimeoutException: Connection failed is located at WebFrontendITCase.java:324.

Possible cause: If maven test first runs testCancelYarn, its request to cancel fail, then there's one job is running, so testCancel asserts no running job fail.;;;","09/Apr/20 01:17;dian.fu;Another instance from master: https://api.travis-ci.org/v3/job/672547894/log.txt;;;","14/Apr/20 10:15;trohrmann;[~lining] any updates on this issue?;;;","21/Apr/20 08:09;trohrmann;Gentle reminder for [~lining].;;;","01/May/20 08:33;rmetzger;Another case in 1.10: https://travis-ci.org/github/apache/flink/jobs/681613001;;;","10/Jun/20 13:12;trohrmann;[~lining] did you have time to work on a fix for this problem?;;;","04/Aug/20 12:05;dian.fu;Another instance on 1.10: https://travis-ci.org/github/apache/flink/jobs/714614632;;;","10/Aug/20 11:49;lining;According to the current message, it can be determined that this is due to
{quote}client.sendGetRequest(""/jobs/"" + jid + ""/yarn-cancel"", getTimeLeft(deadline));
{quote}
timeout.

 ;;;","13/Nov/20 11:34;trohrmann;I think the problem might be caused because of jumping clocks. The jumping clocks might be cause by using {{System.currentTimeMillis}}. I propose to fix this issue by using {{System.nanoTime}}.;;;","19/Nov/20 13:39;trohrmann;Fixed via

1.12.0: f62094d7d4b785c85670ce8cd9f0ed12c7f326ff
1.11.3: 01c5c1432df2fbe2ff9187625ced6793a4912530
1.10.3: d2409c1877ebd6a222dab3f15936af1cd6c4152b;;;",,,,,,,,,,,,
Errors happen in the scheduling of DefaultScheduler are not shown in WebUI,FLINK-15169,13273539,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhuzh,zhuzh,zhuzh,10/Dec/19 07:23,17/Dec/19 12:51,13/Jul/23 08:10,17/Dec/19 12:51,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"WebUI relies on {{ExecutionGraph#failureInfo}} and {{Execution#failureCause}} to generate error info (via {{JobExceptionsHandler#createJobExceptionsInfo}}). 
Errors happen in the scheduling of DefaultScheduler are not recorded into those fields, thus cannot be shown to users in WebUI (nor via REST queries).

To solve it, 
1. global failures should be recorded into {{ExecutionGraph#failureInfo}}, via {{ExecutionGraph#initFailureCause}} which can be exposed as {{SchedulerBase#initFailureCause}}.
2. for task failures, one solution I can think of is to avoid invoking {{DefaultScheduler#handleTaskFailure}} directly on scheduler's internal failures. Instead, we can introduce {{ExecutionVertexOperations#fail(ExecutionVertex)}} to hand the error to {{ExecutionVertex}} as a common failure.

cc [~gjy]",,gjy,zhuzh,,,,,,,,,,,,"zhuzhurk commented on pull request #10541: [FLINK-15169][runtime] Set failures happen in DefaultScheduler to ExecutionGraph/Execution to make them visible to REST
URL: https://github.com/apache/flink/pull/10541
 
 
   ## What is the purpose of the change
   
   WebUI relies on ExecutionGraph#failureInfo and Execution#failureCause to generate error info (via JobExceptionsHandler#createJobExceptionsInfo).
   Errors happen in the scheduling of DefaultScheduler are not recorded into those fields, thus cannot be shown to users in WebUI (nor via REST queries).
   
   ## Brief change log
   
     - *set global failures to ExecutionGraph*
     - *set task deployment failures in DefaultScheduler to Execution via invoking Execution#markFailed*
   
   ## Verifying this change
   
   This change is verified by manually triggering glboal/task failures and checking the WebUI and REST.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**yes** / no / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 08:16;githubbot;600","GJL commented on pull request #10541: [FLINK-15169][runtime] Set failures happen in DefaultScheduler to ExecutionGraph/Execution to make them visible to REST
URL: https://github.com/apache/flink/pull/10541
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 12:46;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 12:51:18 UTC 2019,,,,,,,,,,"0|z09iao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/19 11:13;gjy;How come task failures are not visible? We initialize the failure cause in Execution#processFail;;;","10/Dec/19 11:18;zhuzh;Execution#processFail is not invoked on DefaultScheduler internal task errors (e.g. slot allocation timeout), #handleTaskFailure is directly invoked and it directly cancels all related tasks (in legacy scheduler it fails the error task and cancels other tasks).;;;","10/Dec/19 13:20;gjy;IIRC handling the failure outside of {{DefaultScheduler}}, is not easy. I don't remember details but at least one needs a reference to {{ExecutionVertexVersioner}}. What is the issue with setting the failure cause in {{DefaultScheduler#handleTaskFailure}}?;;;","10/Dec/19 14:29;zhuzh;If we can have an interface {{ExecutionVertexOperations#fail}} which helps to notify failures to ExecutionGraph, then the failure will finally come to {{DefaulScheduler}}, and the version updating will happen as if it is a normal failure. However, we may need to change {{Execution#processFail}} to enable to invoke {{maybeReleasePartitionsAndSendCancelRpcCall}} in this case.

Setting the failure cause directly to Execution, as you mentioned, can be an alternative. However, I think we may also need to set FAILED state timestamp (used by {{JobExceptionsHandler#createJobExceptionsInfo}}  to create error infos), and ExecutionState (so that in the WebUI it is really exhibited as FAILED rather than CANCELED);;;","17/Dec/19 12:51;gjy;1.10:
bb8971e9d699249e835e0373009e27251c6a7dd0
1bf8e89aff64c930b4df0ec99bdcb4e698305465

master:
6ed64abab3c3a041fc65ae954ddb35e026c1e94f
0186e1059db0e7829d6fb57d8b681686bf9b0e43;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception is thrown when using kafka source connector with flink planner,FLINK-15168,13273538,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,hxbks2ks,hxbks2ks,10/Dec/19 07:17,19/Dec/19 21:46,13/Jul/23 08:10,19/Dec/19 21:46,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Legacy Planner,,,,,0,pull-request-available,,,,"when running the following case using kafka as source connector in flink planner, we will get a RuntimeException:
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
env.setParallelism(1);StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);tEnv.connect(new Kafka()
        .version(""0.11"")
        .topic(""user"")
        .startFromEarliest()
        .property(""zookeeper.connect"", ""localhost:2181"")
        .property(""bootstrap.servers"", ""localhost:9092""))
        .withFormat(new Json()
                .failOnMissingField(true)
                .jsonSchema(""{"" +
                        ""  type: 'object',"" +
                        ""  properties: {"" +
                        ""    a: {"" +
                        ""      type: 'string'"" +
                        ""    },"" +
                        ""    b: {"" +
                        ""      type: 'string'"" +
                        ""    },"" +
                        ""    c: {"" +
                        ""      type: 'string'"" +
                        ""    },"" +
                        ""    time: {"" +
                        ""      type: 'string',"" +
                        ""      format: 'date-time'"" +
                        ""    }"" +
                        ""  }"" +
                        ""}""
                ))
        .withSchema(new Schema()
                .field(""rowtime"", Types.SQL_TIMESTAMP)
                .rowtime(new Rowtime()
                        .timestampsFromField(""time"")
                        .watermarksPeriodicBounded(60000))
                .field(""a"", Types.STRING)
                .field(""b"", Types.STRING)
                .field(""c"", Types.STRING))
        .inAppendMode()
        .registerTableSource(""source"");Table t = tEnv.scan(""source"").select(""a"");tEnv.toAppendStream(t, Row.class).print();
tEnv.execute(""test"");
{code}
The RuntimeException detail:
{code:java}
Exception in thread ""main"" java.lang.RuntimeException: Error while applying rule PushProjectIntoTableSourceScanRule, args [rel#26:FlinkLogicalCalc.LOGICAL(input=RelSubset#25,expr#0..3={inputs},a=$t1), Scan(table:[default_catalog, default_database, source], fields:(rowtime, a, b, c), source:Kafka011TableSource(rowtime, a, b, c))]Exception in thread ""main"" java.lang.RuntimeException: Error while applying rule PushProjectIntoTableSourceScanRule, args [rel#26:FlinkLogicalCalc.LOGICAL(input=RelSubset#25,expr#0..3={inputs},a=$t1), Scan(table:[default_catalog, default_database, source], fields:(rowtime, a, b, c), source:Kafka011TableSource(rowtime, a, b, c))] at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:235) at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:631) at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:327) at org.apache.flink.table.plan.Optimizer.runVolcanoPlanner(Optimizer.scala:280) at org.apache.flink.table.plan.Optimizer.optimizeLogicalPlan(Optimizer.scala:199) at org.apache.flink.table.plan.StreamOptimizer.optimize(StreamOptimizer.scala:66) at org.apache.flink.table.planner.StreamPlanner.translateToType(StreamPlanner.scala:389) at org.apache.flink.table.planner.StreamPlanner.org$apache$flink$table$planner$StreamPlanner$$translate(StreamPlanner.scala:180) at org.apache.flink.table.planner.StreamPlanner$$anonfun$translate$1.apply(StreamPlanner.scala:117) at org.apache.flink.table.planner.StreamPlanner$$anonfun$translate$1.apply(StreamPlanner.scala:117) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.StreamPlanner.translate(StreamPlanner.scala:117) at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:351) at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.java:259) at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.java:250) at org.apache.flink.table.api.example.batch.JavaBatchWordCount.main(JavaBatchWordCount.java:64)Caused by: org.apache.flink.table.api.ValidationException: Rowtime field 'rowtime' has invalid type LocalDateTime. Rowtime attributes must be of type Timestamp. at org.apache.flink.table.sources.TableSourceUtil$$anonfun$3.apply(TableSourceUtil.scala:114) at org.apache.flink.table.sources.TableSourceUtil$$anonfun$3.apply(TableSourceUtil.scala:92) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186) at org.apache.flink.table.sources.TableSourceUtil$.computeIndexMapping(TableSourceUtil.scala:92) at org.apache.flink.table.sources.TableSourceUtil$.getPhysicalIndexes(TableSourceUtil.scala:307) at org.apache.flink.table.plan.rules.logical.PushProjectIntoTableSourceScanRule.onMatch(PushProjectIntoTableSourceScanRule.scala:46) at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:208) ... 22 more
{code}",,dwysakowicz,hxbks2ks,jark,leonard,liyu,lzljs3620320,,,,,,,,"dawidwys commented on pull request #10576: [FLINK-15168][table-planner] Fix physical indices computing.
URL: https://github.com/apache/flink/pull/10576
 
 
   ## What is the purpose of the change
   Starting from this PR we use the schema that comes from CatalogTable instead of schema that comes from TableSource.
   
   Computing physical indices is based on the new type hierarchy instead of TypeInformation.
   
   ## Verifying this change
   
   This change added tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**yes** / no)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 21:35;githubbot;600","dawidwys commented on pull request #10576: [FLINK-15168][table-planner] Fix physical indices computing.
URL: https://github.com/apache/flink/pull/10576
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 12:47;githubbot;600","dawidwys commented on pull request #10605: [FLINK-15168][table-planner] Fix physical indices computing. 
URL: https://github.com/apache/flink/pull/10605
 
 
   ## What is the purpose of the change
   Starting from this PR we use the schema that comes from CatalogTable instead of schema that comes from TableSource in legacy planner.
   
   Computing physical indices is based on the new type hierarchy instead of TypeInformation.
   
   ## Verifying this change
   
   This change added tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**yes** / no)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 12:51;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,FLINK-14505,,,,,,,FLINK-15200,FLINK-15316,,FLINK-15323,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 19 21:46:26 UTC 2019,,,,,,,,,,"0|z09iag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/19 09:32;docete;The reason is FLINK-14645 introduces DataType(instead of TypeInformation) de/serialize for Schema. 

Before FLINK-14645, the Types.SQL_TIMESTAMP in the Schema would serialize to ""TIMESTAMP"" and deserialize to Types.SQL_TIMESTAMP back when create the kafka table source. 

After Flink-14645, the Types.SQL_TIMESTAMP in the Schema would serialize to ""TIMESTAMP(3)"" and deserialize to DataTypes.Timestamp(3).bridgedTo(classOf[LocalDateTime]), which convert to Types.LOCAL_DATE_TIME in TableSourceUtil;;;","10/Dec/19 09:39;docete;We should use new type system in TableSourceUtil to avoid such surprise [~jark] [~lzljs3620320];;;","10/Dec/19 09:52;lzljs3620320;> We should use new type system in TableSourceUtil to avoid such surprise

Before this, we should find why it is a DataTypes.Timestamp(3).bridgedTo(classOf[LocalDateTime]) instead of DataTypes.Timestamp(3).bridgedTo(classOf[Timestamp]).;;;","10/Dec/19 10:23;docete;[~lzljs3620320] The serialize/deserialize of DataType in TableSchema ignores conversion class;;;","10/Dec/19 16:20;dwysakowicz;I increased the priority of this issue as it basically makes table sources with time attributes unusable. (at least with legacy planner). I think it is the right assessment that the {{TableSourceUtil}} is the problem. I think it might be even good idea to aim to remove the whole class altogether. I can try and help with that if you have not started working on that [~docete] yet.

;;;","11/Dec/19 03:10;docete;[~dwysakowicz] I have not started preparing PR for this bug, feel free to take it.;;;","12/Dec/19 07:45;leonard;This issue will lead end2end  cron test fail every day too.;;;","12/Dec/19 10:58;jark;Thanks [~dwysakowicz] for working on it. I can help to review once the PR is ready. ;;;","19/Dec/19 21:46;dwysakowicz;Fixed in
master: 39b7992db59fb55d5408b5528d17c924dfce649e
1.10: 422ab8a928413dde21e5c4c8a4068751b7d23fb7;;;",,,,,,,,,,,,,,,,,,,,,,,
SQL CLI library option doesn't work for Hive connector,FLINK-15167,13273533,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,10/Dec/19 06:46,13/Dec/19 09:21,13/Jul/23 08:10,13/Dec/19 09:21,,,,,,,,,,1.10.0,,,,Connectors / Hive,Table SQL / Client,,,,0,pull-request-available,,,,"Launch a standalone cluster. Put all Hive connector dependency jars in a folder and start sql cli like: {{sql-client.sh embedded -l <folder>}}. Hit the following exception:
{noformat}
	at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:186)
	at org.apache.flink.table.catalog.CatalogManager.registerCatalog(CatalogManager.java:102)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.registerCatalog(TableEnvironmentImpl.java:233)
	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:519)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:463)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:156)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:115)
	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:724)
	... 3 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:55)
	... 15 more
Caused by: MetaException(message:org.apache.hadoop.hive.metastore.HiveMetaStoreClient class not found)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getClass(MetaStoreUtils.java:1676)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:131)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:89)
	... 20 more
{noformat}",,jark,lirui,lzljs3620320,,,,,,,,,,,"lirui-apache commented on pull request #10514: [FLINK-15167][sql-client] SQL CLI library option doesn't work for Hive connector
URL: https://github.com/apache/flink/pull/10514
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To fix the issue that Hive connector doesn't work if the dependencies are specified via SQL CLI `library` option. The issue may apply to other connectors as well.
   
   
   ## Brief change log
   
     - Set user class loader as thread context class loader during creating and registering catalogs.
     - Set dependent jars via Configuration so that the job graph carries the dependencies.
     - Added test.
   
   
   ## Verifying this change
   
   New tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 13:05;githubbot;600","KurtYoung commented on pull request #10514: [FLINK-15167][sql-client] Catalogs should be created and registered with user class loader as context loader
URL: https://github.com/apache/flink/pull/10514
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 09:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 13 09:21:29 UTC 2019,,,,,,,,,,"0|z09i9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/19 09:23;lirui;Identified the following issues:
# Both creating and registering catalogs should be done with user class loader as the context loader.
# Need to set the dependent jars via {{PipelineOptions.JARS}} in the configuration, so that our job graph carries these dependencies.

After fixing the above issues, reading Hive tables works. But when I try to write to Hive tables, got the following problem:
{noformat}
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FileSystem
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.<init>(HadoopFileSystem.java:54)
	at org.apache.flink.connectors.hive.HadoopFileSystemFactory.create(HadoopFileSystemFactory.java:46)
	at org.apache.flink.table.filesystem.PartitionTempFileManager.<init>(PartitionTempFileManager.java:73)
	at org.apache.flink.table.filesystem.FileSystemOutputFormat.open(FileSystemOutputFormat.java:104)
	at org.apache.flink.streaming.api.functions.sink.OutputFormatSinkFunction.open(OutputFormatSinkFunction.java:65)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:48)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:986)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:453)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:448)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FileSystem
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 16 more
{noformat}

This is because {{HadoopFileSystem}} is loaded by parent loader (in flink-dist jar), while {{org.apache.hadoop.fs.FileSystem}} (a field of {{HadoopFileSystem}}) needs to be loaded by child loader.
I think one option is to let Hive connector implement its own {{HadoopFileSystem}}. Alternatively, we can remove {{HadoopFileSystem}} from {{flink-dist}}. Since {{flink-dist}} doesn't contain {{org.apache.hadoop.fs.FileSystem}}, I'm not sure what's the point to pack {{HadoopFileSystem}} in the jar.

cc [~lzljs3620320] [~ykt836] What do you think?;;;","10/Dec/19 09:44;lzljs3620320;For the third problem, I think there is a simple way to fix:

Add `flink-hadoop-fs` dependent to hive module. In this way, will use the child ClassLoader always.;;;","10/Dec/19 11:18;lirui;[~lzljs3620320] Unfortunately, that wouldn't work w/o removing {{HadoopFileSystem}} from {{flink-dist}}, because classes with ""org.apache.flink"" prefix will always be loaded by parent class loader :(;;;","11/Dec/19 03:28;lzljs3620320;Hi [~lirui], For the third problem, since it is an independent ticket, I created FLINK-15185 to summarize.;;;","11/Dec/19 04:01;lirui;[~lzljs3620320] Thanks for verifying the issue. Then this PR is ready for review.;;;","13/Dec/19 09:21;ykt836;master: cd4c3ddf

1.10.0: 5a43907e

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle data compression wrongly decrease the buffer reference count.,FLINK-15166,13273520,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,10/Dec/19 04:11,10/Dec/19 10:32,13/Jul/23 08:10,10/Dec/19 10:10,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"FLINK-15140 report two relevant problems which are both triggered by broadcast partitioner, to make it more clear, I create this Jira to addresses the problems separately.

 

For blocking shuffle compression, we recycle the compressed intermediate buffer each time after we write data out, however when the data is not compressed, the return buffer is the original buffer and should not be recycled, but we wrongly recycled it.",,kevin.cyj,zjwang,,,,,,,,,,,,"wsry commented on pull request #10506: [FLINK-15166][runtime] Fix that buffer is wrongly recycled when data compression is enabled for blocking shuffle.
URL: https://github.com/apache/flink/pull/10506
 
 
   ## What is the purpose of the change
   
   For blocking shuffle data compression, the compressed intermediate buffer is recycled each time after data is written out, however when the data can not be compressed, the return buffer is the original buffer which should not be recycled. This PR fixes the wrong recycling problem by checking the returned buffer.
   
   ## Brief change log
   
     - Not recycle the compressed buffer if it is the original buffer.
     - Modify the ```ShuffleCompressionITCase``` to cover the scenario.
   
   
   ## Verifying this change
   
   ```ShuffleCompressionITCase``` is modified to cover the scenario.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 06:47;githubbot;600","zhijiangW commented on pull request #10506: [FLINK-15166][runtime] Fix that buffer is wrongly recycled when data compression is enabled for blocking shuffle.
URL: https://github.com/apache/flink/pull/10506
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 10:08;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 10 10:26:22 UTC 2019,,,,,,,,,,"0|z09i6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/19 10:10;zjwang;Fixed in master : 4d693c4fbc5e6f3ff34ccb3cb3a1d9f35d6bbd76;;;","10/Dec/19 10:26;zjwang;Fixed in release-1.10 : 52a2f03eff658c5ec70b223a2c1551a96b4809dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
japicmp should use 1.9 as the old version,FLINK-15163,13273431,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gjy,gjy,gjy,09/Dec/19 16:43,12/Dec/19 13:38,13/Jul/23 08:10,12/Dec/19 13:34,1.10.0,,,,,,,,,1.10.0,,,,Build System,,,,,0,pull-request-available,,,,We should configure the japicmp-maven-plugin to use the latest Flink 1.9 release as the reference version to compare against. Currently 1.8.0 is used.,,gjy,,,,,,,,,,,,,"GJL commented on pull request #10508: [1.10][FLINK-15163][build] [FLINK-15163][build] Set japicmp.referenceVersion to 1.9.1
URL: https://github.com/apache/flink/pull/10508
 
 
   ## What is the purpose of the change
   
   *This sets japicmp.referenceVersion to 1.9.1*
   
   
   ## Brief change log
   
     - *See commits*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 11:01;githubbot;600","GJL commented on pull request #10509: [FLINK-15163][build] [FLINK-15163][build] Set japicmp.referenceVersion to 1.9.1
URL: https://github.com/apache/flink/pull/10509
 
 
   ## What is the purpose of the change
   
   *This sets japicmp.referenceVersion to 1.9.1*
   
   
   ## Brief change log
   
     - *See commits*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 11:03;githubbot;600","GJL commented on pull request #10509: [FLINK-15163][build] [FLINK-15163][build] Set japicmp.referenceVersion to 1.9.1
URL: https://github.com/apache/flink/pull/10509
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 13:33;githubbot;600","GJL commented on pull request #10508: [FLINK-15163][1.10][build] [FLINK-15163][build] Set japicmp.referenceVersion to 1.9.1
URL: https://github.com/apache/flink/pull/10508
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 13:38;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,FLINK-15207,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 13:34:56 UTC 2019,,,,,,,,,,"0|z09hmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/19 13:34;gjy;1.10: 90f46bea08b575cc9b7012f978fd52cda1703f99
master: 213bdf00cd18d5b6fe744127da82bd918c0828d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make ScalaShell ensureYarnConfig() and fetchConnectionInfo() public,FLINK-15157,13273368,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,aljoscha,aljoscha,09/Dec/19 13:03,10/Dec/19 08:05,13/Jul/23 08:10,10/Dec/19 08:05,,,,,,,,,,1.10.0,,,,Scala Shell,,,,,0,,,,,"This allows users of the Scala Shell, such as Zeppelin to work better with the shell.",,aljoscha,kkl0u,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 10 08:05:27 UTC 2019,,,,,,,,,,"0|z09h8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/19 13:04;aljoscha;[~zjffdu] Could you comment on if the methods need to be public or they can be protected?;;;","09/Dec/19 13:05;zjffdu;[~aljoscha] They needs to be public;;;","09/Dec/19 13:15;kkl0u;This means that we expose the {{ClusterClient}} and the {{configuration (fetchConnectionInfo())}}. This makes me wonder if we expose the {{ClusterClient}}, then what is the whole effort for the {{JobClient}}? Also the {{ClusterClient}} is not public interface and may change.;;;","09/Dec/19 13:43;zjffdu;[~kkl0u] I talked about concept of general FlinkClient which compose the function of JobClient and ClusterClient under this PR. https://github.com/apache/flink/pull/10474#issuecomment-562857131

Actually these methods are public in previous version (but changed to be private in another PR), we didn't introduce new public api. Before we have the general FlinkClient, I think it is fine to just keep these methods to be public as before so that downstream project (Zeppelin) can benefit from this, does this make sense to you ?;;;","09/Dec/19 13:59;kkl0u;[~zjffdu] I understand the reason. If we expose them we should clearly then annotate them as {{internal}} so that users do not depend on them. ;;;","09/Dec/19 13:59;aljoscha;I'm in favour of enabling use cases even if it means that we compromise the API a bit. [~zjffdu] Do you maybe have an example how that API is used by your Zeppelin use case?;;;","09/Dec/19 14:17;zjffdu;[~aljoscha] [~kkl0u] This is how I use it in Zeppelin 
https://github.com/zjffdu/zeppelin/blob/flink_1.10_emr/flink/src/main/scala/org/apache/zeppelin/flink/FlinkScalaInterpreter.scala

Actually I use it very similar as flink scala shell, I just start a flink scala shell as a long running service which will accept requests from external world (web browser), and then run the code and response with the code execution result. So basically I need to do 2 main things

1. Manage the lifecycle of flink cluster. Create/destroy the flink cluster no matter it is local MIniCluster or yarn FlinkCluster.
2. Manage the lifecycle of flink job, Add hooks to the lifecycle of flink job so that I can get more job info during its execution, such as its flink job web url, job progress and etc. ;;;","10/Dec/19 08:05;aljoscha;Implemented on master in a435827b40f1382ae4931edf7f0ab2a187ced04a
Implemented on release-1.10 in bc209522cb2cc0962cbbbd4aa00bcbee21ab835a;;;",,,,,,,,,,,,,,,,,,,,,,,,
Join with a LookupableTableSource: the defined order lookup keys are inconsistent ,FLINK-15155,13273358,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Rockey Cui,Rockey Cui,09/Dec/19 12:33,12/Dec/19 03:45,13/Jul/23 08:10,12/Dec/19 03:45,1.9.1,,,,,,,,,1.9.1,,,,API / DataStream,,,,,0,,,,,"My DQL==>

SELECT
 D1.S_INT ,
 D1.S_LONG ,
 D1.S_NUMBER1 ,
 D1.S_ADDRESS ,
 D1.S_DATE_EVENT ,
 D1.S_TIMESTAMP_EVENT
FROM
 CIRROSTREAM_DIM_GHL_SOURCE_131111_CSL AS F1 LEFT JOIN CIRROSTREAM_DIM_GHL_DIM_131111 FOR SYSTEM_TIME AS OF F1.PROCTIME AS D1
 ON {color:#FF0000}F1.S_INT = D1.S_INT{color}
{color:#FF0000} AND F1.S_LONG = D1.S_LONG{color}
{color:#FF0000} AND F1.S_NUMBER1 = D1.S_NUMBER1{color}
{color:#FF0000} AND F1.S_ADDRESS = D1.S_ADDRESS{color}
WHERE
 F1.S_INT BETWEEN 1501 AND 2000
----
My LookupableTableSource.getAsyncLookupFunction received lookupKeys==>

0 = ""S_ADDRESS""
1 = ""S_INT""
2 = ""S_LONG""
3 = ""S_NUMBER1""
----
 ","win10_x64

idea2019.2.4_x64

jdk1.8.0_211_x64",Rockey Cui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/19 12:32;Rockey Cui;getAsyncLookupFunction.png;https://issues.apache.org/jira/secure/attachment/12988318/getAsyncLookupFunction.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 03:28:51 UTC 2019,,,,,,,,,,"0|z09h68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/19 03:28;Rockey Cui;Sorry, I misunderstood defined order. 
I thought it was the order after the join on condition, but in fact it was the order when registering the LookupableTableSource;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change Flink binding addresses in local mode,FLINK-15154,13273342,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xtsong,cyrusand,cyrusand,09/Dec/19 12:15,13/May/20 08:53,13/Jul/23 08:10,13/May/20 08:53,1.9.1,,,,,,,,,1.11.0,,,,Runtime / Coordination,,,,,0,pull-request-available,usability,,,"Flink (or some of its services) listens on three random TCP ports
during the local[1] execution, e.g., 39951, 41009 and 42849.

[1]: https://ci.apache.org/projects/flink/flink-docs-stable/dev/local_execution.html#local-environment

The sockets listens on `0.0.0.0` and since I need to run some
long-running tests on an Internet-facing machine I was wondering how
to make them listen on `localhost` instead or if there is anything
else I can do to improve the security in this scenario.

Here's what I tried (with little luck):

```
Configuration config = new Configuration();
config.setString(""taskmanager.host"", ""127.0.0.1"");
cconfig.setString(""rest.bind-address"", ""127.0.0.1""); // OK
config.setString(""jobmanager.rpc.address"", ""127.0.0.1"");
StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(StreamExecutionEnvironment.getDefaultLocalParallelism(), config);
```

Only the `rest.bind-address` configuration actually changes the
binding address of one of those ports. Are there other parameters that
I'm not aware of or this is not the right approach in local mode?","```
$ uname -a
Linux xxx 4.19.0-6-amd64 #1 SMP Debian 4.19.67-2+deb10u2 (2019-11-11) x86_64 GNU/Linux
```",cyrusand,felixzheng,sewen,trohrmann,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13717,,,FLINK-15911,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 08:53:25 UTC 2020,,,,,,,,,,"0|z09h2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/20 11:19;sewen;For the TaskManager, this is the same issue as FLINK-13717;;;","21/Feb/20 10:43;xtsong;I'm closing this ticket because it can be resolved with the same solution of FLINK-15911.

[~cyrusand], please reopen it if you think the proposed solution in FLINK-15911 does not meet your demand.;;;","25/Feb/20 04:20;xtsong;Looking more into this issue, I found that this cannot be fully covered by FLINK-15911. Therefore I'm reopening this issue and will work on it after FLINK-15911.

The three ports mentioned in the ticket's description is used for rest server, blob server and RPC service (by default there's only one RPC service shared by JM/TM in local mode). Since binding address for rest server is already configurable, and FLINK-15911 will make binding address for RPC service will configurable, this issue falls back to making binding address of blob server configurable.;;;","07/May/20 11:16;xtsong;Updates:

After FLINK-15911, there are still two ports that Flink binds to whose binding address is not configurable.
- Blob server
- Metrics query RPC service

I've opened a PR trying to fix this issue, with the following changes.
- Make Blob server respect the configuration option 'jobmanager.bind-host' (introduced by FLINK-15911)
- Make metrics query RPC service use Akka local actor system in local execution mode, to avoid unnecessary port binding.;;;","13/May/20 08:53;trohrmann;Fixed via

8b519ca9a25f0e56a811bd7e4b232f781c1e81a6
dbd93e9f80aaddb9a37058201ada3c88ab8eafb9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Job running without periodic checkpoint for stop failed at the beginning,FLINK-15152,13273326,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,klion26,fengjiajie,fengjiajie,09/Dec/19 11:10,11/Nov/20 17:08,13/Jul/23 08:10,16/Jan/20 09:38,1.9.1,,,,,,,,,1.10.0,,,,Runtime / Checkpointing,,,,,0,checkpoint,pull-request-available,scheduler,,"I have a streaming job configured with periodically checkpoint, but after one week running, I found there isn't any checkpoint file.
h2. Reproduce the problem:

1. Job was submitted to YARN:
{code:java}
bin/flink run -m yarn-cluster -p 1 -yjm 1024m -ytm 4096m flink-example-1.0-SNAPSHOT.jar{code}
2. Then immediately, before all the task switch to RUNNING (about seconds), I(actually a job control script) send a ""stop with savepoint"" command by flink cli:
{code:java}
bin/flink stop -yid application_1575872737452_0019 f75ca6f457828427ed3d413031b92722 -p file:///tmp/some_dir
{code}
log in jobmanager.log:
{code:java}
2019-12-09 17:56:56,512 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Checkpoint triggering task Source: Socket Stream -> Map (1/1) of job f75ca6f457828427ed3d413031b92722 is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.
{code}
Then the job task(taskmanager) *continues to run normally without* checkpoint.
h2. The cause of the problem:

1. ""stop with savepoint"" command call the code stopCheckpointScheduler(org/apache/flink/runtime/scheduler/LegacyScheduler.java:612) and then triggerSynchronousSavepoint:
{code:java}
// we stop the checkpoint coordinator so that we are guaranteed
// to have only the data of the synchronous savepoint committed.
// in case of failure, and if the job restarts, the coordinator
// will be restarted by the CheckpointCoordinatorDeActivator.
checkpointCoordinator.stopCheckpointScheduler();{code}
2. but ""before all the task switch to RUNNING"", triggerSynchronousSavepoint failed at org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java:509
{code:java}
LOG.info(""Checkpoint triggering task {} of job {} is not in state {} but {} instead. Aborting checkpoint."",
  tasksToTrigger[i].getTaskNameWithSubtaskIndex(),
  job,
  ExecutionState.RUNNING,
  ee.getState());
throw new CheckpointException(CheckpointFailureReason.NOT_ALL_REQUIRED_TASKS_RUNNING);{code}
3. finally, ""stop with savepoint"" failed, with ""checkpointCoordinator.stopCheckpointScheduler()"" but without the termination of the job. The job is still running without periodically checkpoint. 

 

sample code for reproduce:
{code:java}
public class StreamingJob {

  private static StateBackend makeRocksdbBackend() throws IOException {
    RocksDBStateBackend rocksdbBackend = new RocksDBStateBackend(""file:///tmp/aaa"");
    rocksdbBackend.enableTtlCompactionFilter();
    rocksdbBackend.setPredefinedOptions(PredefinedOptions.SPINNING_DISK_OPTIMIZED);
    return rocksdbBackend;
  }

  public static void main(String[] args) throws Exception {
    // set up the streaming execution environment
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    // 10 sec
    env.enableCheckpointing(10_000L, CheckpointingMode.AT_LEAST_ONCE);
    env.setStateBackend(makeRocksdbBackend());
    env.setRestartStrategy(RestartStrategies.noRestart());

    CheckpointConfig checkpointConfig = env.getCheckpointConfig();
    checkpointConfig.enableExternalizedCheckpoints(
        CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
    checkpointConfig.setFailOnCheckpointingErrors(true);

    DataStream<String> text = env.socketTextStream(""127.0.0.1"", 8912, ""\n"");
    text.map(new MapFunction<String, Tuple2<Long, Long>>() {
      @Override
      public Tuple2<Long, Long> map(String s) {
        String[] s1 = s.split("" "");
        return Tuple2.of(Long.parseLong(s1[0]), Long.parseLong(s1[1]));
      }
    }).keyBy(0).flatMap(new CountWindowAverage()).print();

    env.execute(""Flink Streaming Java API Skeleton"");
  }

  public static class CountWindowAverage extends RichFlatMapFunction<Tuple2<Long, Long>, Tuple2<Long, Long>> {

    private transient ValueState<Tuple2<Long, Long>> sum;

    @Override
    public void flatMap(Tuple2<Long, Long> input, Collector<Tuple2<Long, Long>> out) throws Exception {
      Tuple2<Long, Long> currentSum = sum.value();
      currentSum.f0 += 1;
      currentSum.f1 += input.f1;
      sum.update(currentSum);
      out.collect(new Tuple2<>(input.f0, currentSum.f1));
    }

    @Override
    public void open(Configuration config) {
      ValueStateDescriptor<Tuple2<Long, Long>> descriptor =
          new ValueStateDescriptor<>(
              ""average"", // the state name
              TypeInformation.of(new TypeHint<Tuple2<Long, Long>>() {
              }), // type information
              Tuple2.of(0L, 0L)); // default value of the state, if nothing was set
      sum = getRuntimeContext().getState(descriptor);
    }
  }
}
{code}",,fengjiajie,klion26,liyu,pnowojski,SleePy,,,,,,,,,"klion26 commented on pull request #10824: [FLINK-15152][checkpointing] Restatrt CheckpointCoordinator if StopWithSavepoint failed
URL: https://github.com/apache/flink/pull/10824
 
 
   
   ## What is the purpose of the change
   
   Restart `CheckpointCoordinator` if `StopWithSavepoint` failed.
   
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
     - `JobMasterStopWithSavepointIT#testRestartCheckpointCoordinatorIfStopWithSavepointFails`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: ( no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 08:14;githubbot;600","pnowojski commented on pull request #10824: [FLINK-15152][checkpointing] Restatrt CheckpointCoordinator if StopWithSavepoint failed
URL: https://github.com/apache/flink/pull/10824
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 09:37;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20065,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 16 09:38:46 UTC 2020,,,,,,,,,,"0|z09gz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/19 10:17;klion26;Thanks for reporting the issue, from the description, seems it is a bug, I'll take a look at it.;;;","26/Dec/19 02:23;klion26; After having a look at the code, I think we should re-start {{checkpointCoordinator}} if {{triggerSynchronousSavepoint}} or {{terminateJob}} failed.  we can add a handler to re-start {{checheckpointCoordinator}} such as the below, what do you think?  [~pnowojski] [~kkloudas]

CC [~zhuzh]
{code:java}
// end of SchedulerBase#stopWithSavepoint
savepointFuture.thenCompose((path) ->
   terminationFuture.thenApply(jobStatus -> path))
   .handle(
   (path, throwable) -> {
   if (throwable != null) {
      //re-start the checkpoint coordinator when triggerSynchronousSavepoint or terminateJob failed.
checkpointCoordinator.startCheckpointScheduler();
      throw new CompletionException(ExceptionUtils.stripException(throwable, CompletionException.class));
   }
   return path;
});{code}
 [1] [SchedulerBase#stopWithSavepoint|https://github.com/apache/flink/blob/0ba4a2b4cc2b48886d5a7948d631ea7da0068a0e/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/SchedulerBase.java#L859]

 ;;;","31/Dec/19 13:48;pnowojski;Do you mean to duplicate a logic from {{org.apache.flink.runtime.scheduler.SchedulerBase#triggerSavepoint}}? Maybe that's a simple fix for that. However I'm not sure if restarting the checkpoint coordinator is the best idea, as might introduce some new issues/extra complexity from the more complicated state transitions of the `CheckpointCoordinator`.

I'm wondering if {{CheckpointCoordinator}}'s calls {{#stopCheckpointScheduler}} and {{#triggerSynchronousSavepoint}} should be atomic. Either operation succeeds and checkpoint coordinator shuts itself down, or not?;;;","02/Jan/20 08:00;klion26;[~pnowojski] thanks for your reply.

Yes, duplicate a logic from {{org.apache.flink.runtime.scheduler.SchedulerBase#triggerSavepoint}}.

I think we should {{restart}} the {{CheckpointCoordinator}} in two cases below:
 # {{CheckpointCoordinator#triggerSynchronousSavepoint}} failed
 # stop job failed(even if the synchronous savepoint succeed)

 

For the new issues/extra complexity that might be introduced. IMO, we should make the {{CheckpointCoordinator}} running if the job is not stopped. so I think {{restart}} CheckpointCoordinator is needed.

So I proposed the previous change.;;;","03/Jan/20 09:20;pnowojski;+1

Would you [~klion26] or [~fengjiajie] like to work on that?;;;","03/Jan/20 10:04;klion26;[~pnowojski] thanks for the confirmation, I can help to fix this, please assign it to me.;;;","16/Jan/20 09:38;pnowojski;merged commit e6aff96 into apache:master
merged commit d129d619d4b48dae0740b14bbdfc2c0081d669bf into release-1.10
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange failed on Travis,FLINK-15150,13273310,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,klion26,klion26,09/Dec/19 10:15,22/Jan/20 13:55,13/Jul/23 08:10,22/Jan/20 13:55,1.10.0,,,,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,,,," 
06:37:18.423 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.014 s <<< FAILURE! - in org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase
375406:37:18.423 [ERROR] testJobExecutionOnClusterWithLeaderChange(org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase) Time elapsed: 14.001 s <<< ERROR!
3755java.util.concurrent.ExecutionException: org.apache.flink.util.FlinkException: JobMaster has been shut down.
3756 at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.lambda$testJobExecutionOnClusterWithLeaderChange$1(ZooKeeperLeaderElectionITCase.java:131)
3757 at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:131)
3758Caused by: org.apache.flink.util.FlinkException: JobMaster has been shut down.
3759
 
[https://travis-ci.com/flink-ci/flink/jobs/264972218]
 ",,guoyangze,klion26,liyu,rmetzger,,,,,,,,,,"zentol commented on pull request #10887: [FLINK-15150][tests] Prevent job from reaching terminal state
URL: https://github.com/apache/flink/pull/10887
 
 
   Fixes an instability in the `ZookeeperLeaderElectionITCase` where the shutdown of the Dispatcher caused a slot allocation to fail, resulting in the job failing, reaching a terminal state and afterwards being removed from Zookeeper.
   
   We now prevent the job from reaching a terminal state by enabling a fixed-delay restart strategy. Should the allocation fail the JM will retry until the JM itself is being shut down. On shutdown the JM will suspend the job, allowing it to be recovered by other Dispatchers.
   
   The exact behavior for what happens to running jobs when the Dispatcher is shut down in an orderly fashion is currently undefined, and this PR makes no attempt remedy this.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/20 14:23;githubbot;600","zentol commented on pull request #10887: [FLINK-15150][tests] Prevent job from reaching terminal state
URL: https://github.com/apache/flink/pull/10887
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/20 13:54;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-11835,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 22 13:55:57 UTC 2020,,,,,,,,,,"0|z09gvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/19 12:41;rmetzger;I have observed the same failure on a azure build: [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=3665&view=logs&j=67131492-722d-5683-5ccf-cdfe7eed652d&t=0f864326-4ca6-5e12-40d4-a2b00d6ececf];;;","06/Jan/20 08:40;guoyangze;Another instance https://travis-ci.org/apache/flink/jobs/633117057 .;;;","16/Jan/20 13:59;chesnay;What seems to happen is that, while JobMaster is in the process of allocating slots, the test is calling {{Dispatcher#shutDownCluster}}. The dispatcher starts shutting down components, but the JobMaster is _not_ shut down first.
This results in a job failure since the slotmanager shuts down during the allocation, which fails the allocation, which fails the task, which fails the job.
Job restarts are forbidden by the restart strategy, so the job ends up in a terminal state, and subsequent dispatchers don't recover the job.

MInimized log with some additional log messages added:
{code}
...
18660 [dispatcher-5] INFO  o.a.f.r.dispatcher.StandaloneDispatcher  - Submitting job 411378b9f7181f9ba431960b08b701d7 (Blocking test job).
18660 [dispatcher-5] DEBUG o.a.f.r.jobmanager.ZooKeeperJobGraphStore  - Adding job graph 411378b9f7181f9ba431960b08b701d7 to flink/default/jobgraphs/411378b9f7181f9ba431960b08b701d7.
18670 [dispatcher-5] INFO  o.a.f.r.jobmanager.ZooKeeperJobGraphStore  - Added JobGraph(jobId: 411378b9f7181f9ba431960b08b701d7) to ZooKeeper.
18670 [dispatcher-2] INFO  o.a.f.r.rpc.akka.AkkaRpcService  - Starting RPC endpoint for o.a.f.r.jobmaster.JobMaster at akka://flink/user/jobmanager_4 .
...
18700 [main-EventThread] INFO  o.a.f.r.jobmaster.JobManagerRunnerImpl  - JobManager runner for job Blocking test job (411378b9f7181f9ba431960b08b701d7) was granted leadership with session id 3ac17f36-f3d9-4b63-ae1e-18572db0cb23 at akka://flink/user/jobmanager_4.
18700 [main-EventThread] DEBUG o.a.f.r.highavailability.zookeeper.ZooKeeperRunningJobsRegistry  - Setting scheduling state to RUNNING for job 411378b9f7181f9ba431960b08b701d7.
...
18720 [dispatcher-4] INFO  o.a.f.r.executiongraph.ExecutionGraph  - Job Blocking test job (411378b9f7181f9ba431960b08b701d7) switched from state CREATED to RUNNING.
18720 [main] INFO  org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase  - Initiated cluster shutdown.
...
18736 [dispatcher-2] INFO  o.a.f.r.jobmaster.JobMaster  - JobManager successfully registered at ResourceManager, leader id: a64a7f6db3146cf378a7d898cf6044d1.
18736 [dispatcher-2] INFO  o.a.f.r.jobmaster.slotpool.SlotPoolImpl  - Requesting new slot [SlotRequestId{7a9512bdaf255cc687212211142abd4b}] and profile ResourceProfile{UNKNOWN} from resource manager.
18736 [dispatcher-3] INFO  o.a.f.r.resourcemanager.StandaloneResourceManager  - Request slot with profile ResourceProfile{UNKNOWN} for job 411378b9f7181f9ba431960b08b701d7 with allocation id 0e5171488810f0cc1c3265e9b278bb17.
...
18740 [dispatcher-3] INFO  o.a.f.r.resourcemanager.slotmanager.SlotManagerImpl  - Suspending the SlotManager.
18740 [dispatcher-5] INFO  o.a.f.r.jobmaster.slotpool.SlotPoolImpl  - Failing pending slot request [SlotRequestId{7a9512bdaf255cc687212211142abd4b}]: The slot manager is being suspended.
...
18750 [dispatcher-5] INFO  o.a.f.r.executiongraph.ExecutionGraph  - blocking operator (1/1) (927b48379633675aad810f12af2470a6) switched from SCHEDULED to FAILED.
java.util.concurrent.CompletionException: o.a.f.r.resourcemanager.slotmanager.SlotManagerException: The slot manager is being suspended.
...
18750 [dispatcher-5] INFO  o.a.f.r.executiongraph.ExecutionGraph  - Job Blocking test job (411378b9f7181f9ba431960b08b701d7) switched from state RUNNING to FAILING.
o.a.f.r.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
Caused by: java.util.concurrent.CompletionException: o.a.f.r.resourcemanager.slotmanager.SlotManagerException: The slot manager is being suspended.
...
18760 [dispatcher-5] INFO  o.a.f.r.executiongraph.ExecutionGraph  - Job Blocking test job (411378b9f7181f9ba431960b08b701d7) switched from state FAILING to FAILED.
o.a.f.r.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
Caused by: java.util.concurrent.CompletionException: o.a.f.r.resourcemanager.slotmanager.SlotManagerException: The slot manager is being suspended.
...
18765 [jobmanager-future-thread-2] DEBUG o.a.f.r.jobmaster.JobManagerRunnerImpl  - Job 411378b9f7181f9ba431960b08b701d7 reached globally terminal state FAILED.
18765 [jobmanager-future-thread-2] DEBUG o.a.f.r.jobmaster.JobManagerRunnerImpl  - Unregistering job from 411378b9f7181f9ba431960b08b701d7 from high-availability.
18765 [jobmanager-future-thread-2] DEBUG o.a.f.r.highavailability.zookeeper.ZooKeeperRunningJobsRegistry  - Setting scheduling state to DONE for job 411378b9f7181f9ba431960b08b701d7.
...
18770 [mini-cluster-io-thread-2] INFO  o.a.f.r.jobmanager.ZooKeeperJobGraphStore  - Recovered JobGraph(jobId: 411378b9f7181f9ba431960b08b701d7).
18770 [mini-cluster-io-thread-2] INFO  o.a.f.r.dispatcher.runner.SessionDispatcherLeaderProcess  - Successfully recovered 1 persisted job graphs.
...
23748 [main-EventThread] INFO  o.a.f.r.jobmaster.JobManagerRunnerImpl  - Granted leader ship but job 411378b9f7181f9ba431960b08b701d7 has been finished.
{code}

From what I can tell the shutdown logic should be adjusted to first shutdown the JobMaster before shutting down any service the JM may be relying on.
Unfortunately the shutdown logic is one of the most obfuscated things I have seen in a while, and I can't make heads-or-tails of how it is even supposed to work.

[~trohrmann] WDYT?;;;","22/Jan/20 13:55;chesnay;master: fbc1ef716f8485f9fe0e9e177902a2345af3ac9d
1.10: 2dd322fbd77f822784c32c06e21e946ee379c377;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Fix check that incremental cleanup size must be greater than zero,FLINK-15146,13273286,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,hehuiyuan,hehuiyuan,hehuiyuan,09/Dec/19 09:09,28/Aug/21 10:34,13/Jul/23 08:10,10/Apr/21 13:55,,,,,,,,,,1.14.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,," 

Hi , the value of cleanupSize is grater than or equal 0. Whether that the value is grater than 0 is more practical.

!image-2019-12-09-17-03-59-014.png|width=615,height=108!

!image-2019-12-09-17-09-18-062.png|width=491,height=309!

 ",,azagrebin,hehuiyuan,liyu,yunta,,,,,,,,,,"hehuiyuan commented on pull request #10507: [FLINK-15146][CORE]Fix the value of  that should  be strictly greater than zero 
URL: https://github.com/apache/flink/pull/10507
 
 
   ## What is the purpose of the change
   Hi , the value of cleanupSize is grater than or equal 0. Whether that the value is grater than 0 is more practical.
   ![image](https://user-images.githubusercontent.com/18002496/70502733-98a34700-1b5c-11ea-8b68-88b8ac4f1c80.png)
   ![image](https://user-images.githubusercontent.com/18002496/70502745-9fca5500-1b5c-11ea-89fd-a1bc2486cc1c.png)
   
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 07:00;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/19 09:04;hehuiyuan;image-2019-12-09-17-03-59-014.png;https://issues.apache.org/jira/secure/attachment/12988298/image-2019-12-09-17-03-59-014.png","09/Dec/19 09:09;hehuiyuan;image-2019-12-09-17-09-18-062.png;https://issues.apache.org/jira/secure/attachment/12988297/image-2019-12-09-17-09-18-062.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 10 13:55:41 UTC 2021,,,,,,,,,,"0|z09gq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/19 13:31;azagrebin;[~hehuiyuan] 
I think you are right, although technically zero value will not break the cleanup but practically there will be no cleanup.

We can change cleanupSize to be strictly greater than zero to avoid confusion.

Do you want to work on this?;;;","10/Dec/19 06:12;hehuiyuan;Hi  [~azagrebin]  , I am pleasure to do this.;;;","10/Apr/21 13:55;yunta;Merged into master:
4dd8c533035b471c2655e6e358cdb75dc57c36c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle data compression does not work with BroadcastRecordWriter.,FLINK-15140,13273243,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,09/Dec/19 05:17,13/Dec/19 09:36,13/Jul/23 08:10,13/Dec/19 09:36,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"I tested the newest code of master branch last weekend with more test cases. Unfortunately, several problems were encountered, including a bug of compression.

When BroadcastRecordWriter is used, for pipelined mode, because the compressor copies the data back to the input buffer, however, the underlying buffer is shared when BroadcastRecordWriter is used. So we can not copy the compressed buffer back to the input buffer if the underlying buffer is shared. For blocking mode, we wrongly recycle the buffer when buffer is not compressed, and the problem is also triggered when BroadcastRecordWriter is used.

To fix the problem, for blocking shuffle, the reference counter should be maintained correctly, for pipelined shuffle, the simplest way maybe disable compression when the underlying buffer is shared. I will open a PR to fix the problem.",,kevin.cyj,pnowojski,,,,,,,,,,,,"wsry commented on pull request #10492: [FLINK-15140][runtime] Fix shuffle data compression doesn't work with BroadcastRecordWriter.
URL: https://github.com/apache/flink/pull/10492
 
 
   
   1. Disable data compression for operators which use broadcast partitioner in pipelined mode.
   2. Not recycle buffer if it not compressed in blocking mode.
   
   ## What is the purpose of the change
   We implemented shuffle data compression in FLINK-14845 to reduce disk and network IO, but unfortunately, a bug was introduced, which makes the compression doesn't work when broadcast partitioner is used.
   For pipelined mode, because the compressor copies the data back to the input buffer, however, the underlying buffer is shared when BroadcastRecordWriter is used. So we can not copy the compressed buffer back to the input buffer if the underlying buffer is shared. For blocking mode, we wrongly recycle the buffer when buffer is not compressed, and the problem is also triggered when BroadcastRecordWriter is used.
   This PR tries to fix the problem.
   
   
   ## Brief change log
   
     - Disable data compression for operators which use broadcast partitioner in pipelined mode.
     - Not recycle buffer if it not compressed in blocking mode.=
   
   
   ## Verifying this change
   
   ```ShuffleCompressionITCase``` is modified to cover the scenario.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Dec/19 07:08;githubbot;600","wsry commented on pull request #10547: [FLINK-15140][runtime] Fix shuffle data compression doesn't work with BroadcastRecordWriter.
URL: https://github.com/apache/flink/pull/10547
 
 
   ## What is the purpose of the change
   We implemented shuffle data compression in FLINK-14845 to reduce disk and network IO, but unfortunately, a bug was introduced, which makes the compression doesn't work when broadcast partitioner is used.
   For pipelined mode, because the compressor copies the data back to the input buffer, however, the underlying buffer is shared when BroadcastRecordWriter is used. So we can not copy the compressed buffer back to the input buffer if the underlying buffer is shared. For blocking mode, we wrongly recycle the buffer when buffer is not compressed, and the problem is also triggered when BroadcastRecordWriter is used.
   This PR tries to fix the problem.
   
   
   ## Brief change log
   
     - Disable data compression for operators which use broadcast partitioner in pipelined mode.
     - Not recycle buffer if it not compressed in blocking mode.=
   
   
   ## Verifying this change
   
   ```ShuffleCompressionITCase``` is modified to cover the scenario.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 11:34;githubbot;600","zhijiangW commented on pull request #10492: [FLINK-15140][runtime] Fix shuffle data compression doesn't work with BroadcastRecordWriter.
URL: https://github.com/apache/flink/pull/10492
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 08:34;githubbot;600","zhijiangW commented on pull request #10547: [FLINK-15140][runtime] Fix shuffle data compression doesn't work with BroadcastRecordWriter.
URL: https://github.com/apache/flink/pull/10547
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 09:32;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 13 09:36:28 UTC 2019,,,,,,,,,,"0|z09ggo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/19 09:36;kevin.cyj;Fix via 5139282e7f48ece00528c6ade9480f10fa3dd54b on master.

Fix via 15a76b8fe2e5cee44af504174bcbc86dc147058a on release-1.10.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
misc end to end test failed  cause loss jars in converting to jobgraph,FLINK-15139,13273228,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,leonard,wangxiyuan,wangxiyuan,09/Dec/19 02:49,12/Dec/19 11:39,13/Jul/23 08:10,12/Dec/19 11:38,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"The test Running 'SQL Client end-to-end test (Old planner)' in misc e2e test failed

log:
{code:java}
(a94d1da25baf2a5586a296d9e933743c) switched from RUNNING to FAILED.
org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot load user class: org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink
ClassLoader info: URL ClassLoader:
Class not resolvable through given classloader.
	at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:266)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:430)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:353)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:419)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:353)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:144)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:432)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.resolveClass(InstantiationUtil.java:78)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1868)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550)
	at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511)
	at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:254)
	... 10 more
{code}
link: [https://travis-ci.org/apache/flink/jobs/622261358]

 ",,jark,kkl0u,leonard,lirui,liyu,wangxiyuan,,,,,,,,"leonardBang commented on pull request #10501: [FLINK-15139][table sql / client]misc end to end test failed  cause loss jars in converting to jobgraph
URL: https://github.com/apache/flink/pull/10501
 
 
   misc end to end test failed  cause loss jars in converting to jobgraph.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
   
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
   
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
   
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request fix  loss jars when convert from streamgraph to jobgraph in `ExecutionContext`.*
   
   
   ## Brief change log
   
     - *update file org.apache.flink.table.client.gateway.local.ExecutionContext.java*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Dec/19 15:44;githubbot;600","kl0u commented on pull request #10501: [FLINK-15139][table sql / client]misc end to end test failed  cause loss jars in converting to jobgraph
URL: https://github.com/apache/flink/pull/10501
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 11:39;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 11:38:30 UTC 2019,,,,,,,,,,"0|z09gdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/19 08:00;liyu;Another two instances:
https://api.travis-ci.org/v3/job/622261365/log.txt
https://api.travis-ci.org/v3/job/621979555/log.txt

This test case seems to fail stably recently. ;;;","09/Dec/19 08:56;jark;I will look into this.;;;","09/Dec/19 12:12;leonard;This should be a bug imported by FLINK-14840 after checked recent commits in master branch.

I ran test `/test_sql_client.sh blink`  success before FLINK-14840(commit:c1d0e99c) 
and ran test `/test_sql_client.sh blink` fail in FLINK-14840（commit:99c1aa32） 

cc:
[~aljoscha];;;","09/Dec/19 12:22;jark;I think so. The root cause should be that we miss to pass {{""pipeline.jars""}} into the Configuration. 

https://github.com/apache/flink/blob/master/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java#L655;;;","09/Dec/19 12:23;jark;[~Leonard Xu], I assigned this issue to you. Feel free to open pull request.;;;","09/Dec/19 13:18;leonard;[~jark] thanks，I'll work for this.;;;","11/Dec/19 04:39;liyu;Raising the priority to blocker since this is a major cause of release-1.10 nightly build failure.;;;","11/Dec/19 06:38;leonard;[~liyu] Yes，it‘s a blocker issue.
I had submitted PR and Will push it ASAP.;;;","12/Dec/19 11:38;kkl0u;Merged on master with f283edb3307d476cd78cd9963347c7d663197805
and on release-1.10 with 333ccc9b1dc9aedf5952d6c1d45be3a4ce09aff8;;;",,,,,,,,,,,,,,,,,,,,,,,
types with precision can't be executed in sql client with blink planner,FLINK-15124,13272974,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,ykt836,ykt836,08/Dec/19 02:10,17/Dec/19 13:30,13/Jul/23 08:10,12/Dec/19 12:50,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Client,Table SQL / Planner,,,,0,pull-request-available,,,,"I created a table in sql client with blink planner:  
{noformat}
create table t (
    a int,
    b varchar,
    c decimal(10, 5))
with (
    'connector.type' = 'filesystem',
    'format.type' = 'csv',
    'format.derive-schema' = 'true',
    'connector.path' = 'xxxxxxx'
);
{noformat}
The table description looks good:
{noformat}
Flink SQL> describe t; 
root 
  |-- a: INT 
  |-- b: STRING 
  |-- c: DECIMAL(10, 5){noformat}
But the select query failed:
{noformat}
Flink SQL> select * from t;
[ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression[GeneratedExpression(field$3,isNull$3,,DECIMAL(38, 18),None)] type is [DECIMAL(38, 18)], result type is [DECIMAL(10, 5)]
{noformat}
 ",,jark,,,,,,,,,,,,,"wuchong commented on pull request #10518: [FLINK-15124][table] Fix types with precision defined in DDL can't be executed
URL: https://github.com/apache/flink/pull/10518
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix the problem that types with precision defined in DDL can't be executed.
   The reason is that when code generation, the result type of field access expression is derived from `TableSource#getProducedDataType` (which may lose precision), and the expected result type is from DDL (which keeps precision). And comparing these two types, an exception is thrown. 
   
   With discussion with Jingsong, we thought that the TableSource should carry the correct precision.
   
   ## Brief change log
   
   - Update CsvTableSource to use DataType instead of TypeInformation.
   - throw an readable exception if the TableSource#getProducedDataType doesn't match the precision defined in DDL.
   
   ## Verifying this change
   
   - added a unit test to verify the exception message.
   - added a IT casee to read a table with precision types and write into a table with precision types.
   - added a IT casee to verify CSV with precision types 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 15:50;githubbot;600","wuchong commented on pull request #10518: [FLINK-15124][table] Fix types with precision defined in DDL can't be executed
URL: https://github.com/apache/flink/pull/10518
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 12:40;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-15141,,,,,,,FLINK-15217,,,FLINK-15141,,,,,,FLINK-15151,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 12:50:53 UTC 2019,,,,,,,,,,"0|z09esw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/19 12:50;jark;[hotfix][table-runtime-blink] Fix the watermark assigner operator should emit max watermark in close
 - 1.10.0: 8591e33217c8b5301850aa89e40c7478b5a83d75
 - 1.11.0: 9e916642ffcc9dcc5ca2ab6d8a609ab3894ad960

[FLINK-15124][table] Fix types with precision defined in DDL can't be executed
 - 1.10.0: e1b1fc417d4a7aa4aba50875084f5cf84cc58a22
 - 1.11.0: 3e5a045cdc8a32e1d59bff43b70ce9e36df20d62
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQL-CLI can not execute insert into statement with lowercase ""INSERT INTO"" keyword",FLINK-15107,13272759,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Terry1897,danny0405,danny0405,06/Dec/19 14:06,07/Dec/19 11:58,13/Jul/23 08:10,07/Dec/19 11:58,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"This was introduced by FLINK-15026 which has a always uppercase case ""INSERT INTO"" pattern matcher check.",,danny0405,Terry1897,,,,,,,,,,,,"zjuwangg commented on pull request #10469: [FLINK-15107][SQL-CLIENT]Fix bug that sql cli can not execute statement with lower 'inert into'
URL: https://github.com/apache/flink/pull/10469
 
 
   ## What is the purpose of the change
   
   * We have introduced alter/create/drop database in  #10419  but there is a potential bug that causes Insert into syntax with the lower case can not get executed normally*
   
   
   ## Brief change log
   
     - * [16dc5a8](https://github.com/apache/flink/commit/16dc5a8dfd69c89d46a35dce9860960e535c03e7)  fix bug insert into syntax with lower case can not get executed normally*
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - *Added unit test in LocalExecutorTest#testInsertIntoSqlPattern*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)
     - The serializers: (no )
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 15:57;githubbot;600","KurtYoung commented on pull request #10469: [FLINK-15107][SQL-CLIENT]Fix bug that sql cli can not execute statement with lower 'insert into'
URL: https://github.com/apache/flink/pull/10469
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Dec/19 11:57;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 07 11:58:07 UTC 2019,,,,,,,,,,"0|z09dh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/19 14:10;Terry1897;Sorry to introduce this, I'd like it.;;;","07/Dec/19 11:58;ykt836;master: 6b389ebef0751d54f53f5fa59a359283e29586ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resuming Externalized Checkpoint after terminal failure (rocks, incremental) end-to-end test stalls on travis",FLINK-15105,13272747,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,klion26,liyu,liyu,06/Dec/19 12:51,17/Dec/19 15:32,13/Jul/23 08:10,17/Dec/19 15:32,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"Resuming Externalized Checkpoint after terminal failure (rocks, incremental) end-to-end test fails on release-1.9 nightly build stalls with ""The job exceeded the maximum log length, and has been terminated"".

https://api.travis-ci.org/v3/job/621090394/log.txt",,gjy,klion26,liyu,trohrmann,yunta,,,,,,,,,"klion26 commented on pull request #10588: [FLINK-15105][test] Resuming Externalized Checkpoint after terminal failure (rocks, incremental) end-to-end test stalls on travis
URL: https://github.com/apache/flink/pull/10588
 
 
   ## What is the purpose of the change
   
   Skip all the exception_checks when running `test_resume_externalized_checkpoints.sh`
   
   Currently, when running `test_resume_externalized_checkpoints.sh`, we'll throw Exception in `FailureMapper#map` and `FailureMapper#notifyCheckpointComplete`, if the `Artificail Exception` was thrown in `FailureMapper#map` the test will fail because the log contains error/exception. In another hand, the correctness of the execution is verified by the test itself, so we disable the exception check in `test_resume_externalized_checkpoints.sh` here.
   
   ## Verifying this change
   
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 06:53;githubbot;600","GJL commented on pull request #10588: [FLINK-15105][test] Resuming Externalized Checkpoint after terminal failure (rocks, incremental) end-to-end test stalls on travis
URL: https://github.com/apache/flink/pull/10588
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 15:29;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 15:32:54 UTC 2019,,,,,,,,,,"0|z09deg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/19 13:07;chesnay;If you run into this error message in 99% of the cases some exception occurred and the e2e tests prints the logs of all processes.

The only error I found so far is
{code}
java.lang.RuntimeException: Error while confirming checkpoint
	at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1205)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.Exception: Artificial failure.
	at org.apache.flink.streaming.tests.FailureMapper.notifyCheckpointComplete(FailureMapper.java:70)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:822)
	at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1200)
	... 5 more
{code}

The artificial failure is whitelisted; maybe something has changed how we bubble them up?;;;","06/Dec/19 16:02;yunta;I'm afraid it's not easy to figure out this issue. The shell scripts detect the logs containing errors, however, when it tries to print all logs out it exceeded the maximum log length. In other words, we do not know the actual errors.;;;","09/Dec/19 06:37;klion26;The test complete checkpoint successfully in the first job, and resumed from the checkpoint successfully in the second job, and can complete checkpoint in the seconde job successfully, 
{code:java}
// log for first job
2019-12-05 20:12:17,410 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - SlidingWindowOperator (1/2) (5a5c73dd041a0145bc02dc017e46bf1f) switched from DE
PLOYING to RUNNING.
2019-12-05 20:12:17,970 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 1 @ 1575576737956 for job 39a292088648857cac5f7e110547c18
0.
2019-12-05 20:12:21,095 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 1 for job 39a292088648857cac5f7e110547c180 (261564 bytes in 3114 ms).
2019-12-05 20:12:21,113 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 2 @ 1575576741094 for job 39a292088648857cac5f7e110547c180.
2019-12-05 20:12:22,002 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - FailureMapper (1/1) (4d273da136346ef3ff6e1a54d197f00b) switched from RUNNING to
 FAILED.
java.lang.RuntimeException: Error while confirming checkpoint
        at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1205)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.Exception: Artificial failure.
        at org.apache.flink.streaming.tests.FailureMapper.notifyCheckpointComplete(FailureMapper.java:70)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:822)
        at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1200)
        ... 5 more
2019-12-05 20:12:22,014 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Discarding checkpoint 2 of job 39a292088648857cac5f7e110547c180.
java.lang.RuntimeException: Error while confirming checkpoint
        at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1205)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.Exception: Artificial failure.
        at org.apache.flink.streaming.tests.FailureMapper.notifyCheckpointComplete(FailureMapper.java:70)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:822)
        at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1200)
        ... 5 more

// log for second job
2019-12-05 20:12:27,190 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Starting job 7c862506012fb04c0d565bfda7cc9595 from savepoint file:///home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-02075534631/externalized-chckpt-e2e-backend-dir/39a292088648857cac5f7e110547c180/chk-1 ()
2019-12-05 20:12:27,213 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Reset the checkpoint ID of job 7c862506012fb04c0d565bfda7cc9595 to 2.
2019-12-05 20:12:27,220 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Restoring job 7c862506012fb04c0d565bfda7cc9595 from latest valid checkpoint: Checkpoint 1 @ 0 for 7c862506012fb04c0d565bfda7cc9595.
2019-12-05 20:12:27,231 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - No master state to restore
2019-12-05 20:12:27,232 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunner           - JobManager runner for job General purpose test job (7c862506012fb04c0d565bfda7cc9595) was granted leadership with session id 00000000-0000-0000-0000-000000000000 at akka.tcp://flink@localhost:6123/user/jobmanager_1.
2019-12-05 20:12:27,233 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Starting execution of job General purpose test job (7c862506012fb04c0d565bfda7cc9595) under job master id 00000000000000000000000000000000.
2019-12-05 20:12:27,233 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job General purpose test job (7c862506012fb04c0d565bfda7cc9595) switched from state CREATED to RUNNING.
2019-12-05 20:12:29,668 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 2 for job 7c862506012fb04c0d565bfda7cc9595 (272831 bytes in 1389 ms).
2019-12-05 20:12:29,683 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 3 @ 1575576749668 for job 7c862506012fb04c0d565bfda7cc9595.
{code}
it failed because the log contains some error
{code:java}
Found error in log files:^M 
{code}
if we execute the command(copied from common.sh) 
{code:java}
grep -rv ""GroupCoordinatorNotAvailableException"" log.txt| grep -v ""RetriableCommitFailedException"" | grep -v ""NoAvailableBrokersException"" | grep -v ""Async Kafka commit failed"" | grep -v ""DisconnectException"" | grep -v ""AskTimeoutException"" | grep -v ""Error while loading kafka-version.properties"" | grep -v ""WARN  akka.remote.transport.netty.NettyTransport"" | grep -v ""WARN  org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline"" | grep -v ""jvm-exit-on-fatal-error"" | grep -v '^INFO:.*AWSErrorCode=\[400 Bad Request\].*ServiceEndpoint=\[https://.*\.s3\.amazonaws\.com\].*RequestType=\[HeadBucketRequest\]' | grep -v ""RejectedExecutionException"" | grep -v ""An exception was thrown by an exception handler"" | grep -v ""java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/exceptions/YarnException"" | grep -v ""java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration"" | grep -v ""org.apache.flink.fs.shaded.hadoop3.org.apache.commons.beanutils.FluentPropertyBeanIntrospector  - Error when creating PropertyDescriptor for public final void org.apache.flink.fs.shaded.hadoop3.org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property."" | grep -v ""Error while loading kafka-version.properties :null"" | grep -v ""Failed Elasticsearch item request"" | grep -v ""[Terror] modules"" | grep -i ""error""
{code}
then we'll get the following output 
{code:java}
log.txt:Checking for errors...
log.txt:Found error in log files:
log.txt:java.lang.RuntimeException: Error while confirming checkpoint
log.txt:java.lang.RuntimeException: Error while confirming checkpoint
log.txt:java.lang.RuntimeException: Error while confirming checkpoint
log.txt:java.lang.RuntimeException: Error while confirming checkpoint
log.txt:java.lang.RuntimeException: Error while confirming checkpoint
log.txt:java.lang.RuntimeException: Error while confirming checkpoint
{code}
All the above {{java.lang.RuntimeException: Error while confirming checkpoint}} was caused by Artificial failure.

The Artificial failure was throw when completing checkpoint in [FailureMapper|https://github.com/apache/flink/blob/171020749f7fccfa7781563569e2c88ea5e8b6a1/flink-end-to-end-tests/flink-datastream-allround-test/src/main/java/org/apache/flink/streaming/tests/FailureMapper.java#L66], and seems we have an infinite source from [SequenceGeneratorSource|https://github.com/apache/flink/blob/171020749f7fccfa7781563569e2c88ea5e8b6a1/flink-end-to-end-tests/flink-datastream-allround-test/src/main/java/org/apache/flink/streaming/tests/SequenceGeneratorSource.java#L102], I'm not sure why we need to throw Artificial failure when completing checkpoint, maybe [~trohrmann] know more about this, as he introduced the {{FailureMapper}}.

If we do not rely on throwing Exception when completing checkpoint, I think we can remove the logic throwing Artifical failure when completing checkpoint. and if this is the right direction, I can help to fix it.

 ;;;","10/Dec/19 10:54;trohrmann;I'm not sure whether it is the problem of the {{FailureMapper}}. The failure mapper is an operator which throws exceptions after processing a given amount of records and having completed so and so many checkpoints. This sounds fine to me. I believe the problem is more how the test is set up and maybe the assumptions about the job behaviour are wrong. But then we should rather correct the test or its setup instead of removing the {{FailureMapper}}.;;;","10/Dec/19 15:47;klion26;[~trohrmann]  In the previous comment, I didn't want to remove the whole {{FailureMapper}}, but just want to remove the {{Artificial failure}} throwing statement in {{FailureMapper}}#{{notifyCheckpointComplete just as the comment in the below code block.}}
{code:java}
public T map(T value) throws Exception {
   numProcessedRecords++;

   if (isReachedFailureThreshold()) {
      throw new Exception(""Artificial failure."");
   }

   return value;
}

@Override
public void notifyCheckpointComplete(long checkpointId) throws Exception {
   numCompleteCheckpoints++;

   if (isReachedFailureThreshold()) { ////// =========== just want to remove this ===========
      throw new Exception(""Artificial failure."");
   }
}
{code}
I think the problem here is that we throw Artifical failure when completing checkpoint

After throwing {{Artificial failure}} in {{FailureMapper#notifyCheckpointComplete}}

---> we got the following exception(attached below)

---> test failed when {{check_logs_for_errors}} using the commands in {{common.sh}}.
{code:java}
java.lang.RuntimeException: Error while confirming checkpoint
        at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1205)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.Exception: Artificial failure.
        at org.apache.flink.streaming.tests.FailureMapper.notifyCheckpointComplete(FailureMapper.java:70)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:822)
        at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1200)
        ... 5 more

{code}
Remove the {{Artificial failure throwing}} in {{FailureMapper#notifyCheckpointComplete, we can still throw }}{{Aritifical failure}} in {{FailureMapper#notifyCheckpointComplete}}, IMHO, the {{Artificial failure throwing}} is just needed when the source is finite, but in the test job, we use [SequenceGeneratorSource|https://github.com/apache/flink/blob/171020749f7fccfa7781563569e2c88ea5e8b6a1/flink-end-to-end-tests/flink-datastream-allround-test/src/main/java/org/apache/flink/streaming/tests/SequenceGeneratorSource.java#L102], it is an infinite source.;;;","11/Dec/19 10:59;trohrmann;Wouldn't this affect all tests which use the {{FailureMapper}}? Moreover, we would no longer be able to test the behaviour when {{notifyCheckpointComplete}} fails which imo is a valid case. We would need to check all tests that no other test uses this feature for any test.

Does the above-mentioned test case rely on the {{isReachedFailureThreshold}} in the {{map}} function? If not, then one could maybe reconfigure the {{FailureMapper}} to not throw exceptions at all. Or one extends it so that one can control that it should not throw exceptions in {{notifyCheckpointComplete}}.

Alternatively, one could change the message of the {{RuntimeException}} to not contain ""error"" in it. Then the check should not fail.;;;","11/Dec/19 13:45;klion26;First, answer the last question: we can't just remove ""error"" message in {{RuntimeException}},  we'll fail in {{common.sh#}}{{check_logs_for_exceptions()}} because of the {{RuntimeException}}.

Then I'll try to describe more about the things about {{FailureMapper}}.
 # {{FailureMapper is only used in {{DataStreamAllroundTestProgram.}}}}
 # we'll add a {{FailureMapper}} in {{DataStreamAllroundTestProgram only if we [enabled TEST_SIMULATE_FAILURE|https://github.com/apache/flink/blob/eddad99123525211c900102206384dacaf8385fc/flink-end-to-end-tests/flink-datastream-allround-test/src/main/java/org/apache/flink/streaming/tests/DataStreamAllroundTestProgram.java#L173]}} in {{DataStreamAllroundTestProgram}}
 # {{We'll throw Exception in {{FailureMapper#map}}}} and {{FailureMapper#notifyCheckpointComplete}}
 # {{we'll enable {{TEST_SIMULATE_FAILURE}} }} in {{test_ha_datastream.sh}}, {{test_ha_per_job_cluster_datastream.sh}} and {{test_resume_externalized_checkpoints.sh}}

IIUC, all the above tests are wanna test whether the job can restore from(restore with checkpoint) the last failed job successfully(but we do not care where the exception come from, then Exception thrown from FailureMapper#mapper or FailureMapper#notifyCheckpointComplete have the same effect). If we want to verify that `failure of notifyCheckpointComplete can fail task`, maybe we can add a ut for it.

 

 ;;;","12/Dec/19 12:46;chesnay;Since this tests results in error behavior, wouldn't the simplest solution be to disable the exception check for runs where failures are being simulated?
This would only affect the {{after terminal failure}} variant of the {{Resuming Externalized Checkpoint}} tests.

The correctness of the execution is verified by the test itself; checking that only very specific exceptions are being thrown in case of an error seems a bit strict and a contract we realistically can't enforce.;;;","12/Dec/19 12:58;trohrmann;Sounds good to me. Additionally, we already whitelisted the ""Artificial failure"" {{Exception}} which indicates that we tried to ignore these exceptions. Apparently, this was not fully achieved as there are other stack traces where it occurs. Hence, the easiest solution could be to disable the check altogether.;;;","12/Dec/19 14:02;klion26;Agree that {{disable the exception check}} here is the easiest way to fix this issue, and we don't need to touch any existing code.  Then we'll disable exception check for all test running {{test_resume_externalized_checkpoints.sh}}. I can help to fix this, could someone help to assign this ticket to me?;;;","16/Dec/19 05:34;liyu;Another stalling instance observed but on different case 'Resuming Externalized Checkpoint (rocks, incremental, scale down) end-to-end test': https://api.travis-ci.org/v3/job/625037128/log.txt;;;","16/Dec/19 05:52;liyu;Another stalling instance on 'Resuming Externalized Checkpoint (file, sync, no parallelism change) end-to-end test': https://api.travis-ci.org/v3/job/625037135/log.txt;;;","17/Dec/19 15:32;gjy;1.10: 527d0584065b899e739f652cb5ade3d1440571a9
master: ca02ebc8e2f7bad2c64aa7e35aa8249d94ca180c;;;",,,,,,,,,,,,,,,,,,,
Do not use GlobalJobParameters to pass system configuration,FLINK-15096,13272701,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,dwysakowicz,dwysakowicz,06/Dec/19 09:18,08/Dec/19 05:01,13/Jul/23 08:10,08/Dec/19 05:01,1.10.0,,,,,,,,,1.10.0,,,,API / Python,,,,,0,pull-request-available,,,,"GlobalJobParameters is a user only configuration that should not be used to ship system specific settings.

Right now python uses it to ship information about custom archives, files, executables etc.

A solution would be to pass required configuration when instantiating the operators.",,aljoscha,dwysakowicz,hequn8128,jark,zhongwei,,,,,,,,,"WeiZhong94 commented on pull request #10477: [FLINK-15096][python] Pass python configuration to the python operators via table config instead of global job parameters.
URL: https://github.com/apache/flink/pull/10477
 
 
   ## What is the purpose of the change
   
   *This pull request passes python configuration to the python operators instead of global job parameters.*
   
   
   ## Brief change log
   
     - *Add class `PythonConfig` to store all python configurations.*
     - *Read python configuration from table config when instantiating the python operators.*
     - *Adjust relevant classes.*
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *PythonConfigTest.java*, *test_dependency.py*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Dec/19 05:52;githubbot;600","hequn8128 commented on pull request #10477: [FLINK-15096][python] Pass python configuration to the python operators via table config instead of global job parameters.
URL: https://github.com/apache/flink/pull/10477
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Dec/19 04:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-14514,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 08 05:00:35 UTC 2019,,,,,,,,,,"0|z09d48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/19 09:44;jark;I also created a similar issue FLINK-14514 that currently our {{FunctionContext}} ships configs in {{TableConfig#getConfiguration}} to {{GlobalJobParameters}}. See {{PlannerBase#mergeParameters}} in blink planner.;;;","06/Dec/19 10:17;dwysakowicz;If I understand it correctly though FLINK-14514 suggest the opposite. To make no differentiation between system and user parameters and ship everything.

What I am suggesting here to ship only a map from a single key: ""pipeline.global-job-parameters"" as parameters, nothing more.

See: https://github.com/apache/flink/pull/10431/commits/f2969d974b862d13fe2e2c3d81b8878ca17d70f5;;;","06/Dec/19 10:38;dwysakowicz;[~aljoscha] [~twalthr] What do you think?;;;","06/Dec/19 12:39;aljoscha;I agree that {{GlobalJobParameters}} should only be used by users, otherwise we will potentially have problems with clashing configuration.;;;","06/Dec/19 13:53;zhongwei;[~dwysakowicz] , thanks for creating this JIRA. It will be great if we can find a cleaner way. Just share my thoughts about the reason why we use the GlobalJobParameter here.

We have thought about several solutions for this issues:
 1) The current solution via GlobalJobParameter
 2) Set the custom archives, files, executables to TableConfig and then instantiating the operator with these configs (this seems the same solution as you proposed?)
 3) Set the custom archives, files, executables to the job configuration of JobGraph
 4) Register these configurations to DistributedCached under some predefined name and then operators could retrieve these configurations

All the above solutions have some problems:
 1) Regarding to solution 1, just as you mentioned, GlobalJobParameter is a user oriented configuration. 
 2) Regarding to solution 2, TableConfig seems also not a good place to hold the custom archives, files, executables as it exposes these internal configurations to users. 
 3) Regarding to solution 3, we need to expose job configuration in JobGraph, it seems also not acceptable. 
 4) Regarding to solution 4, it seems too hack for me.

The first solution seems more acceptable as it will not introduce any API changes. What's your thoughts? It will be great if we can find a cleaner way.;;;","06/Dec/19 14:07;dwysakowicz;Yes my suggested solution is somewhat similar to the option 2. The difference is I am not suggesting to add new fields/methods to TableConfig. From the API side it would be exactly the same as it is in the current master. 
The difference is how you access it in the Operator.  I am suggesting to access the settings from {{TableConfig#getConfiguration()}} as you said when instantiating the operator.

In https://github.com/apache/flink/pull/10431 I would like only the map stored under {{pipeline.global-job-parameters}} in {{TableConfig#getConfiguration}} to be shipped to {{GlobalJobParameters}}.;;;","06/Dec/19 15:35;zhongwei;[~dwysakowicz] thanks for your reply. Sounds like a good idea. I'd like to take this JIRA and provide a PR ASAP. Could you help to assign it to me? Thanks a lot.;;;","08/Dec/19 05:00;hequn8128;Resolved in 1.10.0 via 0d7c15703d0dd304d49203c163e9a1397e4e0d9e;;;",,,,,,,,,,,,,,,,,,,,,,,,
StreamExecutionEnvironment does not clear transformations when executing,FLINK-15093,13272691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,danny0405,zjffdu,zjffdu,06/Dec/19 08:50,10/Dec/19 14:41,13/Jul/23 08:10,10/Dec/19 14:39,1.10.0,,,,,,,,,1.10.0,,,,API / DataStream,,,,,0,pull-request-available,,,,"

Use the following code in scala shell to reproduce this issue.

{code}

val data = senv.fromElements(""hello world"", ""hello flink"", ""hello hadoop"")
data.flatMap(line => line.split(""\\s"")).
    map(w => (w, 1)).
    keyBy(0).
    sum(1).
    print

senv.execute()

data.flatMap(line => line.split(""\\s"")).
    map(w => (w, 1)).
    keyBy(0).
    sum(1).
    print

senv.execute()

{code}",,aljoscha,danny0405,godfreyhe,jark,kkl0u,Terry1897,zjffdu,,,,,,,"danny0405 commented on pull request #10491: [FLINK-15093][streaming-java] StreamExecutionEnvironment does not cle…
URL: https://github.com/apache/flink/pull/10491
 
 
   …ar transformations when executing
   
   ## What is the purpose of the change
   
   Add internal interface `StreamExecutionEnvironment#getStreamGraph(String, boolean)` with the
   ability to clean existing transformations
   
   
   ## Brief change log
   
   * Add internal interface
   StreamExecutionEnvironment#getStreamGraph(String, boolean) with the
   ability to clean existing transformations;
   * Add tests for this new interface;
   * Add ITCase in SQL-CLI LocalExecutorITCase because FLINK-15052 can also
   be fixed by this patch.
   
   
   ## Verifying this change
   
   See tests in `StreamExecutionEnvironmentTest` and `LocalExecutorITCase`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not documented
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Dec/19 05:43;githubbot;600","kl0u commented on pull request #10491: [FLINK-15093][streaming-java] StreamExecutionEnvironment does not cle…
URL: https://github.com/apache/flink/pull/10491
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 14:41;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/19 08:57;zjffdu;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12987682/screenshot-1.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 10 14:39:53 UTC 2019,,,,,,,,,,"0|z09d20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/19 08:52;zjffdu;\cc [~ykt836] [~godfreyhe];;;","06/Dec/19 08:56;ykt836;cc [~kkloudas] ;;;","06/Dec/19 09:00;ykt836;It's similar with FLINK-13708, but I don't think it's the same problem. You don't even use table API in this example. ;;;","06/Dec/19 09:45;jark;I changed the component to DataStream, as it is not a problem of Table API.;;;","06/Dec/19 10:18;ykt836;We also met similar problem in sql client: FLINK-15052

I think the reason is we didn't clear the buffered transformation in `StreamExecutionEnvironment`. Currently, both sql client and `StreamExecutionEnvironment#execute()` will get `StreamGraph` first and try to execute the `StreamGraph`. 

Does it make sense to have an extra boolean flag to indicate whether we want to also clear transformations when getting `StreamGraph`? Just like `ExecutionEnvironment#createProgramPlan(String jobName, boolean clearSinks)` did. ;;;","06/Dec/19 13:16;aljoscha;I think [~ykt836] is right, we can add a {{clearTransforms}} parameter.;;;","06/Dec/19 13:22;aljoscha;Anyone wanna cut a PR?;;;","06/Dec/19 13:54;danny0405;[~aljoscha] I can do that ~;;;","10/Dec/19 14:39;kkl0u;Merged on master with 9c44413a805d6d0d809fe2d497c84a3f653f4337

and on 1.10 with 9a15733e22fc4c88e6563e9f96f78f88dc0aafca;;;",,,,,,,,,,,,,,,,,,,,,,,
"Using sql-client excute sql(select sum(cast(null as int)) from t123;) has a TableException",FLINK-15092,13272690,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,xiaojin.wy,xiaojin.wy,06/Dec/19 08:37,17/Dec/19 13:54,13/Jul/23 08:10,17/Dec/19 13:54,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"CREATE TABLE `t123` (
 x INT
) WITH (
 'format.field-delimiter'='|',
 'connector.type'='filesystem',
 'format.derive-schema'='true',
 'connector.path'='hdfs://zthdev/defender_test_data/daily/test_aggregates/sources/t123.csv',
 'format.type'='csv'
);

select sum(cast(null as int)) from t123;

 

Excute the statement above , then you will see such exception:

 

[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Failed to push project into table source! table source with pushdown capability must override and change explainSource() API to explain the pushdown applied!

 

 ",flink master's newest code.,dwysakowicz,jark,xiaojin.wy,,,,,,,,,,,"dawidwys commented on pull request #10512: [FLINK-15092][java, table] Remove the restriction that GenericCsvInputFormat expects at least one parser
URL: https://github.com/apache/flink/pull/10512
 
 
   ## What is the purpose of the change
   
   GenericCsvInputFormat (and CsvInputFormat, RowCsvInputFormat
   transitevely) had an artificial restriction that at least one parser
   must have been provided. This is a limitation for a proper support of
   projection pushdown in flink-table, when no fields from the input are
   required. This lead to a safety condition check in PushProjectIntoTableSourceScanRule failing.
   
   As part of this commit a TableSourceTestBase class was introduced where
   we can check if table sourcec meet such requirements.
   
   
   ## Verifying this change
   
   This change added tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 12:50;githubbot;600","dawidwys commented on pull request #10512: [FLINK-15092][java, table] Remove the restriction that CsvInputFormat expects at least one parser
URL: https://github.com/apache/flink/pull/10512
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 13:52;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-15284,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 13:54:05 UTC 2019,,,,,,,,,,"0|z09d1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/19 08:44;xiaojin.wy; [~ykt836] , could you please assign it to someone?;;;","06/Dec/19 08:52;ykt836;Thanks for reporting this [~xiaojin.wy], I can also reproduce this in my local env. ;;;","17/Dec/19 13:54;dwysakowicz;Fixed in:
master: c3251911d4dbd21e1ffa194aae35e6a9ca99b924
1.10: c2ed1dad89368afef2b7ecb78e296fd6327bacdc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JoinITCase.testFullJoinWithNonEquiJoinPred failed in travis,FLINK-15091,13272674,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,ykt836,ykt836,06/Dec/19 07:03,07/Dec/19 04:45,13/Jul/23 08:10,06/Dec/19 09:02,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"04:45:22.404 [ERROR] Tests run: 21, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.909 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.batch.table.JoinITCase 04:45:22.406 [ERROR] testFullJoinWithNonEquiJoinPred(org.apache.flink.table.planner.runtime.batch.table.JoinITCase) Time elapsed: 0.168 s <<< ERROR! org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.table.planner.runtime.batch.table.JoinITCase.testFullJoinWithNonEquiJoinPred(JoinITCase.scala:344) Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy Caused by: org.apache.flink.runtime.memory.MemoryAllocationException: Could not allocate 32 pages. Only 0 pages are remaining.

 

details: [https://api.travis-ci.org/v3/job/621407747/log.txt]",,,,,,,,,,,,,,,"JingsongLi commented on pull request #10456: [FLINK-15091][table-planner-blink] Fix memory overused in SortMergeJoinOperator
URL: https://github.com/apache/flink/pull/10456
 
 
   
   ## What is the purpose of the change
   
   In SortMergeJoinOperator, we should check if it is a full join, then will use two externalBufferMemory.
   
   ## Brief change log
   
   Using two externalBufferMemory when SortMergeJoinOperator is full join.
   
   ## Verifying this change
   
   This change is already covered by existing tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 07:39;githubbot;600","KurtYoung commented on pull request #10456: [FLINK-15091][table-planner-blink] Fix memory overused in SortMergeJoinOperator
URL: https://github.com/apache/flink/pull/10456
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 09:01;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-15117,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 09:02:00 UTC 2019,,,,,,,,,,"0|z09cy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/19 09:02;ykt836;master: a4072efb809680aa0f0a5f696e186634624f21b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServer dashboard config json out of sync,FLINK-15085,13272595,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,spurthic,spurthic,05/Dec/19 20:45,11/Mar/20 12:31,13/Jul/23 08:10,11/Mar/20 12:31,1.10.0,1.9.2,,,,,,,,1.10.0,1.9.3,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"History server has javascript errors while loading the overview page, because the web-submit feature not found in the config. 
{code:java}
main.9c4be059472ea41d7052.js:1 ERROR TypeError: Cannot read property 'web-submit' of undefined
    at new t (main.9c4be059472ea41d7052.js:1)
    at qr (main.9c4be059472ea41d7052.js:1)
    at Gr (main.9c4be059472ea41d7052.js:1)
    at ko (main.9c4be059472ea41d7052.js:1)
    at Oo (main.9c4be059472ea41d7052.js:1)
    at Object.Bo [as createRootView] (main.9c4be059472ea41d7052.js:1)
    at e.create (main.9c4be059472ea41d7052.js:1)
    at e.create (main.9c4be059472ea41d7052.js:1)
    at t.bootstrap (main.9c4be059472ea41d7052.js:1)
    at main.9c4be059472ea41d7052.js:1
{code}
It seems to be coming since FLINK-13818: [https://github.com/apache/flink/pull/9883]

The issue is that for history server we are not setting the web-submit feature in the conf and the /config endpoint returns 
{code:java}
{""refresh-interval"":10000,""timezone-offset"":-18000000,""timezone-name"":""Eastern Time"",""flink-version"":""<unknown>"",""flink-revision"":""d9f8abb @ 04.12.2019 @ 16:16:24 EST""}{code}
while as in the Jobmanager the /config endpoint returns
{code:java}
{""refresh-interval"":3000,""timezone-name"":""Coordinated Universal Time"",""timezone-offset"":0,""flink-version"":""1.9-criteo-rc1-1573156762"",""flink-revision"":""366237a @ 07.11.2019 @ 20:00:32 UTC"",""features"":{""web-submit"":true}}

{code}
*AppComponent.ts* fails at this line because the feature web-submit is not found in the config:
{code:java}
webSubmitEnabled = this.statusService.configuration.features['web-submit'];

{code}
This can be fixed in two ways:
 # Add defensive check in the *AppComponent.ts* 
{code:java}
 webSubmitEnabled =
    (this.statusService.configuration &&
        this.statusService.configuration.features &&
            this.statusService.configuration.features['web-submit']);{code}

 # Add the features property in the config file that *HistoryServer.java* generates.",,0x26dres,spurthic,,,,,,,,,,,,"zentol commented on pull request #10459: [FLINK-15085][hs] Simplify dashboard config generation
URL: https://github.com/apache/flink/pull/10459
 
 
   Fixes an issue in the HistoryServer where the dashboard configuration JSON was generated manually, resulting in it being out-of-sync with the actual representation.
   
   Instead we now use an `ObjectMapper` to directly map the object to json, just like how the REST handlers work.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 09:09;githubbot;600","zentol commented on pull request #10459: [FLINK-15085][hs] Simplify dashboard config generation
URL: https://github.com/apache/flink/pull/10459
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 16:18;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-16542,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 11 12:31:59 UTC 2020,,,,,,,,,,"0|z09cgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/19 16:18;chesnay;master: 1b0b289bc16fe9f0b519daa36ab2073f57fce38f;;;","26/Feb/20 10:18;0x26dres;If anyone is stuck with a broken version of the history server, there's a possible hot fix.

You have to modify config.json (which should be in historyserver.web.tmpdir) and add ""features"":{""web-submit"":true} to it:

{code}
sed -i 's/""}$/"",""features"":\{""web-submit"":true}}/g' config.json
{code}

This has to be done AFTER the history server has started.;;;","11/Mar/20 12:17;chesnay;Backporting to 1.9.3;;;","11/Mar/20 12:31;chesnay;1.9: 366cdc7d7d90737848e5936bf50e718cf8f4bffb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos App Master does not respect taskmanager.memory.total-process.size,FLINK-15082,13272537,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,azagrebin,gjy,gjy,05/Dec/19 15:54,12/Dec/19 13:41,13/Jul/23 08:10,12/Dec/19 13:41,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"*Description*
 When the Mesos App Master is started with {{taskmanager.memory.total-process.size}}, [the value is not respected|https://github.com/apache/flink/blob/d08beaa3255b3df96afe35f17e257df31a0d71ed/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java#L339]. 

One can reproduce this when starting the App Master with the command below:
{noformat}
/bin/mesos-appmaster.sh \ 
-Dtaskmanager.memory.total-process.size=2048m \
-Djobmanager.heap.size=2048m \
...
{noformat}
The ClusterEntryPoint will fail with an exception (see below). The reason is that the default value of {{mesos.resourcemanager.tasks.mem}} will be taken as the total process memory size (1024 mb).
{noformat}
org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint MesosSessionClusterEntrypoint.
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:187)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:518)
        at org.apache.flink.mesos.entrypoint.MesosSessionClusterEntrypoint.main(MesosSessionClusterEntrypoint.java:126)
Caused by: org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
        at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:261)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:215)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:169)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
        at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
        at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:168)
        ... 2 more
Caused by: org.apache.flink.configuration.IllegalConfigurationException: Sum of configured Framework Heap Memory (134217728 bytes), Framework Off-Heap Memory (134217728 bytes), Task Off-Heap Memory (0 bytes), Managed Memory (719407031 bytes) and Shuffle Memory (80530638 bytes) exceed configured Total Flink Memory (805306368 bytes).
        at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.deriveInternalMemoryFromTotalFlinkMemory(TaskExecutorResourceUtils.java:273)
        at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.deriveResourceSpecWithTotalProcessMemory(TaskExecutorResourceUtils.java:210)
        at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.resourceSpecFromConfig(TaskExecutorResourceUtils.java:108)
        at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.resourceSpecFromConfig(TaskExecutorResourceUtils.java:94)
        at org.apache.flink.mesos.runtime.clusterframework.MesosTaskManagerParameters.create(MesosTaskManagerParameters.java:341)
        at org.apache.flink.mesos.util.MesosUtils.createTmParameters(MesosUtils.java:109)
        at org.apache.flink.mesos.runtime.clusterframework.MesosResourceManagerFactory.createActiveResourceManager(MesosResourceManagerFactory.java:80)
        at org.apache.flink.runtime.resourcemanager.ActiveResourceManagerFactory.createResourceManager(ActiveResourceManagerFactory.java:58)
        at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:170)
        ... 9 more
{noformat}
*Expected Behavior*
 * If taskmanager.memory.total-process.size and mesos.resourcemanager.tasks.mem are both set and differ in their values, an exception should be thrown
 * If only taskmanager.memory.total-process.size is set and mesos.resourcemanager.tasks.mem is not set, then the value configured by the former should be respected
 * If only mesos.resourcemanager.tasks.mem is set and taskmanager.memory.total-process.size is not set, then the value configured by the former should be respected",,aljoscha,gjy,trohrmann,,,,,,,,,,,"azagrebin commented on pull request #10513: [FLINK-15082] Respect new FLIP-49 taskmanager.memory.total-process.size in Mesos RM
URL: https://github.com/apache/flink/pull/10513
 
 
   ## What is the purpose of the change
   
   
   - If `taskmanager.memory.total-process.size` and `mesos.resourcemanager.tasks.mem` are both set and differ in their values, an exception should be thrown
   - If only `taskmanager.memory.total-process.size` is set and `mesos.resourcemanager.tasks.mem` is not set, then the value configured by the former should be respected
   - If only `mesos.resourcemanager.tasks.mem` is set and` taskmanager.memory.total-process.size` is not set, then the value configured by the former should be respected
   
   ## Brief change log
   
     - Add the described configuration change to `MesosTaskManagerParameters#createContaineredTaskManagerParameters`
     - Add unit tests for that
   
   
   ## Verifying this change
   
   Unit tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (the options are already documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 12:59;githubbot;600","tillrohrmann commented on pull request #10513: [FLINK-15082] Respect new FLIP-49 taskmanager.memory.total-process.size in Mesos RM
URL: https://github.com/apache/flink/pull/10513
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 11:12;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-15198,,,FLINK-13980,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 13:41:33 UTC 2019,,,,,,,,,,"0|z09c3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/19 13:41;trohrmann;Fixed via

master:
b8f3e3a77d23076aff7378820056b3f8d43c55d1
792c6749975350eb16826e51cc0763dfbc3eb20a

1.10.0:
952a880b70cd13fe22ca8397611168e9e55d4823
d9eefeb0baa4c78202c116084bc39fddc322cc52;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source thread should be interrupted during the Task cancellation ,FLINK-15076,13272458,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,05/Dec/19 10:54,19/Dec/19 10:54,13/Jul/23 08:10,10/Dec/19 15:12,1.9.1,,,,,,,,,1.10.0,1.9.2,,,Runtime / Task,,,,,0,pull-request-available,,,,"Source thread should be interrupted more or less the same way how task thread is being interrupted.

+/- The `StreamTaskTest#testCancellationNotBlockedOnLock` should also work in case if the mailbox (task) thread is blocked on trying to acquire a `checkpointLock` by some other currently being executed mail (processing time timer/perform checkpoint).

https://github.com/apache/flink/pull/10345#discussion_r353615760",,pnowojski,wind_ljy,,,,,,,,,,,,"pnowojski commented on pull request #10446: [FLINK-15076][task] Fix SourceStreamTask cancellation
URL: https://github.com/apache/flink/pull/10446
 
 
   Source thread should be interrupted more or less the same way how task thread is being interrupted. This is important for example as in the scenario presented in the `SourceStreamTaskTest#testCancellationWithSourceBlockedOnLock()`. `SourceFunction` is blocked while holding checkpointLock, which might prevent task thread from cancelling properly if the `SourceFunction` is not interrupted.
   
   ## Verifying this change
   
   This change is adding a couple of new test cases in `SourceStreamTask` which supersede an old very similar test from `StreamTask` the a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 15:49;githubbot;600","pnowojski commented on pull request #10446: [FLINK-15076][task] Fix SourceStreamTask cancellation
URL: https://github.com/apache/flink/pull/10446
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 10:33;githubbot;600","pnowojski commented on pull request #10511: [FLINK-15076][task] Fix SourceStreamTask cancellation (backport to 1.9)
URL: https://github.com/apache/flink/pull/10511
 
 
   Source thread should be interrupted more or less the same way how task thread is
   being interrupted. This is important for example as in the scenario presented in the
   SourceStreamTaskTest#testCancellationWithSourceBlockedOnLock(). SourceFunction is
   blocked while holding checkpointLock, which might prevent task thread from
   cancelling properly if the SourceFunction is not interrupted.
   
   ## Verifying this change
   
   This PR is adding a couple of tests and modifies some of the existing ones.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 12:46;githubbot;600","pnowojski commented on pull request #10511: [FLINK-15076][task] Fix SourceStreamTask cancellation (backport to 1.9)
URL: https://github.com/apache/flink/pull/10511
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 15:11;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,FLINK-15317,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 10 15:12:00 UTC 2019,,,,,,,,,,"0|z09bm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/19 15:12;pnowojski;merged to release-1.10 as 52a2f03eff..1e723c64ad
merged to master as c29f65936ff94c65b9d7200d45025f94cc2d2bc3..1863bb88e4f2388a2f91b8000bcb3ca1e44d352b
merged to release-1.9 as 7f797e06ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql client fails to run same query multiple times,FLINK-15073,13272424,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,danny0405,ykt836,ykt836,05/Dec/19 09:13,12/Dec/19 07:58,13/Jul/23 08:10,12/Dec/19 07:58,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"Flink SQL> select abs(-1);
[INFO] Result retrieval cancelled.

Flink SQL> select abs(-1);
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Table 'default: select abs(-1)' already exists. Please choose a different name.",,godfreyhe,jark,,,,,,,,,,,,"danny0405 commented on pull request #10523: [FLINK-15073][sql-client] SQL-CLI fails to run same query multiple times
URL: https://github.com/apache/flink/pull/10523
 
 
   ## What is the purpose of the change
   
   After we change the SQL-CLI to stateful in FLINK-14672, each query's
   temporal table was left out so we can not re-registered the same
   object(from the same query).
   
   ## Brief change log
   
     - Remove the temporal table registered after the query was executed
     - Add test cases for same query re-execution
   
   
   ## Verifying this change
   
   See tests in LocalExecutorITCase.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not documented
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Dec/19 02:56;githubbot;600","KurtYoung commented on pull request #10523: [FLINK-15073][sql-client] SQL-CLI fails to run same query multiple times
URL: https://github.com/apache/flink/pull/10523
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 07:55;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-15108,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 07:58:49 UTC 2019,,,,,,,,,,"0|z09beo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/19 07:58;ykt836;1.10.0: 4a5a720024992c12bbfd4fb316d04f24d23a109e ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
executeAsync in ContextEnvironment from CliFrontend cause unexpected exception,FLINK-15072,13272418,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,05/Dec/19 08:43,17/Dec/19 10:42,13/Jul/23 08:10,17/Dec/19 10:41,1.10.0,,,,,,,,,1.10.0,,,,Client / Job Submission,,,,,0,pull-request-available,,,,See also the pull request.,,aljoscha,tison,,,,,,,,,,,,"TisonKun commented on pull request #10434: [FLINK-15072][client] Hijack executeAsync instead of execute in context environment
URL: https://github.com/apache/flink/pull/10434
 
 
   ## What is the purpose of the change
   
   Currently, if users write program using `executeAsync`, it doesn't work if context environment get into use(JarRun endpoint, CLI Frontend). This is because we don't hijack `executeAsync` 
   
   ## Brief change log
   
   - 
   
   
   ## Verifying this change
   
   Existing tests guarded for current behavior, `ClientTest#testExecuteAsync` tests for `executeAsync` with context environment.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes, change codes in envs)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 08:48;githubbot;600","aljoscha commented on pull request #10434: [FLINK-15072][client] executeAsync in ContextEnvironment from CliFrontend cause unexpected exception
URL: https://github.com/apache/flink/pull/10434
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 10:42;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 10:41:18 UTC 2019,,,,,,,,,,"0|z09bdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/19 08:10;aljoscha;Is this still a relevant issue?
;;;","10/Dec/19 08:15;tison;Yes. As described in the pull request, since we hijack {{execute}} instead of {{executeAsync}} in {{ContextEnvironment}}, if user writes program using {{executeAsync}} and submit via {{CliFrontend}} they will get an unexpected exception {{ProgramMissingJobException}}, although the job is actually fired.;;;","17/Dec/19 10:41;aljoscha;Fixed on master in 992f26ff2dd9db78d718ff47f20c848d9706319f and previous commits
Fixed on release-1.10 in 17adf23e60b7b1e11ced380af132f215fa61c563 and previous commits.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN vcore capacity check can not pass when use large slotPerTaskManager,FLINK-15071,13272415,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,huwh,huwh,05/Dec/19 08:34,15/Jun/20 11:48,13/Jul/23 08:10,04/Jun/20 09:04,1.9.0,,,,,,,,,,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"The check of YARN vcore capacity in YarnClusterDescriptor.isReadyForDeployment can not pass If we config large slotsPerTaskManager(such as 96). The dynamic property yarn.containers.vcores does not work.

This is because we set dynamicProperties after check isReadyForDeployment.

 

 ",,huwh,,,,,,,,,,,,,"huwh commented on pull request #10433: [FLINK-15071] YARN vcore capacity check can not pass when use large slotPerTaskManager
URL: https://github.com/apache/flink/pull/10433
 
 
   ## What is the purpose of the change
   
   This pull request load dynamic properties before check YARN vcore capacity.  so that we can use large slotPerTaskManager.
   
   ## Brief change log
   
   Set dynamic properties to configuration before isReadyForDeployment.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 08:43;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 01:35:20 UTC 2020,,,,,,,,,,"0|z09bco:",9223372036854775807,not exist,,,,,,,,,,,,,,,,,,,"28/Feb/20 10:51;chesnay;Just to clarify, is it relevant whether you configure a large slotPerTaskManager, or is it just always ignored regardless of the actual value?;;;","13/Mar/20 01:35;huwh;h4. [Chesnay Schepler|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chesnay], thanks for your reply, It is always ignored the vcores configured by ""yarn.containers.vcores"". ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot run multiple `insert into csvTable values ()`,FLINK-15066,13272409,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,ykt836,ykt836,05/Dec/19 08:18,09/Jun/20 08:20,13/Jul/23 08:10,09/Jun/20 08:20,1.10.1,,,,,,,,,1.11.0,,,,Table SQL / Client,,,,,0,,,,,"I created a csv table in sql client, and tried to insert some data into this table.

The first insert into success, but the second one failed with exception: 
{code:java}
// Caused by: java.io.IOException: File or directory /.../xxx.csv already exists. Existing files and directories are not overwritten in NO_OVERWRITE mode. Use OVERWRITE mode to overwrite existing files and directories.    at org.apache.flink.core.fs.FileSystem.initOutPathLocalFS(FileSystem.java:817)
{code}",,danny0405,jark,liyu,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15570,,,FLINK-15098,,,,,,,FLINK-18130,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 08:20:29 UTC 2020,,,,,,,,,,"0|z09bbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/19 06:47;liyu;Correct me if I'm wrong, but this seems more like a bug to fix instead of something to improve. Thanks.;;;","12/Dec/19 01:24;jark;Yes, this is a bug. Thanks for the checking [~liyu].;;;","13/Dec/19 03:37;danny0405;Thank [~liyu] for checking this, here are something i want to clarify:

1. INSERT OVERWRITE is not supported in SQL-CLI now, without this syntax, it is hard to make sure what the semantic of writing file. So in my personal option, the best way is to keep as it is, until we add full support for INSERT OVERWRITE;
2. The overwrite attribute is not supported now, and the existing file path check is handled by the CsvTableSink, we  may need some refactoring to support this;

So overall, this is not a bug, this is a usability improvement.
;;;","13/Dec/19 05:59;ykt836;[~danny0405] I'm not sure if this issue relates to INSERT OVERWRITE, quite the opposite, I didn't use INSERT OVERWRITE at all. What I did is type 

`INSERT INTO csvTable VALUES(...)` twice.;;;","24/Dec/19 09:27;lzljs3620320;About writing CSV in SQL world:

Hive/spark behavior: the output path must be a directory. It can be checked whether to append or overwrite according to whether ""insert into"" or ""insert overwrite"". 

Flink's current behavior:
 * By default, whether a directory or a single file is based on parallelism. Single parallelism is a file, and multiple parallelism is a directory. For example, ""insert... values..."" must be a file, because there is only one parallelism source of values.
 * The default behavior is to throw exception when a file exists.

 

We can do:
 * Consider the support of ""insert overwrite"". For overwrite that has been supported in the output format layer, only one boolean should be passed in.
 * It is hard to support append mode. Because the file output format layer does not support it, we need to modify FileOutputFormat and other classes to add an append policy of file name generation.

So my conclusion is that we can wait until 1.11, after we introduce filesystem connector, to take a look again to this problem.;;;","13/Jan/20 10:41;TsReaper;From the perspective of a new user, especially a user from the traditional database, insert overwrite is a really strange behavior. It would be nice if the appending mode is made as the default behavior.;;;","13/Jan/20 10:46;lzljs3620320;[~TsReaper] If you are interesting for this, you can take a look to:

https://issues.apache.org/jira/browse/FLINK-14266

https://issues.apache.org/jira/browse/FLINK-14267

Plan to introduce new Csv, and integrated to {{FileSystemOutputFormat}}.;;;","20/May/20 09:20;ykt836;why this is marked as won't fix?;;;","09/Jun/20 08:20;lzljs3620320;Verified. Close this JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,
RocksDB configurable options doc description error,FLINK-15065,13272393,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,liuyufei,liuyufei,05/Dec/19 06:31,06/Jan/20 15:20,13/Jul/23 08:10,20/Dec/19 13:25,1.9.0,,,,,,,,,1.10.0,1.8.4,1.9.2,,Documentation,,,,,0,pull-request-available,,,,"[RocksDB Configurable Options |https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/config.html#rocksdb-configurable-options]

As description, rocksdb has default options:
state.backend.rocksdb.compaction.level.max-size-level-base: 10MB
state.backend.rocksdb.compaction.level.target-file-size-base: 4MB
state.backend.rocksdb.writebuffer.size: 2MB

But I found the actual options is: 
state.backend.rocksdb.compaction.level.max-size-level-base: 256MB
state.backend.rocksdb.compaction.level.target-file-size-base: 64MB
state.backend.rocksdb.writebuffer.size: 64MB

maybe the description is wrong",,azagrebin,gjy,liuyufei,yunta,,,,,,,,,,"Myasuka commented on pull request #10569: [FLINK-15065][docs] Correct default value of RocksDB options in documentation
URL: https://github.com/apache/flink/pull/10569
 
 
   
   ## What is the purpose of the change
   
   This PR refer to https://github.com/facebook/rocksdb/pull/6123 which correctis RocksDB javadoc. 
   
   Please note that we did not include facebook/rocksdb#6123 in FRocksDB but the actual default value already changes as facebook/rocksdb#6123 said.
   
   
   ## Brief change log
   
   Change `RocksDBConfigurableOptions.java` to correct actual default values.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 11:34;githubbot;600","azagrebin commented on pull request #10569: [FLINK-15065][docs] Correct default value of RocksDB options in documentation
URL: https://github.com/apache/flink/pull/10569
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 13:15;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 06 15:20:03 UTC 2020,,,,,,,,,,"0|z09b7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/19 08:05;yunta;[~liuyufei] thanks for your careful check. Actually, we have already corrected the writebuffer size to {{64MB}} in FLINK-14846 and [https://github.com/facebook/rocksdb/pull/5670]. Can't believe official RocksDB javadoc still has so many errors. I will go through all the settings and create one PR to fix Flink's doc and another one to fix RocksDB's java doc.;;;","05/Dec/19 10:22;yunta;After going through all java docs, I created a [PR|https://github.com/facebook/rocksdb/pull/6123] to RocksDB community. However, our new FRocksDB has been release. But we still need to update Flink's doc at least. [~azagrebin] please assign this issue to me if possible.;;;","20/Dec/19 12:51;azagrebin;[~yunta] do you want to cherry pick your rocksdb [PR|https://github.com/facebook/rocksdb/pull/6123] to frocksdb?;;;","20/Dec/19 13:25;azagrebin;merged into master by ef7ee76eb99054b1ff621ae43ba2c97bcf853156

merged into 1.10 by 836b8ae4c178be6c91fbb64580361ab50bfecdc0

merged into 1.9 by ea5f2418331d8d54cb47313edcdf74923f0f19c8

merged into 1.8 by b38ada077c2ce487dde64a04cb0857d91327c04d;;;","06/Jan/20 15:20;gjy;Please do not set fixVersion to both 1.10.0 and 1.11.0 since 1.10.0 has not been released yet, and we don't want the issues to appear again in the release notes of 1.11.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Input group and output group of the task metric are reversed,FLINK-15063,13272378,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lining,lining,lining,05/Dec/19 04:00,24/Jan/20 01:13,13/Jul/23 08:10,06/Dec/19 08:57,1.9.0,,,,,,,,,1.10.0,1.9.2,,,Runtime / Metrics,Runtime / Network,,,,0,pull-request-available,,,,"In [code|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NettyShuffleEnvironment.java#L181-L182], the input group and output group of the task metric are reversed.",,azagrebin,lining,,,,,,,,,,,,"jinglining commented on pull request #10428: [FLINK-15063][metric]fix input group and output group of the task met…
URL: https://github.com/apache/flink/pull/10428
 
 
   
   
   ## What is the purpose of the change
   
   
   This pull request is for fix bug which input group and output group of the task metric are reversed
   
   
   ## Brief change log
   
   - fix it in org.apache.flink.runtime.io.network.NettyShuffleEnvironment#createShuffleIOOwnerContext
   
   
   ## Verifying this change
   
   
   
   This change is a hotfix.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no )
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no )
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( no)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 05:21;githubbot;600","azagrebin commented on pull request #10428: [FLINK-15063][metric]fix input group and output group of the task met…
URL: https://github.com/apache/flink/pull/10428
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 08:52;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 08:57:03 UTC 2019,,,,,,,,,,"0|z09b4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/19 04:12;lining;[~azagrebin],  in FLINK-12555 you added [code|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NettyShuffleEnvironment.java#L181-L182], here input the wrong metric group. Which define in [code|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/shuffle/ShuffleIOOwnerContext.java#L33-L34], first output, then input. I created pull request for it, could you assign it to me and review it？;;;","05/Dec/19 04:14;lining;cc [~chesnay];;;","05/Dec/19 08:37;azagrebin;Thanks for noticing this [~lining] I am assigning you to the issue;;;","06/Dec/19 08:57;azagrebin;merged into master by fa1dadcd16c52abb93723f7b67f77a4a96c35c2c

merged into 1.9 by 1be72a8d773a44d146862f75b403fdd12585cb4f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Orc reader should use java.sql.Timestamp to read for respecting time zone,FLINK-15062,13272376,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,05/Dec/19 03:50,05/Dec/19 11:41,13/Jul/23 08:10,05/Dec/19 11:41,,,,,,,,,,1.10.0,,,,Connectors / ORC,,,,,0,pull-request-available,,,,"Hive orc use java.sql.Timestamp to read and write orc files... default, timestamp will consider time zone to adjust seconds.",,lzljs3620320,,,,,,,,,,,,,"JingsongLi commented on pull request #10426: [FLINK-15062][orc] Orc reader should use java.sql.Timestamp to read for respecting time zone
URL: https://github.com/apache/flink/pull/10426
 
 
   
   ## What is the purpose of the change
   
   Hive orc use java.sql.Timestamp to read and write orc files... default, timestamp will consider time zone to adjust seconds.
   Our vector Orc reader should use java.sql.Timestamp to read for respecting time zone
   
   ## Brief change log
   
   - OrcTimestampColumnVector should get SqlTimestamp from java.sql.Timestamp.
   - AbstractOrcColumnVector.createTimestampVector should fill data with java.sql.Timestamp.
   
   ## Verifying this change
   
   OrcColumnarRowSplitReaderTest
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 03:54;githubbot;600","KurtYoung commented on pull request #10426: [FLINK-15062][orc] Orc reader should use java.sql.Timestamp to read for respecting time zone
URL: https://github.com/apache/flink/pull/10426
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 11:41;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 05 11:41:50 UTC 2019,,,,,,,,,,"0|z09b40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/19 11:41;ykt836;master: 61f9f2f60f5ee2911a5a716523b805878f350421;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create/alter table/databases properties should be case sensitive stored in catalog,FLINK-15061,13272373,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Terry1897,Terry1897,Terry1897,05/Dec/19 02:47,18/Dec/19 11:42,13/Jul/23 08:10,12/Dec/19 04:02,,,,,,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Now in the class `SqlToOperationConverter`, when creating a table the logic will convert all properties key to lower format, which will cause the properties stored in catalog to lose the case style and not intuitively be observed to user.",,jark,phoenixjiangnan,Terry1897,,,,,,,,,,,"zjuwangg commented on pull request #10493: [FLINK-15061][table]create/alter and table/databases properties should be case sensitive stored in catalog
URL: https://github.com/apache/flink/pull/10493
 
 
   ## What is the purpose of the change
   
   *Now in the class `SqlToOperationConverter`, when creating a table the logic will convert all properties key to lower format, which will cause the properties stored in catalog to lose the case style and not intuitively be observed to user.*
   
   
   ## Brief change log
   
     - * [a79225d](https://github.com/apache/flink/commit/a79225d75a53240951a4378380ff2aac457c1278) *
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Dec/19 07:33;githubbot;600","bowenli86 commented on pull request #10493: [FLINK-15061][table]create/alter and table/databases properties should be case sensitive stored in catalog
URL: https://github.com/apache/flink/pull/10493
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 04:01;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 12 04:02:18 UTC 2019,,,,,,,,,,"0|z09b3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/19 04:02;phoenixjiangnan;master: ab4c31c5266dac4d1040491aa324a1d0a566c4aa
1.10: c35ae053942860d7bb4677124cb21821db968954;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataTypeExtractorTest fails on travis,FLINK-15059,13272331,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,dwysakowicz,dwysakowicz,04/Dec/19 22:25,05/Dec/19 17:58,13/Jul/23 08:10,05/Dec/19 17:58,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"https://api.travis-ci.org/v3/job/620607358/log.txt

{code}
16:28:41.138 [ERROR] testExtraction[27](org.apache.flink.table.types.extraction.DataTypeExtractorTest)  Time elapsed: 0.059 s  <<< ERROR!
org.apache.flink.table.api.ValidationException: Could not extract a data type from 'class org.apache.flink.table.types.extraction.DataTypeExtractorTest$SimplePojoWithAssigningConstructor'. Please pass the required data type manually or allow RAW types.
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367)
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)
Caused by: org.apache.flink.table.api.ValidationException: Could not extract a data type from 'class org.apache.flink.table.types.extraction.DataTypeExtractorTest$SimplePojoWithAssigningConstructor'. Interpreting it as a structured type was also not successful.
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367)
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)
Caused by: java.lang.UnsupportedOperationException: This feature requires ASM7
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367)
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)

16:28:41.141 [ERROR] testExtraction[28](org.apache.flink.table.types.extraction.DataTypeExtractorTest)  Time elapsed: 0.002 s  <<< ERROR!
org.apache.flink.table.api.ValidationException: Could not extract a data type from 'class org.apache.flink.table.types.extraction.DataTypeExtractorTest$PojoWithCustomFieldOrder'. Please pass the required data type manually or allow RAW types.
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367)
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)
Caused by: org.apache.flink.table.api.ValidationException: Could not extract a data type from 'class org.apache.flink.table.types.extraction.DataTypeExtractorTest$PojoWithCustomFieldOrder'. Interpreting it as a structured type was also not successful.
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367)
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)
Caused by: java.lang.UnsupportedOperationException: This feature requires ASM7
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.runExtraction(DataTypeExtractorTest.java:367)
	at org.apache.flink.table.types.extraction.DataTypeExtractorTest.testExtraction(DataTypeExtractorTest.java:363)
{code}",,dwysakowicz,jark,twalthr,,,,,,,,,,,"twalthr commented on pull request #10439: [FLINK-15059][table-common] Update ASM opcode to 7
URL: https://github.com/apache/flink/pull/10439
 
 
   ## What is the purpose of the change
   
   This should fix the failing builds using JDK11.
   
   ## Brief change log
   
   - Updates ASM opcodes to 7
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 12:28;githubbot;600","asfgit commented on pull request #10439: [FLINK-15059][table-common] Update ASM opcode to 7
URL: https://github.com/apache/flink/pull/10439
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 17:57;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 05 17:58:47 UTC 2019,,,,,,,,,,"0|z09au0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/19 22:26;dwysakowicz;cc [~twalthr];;;","05/Dec/19 17:58;twalthr;Fixed in 1.10.0: 23ea91bb80ade0e3a715498d52c1564c45b80d5b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set taskmanager.memory.total-process.size in jepsen tests,FLINK-15057,13272283,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gjy,gjy,gjy,04/Dec/19 16:30,06/Dec/19 13:26,13/Jul/23 08:10,06/Dec/19 13:26,1.10.0,,,,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,,,,"Set {{taskmanager.memory.total-process.size}} in {{flink-conf.yaml}} used by tests. Currently the taskmanager process fails due to
{noformat}
org.apache.flink.configuration.IllegalConfigurationException: Either Task Heap Memory size and Managed Memory size, or Total Flink Memory size, or Total Process Memory size need to be configured explicitly.
        at org.apache.flink.runtime.clusterframework.TaskExecutorResourceUtils.resourceSpecFromConfig(TaskExecutorResourceUtils.java:110)
        at org.apache.flink.runtime.taskexecutor.TaskManagerServicesConfiguration.fromConfiguration(TaskManagerServicesConfiguration.java:219)
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManager(TaskManagerRunner.java:357)
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.<init>(TaskManagerRunner.java:153)
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:327)
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:298)
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:295)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
        at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.main(TaskManagerRunner.java:295)
{noformat}",,gjy,,,,,,,,,,,,,"GJL commented on pull request #10422: [FLINK-15057][tests] Set JM and TM memory config in flink-conf.yaml
URL: https://github.com/apache/flink/pull/10422
 
 
   ## What is the purpose of the change
   
   *This sets the JM and TM memory configuration in flink-conf.yaml used in Jepsen tests.*
   
   
   ## Brief change log
   
     - *Set JM and TM memory config in flink-conf.yaml*
   
   ## Verifying this change
   
   
   This change is already covered by existing tests, such as *the test that is being changed*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 16:38;githubbot;600","GJL commented on pull request #10422: [FLINK-15057][tests] Set JM and TM memory config in flink-conf.yaml
URL: https://github.com/apache/flink/pull/10422
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 13:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 13:26:14 UTC 2019,,,,,,,,,,"0|z09ajc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/19 13:26;gjy;master: 953f1a0cb511091721e725373f2960a0deb77e15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configurations with values contains space may cause TM failures on Yarn,FLINK-15053,13272244,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,xtsong,xtsong,04/Dec/19 14:10,07/Jan/20 08:19,13/Jul/23 08:10,07/Jan/20 08:19,1.10.0,,,,,,,,,1.10.0,,,,Deployment / YARN,Runtime / Coordination,,,,0,pull-request-available,,,,"Currently on Yarn setups, we are passing task executor specific configurations through dynamic properties in the starting command (see FLINK-13184).

If the value of configuration contains space, the dynamic properties may not be correctly parsed, which could cause task executor failures. On occurrence can be found in FLINK-15047.

It would be good to allow spaces when passing dynamic properties. E.g., surrounding the values with double quotation marks, or escaping special characters.

cc [~fly_in_gis]",,gjy,trohrmann,wangyang0918,xtsong,,,,,,,,,,"wangyang0918 commented on pull request #10532: [FLINK-15053][runtime] Escape all dynamical property values for taskmanager
URL: https://github.com/apache/flink/pull/10532
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   Currently on Yarn setups, we are passing task executor specific configurations through dynamic properties in the starting command (see FLINK-13184).
   
   If the value of configuration contains space, the dynamic properties may not be correctly parsed, which could cause task executor failures. On occurrence can be found in FLINK-15047.
   
   It would be good to allow spaces when passing dynamic properties. E.g., surrounding the values with single quotes. 
   
   
   ## Brief change log
   
   * Each value will put in single quotes. This works for all chars except single quote itself. To escape the single quote, close the quoting before it, insert the escaped single quote, and then re-open the quoting. For example, the value is `my'value` and the escaped value is `'my'\''value'`.
   
   
   ## Verifying this change
   
   * Update the existing unit test to cover space values case
   * Add a new ITCase to verify passing and loading dynamic properties for TaskManager
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Dec/19 12:45;githubbot;600","tillrohrmann commented on pull request #10532: [FLINK-15053][runtime] Escape all dynamical property values for taskmanager
URL: https://github.com/apache/flink/pull/10532
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 08:18;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,FLINK-15047,,,,,FLINK-13184,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 08:19:12 UTC 2020,,,,,,,,,,"0|z09aao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/19 02:32;wangyang0918;[~xintongsong] Thanks for creating this ticket. I think it just because we have introduced many updated configs from 1.10. These configs may contains some characters need to be escaped. FLINK-13184 uses dynamic properties instead of always uploading a taskmanage-conf.yam file to dfs.

For the solution, i think we always use single quotes for the values. If the value contains single quote, it needs to be escaped. For example, `{{-D taskmanager.memory.managed.size='1234 bytes' -D key1='value\'xxxx\'yyyy'`.}};;;","05/Dec/19 02:37;xtsong;Does single quotation marks also works for other special characters?
[~gjy] also pointed out in FLINK-15047 that there are other characters that should be escaped.
 https://stackoverflow.com/questions/15783701/which-characters-need-to-be-escaped-when-using-bash;;;","05/Dec/19 02:51;wangyang0918;[~xintongsong] I think single quotation marks works for all the other special characters except itself. And i have a simple test for sh & bash, it works as expected. [https://www.gnu.org/software/bash/manual/html_node/Single-Quotes.html]

[~GJL] do you have other concerns?;;;","06/Dec/19 08:42;gjy;Right, within single quotes, only single quotes have to be escaped. I do not see a reason why this shouldn't work.;;;","09/Dec/19 11:06;wangyang0918;All the value of dynamic properties will be put into a pair of single quotes. If the value contains single quote, will be replaced with <SINGLE_QUOTE>. When loading the dynamic properties, it will be restored to single quote. I do not suggest to use \' to escape, since it will not work for start command of bash.
{code:java}
// start command
bash -c ""$JAVA_HOME/bin/java -Xmx424m ... ... org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint -Dkey1='\'value' -Dkey2='20 s' -Dkey3='value3'""

bash -c ""$JAVA_HOME/bin/java -Xmx424m ... ... org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint -Dkey1='<SINGLE_QUOTE>value' -Dkey2='20 s' -Dkey3='value3'""{code}
[~gjy] What do you think?;;;","10/Dec/19 10:08;trohrmann;What exactly does not work [~fly_in_gis]? Would it work to close the single quote before escaping {{'}} (instead of {{'\'value'}} one would {{\''value'}} as described in the SO answer?;;;","10/Dec/19 10:11;trohrmann;Btw: Shouldn't this issue be a blocker issue because whenever a user configures Flink to have a config entry with spaces, Yarn won't work?;;;","10/Dec/19 10:12;trohrmann;cc [~gjy], I've escalated this issue to be a blocker. Downgrade if you disagree.;;;","10/Dec/19 11:40;wangyang0918;[~trohrmann], thanks for your comment. I will try to give a fix for this issue. Could you assign this ticket to me?

 

This problem only comes up when the JM updated configuration contains space. If the user puts config options in flink-conf.yaml or via `-yD`, it works fine. Because we calculate the differences between client uploaded flink configuration and JM updated configuration, then dynamic properties are used to pass them to TaskManager.

 

Closing the quoting before escaping single quote should work. If the config option value is my'value, the passing dynamic property will be -Dkey1='my'\''value'.;;;","10/Dec/19 14:19;trohrmann;Thanks for the clarification. I think it would be good to fix this problem. I've assigned the ticket to you [~fly_in_gis]. Please set it in progress.;;;","11/Dec/19 13:03;wangyang0918;[~gjy] [~trohrmann]

I have attached a PR for this ticket. Please take a look at your convenience.;;;","07/Jan/20 08:19;trohrmann;Fixed via

master: 2ff1de776dc6675aa6d5a211296c54df9f27c6af
1.10.0: 76b23e8907789e50abfa8c34301508bd412d23ff;;;",,,,,,,,,,,,,,,,,,,,
Test transformation.clear() in sqlClient,FLINK-15052,13272204,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,danny0405,ykt836,ykt836,04/Dec/19 12:15,10/Dec/19 14:40,13/Jul/23 08:10,10/Dec/19 14:40,,,,,,,,,,1.10.0,,,,Table SQL / Client,,,,,0,,,,,"when executing multiple commands from sql client, the later job graph will include all job graphs which already executed. ",,jark,kkl0u,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 10 14:40:58 UTC 2019,,,,,,,,,,"0|z09a1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/19 14:40;kkl0u;Merged on master with a11893f63cc0610ec714e54eac24bd2f2b4dab53

and on 1.10 with 21c295506d45aaf3b8ee00022f510869d2e7dfaa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in RocksDBStateBackend getNumberOfTransferingThread,FLINK-15051,13272193,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sewen,sewen,sewen,04/Dec/19 11:38,04/Dec/19 21:56,13/Jul/23 08:10,04/Dec/19 21:56,1.9.1,,,,,,,,,1.10.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"""getNumberOfTransferingThreads"" has a typo, it should be either
  - ""getNumberOfTransferringThreads"" (with two r)
  - getNumberOfTransferThread

I personally find the second option nicer.",,sewen,,,,,,,,,,,,,"StephanEwen commented on pull request #10420: [FLINK-15051][state-backends] Fix typo in RocksDBStateBackend.getNumberOfTransferingThreads
URL: https://github.com/apache/flink/pull/10420
 
 
   ## What is the purpose of the change
   
   This fixes a simple typo in the `RocksDBStateBackend.getNumberOfTransferingThreads()` changing to `RocksDBStateBackend.getNumberOfTransferThreads()`.
   
   The original method is retained and deprecated.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 11:42;githubbot;600","asfgit commented on pull request #10420: [FLINK-15051][state-backends] Fix typo in RocksDBStateBackend.getNumberOfTransferingThreads
URL: https://github.com/apache/flink/pull/10420
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 21:54;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 21:56:22 UTC 2019,,,,,,,,,,"0|z099zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/19 21:56;sewen;Fixed in 1.10.0 via 9829b9cbc1f48060b0ef01739f1ead413e962715;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFormatConverters should support any TIMESTAMP WITHOUT TIME ZONE types,FLINK-15050,13272140,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,docete,lzljs3620320,lzljs3620320,04/Dec/19 07:49,20/Dec/19 06:00,13/Jul/23 08:10,20/Dec/19 06:00,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,Should have tests to cover these behaviors.,,jark,lirui,lzljs3620320,,,,,,,,,,,"docete commented on pull request #10418: [FLINK-15050][table-planner-blink] DataFormatConverters should suppor…
URL: https://github.com/apache/flink/pull/10418
 
 
   …t any TIMESTAMP WITHOUT TIME ZONE types
   
   
   ## What is the purpose of the change
   
   User may pass any TIMESTAMP WITHOUT TIME ZONE in, e.g.:
   
   `DataTypes.TIMESTAMP(9).bridgedTo(LocalDateTime.class)`,
   `DataTypes.TIMESTAMP(9).bridgedTo(Timestamp.class)`,
   `DataTypes.TIMESTAMP(3)`,
   ```
   new AtomicDataType(
     new LegacyTypeInformationType<>(
       LogicalTypeRoot.TIMESTAMP_WITHOUT_TIME_ZONE,
       SqlTimeTypeInfo.TIMESTAMP))
   ```,
   ```
   new AtomicDataType(
     new LegacyTypeInformationType<>(
       LogicalTypeRoot.TIMESTAMP_WITHOUT_TIME_ZONE,
       new LegacyTimestampTypeInfo(7)))
   ```
   In such situations, we should return proper converters.
   
   ## Brief change log
   
   - Let DataFormatConverters support any TIMESTAMP_WITHOUT_TIME_ZONE types
   - Add tests to verify
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *DataFormatConvertersTest)*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**yes** / no)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 09:36;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 06:00:11 UTC 2019,,,,,,,,,,"0|z099nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/19 09:07;docete;User may pass any TIMESTAMP WITHOUT TIME ZONE in, e.g.:

DataTypes.TIMESTAMP(9).bridgedTo(LocalDateTime.class),
DataTypes.TIMESTAMP(9).bridgedTo(Timestamp.class),
DataTypes.TIMESTAMP(3),
new AtomicDataType(
  new LegacyTypeInformationType<>(
    LogicalTypeRoot.TIMESTAMP_WITHOUT_TIME_ZONE,
    SqlTimeTypeInfo.TIMESTAMP)),
new AtomicDataType(
  new LegacyTypeInformationType<>(
    LogicalTypeRoot.TIMESTAMP_WITHOUT_TIME_ZONE,
    new LegacyTimestampTypeInfo(7)));;;","12/Dec/19 06:01;lzljs3620320;[~jark] Can you assign this to [~docete] ?;;;","20/Dec/19 06:00;jark;1.11.0: 6053e7ad39e6c3aa4427803f50c4ee828bcd1f59
1.10.0: 3c339dad1db229099236997880eb1f951ab69e08;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compile error when hash join with timestamp type key,FLINK-15049,13272134,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,docete,lzljs3620320,lzljs3620320,04/Dec/19 07:11,18/Dec/19 07:14,13/Jul/23 08:10,18/Dec/19 07:14,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Now, internal format of timestamp type has been modified to SqlTimestamp, but in LongHashJoinGenerator, still convert it to long directly in genGetLongKey.

This should be a bug when hash join with timestamp type key.",,jark,lzljs3620320,,,,,,,,,,,,"docete commented on pull request #10415: [FLINK-15049][table-planner-blink] Compile error when hash join with …
URL: https://github.com/apache/flink/pull/10415
 
 
   …timestamp type key
   
   
   ## What is the purpose of the change
   
   LongHashJoinGenerator should convert SqlTimestamp to long by genGetLongKey when precision of TimestampType is less than  or equals to 3.
   
   ## Brief change log
   
   - Convert SqlTimestamp to long in genGetLongKey when precision of Timestamp <= 3
   - Add JoinOn Timestmap(3) case to verify
   
   ## Verifying this change
   
   This change is already covered by tests, such as *(TimestampITCase)*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**yes** / no)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 07:47;githubbot;600","wuchong commented on pull request #10415: [FLINK-15049][table-planner-blink] Compile error when hash join with …
URL: https://github.com/apache/flink/pull/10415
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/19 07:13;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 07:14:52 UTC 2019,,,,,,,,,,"0|z099m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/19 07:43;docete;LongHashJoinGenerator should convert SqlTimestamp to Long by genGetLongKey when precision of TimestampType is less than  or equals to 3.;;;","12/Dec/19 06:01;lzljs3620320;[~jark] Can you assign this to [~docete] ?;;;","12/Dec/19 10:03;jark;Sure.;;;","18/Dec/19 07:14;jark;1.10.0: e6fc18c75af4a46bf289c11f86a3b4aac735e162
1.11.0: d6e3bd985b2cd59dfd8e6feb49b84af52b940ccc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnDistributedCacheITCase is unstable,FLINK-15047,13272120,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,tison,tison,04/Dec/19 05:52,06/Dec/19 09:32,13/Jul/23 08:10,04/Dec/19 15:34,1.10.0,,,,,,,,,1.10.0,,,,Tests,,,,,0,test-stability,,,,"See also https://api.travis-ci.com/v3/job/262854881/log.txt

cc [~ZhenqiuHuang]",,aljoscha,azagrebin,gjy,rmetzger,tison,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15088,,,,,,FLINK-15053,FLINK-15055,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 15:34:41 UTC 2019,,,,,,,,,,"0|z099j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/19 09:34;gjy;-Running this test locally failed 10 out of 10 times. I am promoting the priority to Blocker.- 

Edit: I forgot to recompile Flink so I am not sure if it fails 10/10 times.;;;","04/Dec/19 13:20;xtsong;[~tison], [~hpeter], [~gjy]
I've also looked into this failure case, and have already find the cause.
A solution is already included in the linked PR, and the explanation of the cause is as follows.

The problem is caused together by FLINK-13983 and FLINK-13184. The test case [~hpeter] introduced in FLINK-14033 just triggers the PR. It seems that FLINK-14033 and FLINK-13984 passed travis and get merged separately, but when both changes are merged into the master branch, the test failure is triggered.

When I try to debug this case, I find that this case does not output 'jobmanager.log' and 'taskmanager.log', because it does not specify {{YarnConfigOptionsInternal#APPLICATION_LOG_CONFIG_FILE}}. So I specified this configuration in the case, and surprisingly find that the case is fixed.

After a few tries, I find that the test case fails when {{configuration}} contains less than 2 key-value pairs, and success when there's more than 2. (This is later proved not always true, but it helped at this time.) So I removed {{AkkaOptions.ASK_TIMEOUT}} from the {{configuration}}, keeps only the one for log4j config file. In this way I finally get log files of a failure case.

Looking into the log files, I find an exception in 'taskmanager.log' saying 'jobmanager.rpc.address' is null, which caused the task executor fail. Currently, we are passing task executor specific configurations through dynamic properties in the starting command (see FLINK-13184). So I looked in to the starting command of the task executors.

This is the starting command in a failure case:
{code:java}/bin/bash -c /Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home/bin/java -Xmx268435450 -Xms268435450 -XX:MaxDirectMemorySize=214748366 -XX:MaxMetaspaceSize=134217728 -Dlog.file=/Users/xintongsong/workspace/xintongsong-flink/flink-yarn-tests/target/flink-yarn-tests-with-distributed-cache/flink-yarn-tests-with-distributed-cache-logDir-nm-1_0/application_1575456859645_0001/container_1575456859645_0001_01_000002/taskmanager.log -Dlog4j.configuration=file:./log4j.properties org.apache.flink.yarn.YarnTaskExecutorRunner -D taskmanager.memory.shuffle.max=80530638b -D taskmanager.memory.framework.off-heap.size=134217728b -D taskmanager.memory.framework.heap.size=134217728b -D taskmanager.memory.managed.size=322122552b -D taskmanager.memory.task.heap.size=134217722b -D taskmanager.memory.task.off-heap.size=0b -D taskmanager.memory.managed.off-heap.size=322122552b -D taskmanager.memory.shuffle.min=80530638b --configDir . -Dweb.port=0 -Dtaskmanager.memory.managed.size=322122552 bytes -Djobmanager.rpc.address=30.25.94.103 -Drest.address=30.25.94.103 -Dweb.tmpdir=/var/folders/nk/cj8wv8r97rn7w7dhwqzghpr40000gn/T/flink-web-ceab8aec-cf07-4c83-9cfc-9599d39a5980 -Djobmanager.rpc.port=53359 1> /Users/xintongsong/workspace/xintongsong-flink/flink-yarn-tests/target/flink-yarn-tests-with-distributed-cache/flink-yarn-tests-with-distributed-cache-logDir-nm-1_0/application_1575456859645_0001/container_1575456859645_0001_01_000002/taskmanager.out 2> /Users/xintongsong/workspace/xintongsong-flink/flink-yarn-tests/target/flink-yarn-tests-with-distributed-cache/flink-yarn-tests-with-distributed-cache-logDir-nm-1_0/application_1575456859645_0001/container_1575456859645_0001_01_000002/taskmanager.err{code}

And this is the starting command in a success case:
{code:java}
/bin/bash -c /Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home/bin/java -Xmx268435450 -Xms268435450 -XX:MaxDirectMemorySize=214748366 -XX:MaxMetaspaceSize=134217728 -Dlog.file=/Users/xintongsong/workspace/xintongsong-flink/flink-yarn-tests/target/flink-yarn-tests-with-distributed-cache/flink-yarn-tests-with-distributed-cache-logDir-nm-1_0/application_1575456859645_0001/container_1575456859645_0001_01_000002/taskmanager.log -Dlog4j.configuration=file:./log4j.properties org.apache.flink.yarn.YarnTaskExecutorRunner -D taskmanager.memory.shuffle.max=80530638b -D taskmanager.memory.framework.off-heap.size=134217728b -D taskmanager.memory.framework.heap.size=134217728b -D taskmanager.memory.managed.size=322122552b -D taskmanager.memory.task.heap.size=134217722b -D taskmanager.memory.task.off-heap.size=0b -D taskmanager.memory.managed.off-heap.size=322122552b -D taskmanager.memory.shuffle.min=80530638b --configDir . -Dweb.port=0 -Dtaskmanager.memory.managed.size=322122552 bytes -Djobmanager.rpc.address=30.25.94.103 -Drest.address=30.25.94.103 -Dweb.tmpdir=/var/folders/nk/cj8wv8r97rn7w7dhwqzghpr40000gn/T/flink-web-ceab8aec-cf07-4c83-9cfc-9599d39a5980 -Djobmanager.rpc.port=53359 1> /Users/xintongsong/workspace/xintongsong-flink/flink-yarn-tests/target/flink-yarn-tests-with-distributed-cache/flink-yarn-tests-with-distributed-cache-logDir-nm-1_0/application_1575456859645_0001/container_1575456859645_0001_01_000002/taskmanager.out 2> /Users/xintongsong/workspace/xintongsong-flink/flink-yarn-tests/target/flink-yarn-tests-with-distributed-cache/flink-yarn-tests-with-distributed-cache-logDir-nm-1_0/application_1575456859645_0001/container_1575456859645_0001_01_000002/taskmanager.err
{code}

You will find ""-Dtaskmanager.memory.managed.size=322122552 bytes"" in both commands. The space between the number and ""bytes"" prevents parsing of the subsequent dynamic properties. In the failure case, the space comes before ""jobmanager.rpc.address"", so the address is not parsed. In the success case, the space comes after the address, so the address is not affected.

The dynamic properties are generated by {{Utils#getDynamicProperties}}, where {{streram().flatMap().toArray()}} is used. So the order of the properties depends on the internal implementation of java stream, probably related to number of configurations.

The cause of the space in this case is that, in FLINK-13983 we use {{tmResourceSpec.getManagedMemorySize().toString()}} in {{ActiveResourceManagerFactory#createActiveResourceManagerConfiguration}} to explicitly set managed memory size into the {{configuration}}. {{MemorySize#toString}} generates strings with spaces. I've verified that changing it to {{tmResourceSpec.getManagedMemorySize().getBytes + ""b""}} can fix the problem.

I already included the fix in our PR, so the test case should be fixed soon. 

And according to the findings during debugging this case, I would suggest two follow-ups.
- {{YarnDistributedCacheITCase}} should generate jobmanager / taskmanager logs. {{YarnTestUtils.createClusterDescriptorWithLogging}} may be used here.
- Flink allows config options to have values that contain spaces. E.g., the default value of {{AkkaOption#ASK_TIMEOUT}} is ""10 s"". To prevent such problem in future, we should also allow spaces when dynamic properties. E.g., surround the values with double quotation marks, or escaping special characters.

What do you think?;;;","04/Dec/19 14:18;gjy;I just want to point out that there are also other characters that potentially need to be escaped https://stackoverflow.com/questions/15783701/which-characters-need-to-be-escaped-when-using-bash;;;","04/Dec/19 14:42;rmetzger;If I'm not mistaken, the test failed in all recent travis builds that had no caching issues:

[https://travis-ci.org/apache/flink/builds/620414836]

[https://travis-ci.org/apache/flink/builds/620386904]

[https://travis-ci.org/apache/flink/builds/620569917]

[https://travis-ci.org/apache/flink/builds/620497074]

 

 ;;;","04/Dec/19 15:29;xtsong;True, the master branch is broken.
Two PRs passed ci tests separately without each other. The failure is triggered when both of them are merged.
Andrey is merging the fix right now.;;;","04/Dec/19 15:34;azagrebin;merged into master by 4060acc46ebd31184f0e74ce11a3e3479ebef9dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix python compatibility by excluding the Env.executeAsync() (FLINK-14854),FLINK-15042,13271973,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kkl0u,kkl0u,kkl0u,03/Dec/19 15:06,06/Dec/19 20:59,13/Jul/23 08:10,03/Dec/19 15:11,1.10.0,,,,,,,,,1.10.0,,,,API / Python,,,,,0,pull-request-available,,,,,,kkl0u,,,,,,,,,,,,,"kl0u commented on pull request #10404: FLINK-15042][python] Fix python compatibility by excluding executeAsync
URL: https://github.com/apache/flink/pull/10404
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/19 15:10;githubbot;600","kl0u commented on pull request #10404: FLINK-15042][python] Fix python compatibility by excluding executeAsync
URL: https://github.com/apache/flink/pull/10404
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/19 15:10;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-14854,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 03 15:11:52 UTC 2019,,,,,,,,,,"0|z098mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/19 15:11;kkl0u;Fixed with 89bd90dc67cf35575ec7b8de1d1d2cdf512fffdf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Container startup error will be handled out side of the YarnResourceManager's main thread,FLINK-15036,13271914,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,trohrmann,trohrmann,03/Dec/19 11:02,04/Dec/19 21:17,13/Jul/23 08:10,04/Dec/19 21:17,1.10.0,1.8.3,1.9.2,,,,,,,1.10.0,1.8.3,1.9.2,,Deployment / YARN,,,,,0,pull-request-available,,,,"With FLINK-13184, we replaced the {{NMClient}} with the {{NMClientAsync}}. As part of this change, container start up errors are now handled by a callback to {{NMClientAsync.CallbackHandler}}. The implementation of {{NMClientAsync.CallbackHandler#onStartContainerError}} will be called by the {{NMClientAsync}}. Since the implementation does state changing operations, it needs to happen inside of the {{YarnResourceManager}} main thread.",,hequn8128,SleePy,trohrmann,wangyang0918,,,,,,,,,,"wangyang0918 commented on pull request #10407: [FLINK-15036][yarn] Container startup error should be handled inside …
URL: https://github.com/apache/flink/pull/10407
 
 
   …of the YarnResourceManager's main thread
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   With [FLINK-13184](https://issues.apache.org/jira/browse/FLINK-15036), we replaced the `NMClient` with the `NMClientAsync`. As part of this change, container start up errors are now handled by a callback to NMClientAsync.CallbackHandler. The implementation of `NMClientAsync.CallbackHandler#onStartContainerError` will be called by the `NMClientAsync`. Since the implementation does state changing operations, it needs to happen inside of the YarnResourceManager main thread.
   
   
   ## Brief change log
   
   * Wrap all codes of `onStartContainerError` in `runAsync(() -> {xxxx})`
   
   
   ## Verifying this change
   
   * The change could be covered by unit test `YarnResourceManagerTest#testOnStartContainerError`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 01:07;githubbot;600","tillrohrmann commented on pull request #10407: [FLINK-15036][yarn] Container startup error should be handled inside …
URL: https://github.com/apache/flink/pull/10407
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 21:14;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 21:17:42 UTC 2019,,,,,,,,,,"0|z0989s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/19 11:02;trohrmann;FYI [~yangwang166];;;","03/Dec/19 13:45;wangyang0918;[~trohrmann] You are right. It should be handled in the main thread of {{YarnResourceManager}}. Otherwise, concurrent exceptions may happen. We could wrap all the codes of {{onStartContainerError}} into {{runAsync}} for a quick fix.;;;","04/Dec/19 01:30;hequn8128;Hi [~trohrmann][~fly_in_gis] The 1.8.3-rc2 is under voting. Is this issue a blocker for it? Or if we can leave the fix into the next release?;;;","04/Dec/19 01:48;wangyang0918;[~hequn8128] Even there's a very small probability of that happening, i think it is better merged to 1.8.3 if possible. Sorry for the inconvenience. 

I have attached a PR to fix this. [~trohrmann] Could you please take a look.;;;","04/Dec/19 10:23;trohrmann;I actually think that this issue should be a blocker. In particular since {{1.8.3}} will most likely be our last bug fix release for Flink {{1.8}}.;;;","04/Dec/19 10:26;hequn8128;[~trohrmann] [~fly_in_gis] OK, let's make the fix in 1.8.3. Thanks a lot for your suggestions. ;;;","04/Dec/19 21:17;trohrmann;Fixed via

1.10.0: 560726a796b0de86869bb63ecdbd980d3cdb6a32
1.9.2: 0071401f3dee67f45684d349864f94bf3cc6359a
1.8.3: 2f0e93fa4f1a534c994d2e7da810cb140fe88790;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Potential deadlock for bounded blocking ResultPartition.,FLINK-15030,13271877,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,03/Dec/19 07:14,14/Dec/19 16:45,13/Jul/23 08:10,14/Dec/19 16:45,1.9.0,1.9.1,,,,,,,,1.9.2,,,,Runtime / Network,,,,,0,pull-request-available,,,,"Currently, the BoundedBlockingSubpartition relies on the add of the next BufferConsumer to flush and recycle the previous one, which means we need at least (numsubpartition + 1) buffers to make the bounded blocking ResultPartition work. However, the ResultPartitionFactory gives only (numsubpartition) required buffers to the BoundedBlockingSubpartition which may lead to deadlock.

This problem exists only in version 1.9. In version 1.10 (master), this problem has been fixed by this commit: 2c8b4ef572f05bf4740b7e204af1e5e709cd945c.",,aljoscha,kevin.cyj,pnowojski,,,,,,,,,,,"wsry commented on pull request #10395: [FLINK-15030][runtime] Fix deadlock problem of blocking ResultPartition with minimum required buffers.
URL: https://github.com/apache/flink/pull/10395
 
 
   ## What is the purpose of the change
   Currently, BoundedBlockingSubpartition relies on the add of next BufferConsumer to flush and recycle the previous one, which means at least (numSubpartition + 1) buffers is needed to make the bounded blocking ResultPartition work. However, ResultPartitionFactory gives only (numSubpartition) required buffers to BufferPool of bounded blocking ResultPartition which may lead to deadlock problem. This PR tries to fix the problem by increasing the number of required buffers by 1.
   
   ## Brief change log
   
     - The number of required buffers of the BufferPool of ResultPartition is increased by one to (numSubpartition + 1).
     - A new test case is added to verify the change.
   
   
   ## Verifying this change
   
   A new test case ResultPartitionTest#testWriteToBlockingResultPartitionWithMinimumBuffers is added.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/19 10:15;githubbot;600","pnowojski commented on pull request #10395: [FLINK-15030][runtime] Fix deadlock problem of blocking ResultPartition with minimum required buffers.
URL: https://github.com/apache/flink/pull/10395
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Dec/19 16:45;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 14 16:45:47 UTC 2019,,,,,,,,,,"0|z0981k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/19 16:45;pnowojski;merged commit 1325727;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource directory not bundled in jar,FLINK-15025,13271862,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,03/Dec/19 05:37,04/Dec/19 13:33,13/Jul/23 08:10,04/Dec/19 13:33,1.10.0,,,,,,,,,1.10.0,,,,API / Python,Build System,,,,0,pull-request-available,,,,FLINK-14581 accidentally excluded the directory src/main/resources from the jar of the flink-python. This results into that the license files of the bundled third-party dependencies which are not of apache license are not bundled into the jar. The content of the NOTICE file defined in src/main/resources is also not included in the jar.,,dian.fu,,,,,,,,,,,,,"dianfu commented on pull request #10391: [FLINK-15025][python][legal] Fix the license and notice issue of flink-python module
URL: https://github.com/apache/flink/pull/10391
 
 
   
   ## What is the purpose of the change
   
   *The license files of the bundled third-party dependencies which are not of apache license are not bundled in the jar of flink-python and the NOTICE file is also incorrect. This PR tries to solve this issue*
   
   ## Brief change log
   
     - *Update the pom of flink-python to include src/main/resources into the jar
   
   ## Verifying this change
   
   This change is a license/notice file correction work without any test coverage. Have verified manually.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/19 05:48;githubbot;600","zentol commented on pull request #10391: [FLINK-15025][python][legal] Fix the license and notice issue of flink-python module
URL: https://github.com/apache/flink/pull/10391
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 13:32;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 13:33:08 UTC 2019,,,,,,,,,,"0|z097y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/19 13:33;chesnay;master: 7a37209818694d3dacd75d061f244630de4c35e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink (on YARN) sometimes needs too many slots,FLINK-15013,13271708,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,aljoscha,aljoscha,02/Dec/19 13:54,17/Dec/19 17:08,13/Jul/23 08:10,13/Dec/19 17:28,1.10.0,,,,,,,,,1.10.0,1.9.2,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"*THIS IS DIFFERENT FROM FLINK-15007, even though the text looks almost the same.*

This was discovered while debugging FLINK-14834. In some cases a Flink needs needs more slots to execute than expected. You can see this in some of the logs attached to FLINK-14834.

You can reproduce this using [https://github.com/aljoscha/docker-hadoop-cluster] to bring up a YARN cluster and then running a compiled Flink in there.

When you run
{code:java}
bin/flink run -m yarn-cluster -p 3 -yjm 1224 -ytm 1224 /root/DualInputWordCount.jar --input hdfs:///wc-in-1 --output hdfs:///wc-out && hdfs dfs -rm -r /wc-out
{code}
and check the logs afterwards you will sometimes see three ""Requesting new slot..."" statements and sometimes you will see four.

This is the {{git bisect}} log that identifies the first faulty commit ([https://github.com/apache/flink/commit/2ab8b61f2f22f1a1ce7f92cd6b8dd32d2c0c227d|https://github.com/apache/flink/commit/2ab8b61f2f22f1a1ce7f92cd6b8dd32d2c0c227d]):
{code:java}
git bisect start
# good: [09f2f43a1d73c76bf4d3f4a1205269eb860deb14] [FLINK-14154][ml] Add the class for multivariate Gaussian Distribution.
git bisect good 09f2f43a1d73c76bf4d3f4a1205269eb860deb14
# bad: [01d6972ab267807b8afccb09a45c454fa76d6c4b] [hotfix] Refactor out slots creation from the TaskSlotTable constructor
git bisect bad 01d6972ab267807b8afccb09a45c454fa76d6c4b
# bad: [7a61c582c7213f123e10de4fd11a13d96425fd77] [hotfix] Fix wrong Java doc comment of BroadcastStateBootstrapFunction.Context
git bisect bad 7a61c582c7213f123e10de4fd11a13d96425fd77
# good: [edeec8d7420185d1c49b2739827bd921d2c2d485] [hotfix][runtime] Replace all occurrences of letter to mail to unify wording of variables and documentation.
git bisect good edeec8d7420185d1c49b2739827bd921d2c2d485
# bad: [1b4ebce86b71d56f44185f1cb83d9a3b51de13df] [FLINK-14262][table-planner-blink] support referencing function with fully/partially qualified names in blink
git bisect bad 1b4ebce86b71d56f44185f1cb83d9a3b51de13df
# good: [25a3d9138cd5e39fc786315682586b75d8ac86ea] [hotfix] Move TaskManagerSlot to o.a.f.runtime.resourcemanager.slotmanager
git bisect good 25a3d9138cd5e39fc786315682586b75d8ac86ea
# good: [362d7670593adc2e4b20650c8854398727d8102b] [FLINK-12122] Calculate TaskExecutorUtilization when listing available slots
git bisect good 362d7670593adc2e4b20650c8854398727d8102b
# bad: [7e8218515baf630e668348a68ff051dfa49c90c3] [FLINK-13969][Checkpointing] Do not allow trigger new checkpoitn after stop the coordinator
git bisect bad 7e8218515baf630e668348a68ff051dfa49c90c3
# bad: [269e7f007e855c2bdedf8bad64ef13f516a608a6] [FLINK-12122] Choose SlotSelectionStrategy based on ClusterOptions#EVENLY_SPREAD_OUT_SLOTS_STRATEGY
git bisect bad 269e7f007e855c2bdedf8bad64ef13f516a608a6
# bad: [2ab8b61f2f22f1a1ce7f92cd6b8dd32d2c0c227d] [FLINK-12122] Add EvenlySpreadOutLocationPreferenceSlotSelectionStrategy
git bisect bad 2ab8b61f2f22f1a1ce7f92cd6b8dd32d2c0c227d
# first bad commit: [2ab8b61f2f22f1a1ce7f92cd6b8dd32d2c0c227d] [FLINK-12122] Add EvenlySpreadOutLocationPreferenceSlotSelectionStrategy
{code}

I'm using the streaming WordCount example that I modified to have two ""inputs"", similar to how the WordCount example is used in the YARN/kerberos/Docker test. Instead of using the input once we use it like this:
{code}
text = env.readTextFile(params.get(""input"")).union(env.readTextFile(params.get(""input"")));
{code}
to create two inputs from the same path. A jar is attached.",,aljoscha,gjy,klion26,maguowei,trohrmann,wangyang0918,zhuzh,,,,,,,"tillrohrmann commented on pull request #10555: [FLINK-15013] Fix non local slot selection and root slot resolution
URL: https://github.com/apache/flink/pull/10555
 
 
   ## What is the purpose of the change
   
   This PR fixes two problems related to Flink's scheduling:
   
   In order to prevent a race condition where Executions are scheduled because
   their inputs' locations have been assigned but the underlying root slot not
   being marked as resolved and, hence, not being available for location based
   scheduling, this commit enforces that we first resolve the root slot before
   completing the associated MultiTaskSlot.
   
   The LocationPreferenceSlotSelectionStrategy ignored NON_LOCAL slots due to initializing the initial
   candidate score to a positive value. As NON_LOCAL candidates have a value of 0, the initial candidate score needs to be negative, otherwise NON_LOCAL candidates will be ignored.
   
   ## Verifying this change
   
   - Added `LocationPreferenceSlotSelectionStrategyTest#returnsNonLocalMatchingIfResourceProfileCanBeFulfilledButNotTheTMLocationPreferences` and `SlotSharingManagerTest#shouldResolveRootSlotBeforeCompletingChildSlots`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 18:29;githubbot;600","tillrohrmann commented on pull request #10555: [FLINK-15013] Fix non local slot selection and root slot resolution
URL: https://github.com/apache/flink/pull/10555
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 17:26;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-12122,,FLINK-14834,,,,,,,,,,,,,"02/Dec/19 13:53;aljoscha;DualInputWordCount.jar;https://issues.apache.org/jira/secure/attachment/12987299/DualInputWordCount.jar",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 13 17:28:50 UTC 2019,,,,,,,,,,"0|z09700:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/19 06:37;zhuzh;Thanks for [~aljoscha] to find out the faulty commit.
The root cause should be this change in the faulty commit:

{code:java}
-		int bestCandidateScore = Integer.MIN_VALUE;
+		double bestCandidateScore = Double.MIN_VALUE;
{code}

Since Integer.MIN_VALUE is negative but Double.MIN_VALUE is a very small positive value. So that slot with 0 score (Locality==NON_LOCAL) will fail to match the slot request.
*change it to {{""double bestCandidateScore = -1.0;""}} should fix this issue.*

However, here comes another question: the {{NON_LOCAL}} locality should not happen in this case. Theoretically it should be able to find a slot of its upstream node which is {{LOCAL}}, since the task is scheduled when its upstream tasks are assigned slots, its parallelism is not larger, and all tasks are in the same slot sharing group. 
I did some experiment and find the cause, it's a bit complicated to explain.
1. To be simple, the root cause is that a slot offer for a shared slot completes the location of inner tasks before making the shared slot resolved with location. (due to unexpected {{CompletableFuture}} callback)
2. The task location completion triggers its downstream task scheduling, but the downstream task would fail to see resolved shared slot of its upstream task due to #1. So it make pick a random pending slot sharing group which is against the input locality constraint. (see  {{SchedulerImpl#allocateMultiTaskSlot}})
3. After #2 happened. A later scheduled task may find there is no pending slot sharing group, and the resolved slot sharing group cannot fulfill its locality constraint. Then it will request a new slot.

Looks to me the most simple way to fix it would be: Resolve shared slot locations when it's assigned a physical slot, before completing the location futures of tasks. So that the tasks would always be possible to find a resolved slot.

Not that this may not be a new issue since the problematic logics has been there for versions. Fixing it is not necessary to resolve this ticket, but can make the input locality constraint work as expected.
;;;","03/Dec/19 10:45;gjy;[~trohrmann] Do you have capacities to work on this?;;;","10/Dec/19 11:04;trohrmann;Thanks a lot for the analysis of the problem [~zhuzh]. I have capacities for working on this issue [~gjy].;;;","11/Dec/19 14:09;aljoscha;The issue seems to be fixed on master. I'm currently bisecting to identify the ""culprit"".
;;;","11/Dec/19 15:54;aljoscha;Scratch that, it seems to happen in roughly 20 % of cases.;;;","11/Dec/19 16:23;trohrmann;[~zhuzh] could you help me to understand the second problem. Are you saying that https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/SlotSharingManager.java#L172 will be executed after https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/SlotSharingManager.java#L692 so that there can be the situation that a {{SingleTaskSlot}} is completed and the underlying {{MultiTaskSlot}} has not been moved to {{SlotSharingManager#resolvedRootSlots}}? Maybe you could provide me with your test setup to reproduce the problem. I tried to write some tests but they passed so far.;;;","12/Dec/19 06:39;zhuzh;[~trohrmann] yes that's what I mean. And location preferences are not respected when fulfilling a slot request with {{unresolvedRootSlots}}. 
This [commit|https://github.com/zhuzhurk/flink/commit/464396283b96b670998d11c88cd102855c19c385] contains a test to reproduce the input locality issue.;;;","12/Dec/19 14:41;trohrmann;All right, let me look into the test case to see whether I can find the culprit.

I think it is ok to not respect input preferences if you falls back to using {{unresolvedRootSlots}}.;;;","12/Dec/19 15:35;zhuzh;Yes, it is ok to not respect input preferences if falling back to using unresolvedRootSlots.
In this case, what is unexpected is the falling back to unresolvedRootSlots. The cause, as discussed above, is that the slot offer first completes the SingleTaskSlot in a shared slot(MultiTaskSlot), before that shared slot is moved to resolvedRootSlots. The completed SingleTaskSlot also completes the input locality future, and triggers the scheduling and slot request of downstream tasks. The triggered slot request would not see the just fulfilled shared slot as resolved and so it would randomly pick an unresolved slot. In this way the input locality is broken.;;;","13/Dec/19 17:28;trohrmann;Fixed via

master:
632586f27487836d3b336aa90b1754f150359250
9493c3dbaab3d171ad6a478ec1e5280048143900

1.10.0:
e3171032252c9cb63f4c4698c81a5a6114fb24a6
7f4dc2cf32178babfb812832b7fd43bfd7fd12ed

1.9.2:
119b4e7089391a73b90cfc87ec5aa0c8fa7543a8
02755b973ccae3f9e756a6d82fe93435d1fb368c;;;",,,,,,,,,,,,,,,,,,,,,,
Temp directories flink-netty-shuffle-* are not cleaned up,FLINK-15010,13271697,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,nkruber,nkruber,02/Dec/19 12:44,06/Feb/20 12:57,13/Jul/23 08:10,04/Feb/20 04:07,1.9.0,1.9.1,1.9.2,,,,,,,1.10.0,1.9.3,,,Runtime / Network,,,,,1,pull-request-available,,,,Starting a Flink cluster with 2 TMs and stopping it again will leave 2 temporary directories (and not delete them): flink-netty-shuffle-<uid>,,aljoscha,gaoyunhaii,gjy,nkruber,pnowojski,zjwang,,,,,,,,"gaoyunhaii commented on pull request #10736: [FLINK-15010][Network] Add shutdown hook to ensure cleanup netty shuffle directories
URL: https://github.com/apache/flink/pull/10736
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   When shutting down the cluster in standalone mode, the task manager is shutdown by emit SIG_TERM signal. In this case, the directories are cleaned up by the shutdown hooks. However, `NettyShuffleEnvironment` does not register shutdown hook normally, which causes the `flink-netty-shuffle-*` directories not  cleaned finally.
   
   To fix this issue, we registered the corresponding shutdown hooks. To ensure the directories could be removed, the hook must be registered before creating the directories. Therefore, it is not suitable to register shutdown hook in `NettyShuffleEnvironment`. Instead, we chose to allow users to register shutdown hook for each `FileChannelManagerImpl`.   
   
   ## Brief change log
   
   - 1a6b51a57cbf23b054b3b20dd5dbb30a03b561ef registered shutdown hook for `FileChannelManagerImpl` used in `NettyShuffleEnvironment`. 
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Added test that validates that `flink-netty-shuffle-*` get cleared after task manager received SIG_TERM signals. 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Jan/20 15:45;githubbot;600","gaoyunhaii commented on pull request #11001: [FLINK-15010][Network] Add shutdown hook to ensure cleanup netty shuffle directories
URL: https://github.com/apache/flink/pull/11001
 
 
   
   ## What is the purpose of the change
   
   This PR picks [pull-10736](https://github.com/apache/flink/pull/10736) to release-1.9
   
   ## Brief change log
   
   
   ## Verifying this change
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/20 14:40;githubbot;600","gaoyunhaii commented on pull request #11002: [FLINK-15010][Network] Add shutdown hook to ensure cleanup netty shuffle directories
URL: https://github.com/apache/flink/pull/11002
 
 
   
   ## What is the purpose of the change
   
   This PR picks [pull-10736](https://github.com/apache/flink/pull/10736) to release-1.10.
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/20 14:44;githubbot;600","zhijiangW commented on pull request #10736: [FLINK-15010][Network] Add shutdown hook to ensure cleanup netty shuffle directories
URL: https://github.com/apache/flink/pull/10736
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/20 03:54;githubbot;600","zhijiangW commented on pull request #11001: [FLINK-15010][Network] Add shutdown hook to ensure cleanup netty shuffle directories
URL: https://github.com/apache/flink/pull/11001
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/20 04:01;githubbot;600","zhijiangW commented on pull request #11002: [FLINK-15010][Network] Add shutdown hook to ensure cleanup netty shuffle directories
URL: https://github.com/apache/flink/pull/11002
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Feb/20 04:04;githubbot;600",,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 05 17:31:20 UTC 2020,,,,,,,,,,"0|z096xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/19 09:18;zjwang;Hey [~NicoK], thanks for reporting this. I want to further confirm that what 's the mode for flink cluster, standalone / session ?  Or how can I re-produce this issue?;;;","17/Dec/19 12:37;nkruber;I used {{start-cluster.sh}} and added {{localhost}} twice into {{conf/slaves}};;;","18/Dec/19 05:26;zjwang;Thanks for the information and it is easy to re-produce this issue. I would assign to gaoyun for solving it.;;;","01/Jan/20 16:34;gaoyunhaii;The reason for this issue should be in standalone mode TaskManagers are shutdown by SIG_TERM signal, and the cleanup of directories rely on shutdown hooks, however, there are no shutdown hook registered for netty shuffle environment. 

An intuitive thought is to add shutdown hook directly for _NettyShuffleEnvironment_, however, it cannot ensure the directories get cleaned up in all cases, since the directories are created in the constructor of _FileChannelManagerImpl_, which comes before registering  shutdown hook in _NettyShuffleEnvironment's_ constructor_._ If __ task __ managers receive SIG_TERM between the two actions, the directories will not be cleaned. Therefore, the current PR enhance _FileChannelManagerImpl_ by allowing the callers to specify whether to register a shutdown hook for the manager, and the hook is registered before creating the directories. 

Besides, The above issue also exist for the existing _FileChannelManagerImpl_ usage in _IOManager_. If the current fix is acceptable, we might also fix the _IOManager_ case in similar way.;;;","04/Feb/20 04:07;zjwang;Merged in master: 5036334ca00405cd4cdd5a798dca012bb3cc7bbf

Merged in release-1.10: 57da4589e89672bcb042c8656b269e4faace1664

Merged in release-1.9: b8221b0758a2e0946f9ddd6ca537ce2a6e6a133a;;;","05/Feb/20 14:56;gjy;Is this an improvement or bug?;;;","05/Feb/20 17:31;gaoyunhaii;Hi [~gjy], I think this issue should be a bug, since the impact is that there may be directories leakage after starting and stoping standalone cluster.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Tests in flink-yarn-tests fail with ClassNotFoundException (JDK11),FLINK-15008,13271664,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,gjy,gjy,02/Dec/19 10:30,11/Dec/19 09:32,13/Jul/23 08:10,11/Dec/19 09:32,1.10.0,,,,,,,,,1.10.0,,,,Deployment / YARN,Tests,,,,0,jdk11,pull-request-available,,,"{noformat}
1) Error injecting constructor, java.lang.NoClassDefFoundError: javax/activation/DataSource
  at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver.<init>(JAXBContextResolver.java:41)
  at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:51)
  while locating org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver

1 error
	at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:987)
	at com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1013)
	at com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory$GuiceInstantiatedComponentProvider.getInstance(GuiceComponentProviderFactory.java:332)
	at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory$ManagedSingleton.<init>(IoCProviderFactory.java:179)
	at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory.wrap(IoCProviderFactory.java:100)
	at com.sun.jersey.core.spi.component.ioc.IoCProviderFactory._getComponentProvider(IoCProviderFactory.java:93)
	at com.sun.jersey.core.spi.component.ProviderFactory.getComponentProvider(ProviderFactory.java:153)
	at com.sun.jersey.core.spi.component.ProviderServices.getComponent(ProviderServices.java:251)
	at com.sun.jersey.core.spi.component.ProviderServices.getProviders(ProviderServices.java:148)
	at com.sun.jersey.core.spi.factory.ContextResolverFactory.init(ContextResolverFactory.java:83)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1271)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:169)
	at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:775)
	at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:771)
	at com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.initiate(WebApplicationImpl.java:771)
	at com.sun.jersey.guice.spi.container.servlet.GuiceContainer.initiate(GuiceContainer.java:121)
	at com.sun.jersey.spi.container.servlet.ServletContainer$InternalWebComponent.initiate(ServletContainer.java:318)
	at com.sun.jersey.spi.container.servlet.WebComponent.load(WebComponent.java:609)
	at com.sun.jersey.spi.container.servlet.WebComponent.init(WebComponent.java:210)
	at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:373)
	at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:710)
	at com.google.inject.servlet.FilterDefinition.init(FilterDefinition.java:114)
	at com.google.inject.servlet.ManagedFilterPipeline.initPipeline(ManagedFilterPipeline.java:98)
	at com.google.inject.servlet.GuiceFilter.init(GuiceFilter.java:172)
	at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)
	at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)
	at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)
	at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)
	at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)
	at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)
	at org.mortbay.jetty.Server.doStart(Server.java:224)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:936)
	... 50 more
Caused by: java.lang.NoClassDefFoundError: javax/activation/DataSource
	at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl.<clinit>(RuntimeBuiltinLeafInfoImpl.java:457)
	at com.sun.xml.bind.v2.model.impl.RuntimeTypeInfoSetImpl.<init>(RuntimeTypeInfoSetImpl.java:65)
	at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:133)
	at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:85)
	at com.sun.xml.bind.v2.model.impl.ModelBuilder.<init>(ModelBuilder.java:156)
	at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.<init>(RuntimeModelBuilder.java:93)
	at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getTypeInfoSet(JAXBContextImpl.java:473)
	at com.sun.xml.bind.v2.runtime.JAXBContextImpl.<init>(JAXBContextImpl.java:319)
	at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1170)
	at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:145)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:297)
	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:286)
	at javax.xml.bind.ContextFinder.find(ContextFinder.java:409)
	at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:721)
	at com.sun.jersey.api.json.JSONJAXBContext.<init>(JSONJAXBContext.java:246)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver.<init>(JAXBContextResolver.java:65)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver$$FastClassByGuice$$6a7be7f6.newInstance(<generated>)
	at com.google.inject.internal.cglib.reflect.$FastConstructor.newInstance(FastConstructor.java:40)
	at com.google.inject.internal.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:60)
	at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85)
	at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:254)
	at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)
	at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1031)
	at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)
	at com.google.inject.Scopes$1$1.get(Scopes.java:65)
	at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:40)
	at com.google.inject.internal.InjectorImpl$4$1.call(InjectorImpl.java:978)
	at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1024)
	at com.google.inject.internal.InjectorImpl$4.get(InjectorImpl.java:974)
	... 89 more
Caused by: java.lang.ClassNotFoundException: javax.activation.DataSource
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:583)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
	... 122 more
{noformat}

https://api.travis-ci.org/v3/job/619217945/log.txt",,dwysakowicz,gjy,liyu,yunta,,,,,,,,,,"Myasuka commented on pull request #10467: [FLINK-15008][tests] Fix ClassNotFoundException of flink-yarn-tests module under JDK11
URL: https://github.com/apache/flink/pull/10467
 
 
   ##  What is the purpose of the change
   
   Refer to [HADOOP-15775](https://issues.apache.org/jira/browse/HADOOP-15775) to add missing  javax.activation-api dependency to fix broken tests under JDK11
   
   ## Brief change log
   Add missing  javax.activation-api dependency.
   
   ## Verifying this change
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **yes**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 15:28;githubbot;600","zentol commented on pull request #10467: [FLINK-15008][tests] Fix ClassNotFoundException of flink-yarn-tests module under JDK11
URL: https://github.com/apache/flink/pull/10467
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 16:53;githubbot;600","zentol commented on pull request #10473: [FLINK-15008][build] Bundle javax.activation-api for Java 11
URL: https://github.com/apache/flink/pull/10473
 
 
   In FLINK-14905 we modified the build process to bundle `jaxb-api` in flink-dist.
   As part of that a dependencyManagement entry was added pinning the version of jaxb-api to 2.3.0 .
   Unfortunately 2.3.0 no longer declares a dependency on `javax.activation-api`; from what I gathered this version was released before Java 11 and hence assumed this dependency to be available anyway.
   
   Hence, this PR
   1) bumps the version of jaxb-api to 2.3.1 which again declares the activation-api dependency, which should prevent any issues caused by the lack of the transitive dependency
   2) bundle `javax.activation-api` in flink-dist similar to jaxb-api (i.e., java 11 exclusive)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 17:14;githubbot;600","zentol commented on pull request #10473: [FLINK-15008][build] Bundle javax.activation-api for Java 11
URL: https://github.com/apache/flink/pull/10473
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Dec/19 09:22;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,FLINK-15028,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 11 09:32:34 UTC 2019,,,,,,,,,,"0|z096q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/19 08:33;yunta;This is because the [removal of {{java.activation}}|https://docs.oracle.com/en/java/javase/11/migrate/index.html#JSMIG-GUID-F640FA9D-FB66-4D85-AD2B-D931174C09A3] in JDK11. We could manually add dependency below to fix this:
{code:java}
<dependency>
    <groupId>javax.activation</groupId>
    <artifactId>activation</artifactId>
    <version>1.1.1</version>
</dependency>
{code};;;","06/Dec/19 16:22;gjy;Do we know why it started failing only recently? Is it due to FLINK-14905? 

cc: [~chesnay];;;","06/Dec/19 16:41;chesnay;Yes, this is caused by FLINK-14905. [~yunta] I've assigned this issue to myself since I know what exactly must be changed.;;;","07/Dec/19 06:41;yunta;[~chesnay] It seems I have missed some context, and really glad to see you could help resolve it in a better way.;;;","10/Dec/19 08:15;dwysakowicz;Another instance: https://api.travis-ci.org/v3/job/622756873/log.txt (Also other profiles in this run);;;","11/Dec/19 09:32;chesnay;master:
886bf6fe815ffc3bb8b4e21f76e473fc0f081e7d
eddad99123525211c900102206384dacaf8385fc
1.10:
97899cf0a76e163820e1d32f9ba715e742315012 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Flink on YARN does not request required TaskExecutors in some cases,FLINK-15007,13271651,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,aljoscha,aljoscha,02/Dec/19 09:09,04/Dec/19 09:04,13/Jul/23 08:10,04/Dec/19 09:04,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Coordination,Runtime / Task,,,,0,,,,,"This was discovered while debugging FLINK-14834. In some cases Flink does not request new {{TaskExecutors}} even though new slots are requested. You can see this in some of the logs attached to FLINK-14834.

You can reproduce this using [https://github.com/aljoscha/docker-hadoop-cluster] to bring up a YARN cluster and then running a compiled Flink in there.

When you run
{code:java}
bin/flink run -m yarn-cluster -p 3 -yjm 1200 -ytm 1200 /root/DualInputWordCount.jar --input hdfs:///wc-in-1 --output hdfs:///wc-out && hdfs dfs -rm -r /wc-out
{code}
the job waits and eventually fails because it does not have enough slots. (You can see in the log that 3 new slots are requested but only 2 {{TaskExecutors}} are requested.

When you run
{code:java}
bin/flink run -m yarn-cluster -p 3 -yjm 1224 -ytm 1224 /root/DualInputWordCount.jar --input hdfs:///wc-in-1 --output hdfs:///wc-out && hdfs dfs -rm -r /wc-out
{code}
runs successfully.

This is the {{git bisect}} log that identifies the first faulty commit ([https://github.com/apache/flink/commit/9fed0ddc5bc015f98246a2d8d9adbe5fb2b91ba4|https://github.com/apache/flink/commit/9fed0ddc5bc015f98246a2d8d9adbe5fb2b91ba4]):
{code:java}
git bisect start
# good: [09f2f43a1d73c76bf4d3f4a1205269eb860deb14] [FLINK-14154][ml] Add the class for multivariate Gaussian Distribution.
git bisect good 09f2f43a1d73c76bf4d3f4a1205269eb860deb14
# bad: [85905f80e9711967711c2992612dccdd2cc211ac] [FLINK-14834][tests] Disable flaky yarn_kerberos_docker (default input) test
git bisect bad 85905f80e9711967711c2992612dccdd2cc211ac
# good: [c9c8a29b1b2e4f2886fba1524432f9788b564e61] [FLINK-14759][coordination] Remove unused class TaskManagerCliOptions
git bisect good c9c8a29b1b2e4f2886fba1524432f9788b564e61
# good: [c9c8a29b1b2e4f2886fba1524432f9788b564e61] [FLINK-14759][coordination] Remove unused class TaskManagerCliOptions
git bisect good c9c8a29b1b2e4f2886fba1524432f9788b564e61
# bad: [ae539c97c858b94e0e2504b54a8517ac1383482a] [hotfix][runtime] Check managed memory fraction range when setting it into StreamConfig
git bisect bad ae539c97c858b94e0e2504b54a8517ac1383482a
# good: [01d6972ab267807b8afccb09a45c454fa76d6c4b] [hotfix] Refactor out slots creation from the TaskSlotTable constructor
git bisect good 01d6972ab267807b8afccb09a45c454fa76d6c4b
# bad: [d32e1d00854e24bc4bb3aad6d866c2d709acd993] [FLINK-14594][core] Change Resource to use BigDecimal as its value
git bisect bad d32e1d00854e24bc4bb3aad6d866c2d709acd993
# bad: [25f87ec208a642283e995811d809632129ca289a] [FLINK-11935][table-planner] Fix cast timestamp/date to string to avoid Gregorian cutover
git bisect bad 25f87ec208a642283e995811d809632129ca289a
# bad: [21c6b85a6f5991aabcbcd41fedc860d662d478fb] [FLINK-14842][table] add logging for loaded modules and functions
git bisect bad 21c6b85a6f5991aabcbcd41fedc860d662d478fb
# bad: [48986aa0b89de731d1b9136b59d409933cc15408] [hotfix] Remove unnecessary comments about memory size calculation before network init
git bisect bad 48986aa0b89de731d1b9136b59d409933cc15408
# bad: [4c4652efa43ed8ab456f5f63c89b57d8c4a621f8] [hotfix] Remove unused number of slots in MemoryManager
git bisect bad 4c4652efa43ed8ab456f5f63c89b57d8c4a621f8
# bad: [9fed0ddc5bc015f98246a2d8d9adbe5fb2b91ba4] [FLINK-14400] Shrink the scope of MemoryManager from TaskExecutor to slot
git bisect bad 9fed0ddc5bc015f98246a2d8d9adbe5fb2b91ba4
# first bad commit: [9fed0ddc5bc015f98246a2d8d9adbe5fb2b91ba4] [FLINK-14400] Shrink the scope of MemoryManager from TaskExecutor to slot
{code}",,aljoscha,azagrebin,gjy,klion26,tison,wangyang0918,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14834,,,,,,,,,,,,,"02/Dec/19 10:43;aljoscha;DualInputWordCount.jar;https://issues.apache.org/jira/secure/attachment/12987281/DualInputWordCount.jar",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 09:04:27 UTC 2019,,,,,,,,,,"0|z096nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/19 09:14;tison;CC [~fly_in_gis] do you have some ideas?;;;","02/Dec/19 10:49;aljoscha;Forgot to attach the example. This is the streaming WordCount example that I modified to have two ""inputs"", similar to how the WordCount example is used in the YARN/kerberos/Docker test. Instead of using the input once we use it like this:
{code}
text = env.readTextFile(params.get(""input"")).union(env.readTextFile(params.get(""input"")));
{code}
to create two inputs from the same path.;;;","02/Dec/19 14:20;zhuzh;-Each TM contains 3 slots, and one TM should be able to carry the job. So the issue may be that why the RM requested 2 new TMs when only 3 new slots are requested.-
-Another issue is that why the 2nd requested TM cannot be fulfilled, yet I'm not sure if it's an expected cluster limitation.-
Sorry I made a mistake here. Just ignore it.;;;","03/Dec/19 07:18;xtsong;I think I find the cause of this issue.

I was looking at this [log|https://api.travis-ci.org/v3/job/614505046/log.txt] that [~gjy] reported in [FLINK-14834|https://issues.apache.org/jira/browse/FLINK-14834]. The corresponding commit is 4d28d3f783e48708ab1175b323aa9ed332cf207e.

According to the log, RM first received on slot request, and started a TM for that request. After the first TM is registered, RM received another 2 slot requests, this time it started only 1 TM. We have seen similar problems before.

When RM starts a TM, it also generates pending task slots according to the number of slots the new TM should have (in our case 1), so that RM can assign slot requests to pending slots before the TM is registered. Later when TM registers to RM, the registered actual slots will be mapped to the pending slots if they have the same profile, and slot requests assigned to the pending slot will be assigned to the corresponding actual slot (see {{SlotManagerImpl#registerSlot}}). If the profile of registered slot is different from the pending slot, the pending slot will not be consumed.

In our case, when the first request arrives, RM starts a new TM, generates a pending slot, and assign the request to the pending slot. When the TM is registered, the pending slot is not consumed because the slot profile doesn't match. Yet, the request will be assigned to the registered slot, and the pending slot becomes unassigned. Then the next 2 slot requests arrive, 1 of the request will be assigned to the pending slot, so RM only starts new TM for the other request. As a result, there's no pending TM and the slot request assigned to the pending slot will never be satisfied.

The reason that pending slot and registered slot may have different profile, is because memory sizes on RM / TM are calculated from configuration in different code paths, so the calculated results may suffer from slight rounding errors due to the fraction multiplying. Previously in 1.9, we solve this problem by explicitly overwriting managed memory size in the configuration to make sure RM / TM have the same managed memory size, which is the only value in slot profile the is actually used. That solves most of the problem. But we failed to discover the problem in 1.9 that there is a bytes -> megabytes -> bytes conversion of managed memory size on TM side, which may also introduce errors. In 1.9 all the fields of ResourceProfile are represented by MB, so the conversion should not cause problems. But recently we changed them to MemorySize that stores the bytes value, which uncovers the accuracy loss.

I think this also explains [~aljoscha]'s finding that not all the value of `taskmanager.heap.size` triggers the problem. Because different value of `taskmanager.heap.size` can result in different managed memory size, and not all managed memory sizes have accuracy loss in the conversions.

The problem should be fixed by FLIP-49, which will be merged soon. In FLIP-49 we calculated the memory sizes with exactly same code paths, to eliminate the differences between resource profiles calculated on RM / TM sides.;;;","03/Dec/19 10:07;wangyang0918;I think [~xintongsong] is right. I am developing the active Flink Kubernetes Integration and come across the same problem. Set different `taskmanager.heap.size` may lead to different result. The root cause is precision loss in TaskManager when convert to bytes value to mega bytes value.;;;","03/Dec/19 15:47;azagrebin;I checked, the problem resolves after merging FLIP-49: [https://github.com/apache/flink/pull/10161]
 It would be still nice to understand in detail where in source code the slot request was rejected for the newly launched TM
 [~xintongsong] could you post references in source code where it happens? and maybe where the calculations diverged;;;","03/Dec/19 16:11;xtsong;The matching between pending slots and registered slots happens in {{SlotManagerImpl#registerSlot}}, {{findExactlyMatchingPendingTaskManagerSlot}} to be specific. If the resource profile does not match, {{findExactlyMatchingPendingTaskManagerSlot}} will return {{null}}, thus {{pendingTaskManagerSlot}} will also be {{null}}, and the codes will not executed to the else branch below, where {{pendingTaskManagerSlot}} is removed from {{pendingSlots}}.

If you look at {{ActiveResourceManagerFactory#createActiveResourceManagerConfiguration}} in commit {{4d28d3f783e48708ab1175b323aa9ed332cf207e}} (or other commits with this problem), you will find that managed memory size is explicitly set into configuration in bytes. This value is later used in {{ResourceManager#createWorkerSlotProfiles}} for creating profiles of pending slots.

On the TM side, the explicitly configured managed memory size is first read in {{TaskManagerServicesConfiguration#fromConfiguration}}, by {{ConfigurationParserUtils#getManagedMemorySize}} which parses the bytes value but only returns the megabytes value. Then in {{TaskManagerServices#calculateMemorySizeByType}}, this megabytes value is converted back to bytes, which is used for calculating slot profile on the TM side in {{TaskManagerServices#computeSlotResourceProfile}}.

The conversion from bytes into megabytes ({{>> 20}}), then to bytes again ({{<< 20}}) leads to accuracy loss.;;;","03/Dec/19 16:25;azagrebin;Thanks for the explanation [~xintongsong], makes sense to me.
[~aljoscha] I think we can close this issue if you agree;;;","04/Dec/19 09:04;aljoscha;I checked this myself and it is in fact fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,
The digest of sub-plan reuse should contain retraction traits for stream physical nodes,FLINK-15001,13271592,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,02/Dec/19 02:55,16/Dec/19 12:02,13/Jul/23 08:10,14/Dec/19 03:28,1.9.0,1.9.1,,,,,,,,1.10.0,1.9.2,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"This bug is found in [FLINK-14946| https://issues.apache.org/jira/browse/FLINK-14946]:

The plan for the given sql in [FLINK-14946| https://issues.apache.org/jira/browse/FLINK-14946] is
 !image-2019-12-02-10-49-46-916.png! 

however, the plan after sub-plan reuse is:
 !image-2019-12-02-10-52-01-399.png! 

in the first picture, we could find that the accMode of two joins are different, but the two joins are reused in the second picture. 

The reason is the digest of sub-plan reuse does not contain retraction traits for stream physical nodes now.
",,godfreyhe,hequn8128,jark,,,,,,,,,,,"godfreyhe commented on pull request #10377: [FLINK-15001] [table-planner-blink] The digest of sub-plan reuse should contain retraction traits for stream physical nodes
URL: https://github.com/apache/flink/pull/10377
 
 
   
   ## What is the purpose of the change
   
   *currently, the digest of sub-plan reuse does not contain retraction traits for stream physical nodes now, which maybe cause two sub-plans be reused even if they have different retraction traits and the result is wrong. The pr aims to fix this issue.*
   
   
   ## Brief change log
   
     - *Add retraction traits to digest in RelDigestWriterImpl*
     - *refactor getDigest method and replace RelDigestWriterImpl with RelTreeWriterImpl*
   
   
   ## Verifying this change
   
   
   This change is already covered by existing tests, such as *DagOptimizationTest*.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Dec/19 08:08;githubbot;600","hequn8128 commented on pull request #10377: [FLINK-15001] [table-planner-blink] The digest of sub-plan reuse should contain retraction traits for stream physical nodes
URL: https://github.com/apache/flink/pull/10377
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 12:36;githubbot;600","godfreyhe commented on pull request #10561: [FLINK-15001] [table-planner-blink] The digest of sub-plan reuse should contain retraction traits for stream physical nodes
URL: https://github.com/apache/flink/pull/10561
 
 
   
   
   ## What is the purpose of the change
   
   *Backport https://github.com/apache/flink/pull/10377 to release-1.9*
   
   
   ## Brief change log
   none
   
   
   ## Verifying this change
   none
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no)**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no)**
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no)**
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 02:30;githubbot;600","hequn8128 commented on pull request #10561: [FLINK-15001] [table-planner-blink] The digest of sub-plan reuse should contain retraction traits for stream physical nodes
URL: https://github.com/apache/flink/pull/10561
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 12:02;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14946,,,,,,,,"02/Dec/19 02:49;godfreyhe;image-2019-12-02-10-49-46-916.png;https://issues.apache.org/jira/secure/attachment/12987259/image-2019-12-02-10-49-46-916.png","02/Dec/19 02:52;godfreyhe;image-2019-12-02-10-52-01-399.png;https://issues.apache.org/jira/secure/attachment/12987258/image-2019-12-02-10-52-01-399.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 14 03:27:47 UTC 2019,,,,,,,,,,"0|z096a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/19 02:58;godfreyhe;I would like to fix this;;;","12/Dec/19 12:38;hequn8128;Fixed in 
1.11.0 via ef0a033f428bcd2662fa7da64d015cb47e7cae81
1.10.0 via 2020f99486a6c8f57fa9a9453b335658b089f5b5;;;","12/Dec/19 14:07;hequn8128;Also need backport to release-1.9 branch;;;","14/Dec/19 03:27;hequn8128;Fixed in 1.9.2 via 9ea04933ee8294da757b5ebc73611cfc3f2a4915 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN IT Test Case log config is mistakenly disabled,FLINK-14982,13271143,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,28/Nov/19 07:20,20/May/20 08:41,13/Jul/23 08:10,19/May/20 16:37,1.10.0,,,,,,,,,,,,,Deployment / YARN,,,,,0,,,,,"The [FLINK-14630] Make the yarn APPLICATION_LOG_CONFIG_FILE an internal option changed how log config is shipped in YarnClusterDescritor. Currently, we need to rely on the yarn.log-config-file to specify which log file to ship in flink conf. But currently all YARN IT test cases haven't enabled it. It will cause the IT test to fail catch issue by looking into JM, TM log files.",,kkl0u,rmetzger,trohrmann,wangyang0918,ZhenqiuHuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 16:36:21 UTC 2020,,,,,,,,,,"0|z093ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Dec/19 18:28;rmetzger;Thanks for spotting. Assigning you.;;;","18/May/20 14:21;trohrmann;What is the state of this issue [~ZhenqiuHuang]?;;;","19/May/20 11:50;wangyang0918;[~trohrmann] IIRC, it is not a problem now. The JM/TM logs could output correctly. {{YarnTestUtils#createClusterDescriptorWithLogging}} will set the logging configuration. And you have a double check with that.;;;","19/May/20 13:18;trohrmann;I don't think that the issue has been fixed. It only works for the Yarn application mode. [~kkl0u] why are we calling {{YarnLogConfigUtil.setLogConfigFileInConfig}} only for the application mode and not in {{YarnClusterDescriptor.deployInternal}}?;;;","19/May/20 13:47;kkl0u;In all other cases, job submission goes through the {{FlinkYarnSessionCli}} and the same method will be called in the {{FlinkYarnSessionCli.applyDescriptorOptionToConfig()}}. It is true that it is better to move it to the {{deployInternal()}} though as this will fix the behaviour for the per-job and session when submitting through the {{ExecutorCLI}}.;;;","19/May/20 14:04;kkl0u;[~ZhenqiuHuang] are you planning to integrate [~trohrmann]'s suggestion now that you are working on it?;;;","19/May/20 14:18;trohrmann;[~kkl0u] is it always guaranteed that {{CliFrontend.getConfigurationDirectoryFromEnv()}} will work everywhere where {{YarnClusterDescriptor.deployApplicationCluster}} is called? This looks a bit brittle to me.;;;","19/May/20 14:20;trohrmann;At the moment I don't understand what exactly the problem is. Could you please exactly specify which test does not create logs [~ZhenqiuHuang]? I agree that the current way how the logging configuration is set is not optimal since it is spread over different classes but this is a different problem.;;;","19/May/20 14:21;trohrmann;Or do you want to make the Yarn tests independent of {{FLINK_HOME/conf/log4j.properties}}?;;;","19/May/20 14:31;kkl0u;[~trohrmann] the {{YarnClusterDescriptor.deployApplicationCluster}} is expected to be called at the client who submits the job. So the semantics are not any different from what is happening with other deployment modes.

The only problem I can find (although I have to investigate further) is that for other modes (apart from the Application Mode), we do not call the {{YarnLogConfigUtil.setLogConfigFileInConfig}} when we submit using the {{ExecutorCLI}}. And if this is the case, then this it is a bug and it has to be fixed.

Finally for the problem here, I also do not fully understand it.;;;","19/May/20 14:31;kkl0u;I would be up also for making the option visible and not internal anymore.;;;","19/May/20 15:50;wangyang0918;I think it is a very old problem and has been fixed in this commit[1]. Before this change, all the JM/TM logs in {{YarnITCase}} could not output correctly.

 

[1]. [https://github.com/apache/flink/pull/10550/commits/86ab4706488b98bd95cfd337e7752404f0bce95e];;;","19/May/20 16:36;ZhenqiuHuang;[~fly_in_gis] [~trohrmann]

Sorry for the late reply. Yes, it is fixed already. We can close the ticket now.;;;",,,,,,,,,,,,,,,,,,,
"Cassandra Connector leaks Semaphore on Throwable; hangs on close",FLINK-14976,13270990,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mchro,mchro,mchro,27/Nov/19 13:21,29/Jan/20 03:48,13/Jul/23 08:10,29/Nov/19 18:55,1.10.0,1.8.2,1.9.1,,,,,,,1.10.0,1.8.3,1.9.2,,Connectors / Cassandra,,,,,0,pull-request-available,,,,"This issue was mostly fixed in FLINK-13059; unfortunately, the fix only caught {{Exception}} so any non-{{Exception Throwable}} can still cause the issue of leaking semaphores.",,azagrebin,mchro,,,,,,,,,,,,"mchro commented on pull request #10339: [FLINK-14976][cassandra] Release semaphore on all Throwable's in send()
URL: https://github.com/apache/flink/pull/10339
 
 
   ## What is the purpose of the change
   
   https://github.com/apache/flink/pull/8967 changed the Cassandra connector to catch `Exception` and release the semaphore correctly, however this is not general enough as e.g. `Error` can still lead to deadlock. Change code to catch all `Throwable` instead.
   
   
   ## Brief change log
   
     - Release Semaphore correctly on all Throwable in send()
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - Added unit test that validates semaphore released if `Error` in send(); test fails before change.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Nov/19 14:07;githubbot;600","azagrebin commented on pull request #10339: [FLINK-14976][cassandra] Release semaphore on all Throwable's in send()
URL: https://github.com/apache/flink/pull/10339
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Nov/19 17:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 29 18:55:01 UTC 2019,,,,,,,,,,"0|z092kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/19 18:55;azagrebin;merged into master by 07b66b60dd8c72f5ecd98054fa322d768c774e74

merged into 1.9 by 6039e11c1cad20fe3468715ff594a49cbdc8d95e

merged into 1.8 by 8747e3d4ec29394fa65e875b9da68b2af863f92a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OSS filesystem does not relocate many dependencies,FLINK-14973,13270958,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,plucas,plucas,27/Nov/19 11:16,23/Mar/21 12:16,13/Jul/23 08:10,23/Mar/21 12:16,1.9.1,,,,,,,,,1.10.0,,,,FileSystems,,,,,0,,,,,"Whereas the Azure and S3 Hadoop filesystem jars relocate all of their depdendencies:

{noformat}
$ jar tf opt/flink-azure-fs-hadoop-1.9.1.jar | grep -v '^org/apache/fs/shaded/' | grep -v '^META-INF' | grep '/' | cut -f -2 -d / | sort | uniq
org/
org/apache

$ jar tf opt/flink-s3-fs-hadoop-1.9.1.jar | grep -v '^org/apache/fs/shaded/' | grep -v '^META-INF' | grep '/' | cut -f -2 -d / | sort | uniq
org/
org/apache
{noformat}

The OSS Hadoop filesystem leaves many things un-relocated:

{noformat}
$ jar tf opt/flink-oss-fs-hadoop-1.9.1.jar | grep -v '^org/apache/fs/shaded/' | grep -v '^META-INF' | grep '/' | cut -f -2 -d / | sort | uniq
assets/
assets/org
avro/
avro/shaded
com/
com/ctc
com/fasterxml
com/google
com/jcraft
com/nimbusds
com/sun
com/thoughtworks
javax/
javax/activation
javax/el
javax/servlet
javax/ws
javax/xml
jersey/
jersey/repackaged
licenses/
licenses/LICENSE.asm
licenses/LICENSE.cddlv1.0
licenses/LICENSE.cddlv1.1
licenses/LICENSE.jdom
licenses/LICENSE.jzlib
licenses/LICENSE.paranamer
licenses/LICENSE.protobuf
licenses/LICENSE.re2j
licenses/LICENSE.stax2api
net/
net/jcip
net/minidev
org/
org/apache
org/codehaus
org/eclipse
org/jdom
org/objectweb
org/tukaani
org/xerial
{noformat}

The first symptom of this I ran into was that Flink is unable to restore from a savepoint if both the OSS and Azure Hadoop filesystems are on the classpath, but I assume this has the potential to cause further problems, at least until more progress is made on the module/classloading front.

h3. Steps to reproduce

# Copy both the Azure and OSS Hadoop filesystem JARs from opt/ into lib/
# Run a job that restores from a savepoint (the savepoint might need to be stored on OSS)
# See a crash and traceback like:

{noformat}
2019-11-26 15:59:25,318 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Fatal error occurred in the cluster entrypoint.
org.apache.flink.runtime.dispatcher.DispatcherException: Failed to take leadership with session id 00000000-0000-0000-0000-000000000000.
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$null$30(Dispatcher.java:915) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_232]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_232]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_232]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_232]
	at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:691) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_232]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_232]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_232]
	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575) ~[?:1.8.0_232]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:753) ~[?:1.8.0_232]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_232]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.actor.Actor.aroundReceive(Actor.scala:517) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.actor.Actor.aroundReceive$(Actor.scala:515) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
	at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_232]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	... 4 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
	at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:152) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:83) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:375) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_232]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	... 4 more
Caused by: java.lang.NoSuchMethodError: org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(Lcom/google/common/util/concurrent/ListeningExecutorService;IZ)V
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem.open(AliyunOSSFileSystem.java:570) ~[flink-oss-fs-hadoop-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem.open(FileSystem.java:950) ~[flink-azure-fs-hadoop-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.open(HadoopFileSystem.java:120) ~[flink-oss-fs-hadoop-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.open(HadoopFileSystem.java:37) ~[flink-oss-fs-hadoop-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:68) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.checkpoint.Checkpoints.loadAndValidateCheckpoint(Checkpoints.java:141) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1132) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.scheduler.LegacyScheduler.tryRestoreExecutionGraphFromSavepoint(LegacyScheduler.java:237) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.scheduler.LegacyScheduler.createAndRestoreExecutionGraph(LegacyScheduler.java:196) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.scheduler.LegacyScheduler.<init>(LegacyScheduler.java:176) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.scheduler.LegacySchedulerFactory.createInstance(LegacySchedulerFactory.java:70) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:275) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:265) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:146) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:83) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:375) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_232]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) ~[flink-dist_2.12-1.9.1-stream2.jar:1.9.1-stream2]
	... 4 more
{noformat}",,Eric Lee,plucas,pnowojski,uce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14930,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 19 08:11:26 UTC 2021,,,,,,,,,,"0|z092dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/19 03:57;Eric Lee;Hi, [~plucas], I think this problem is caused by the chaos of the management of flink file system.

All flink systems are all based on the Hadoop file system. But in these submodules' maven settings, all of them use shade plugin to relocate the package `org.apache.hadoop` but somehow some of them ignore the dependencies of Hadoop such as `com.google.common` and oss file system even doesn't relocate it.

As a result, when oss and azure package are both added into the classpath and flink is going to restore from a savepoint in oss, it will initialize `com.google.common.xx.ListeningExecutorService` object and then try to find class `org.apache.flink.fs.shaded.hadoop3.org.apache.commons.SemaphoredDelegatingExecutor` which exists both in oss and azure jar package as they follow the same relocation rules. Then, JVM will hold the class SemaphoredDelegatingExecutor's definition in the azure jar cause the classloader will firstly load an azure package.

Due to different relocation rules, in the azure package, SemaphoredDelegatingExecutor holds the definition of `org.apache.flink.fs.{color:#FF0000}azure{color}.shaded.com.google.common.xx.ListeningExecutorService` rather than `com.google.common.xx.ListeningExecutorService`.

So after oss try to use `com.google.common.xx.ListeningExecutorService` to new instance of `SemaphoredDelegatingExecutor(org.apache.flink.fs.{color:#FF0000}azure{color}.shaded.com.google.common.xx.ListeningExecutorService)`, JVM will encounter conflict and throw failure.

 
----

My suggestion:
      Maybe writing a README specifying all the rules the sub filesystems need to follow in the flink filesystem is needed and also a common relocation rule.;;;","05/Dec/19 07:17;pnowojski;I think this issue could be workaround now by loading the azure and oss file systems (or at least one of them) [as plugins|https://ci.apache.org/projects/flink/flink-docs-stable/ops/filesystems/#pluggable-file-systems]. Because of that, unless there is some particular issue with those file systems that they can not work as plugins, I would be inclined to close this as won't fix. Note also that as we gain coincidence that some particular Filesystem is working as plugin, we will be removing shading from it anyway.

If someone will be trying to out to solve this by using plugins, please note that plugins handling in yarn clusters [is or will be fixed in 1.9.2 or 1.10.0 release|https://issues.apache.org/jira/browse/FLINK-14382];;;","09/Dec/19 01:48;Eric Lee;Hi [~pnowojski], Thanks for your comments. It seems using the plugin mechanism is much more rational. I will take a try later.;;;","19/Mar/21 08:11;chesnay;Solved by using the plugin mechanism.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
CatalogTableStatistics UNKNOWN should be consistent with TableStats UNKNOWN,FLINK-14965,13270900,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,Terry1897,Terry1897,27/Nov/19 07:38,06/Dec/19 01:40,13/Jul/23 08:10,06/Dec/19 01:40,1.10.0,,,,,,,,,1.10.0,,,,,,,,,0,pull-request-available,,,,"UNKNOWN stats in
` org.apache.flink.table.catalog.stats

public class CatalogTableStatistics {
	public static final CatalogTableStatistics UNKNOWN = new CatalogTableStatistics(0, 0, 0, 0);
`
and 
`
org.apache.flink.table.plan.stats
public final class TableStats {
	public static final TableStats UNKNOWN = new TableStats(-1, new HashMap<>());
`
are not consistent which will cause some cbo unexpect behavior

",,godfreyhe,Terry1897,,,,,,,,,,,,"godfreyhe commented on pull request #10429: [FLINK-14965] [table-planner-blink] correct the logic of convertToTableStats method in CatalogTableStatisticsConverter
URL: https://github.com/apache/flink/pull/10429
 
 
   
   
   ## What is the purpose of the change
   
   *[FLINK-14662](https://issues.apache.org/jira/browse/FLINK-14662) had let CatalogTableStatistics.UNKNOWN be consistent with TableStats.UNKNOWN.
   however, the logic in CatalogTableStatisticsConverter#convertToTableStats method is also not correct:
   1. HiveCatalog will create a new instance even if all attributes in CatalogTableStatistics is unknown instead of using CatalogTableStatistics.UNKNOWN. And CatalogTableStatistics does not override equals method. So tableStatistics.equals(CatalogTableStatistics.UNKNOWN) is always false.
   2. similar logic about equals on CatalogColumnStatistics
   3. even if tableStatistics is null or tableStatistics is CatalogTableStatistics.UNKNOWN, the columnStatistics may be not UNKNOWN. It also needs to be convert to column stats.
   
   the pr mainly fixes the above incorrect logic.*
   
   
   ## Brief change log
   
     - *correct the logic of convertToTableStats method in CatalogTableStatisticsConverter*
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added tests for unknown row count with known column stats*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no)**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no)**
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no)**
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 07:35;githubbot;600","KurtYoung commented on pull request #10429: [FLINK-14965] [table-planner-blink] correct the logic of convertToTableStats method in CatalogTableStatisticsConverter
URL: https://github.com/apache/flink/pull/10429
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 01:39;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 01:40:06 UTC 2019,,,,,,,,,,"0|z0920g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/19 07:40;Terry1897;HiveStatsUtil.DEFAULT_STATS_ZERO_CONST should also be modified.;;;","27/Nov/19 07:40;Terry1897;cc [~godfreyhe];;;","27/Nov/19 08:26;godfreyhe;I would like to fix this;;;","05/Dec/19 07:18;godfreyhe;[FLINK-14662|https://issues.apache.org/jira/browse/FLINK-14662] had let CatalogTableStatistics.UNKNOWN be consistent with TableStats.UNKNOWN. 
however, the logic in {{CatalogTableStatisticsConverter#convertToTableStats}} method is also not correct: 
1. {{HiveCatalog}} will create a new instance even if all attributes in {{CatalogTableStatistics}} is unknown instead of using {{CatalogTableStatistics.UNKNOWN}}. And {{CatalogTableStatistics}} does not override {{equals}} method. So {{tableStatistics.equals(CatalogTableStatistics.UNKNOWN)}} is always false.
2. similar logic about {{equals}} on {{CatalogColumnStatistics}}
3. even if tableStatistics is null or tableStatistics is {{CatalogTableStatistics.UNKNOWN}}, the {{columnStatistics}} may be not UNKNOWN. It also needs to be convert to column stats.

the issue mainly fixes the above incorrect logic.;;;","06/Dec/19 01:40;ykt836;master: c20697e149f6fd97b6514c870a3b3f19de145421;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Dependency shading of table modules test fails on Travis,FLINK-14960,13270862,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,TsReaper,TsReaper,27/Nov/19 02:27,29/Jan/20 08:58,13/Jul/23 08:10,04/Dec/19 18:10,1.10.0,1.9.1,,,,,,,,1.10.0,1.9.2,,,Table SQL / Planner,Tests,,,,0,pull-request-available,test-stability,,,"e2e - misc cron job fails on Travis. The messages are as follows:

{code}
==============================================================================
Running 'Dependency shading of table modules test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-41270732894
Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT
Success: There are no unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-api-java/target/flink-table-api-java-1.10-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-api-scala/target/flink-table-api-scala_2.11-1.10-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-api-java-bridge/target/flink-table-api-java-bridge_2.11-1.10-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-api-scala-bridge/target/flink-table-api-scala-bridge_2.11-1.10-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-planner/target/flink-table-planner_2.11-1.10-SNAPSHOT.jar jar.
Failure: There are unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-planner-blink/target/flink-table-planner-blink_2.11-1.10-SNAPSHOT.jar jar:       -> com.esotericsoftware.kryo                          not found
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/log/*.out: No such file or directory
No non-empty .out files.

[FAIL] 'Dependency shading of table modules test' failed after 0 minutes and 30 seconds! Test exited with exit code 1
{code}

See https://api.travis-ci.org/v3/job/617187809/log.txt for full message",,aljoscha,dwysakowicz,gjy,jark,liyu,sewen,TsReaper,,,,,,,"wuchong commented on pull request #10414: [FLINK-14960][e2e] Fix failed dependency shading test of table modules
URL: https://github.com/apache/flink/pull/10414
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The `com.esotericsoftware.kryo.Serializer` import in `DummyStreamExecutionEnvironment` fails the `test_table_shaded_dependencies.sh`. As discussed in the JIRA, it's safe to add it to the whilelist of `checkCodeDependencies()` method as it is from flink-core. 
   
   ## Brief change log
   
   - Exclude `com.esotericsoftware.kryo` in `checkCodeDependencies()`
   - Add note in `DummyStreamExecutionEnvironment` to remove the check back when removing `DummyStreamExecutionEnvironment`.
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 07:26;githubbot;600","dawidwys commented on pull request #10414: [FLINK-14960][e2e] Fix failed dependency shading test of table modules
URL: https://github.com/apache/flink/pull/10414
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 18:07;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-14988,FLINK-15029,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 18:10:16 UTC 2019,,,,,,,,,,"0|z091s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/19 08:08;dwysakowicz;[~jark] [~ykt836] Could you check this?;;;","29/Nov/19 08:25;gjy;Another instance: https://api.travis-ci.org/v3/job/618179297/log.txt;;;","30/Nov/19 08:24;jark;Hi [~dwysakowicz], I looked into this issue. I think the reason is the {{DummyStreamExecutionEnvironment}} we introduced in FLINK-13708 has {{com.esotericsoftware.kryo.Serializer}} dependency in imports. However, {{test_table_shaded_dependencies.sh}} will check the code dependcies and think kryo is not in the allowed list.

One solution is add kryo to the {{grep -v}} list of jdeps, because it is flink-core dependency.;;;","03/Dec/19 18:22;sewen;[~jark] If this is a new dependency that was not there before, is it correct to remove it from the test check?

Should it maybe not be packaged instead?;;;","03/Dec/19 19:17;dwysakowicz;I think we can add it to the whitelist in {{checkCodeDependencies}} method. This method checks that we do not use unwanted classes. 
There is a second method {{checkAllowedPackages}} that checks actual classes in the packaged jar. We should not add an exclusion there.

[~sewen] As Jark said this code usage comes from the {{flink-core}}, which does not relocate this package. The {{flink-core}} is in a {{provided}} scope. We did not add any new pacakged dependencies. I think it is safe to add that exclusion.
We should probably though add a note to the {{DummyStreamExecutionEnvironment}} to remove that check back when we decide to remove it.;;;","04/Dec/19 05:27;jark;Thanks [~dwysakowicz], I will open a pull request soon. ;;;","04/Dec/19 18:10;dwysakowicz;Fixed in:
master: 2cad7434973941ea738cd8ecaa4e90661810a833
1.9.2: c388c268635ed02df96ef3440ef0dbc4fab9401b;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Parquet table source should use schema type to build FilterPredicate,FLINK-14953,13270748,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,26/Nov/19 16:02,11/Dec/19 06:46,13/Jul/23 08:10,10/Dec/19 09:18,1.8.0,1.8.2,1.9.0,1.9.1,,,,,,1.10.0,1.9.2,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,"The issue happens when the data type of value in predicate inferred from SQL doesn't match the parquet schema. For example, foo is a long type, foo < 1 is the predicate. Literal will be recognized as an integration. It causes the parquet FilterPredicate is mistakenly created for the column of Integer type. Then, the exception comes.

java.lang.UnsupportedOperationException
	at org.apache.parquet.filter2.recordlevel.IncrementallyUpdatedFilterPredicate$ValueInspector.update(IncrementallyUpdatedFilterPredicate.java:71)
	at org.apache.parquet.filter2.recordlevel.FilteringPrimitiveConverter.addLong(FilteringPrimitiveConverter.java:105)
	at org.apache.parquet.column.impl.ColumnReaderImpl$2$4.writeValue(ColumnReaderImpl.java:268)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:367)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.flink.formats.parquet.utils.ParquetRecordReader.readNextRecord(ParquetRecordReader.java:235)
	at org.apache.flink.formats.parquet.utils.ParquetRecordReader.reachEnd(ParquetRecordReader.java:207)
	at org.apache.flink.formats.parquet.ParquetInputFormat.reachedEnd(ParquetInputFormat.java:233)
	at org.apache.flink.api.common.operators.GenericDataSourceBase.executeOnCollections(GenericDataSourceBase.java:231)
	at org.apache.flink.api.common.operators.CollectionExecutor.executeDataSource(CollectionExecutor.java:219)
	at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:155)
	at org.apache.flink.api.common.operators.CollectionExecutor.executeUnaryOperator(CollectionExecutor.java:229)
	at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:149)
	at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:131)
	at org.apache.flink.api.common.operators.CollectionExecutor.executeDataSink(CollectionExecutor.java:182)
	at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:158)
	at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:131)
	at org.apache.flink.api.common.operators.CollectionExecutor.execute(CollectionExecutor.java:115)
	at org.apache.flink.api.java.CollectionEnvironment.execute(CollectionEnvironment.java:38)
	at org.apache.flink.test.util.CollectionTestEnvironment.execute(CollectionTestEnvironment.java:52)
	at org.apache.flink.test.util.CollectionTestEnvironment.execute(CollectionTestEnvironment.java:47)
	at org.apache.flink.api.java.DataSet.collect(DataSet.java:413)
	at org.apache.flink.formats.parquet.ParquetTableSourceITCase.testScanWithProjectionAndFilter(ParquetTableSourceITCase.java:91)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)",,dian.fu,lzljs3620320,ZhenqiuHuang,,,,,,,,,,,"KurtYoung commented on pull request #10371: [FLINK-14953][formats] use table type to build parquet FilterPredicate
URL: https://github.com/apache/flink/pull/10371
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Dec/19 06:26;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 11 06:46:08 UTC 2019,,,,,,,,,,"0|z0912o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/19 02:44;ZhenqiuHuang;Created pull request. https://github.com/apache/flink/pull/10371;;;","03/Dec/19 15:37;ZhenqiuHuang;[~danny0405] Would you please help to review this PR?
https://github.com/apache/flink/pull/10371;;;","10/Dec/19 05:13;ZhenqiuHuang;[~ykt836] Would you please help to merge this PR https://github.com/apache/flink/pull/10371?;;;","10/Dec/19 09:18;ykt836;1.10.0: b6801d50ade388b28d349711fdf75e9f75b562da

1.9.2: f65be9b37f0405b1605ecec533614ff2aab3cd52;;;","11/Dec/19 06:24;dian.fu;Hi [~ykt836], the 1.10 release branch has been cut yesterday and it seems that this PR was not cherry-pick to 1.10 branch.;;;","11/Dec/19 06:33;lzljs3620320;[~dian.fu] I can find in 1.10 branch: [FLINK-14953][formats] use table type to build parquet FilterPredicate;;;","11/Dec/19 06:46;dian.fu;[~lzljs3620320] You're right. I missed it. Thanks for your information! (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,
Yarn containers can exceed physical memory limits when using BoundedBlockingSubpartition.,FLINK-14952,13270721,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zjwang,pnowojski,pnowojski,26/Nov/19 13:40,13/Dec/19 04:07,13/Jul/23 08:10,13/Dec/19 04:07,1.9.1,,,,,,,,,1.10.0,,,,Deployment / YARN,Runtime / Network,,,,0,pull-request-available,,,,"As [reported by a user on the user mailing list|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/CoGroup-SortMerger-performance-degradation-from-1-6-4-1-9-1-td31082.html], combination of using {{BoundedBlockingSubpartition}} with yarn containers can cause yarn container to exceed memory limits.
{quote}2019-11-19 12:49:23,068 INFO org.apache.flink.yarn.YarnResourceManager - Closing TaskExecutor connection container_e42_1574076744505_9444_01_000004 because: Container [pid=42774,containerID=container_e42_1574076744505_9444_01_000004] is running beyond physical memory limits. Current usage: 12.0 GB of 12 GB physical memory used; 13.9 GB of 25.2 GB virtual memory used. Killing container.
{quote}
This is probably happening because memory usage of mmap is not capped and not accounted by configured memory limits, however yarn is tracking this memory usage and once Flink exceeds some threshold, container is being killed.

Workaround is to overrule default value and force Flink to not user mmap, by setting a secret (🤫) config option:
{noformat}
taskmanager.network.bounded-blocking-subpartition-type: file
{noformat}",,azagrebin,gjy,kevin.cyj,kisimple,klion26,pnowojski,wangyang0918,wind_ljy,zhuzh,zjwang,,,,"zhijiangW commented on pull request #10539: [FLINK-14952][network] Solve the issue of exceeding memory limits when using mmap blocking partition
URL: https://github.com/apache/flink/pull/10539
 
 
   ## What is the purpose of the change
   
   *As reported by users in mail list, the container would be killed by yarn because of exceeding physical memory limits. It is mainly because the memory usage of mmap in `BoundedBlockingSubpartition` is not capped and not accounted by configured memory limits, but yarn would track this memory usage and kill the container once exceeding some threshold.*
       
   *To solve this issue, we adjust the default option of `taskmanager.network.bounded-blocking-subpartition-type` from `auto` to `file` for safety usage. And users can still enable mmap way via configuration if they make a proper adjustment in yarn configuration to avoid container killing.*
   
   ## Brief change log
   
     - *Generates the respective html files for previous changed configurations*
     - *Adjusts the default option of blocking subpartition type from `auto` to `file`*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**yes** / no)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 05:40;githubbot;600","zhijiangW commented on pull request #10539: [FLINK-14952][network] Solve the issue of exceeding memory limits when using mmap blocking partition
URL: https://github.com/apache/flink/pull/10539
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 15:16;githubbot;600","zhijiangW commented on pull request #10554: [FLINK-14952][network] Solve the issue of exceeding memory limits when using mmap blocking partition
URL: https://github.com/apache/flink/pull/10554
 
 
   ## What is the purpose of the change
   
   backport to release-1.10
   
   ## Brief change log
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 15:33;githubbot;600","zhijiangW commented on pull request #10557: [FLINK-14952][network] Solve the issue of exceeding memory limits when using mmap blocking partition
URL: https://github.com/apache/flink/pull/10557
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 19:23;githubbot;600","zhijiangW commented on pull request #10557: [FLINK-14952][network] Solve the issue of exceeding memory limits when using mmap blocking partition
URL: https://github.com/apache/flink/pull/10557
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 01:50;githubbot;600","zhijiangW commented on pull request #10554: [FLINK-14952][network] Solve the issue of exceeding memory limits when using mmap blocking partition
URL: https://github.com/apache/flink/pull/10554
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 03:47;githubbot;600",,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,FLINK-12070,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 13 04:07:02 UTC 2019,,,,,,,,,,"0|z090wo:",9223372036854775807,"This changes the option key and default value for the type of BoundedBlockingSubpartition in batch jobs. 

The previous key `taskmanager.network.bounded-blocking-subpartition-type` was changed to `taskmanager.network.blocking-shuffle.type` now. 

And the respective option default value was also changed from `auto` to `file` for avoiding yarn killing task manager container when memory usage of mmap exceeds some threshold.",,,,,,,,,,,,,,,,,,,"26/Nov/19 15:21;chesnay;would it be sufficient to document the config option? [~azagrebin] will the new memory management handle this in a better way?;;;","26/Nov/19 15:39;azagrebin;At the moment, we do not explicitly account for this memory usage in our new memory model for the Flink process, except that  in general this kind of things can be accounted for in a config option for the framework off-heap abs memory ([https://jira.apache.org/jira/browse/FLINK-14637]).
it would be nice to understand how to manage this memory usage by mmap firstly to account for it in some better way.;;;","27/Nov/19 02:36;wangyang0918;[~pnowojski]

FYI, all the memory used by Flink TaskManager should be counted and allocate from Yarn ResourceManager. Otherwise, it may be killed by Yarn NodeManager. The NodeManager tracks the memory usage of the TaskManager process tree. It is usually rss mem, read the value from `/proc/\{pid}/stat'. Yarn NodeManager does not use cgroup to set the memory limit, it starts a `ContainersMonitor` thread to monitor the memory usage of each container. If it overused, the container will be killed by NodeManager and get the log ""is running beyond physical memory limits"".;;;","27/Nov/19 07:23;pnowojski;What options do we have? I think there is no way without cgroups to control the memory usage of the mmap, *or is there*? If not, we have only the following options:

# Disallow mmap for yarn
# Disable mmap memory accounting/container killing in yarn 

I think there are some options to achieve the 2., but probably require user to change yarn configuration? So maybe the default should be to not use mmap for yarn, but document how to enable it? ;;;","27/Nov/19 11:01;kevin.cyj;Can we manage the memory mapped region and restrict its size? For example, we can implement a rolling map and release the previous region before mapping next region and restrict the total size of mmapped memory can be used by a single TM.;;;","27/Nov/19 11:36;wangyang0918;[~pnowojski] I think it is not easy to disable mmap memory accounting in yarn. Even we set the `yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=true`, the NodeManager calculates mmaped pages as consumed. So there is no way to disable mmap memory accounting. If users want to disable container killing, `yarn.nodemanager.pmem-check-enabled=false` could be used. 

 

[~kevin.cyj] If we can control the memory used by mmap, i think it is better solution. The memory for mmap should be allocated from Yarn to avoid killing.

 ;;;","27/Nov/19 13:51;pnowojski;{quote}For example, we can implement a rolling map and release the previous region before mapping next region and restrict the total size of mmapped memory can be used by a single TM.{quote}
[~kevin.cyj] doesn't this defeat the purpose of using mmap in the first place? For it to be beneficial two different readers of the mmap'ed region would have to closely follow one another, right?

{quote}If users want to disable container killing, `yarn.nodemanager.pmem-check-enabled=false` could be used. {quote}
Assuming that cluster is properly configured I guess this is a valid option.
;;;","28/Nov/19 03:02;kevin.cyj;> Doesn't this defeat the purpose of using mmap in the first place? For it to be beneficial two different readers of the mmap'ed region would have to closely follow one another, right?

You are right, if we decide to manage the mmapped region, we need to consider when and which region to recycle. Implementing a memory management algorithm is possible but can be complicated, currently, OS does it for us.

Maybe there's another choice we can consider - using the FILE-FILE mode in the first place. The FILE-FILE mode can also leverage the capability of OS page cache and the only problem is that it uses some unpooled heap memory for reading, which has a potential of OOM. If we can manage those memory in the future, there should be no problem with FILE-FILE mode.;;;","04/Dec/19 15:42;pnowojski;It would be nice to try to keep {{MMAP}} as default for non yarn deployments, but if that's non trivial +1 for setting {{FILE}} as the default.;;;","04/Dec/19 17:23;gjy;{quote}Maybe there's another choice we can consider - using the FILE-FILE mode in the first place. The FILE-FILE mode can also leverage the capability of OS page cache and the only problem is that it uses some unpooled heap memory for reading, which has a potential of OOM. If we can manage those memory in the future, there should be no problem with FILE-FILE mode.{quote}

[~kevin.cyj] Can you explain what FILE-FILE mode means?;;;","04/Dec/19 17:50;pnowojski;Initially the {{BlockingBoundedPartition}} was supposed to be working purely using mmap:
1. Results are produced and written to a mmap'ed file, from which they are being read
This had some blocking issues and was replaced by:
2. Results are written to a file directly, but read using mmap (improves performance when reading the same partition multiple times). This is what we are calling ""mmap""/""MMAP"" mode in this ticket.
3. Results are written to a file and read from a file, without using mmap at all. ""file"" or ""FILE-FILE"" mode;;;","06/Dec/19 06:50;kevin.cyj;[~gjy] By ""FILE-FILE"" mode, I mean using BoundedBlockingSubpartitionType.FILE. As explained by [~pnowojski], results are written to a file and read from a file using read/write API of FileChannel. When reading from files, two buffers are needed by each subpartition to store the read data. For MMAP read mode, the extra read buffers are not needed.;;;","11/Dec/19 03:47;zjwang;Generally speaking, we want to provide a better performance setting as a default config. 

But considering the mmap way might bring unstable concern in resource framework cluster, then I also prefer to adjust the default config option ""*taskmanager.network.bounded-blocking-subpartition-type*"" from `auto` to `file` mode.

Furthermore we can also supplement some descriptions of limitation/concerns for `mmap` way in options. Then when users want to enable `mmap` way for better performance, they are aware of the respective risks and the scenarios.;;;","11/Dec/19 05:37;zjwang;As [~kevin.cyj] mentioned above, the current blocking partition with file type has some potential concern for memory overhead. I created a separate ticket FLINK-15187 for tracking this issue and I do not tag it as a blocker for release-1.10 ATM.;;;","13/Dec/19 04:07;zjwang;Fixed in master: 7600e8b9d4cb8fee928c9edc9d2483787dc10a3c

Fixed in release-1.10: b52efff51f6494c442e32181a5d6896feec4e990;;;",,,,,,,,,,,,,,,,,
State TTL backend end-to-end test fail when taskManager has multiple slot,FLINK-14951,13270712,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,26/Nov/19 12:37,30/Nov/21 20:38,13/Jul/23 08:10,11/Dec/19 09:21,1.7.2,,,,,,,,,1.10.0,1.8.4,1.9.2,,Runtime / State Backends,Tests,,,,0,pull-request-available,,,,"When I run flink end to end tests, the State TTL backend tests fail. The log of TaskManager show below:
2019-11-26 20:22:03,837 INFO  org.apache.flink.runtime.taskmanager.Task                     - TtlVerifyUpdateFunction -> Sink: PrintFailedVerifications (3/3) (23f969ddb3e13fcdd3ba9823f50b0eab) switched from RUNNING to FAILED.
java.lang.IllegalStateException: Timestamps before and after the update do not match.
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:195)
	at org.apache.flink.streaming.tests.TtlVerifyUpdateFunction.performUpdate(TtlVerifyUpdateFunction.java:124)
	at org.apache.flink.streaming.tests.TtlVerifyUpdateFunction.generateUpdateAndVerificationContext(TtlVerifyUpdateFunction.java:101)
	at org.apache.flink.streaming.tests.TtlVerifyUpdateFunction.flatMap(TtlVerifyUpdateFunction.java:88)
	at org.apache.flink.streaming.tests.TtlVerifyUpdateFunction.flatMap(TtlVerifyUpdateFunction.java:67)
	at org.apache.flink.streaming.api.operators.StreamFlatMap.processElement(StreamFlatMap.java:50)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:284)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:155)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:445)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:834)

It is cause by the MonotonicTTLTimeProvider:freeze and MonotonicTTLTimeProvider:unfreezeTime called by multithread when taskmanager.numberOfTaskSlots set greater than 1. We could set it to 1 in test_stream_state_ttl.sh. That will fix the problem.","centos 7
java 8",azagrebin,guoyangze,,,,,,,,,,,,"KarmaGYZ commented on pull request #10348: [FLINK-14951][tests] Harden the thread safety of State TTL backend tests
URL: https://github.com/apache/flink/pull/10348
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The State TTL RocksDb/file backend tests now contains thread safety issue during the call of MonotonicTTLTimeProvider. This PR harden the thread safety of that class.
   
   
   ## Brief change log
   
   Replace the freeze/unfreeze function with `doWithFrozenTime`, which run user defined function in synchronize state.
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
    - Set `taskmanager.numberOfTaskSlots` to `PARALLELISM` and run State TTL RocksDb/file backend end to end test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
   cc @azagrebin 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Nov/19 13:44;githubbot;600","azagrebin commented on pull request #10348: [FLINK-14951][tests] Harden the thread safety of State TTL backend tests
URL: https://github.com/apache/flink/pull/10348
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Dec/19 09:11;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 11 09:21:56 UTC 2019,,,,,,,,,,"0|z090uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/19 12:39;guoyangze;Could someone kindly assign this issue to me? :);;;","26/Nov/19 14:47;azagrebin;[~guoyangze] 
Could we fix the underlying concurrency problem?
This should be doable if we substitute _freeze/unfreeze_ methods with e.g. this kind of method:
{code:java}
<T> MonotonicTTLTimeProvider#doWithFrozenTime(Function<Long, T> action) {
  synchronized (lock) {
    return action.accept(getCurrentTimestamp());
  }
}{code}
and use it in TtlVerifyUpdateFunction#performUpdate like:

 
{code:java}
return MonotonicTTLTimeProvider.doWithFrozenTime(frozenTimestamp -> {
  State state = states.get(verifier.getId());
  Object valueBeforeUpdate = verifier.get(state);
  verifier.update(state, update);
  Object updatedValue = verifier.get(state);
  return new TtlUpdateContext<>(valueBeforeUpdate, update, updatedValue, frozenTimestamp);
});
{code}
wdyt?;;;","26/Nov/19 14:52;azagrebin;btw, taskmanager.numberOfTaskSlots is already 1 by default, is it changed somewhere for the TTL e2e test?;;;","27/Nov/19 01:57;guoyangze;Yes, it is changed by other e2e test and not be reverted after other test failed.;;;","27/Nov/19 02:16;guoyangze;Agreed, that will prevent the multithread call issue. ;;;","27/Nov/19 11:01;azagrebin;would you like to work on the discussed fix? should I assign you?;;;","27/Nov/19 11:03;guoyangze;Yes, please assign it to me.;;;","09/Dec/19 06:18;guoyangze;[~azagrebin] Hi, andrey. I notice the fix version of this issue is setted to 1.10. Do you think its a blocker for 1.10 release? If so, could you take a look at the PR? If not, we should edit this field.;;;","09/Dec/19 13:08;azagrebin;[~guoyangze]
Hi yangze, I do not actually see this test failing in master cron e2e tests run.
Could you provide more details which test does not revert the default 1 slot option or how did you run it to get this failure scenario?

For now, I change it to major but technically it was written originally under assumption of parallelism one so I would say we keep it for now as a nice to have and not blocking the release.;;;","09/Dec/19 14:21;guoyangze;Actually, I execute it by run_singe_test.sh. Many tests will touch ""taskmanager.numberOfTaskSlots"" configuration, such as ""test_high_parallelism_iterations.sh"". After it failure, the cleanup function will not be called to revert the configuration. Thus, I met this problem.

Since the test case has thread safety issue, I think we should either set the ""taskmanager.numberOfTaskSlots"" explicitly to 1 or harden the test case. WDYT?

Agreed that it should not block the 1.10 release. What about setting the fix version to 1.11?
;;;","09/Dec/19 15:38;azagrebin;True we could firstly set ""taskmanager.numberOfTaskSlots"" explicitly to 1 but let's see maybe we can merge it soon. I have reviewed the PR and the requested change should be easy to resolve.;;;","11/Dec/19 09:21;azagrebin;merged into master by 2c11291fda7776baa0b2626311de97ea04393f65
merged into 1.9 by 8a73d680869da0d7bb4d543bfc197d01f3b0e068
merged into 1.8 by dfcbf68dbd792fc27c997078be7a59a594005d8b;;;",,,,,,,,,,,,,,,,,,,,
Task cancellation can be stuck against out-of-thread error,FLINK-14949,13270646,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hwanju,hwanju,hwanju,26/Nov/19 07:22,10/Mar/20 17:54,13/Jul/23 08:10,06/Dec/19 13:47,1.8.2,,,,,,,,,1.10.0,1.9.2,,,Runtime / Task,,,,,0,pull-request-available,,,,"Task cancellation ([_cancelOrFailAndCancelInvokable_|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L991]) relies on multiple separate threads, which are _TaskCanceler_, _TaskInterrupter_, and _TaskCancelerWatchdog_. While TaskCanceler performs cancellation itself, TaskInterrupter periodically interrupts a non-reacting task and TaskCancelerWatchdog kills JVM if cancellation has never been finished within a certain amount of time (by default 3 min). Those all ensure that cancellation can be done or either aborted transitioning to a terminal state in finite time (FLINK-4715).

However, if any asynchronous thread creation is failed such as by out-of-thread (_java.lang.OutOfMemoryError: unable to create new native thread_), the code transitions to CANCELING, but nothing could be performed for cancellation or watched by watchdog. Currently, jobmanager does [retry cancellation|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java#L1121] against any error returned, but a next retry [returns success once it sees CANCELING|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L997], assuming that it is in progress. This leads to complete stuck in CANCELING, which is non-terminal, so state machine is stuck after that.

One solution would be that if a task has transitioned to CANCELLING but it gets fatal error or OOM (i.e., _isJvmFatalOrOutOfMemoryError_ is true) indicating that it could not reach spawning TaskCancelerWatchdog, it could immediately consider that as fatal error (not safely cancellable) calling _notifyFatalError_, just as TaskCancelerWatchdog does but eagerly and synchronously. That way, it can at least transition out of the non-terminal state and furthermore clear potentially leaked thread/memory by restarting JVM. The same method is also invoked by _failExternally_, but transitioning to FAILED seems less critical as it's already terminal state.

How to reproduce is straightforward by running an application that keeps creating threads, each of which never finishes in a loop, and has multiple tasks so that one task triggers failure and then the others are attempted to be cancelled by full fail-over. In web UI dashboard, some tasks from a task manager where any of cancellation-related threads failed to be spawned are stuck in CANCELLING for good.",,aitozi,azagrebin,gjy,hwanju,mxm,Paul Lin,pnowojski,rmetzger,txhsj,yunta,,,,"hwanju commented on pull request #10387: [FLINK-14949] [Runtime/Task] Task cancellation can be stuck against out-of-thread error
URL: https://github.com/apache/flink/pull/10387
 
 
   ##  What is the purpose of the change
   
   This pull request added an additional safety net against fatal error thrown from starting task cancellation by notifying the fatal error to taskmanager. Without it, if a fatal or out-of-memory error happens while initiating task cancellation, one or more critical threads that are responsible for cancellation either gracefully or forcefully may not be spawned, thereby job state machine being permanently stuck in a non-terminal state such as FAILING (by stuck task cancellation). The fatal error notification added by this patch can restart JVM cleaning up the state letting job state machine make progress.
   
   ## Brief change log
   
     - Catch fatal or OOM error from task cancellation initiation and notify fatal error to taskmanager.
     - Added two unit tests for cancelling task and failing task externally by mocking the fatal error.
   
   NOTE: Instead of catching fatal/OOM errors from each thread start, added a catch at a higher level by internalizing `cancelOrFailAndCancelInvokable`, as it would make sense to notify fatal error no matter where such error comes from during the function. In addition, doing so made unit testing easier by mocking.
   
   ## Verifying this change
   
     - Added two unit tests for cancelling task and failing task externally.
     - Manually verified that by using an app intentionally leaking threads, job state machine is not stuck by restarting taskmanager JVM where thread limit was hit and everything else was impacted by that. Without the patch, one or more tasks have cancellation completely stuck, leading to stuck job state machine.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Dec/19 23:25;githubbot;600","pnowojski commented on pull request #10387: [FLINK-14949] [Runtime/Task] Task cancellation can be stuck against out-of-thread error
URL: https://github.com/apache/flink/pull/10387
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 13:34;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-16511,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 13:47:30 UTC 2019,,,,,,,,,,"0|z090g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/19 09:56;azagrebin;[~hwanju]
Thanks for filing this problem. It looks like a bug in Flink. The proposed solution also looks good because if we are unable to spawn the cancelation threads we cannot do much about this except fatally terminating JVM. After talking to [~pnowojski], we do not have plans to handle the cancelation differently at the moment so we have to introduce another try/catch surrounding spawning the cancelation threads.

Do you have time to work on the suggested fix for this and want to be assigned to the issue?;;;","26/Nov/19 10:05;hwanju;[~azagrebin], thanks for the quick answer and sure, I can work on this.;;;","26/Nov/19 10:21;rmetzger;Thanks. I assigned you;;;","06/Dec/19 13:47;pnowojski;merged commit 0f47614 to master branch and as 9a9548948563e7778a465d76bc4319c06a29fe7b to release-1.9


Back porting to 1.8 would require to rewrite the test.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
The AbstractTableInputFormat#nextRecord in hbase connector will handle the same rowkey twice once encountered any exception,FLINK-14941,13270431,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,openinx,openinx,openinx,25/Nov/19 12:13,13/Dec/19 06:11,13/Jul/23 08:10,13/Dec/19 06:10,1.9.1,,,,,,,,,1.10.0,,,,Connectors / HBase,,,,,0,pull-request-available,,,,"In the mail list [1].   The user complain that it will see the same row twice if encountered any HBase exception. 
The problem is here: 
{code}
public T nextRecord(T reuse) throws IOException {
		if (resultScanner == null) {
			throw new IOException(""No table result scanner provided!"");
		}
		try {
			Result res = resultScanner.next();
			if (res != null) {
				scannedRows++;
				currentRow = res.getRow();
				return mapResultToOutType(res);
			}
		} catch (Exception e) {
			resultScanner.close();
			//workaround for timeout on scan
			LOG.warn(""Error after scan of "" + scannedRows + "" rows. Retry with a new scanner..."", e);
			scan.setStartRow(currentRow);
			resultScanner = table.getScanner(scan);
			Result res = resultScanner.next();
			if (res != null) {
				scannedRows++;
				currentRow = res.getRow();
				return mapResultToOutType(res);
			}
		}

		endReached = true;
		return null;
	}
{code}

We will set the startRow of the new scan to the currentRow which has been seen,  that means the currentRow will be seen twice.   Actually, we should replace the scan.setStartRow(currentRow) as scan.withStartRow(currentRow, false) , the false means exclude the currentRow. 


[1]. http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/DataSet-API-HBase-ScannerTimeoutException-and-double-Result-processing-td31174.html",,jark,modavis,openinx,,,,,,,,,,,"openinx commented on pull request #10314: [FLINK-14941][hbase] The AbstractTableInputFormat#nextRecord in hbase connector will handle the same rowkey twice once encountered any exception
URL: https://github.com/apache/flink/pull/10314
 
 
   ## What is the purpose of the change
   
   In the [mail list](http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/DataSet-API-HBase-ScannerTimeoutException-and-double-Result-processing-td31174.html). The user complain that it will see the same row twice if encountered any HBase exception.
   The problem is here:
   
   ```java
   public T nextRecord(T reuse) throws IOException {
   		if (resultScanner == null) {
   			throw new IOException(""No table result scanner provided!"");
   		}
   		try {
   			Result res = resultScanner.next();
   			if (res != null) {
   				scannedRows++;
   				currentRow = res.getRow();
   				return mapResultToOutType(res);
   			}
   		} catch (Exception e) {
   			resultScanner.close();
   			//workaround for timeout on scan
   			LOG.warn(""Error after scan of "" + scannedRows + "" rows. Retry with a new scanner..."", e);
   			scan.setStartRow(currentRow);
   			resultScanner = table.getScanner(scan);
   			Result res = resultScanner.next();
   			if (res != null) {
   				scannedRows++;
   				currentRow = res.getRow();
   				return mapResultToOutType(res);
   			}
   		}
   
   		endReached = true;
   		return null;
   }
   ```
   
   We will set the startRow of the new scan to the currentRow which has been seen, that means the currentRow will be seen twice. Actually, we should replace the scan.setStartRow(currentRow) as scan.withStartRow(currentRow, false) , the false means exclude the currentRow.
   
   
   
   ## Brief change log
   
   - 12e35abc0a [FLINK-14941][hbase] The AbstractTableInputFormat#nextRecord in hbase connector will handle the same rowkey twice once encountered any exception
   
   
   ## Verifying this change
   
   This change does not have a test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Nov/19 13:33;githubbot;600","wuchong commented on pull request #10314: [FLINK-14941][hbase] The AbstractTableInputFormat#nextRecord in hbase connector will handle the same rowkey twice once encountered any exception
URL: https://github.com/apache/flink/pull/10314
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 06:10;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 13 06:11:44 UTC 2019,,,,,,,,,,"0|z08z48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/19 13:37;openinx;FYI  [~jark].;;;","25/Nov/19 14:47;modavis;I am not sure that the proposed solution is correct.

It uses the relatively new HBase API, not available in pre-v.2.0 HBase. It is better to use setStartRow() with an extra '0x0' at the end.

And it skips the row if the row processing (mapResultToOutType(res)) fails. The row must be skipped only if it is a scanner exception.;;;","26/Nov/19 06:42;jark;Hi [~modavis], I checked the API and the {{withStartRow}} is in v1.4.3, but I agree with you we should only re-scan if it is an HBase exception.

 

What do you think about [~modavis]' s point? [~openinx];;;","27/Nov/19 01:39;openinx;> but I agree with you we should only re-scan if it is an HBase exception. 
+1,  will update the patch. Thanks [~modavis] & [~jark].;;;","13/Dec/19 06:11;jark;1.11.0: 127ef48ec510663debb31e96a0bca9322625e2fd
1.10.0: 8b0278c72b0662e9c44065a9e8aceff28c80f7e4
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Travis build passes despite Test failures,FLINK-14940,13270407,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,gjy,gjy,25/Nov/19 09:41,27/Nov/19 14:04,13/Jul/23 08:10,27/Nov/19 14:04,1.10.0,,,,,,,,,1.10.0,,,,Test Infrastructure,Travis,,,,0,pull-request-available,,,,Build https://travis-ci.org/apache/flink/jobs/616462870 is green despite the presence of Test failures.,,aljoscha,dwysakowicz,gjy,,,,,,,,,,,"zentol commented on pull request #10337: [FLINK-14940][travis] Fix error code handling for maven calls
URL: https://github.com/apache/flink/pull/10337
 
 
   `run_with_watchdog` automatically updates `EXIT_CODE`, but we were still checking the exit_code of run_with_watchdog after executing the e2e tests. Since the function ran without an error it returns 0, causing us to overwrite the value written into EXIT_CODE.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Nov/19 13:53;githubbot;600","zentol commented on pull request #10337: [FLINK-14940][travis] Fix error code handling for maven calls
URL: https://github.com/apache/flink/pull/10337
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Nov/19 13:58;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-14939,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 27 14:04:21 UTC 2019,,,,,,,,,,"0|z08yyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/19 16:49;gjy;cc: [~chesnay];;;","27/Nov/19 14:04;chesnay;master: 4b1a9e4474cdfb624b40d771495ec3eafde176dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingKafkaITCase fails due to distDir property not being set,FLINK-14939,13270406,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,gjy,gjy,25/Nov/19 09:40,27/Nov/19 14:04,13/Jul/23 08:10,27/Nov/19 14:04,1.10.0,,,,,,,,,1.10.0,,,,Connectors / Kafka,Test Infrastructure,Travis,,,0,pull-request-available,,,,"https://api.travis-ci.org/v3/job/616462870/log.txt

{noformat}
08:12:34.965 [INFO] -------------------------------------------------------
08:12:34.965 [INFO]  T E S T S
08:12:34.965 [INFO] -------------------------------------------------------
08:12:35.868 [INFO] Running org.apache.flink.tests.util.kafka.StreamingKafkaITCase
08:12:35.893 [ERROR] Tests run: 3, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 0.02 s <<< FAILURE! - in org.apache.flink.tests.util.kafka.StreamingKafkaITCase
08:12:35.893 [ERROR] testKafka[0: kafka-version:0.10.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 0.009 s  <<< FAILURE!
java.lang.AssertionError: The distDir property was not set. You can set it when running maven via -DdistDir=<path> .
	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.<init>(StreamingKafkaITCase.java:71)

08:12:35.893 [ERROR] testKafka[1: kafka-version:0.11.0.2](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 0.001 s  <<< FAILURE!
java.lang.AssertionError: The distDir property was not set. You can set it when running maven via -DdistDir=<path> .
	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.<init>(StreamingKafkaITCase.java:71)

08:12:35.893 [ERROR] testKafka[2: kafka-version:2.2.0](org.apache.flink.tests.util.kafka.StreamingKafkaITCase)  Time elapsed: 0.001 s  <<< FAILURE!
java.lang.AssertionError: The distDir property was not set. You can set it when running maven via -DdistDir=<path> .
	at org.apache.flink.tests.util.kafka.StreamingKafkaITCase.<init>(StreamingKafkaITCase.java:71)

08:12:36.233 [INFO] 
08:12:36.233 [INFO] Results:
08:12:36.233 [INFO] 
08:12:36.233 [ERROR] Failures: 
08:12:36.233 [ERROR]   StreamingKafkaITCase.<init>:71 The distDir property was not set. You can set it when running maven via -DdistDir=<path> .
08:12:36.233 [ERROR]   StreamingKafkaITCase.<init>:71 The distDir property was not set. You can set it when running maven via -DdistDir=<path> .
08:12:36.233 [ERROR]   StreamingKafkaITCase.<init>:71 The distDir property was not set. You can set it when running maven via -DdistDir=<path> .
08:12:36.233 [INFO] 
08:12:36.233 [ERROR] Tests run: 3, Failures: 3, Errors: 0, Skipped: 0
08:12:36.233 [INFO] 
{noformat}",,aljoscha,clay4megtr,gjy,,,,,,,,,,,"zentol commented on pull request #10338: [FLINK-14939][e2e] Set distDir property
URL: https://github.com/apache/flink/pull/10338
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Nov/19 13:53;githubbot;600","zentol commented on pull request #10338: [FLINK-14939][e2e] Set distDir property
URL: https://github.com/apache/flink/pull/10338
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Nov/19 14:03;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14940,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 27 14:04:31 UTC 2019,,,,,,,,,,"0|z08yyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/19 16:49;gjy;cc: [~chesnay];;;","26/Nov/19 15:09;gjy;[~clay4megtr] Why did you close this issue?;;;","27/Nov/19 12:29;clay4megtr;I didn't know I have do that.......  My area is very slow to open jara,  so this may be an accident,  I am very sorry about this...;;;","27/Nov/19 14:04;chesnay;master: 3d657b4a9f12d0cb78462be9c64448d96306ede8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink elasticsearch failure handler re-add indexrequest causes ConcurrentModificationException,FLINK-14938,13270388,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ysn2233,ysn2233,ysn2233,25/Nov/19 08:32,24/Jun/20 09:38,13/Jul/23 08:10,24/Jun/20 09:38,1.8.1,,,,,,,,,1.11.0,,,,Connectors / ElasticSearch,,,,,3,pull-request-available,,,," 

When use Elasticsearch connector failure handler (from official example) to re-add documents, Flink encountered ConcurrentModificationException.
{code:java}
input.addSink(new ElasticsearchSink<>(
    config, transportAddresses,
    new ElasticsearchSinkFunction<String>() {...},
    new ActionRequestFailureHandler() {
        @Override
        void onFailure(ActionRequest action,
                Throwable failure,
                int restStatusCode,
                RequestIndexer indexer) throw Throwable {

            if (ExceptionUtils.findThrowable(failure, EsRejectedExecutionException.class).isPresent()) {
                // full queue; re-add document for indexing
                indexer.add(action);
            }
        }
}));
{code}
I found that in method BufferingNoOpRequestIndexer$processBufferedRequests, it will iterator a list of ActionRequest. However the failure handler will keep re-adding request to that list after bulk, which causes ConcurrentModificationException.
{code:java}
void processBufferedRequests(RequestIndexer actualIndexer) {
   for (ActionRequest request : bufferedRequests) {
      if (request instanceof IndexRequest) {
         actualIndexer.add((IndexRequest) request);
      } else if (request instanceof DeleteRequest) {
         actualIndexer.add((DeleteRequest) request);
      } else if (request instanceof UpdateRequest) {
         actualIndexer.add((UpdateRequest) request);
      }
   }

   bufferedRequests.clear();
}{code}
I think it should be a multi-thread bug and is it ok to use concurrent queue to maintain the failure request?

 ",,aljoscha,f.pompermaier,hwanju,karthitect,klion26,liyu,pqf,rmetzger,ysn2233,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 09:38:34 UTC 2020,,,,,,,,,,"0|z08yuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/19 08:41;ysn2233;If it is a bug, could please assign this ticket to me to solve?;;;","12/Dec/19 13:25;rmetzger;Thanks for looking into this. I've assigned you!;;;","25/Feb/20 02:33;pqf;Any update on this issue?;;;","15/Mar/20 14:27;ysn2233;The easiest way to solve this issue is to use ConcurrentLinkedQueue instead of ArrayList in BufferingNoOpRequestIndexer. However use concurrent queue 'may' affect performance if users don't have any failure handles or have nothing to do with concurrent concerns. Therefore I'd like to create a new class ConcurrentBufferingNoOpRequestIndexer which use concurrent queue and users can decided which RequestIndexer to use when they build ElasticsearchSink. 

 

What do you think of this solution? [~rmetzger] Thank you very much.;;;","27/Apr/20 06:32;hwanju;[~ysn2233] - I have a question on your proposed solution. I wonder if you've gotten any performance measurement, which would've led you to that hybrid solution. IMO, since this ElasticSearch sink path is involved in I/O to external services, any CPU penalty incurred by concurrency-aware data structure may be dwarfed or invisible by much high I/O latency, so using CurrentLinkedQueue seems to be just fine to me (I mean in terms of latency, but it could be some additional CPU cost, which I am also not sure about its significance quantitatively). We are also looking to the resolution of this problem, but wanted to check if you have performance test to confirm whether such cost matters.;;;","24/Jun/20 09:35;aljoscha;[~ysn2233] opened a PR with the simpler solution. I agree with [~hwanju] that it shouldn't be a problem. If it turns out to be a problem we need to investigate further.;;;","24/Jun/20 09:38;aljoscha;master: 44ea896d6c5bbfcabb79d9649dd15c834741c4b9
release-1.11: bb1f162d8e7cf3d24c01679492e53b3a041cdde9;;;",,,,,,,,,,,,,,,,,,,,,,,,,
OSS Filesystem Uses Wrong Shading Prefix,FLINK-14930,13270173,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,knaufk,knaufk,knaufk,22/Nov/19 20:09,27/Nov/19 11:16,13/Jul/23 08:10,27/Nov/19 10:46,1.9.1,,,,,,,,,1.10.0,1.9.2,,,FileSystems,,,,,0,pull-request-available,,,,The relevant classes (CredentialsProviders) are relocated to {{org.apache.flink.fs.osshadoop.shaded.}} not {{org.apache.flink.fs.shaded.hadoop3.}}.,,knaufk,,,,,,,,,,,,,"knaufk commented on pull request #10295: [FLINK-14930] fix FLINK_SHADING_PREFIX in OSSFileSystemFactory
URL: https://github.com/apache/flink/pull/10295
 
 
   ## What is the purpose of the change
   
   Fix the shading prefix in OSS Filesystem to enable configuration of custom CredentialsProviders
   
   ## Brief change log
   
     - Fix the shading prefix in OSS Filesystem to enable configuration of custom CredentialsProviders
   
   ## Verifying this change
   
   Build Flink and Run the Job with Checkpointing to OSS with a custom CredentialsProvider: 
   
   flink-conf.yaml
   
   fs.oss.credentials.provider: com.aliyun.oss.common.auth.EnvironmentVariableCredentialsProvider
   fs.oss.endpoint: https://oss-eu-central-1.aliyuncs.com
   
   state.checkpoints.dir: oss:/...
   state.savepoints.dir: oss://...
   
   Environment:
   
   export OSS_ACCESS_KEY_SECRET=...
   export OSS_ACCESS_KEY_ID=....  
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency):  
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: no
     - The S3 file system connector: no
   
   ## Documentation
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented?  docs 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Nov/19 21:01;githubbot;600","zentol commented on pull request #10295: [FLINK-14930] fix FLINK_SHADING_PREFIX in OSSFileSystemFactory
URL: https://github.com/apache/flink/pull/10295
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Nov/19 10:44;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-14973,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 27 10:46:27 UTC 2019,,,,,,,,,,"0|z08xiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/19 06:36;knaufk;[~chesnay] Could you assign me to this?;;;","27/Nov/19 10:46;chesnay;master:
17e7fe9bfd800f8f8adbf941923bfe3a5b3b23cd
d79dbec2bcf2c01d57f9301faec9bc9d330a7bd0

1.9:
49cc9fe2e4ce56bcc348c14d17b2f8466caa34be
1663c21f71c76d6fe16dc4637372f77c20ad1197 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ContinuousFileProcessingCheckpointITCase sporadically fails due to FileNotFoundException,FLINK-14929,13270110,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gjy,gjy,gjy,22/Nov/19 15:19,17/Dec/19 12:26,13/Jul/23 08:10,17/Dec/19 10:48,1.10.0,,,,,,,,,1.10.0,,,,Connectors / Common,Tests,,,,0,pull-request-available,test-stability,,,"*Description*
Test fails locally approximately 1 out of 200 times.

*Stacktrace*
{noformat}
org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 73b2ee613731d76fb9ab20142c3ba52f)
 at org.apache.flink.client.ClientUtils.submitJobAndWaitForResult(ClientUtils.java:144)
 at org.apache.flink.test.checkpointing.StreamFaultToleranceTestBase.runCheckpointedProgram(StreamFaultToleranceTestBase.java:132)
 at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
 at org.junit.rules.RunRules.evaluate(RunRules.java:20)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
 at org.junit.runners.Suite.runChild(Suite.java:128)
 at org.junit.runners.Suite.runChild(Suite.java:27)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
 at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
 at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:54)
 at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
 at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
 at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
 at org.apache.flink.client.ClientUtils.submitJobAndWaitForResult(ClientUtils.java:142)
 ... 34 more
Caused by: java.lang.IllegalStateException: Cannot process mail Report throwable java.io.FileNotFoundException: File file:/home/gary/code/flink/flink-tests/target/localfs/fs_tests/..file3.crc does not exist or the user running Flink ('gary') has insufficient permissions to access it.
 at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:68)
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:213)
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:154)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:445)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
 at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.WrappingRuntimeException: java.io.FileNotFoundException: File file:/home/gary/code/flink/flink-tests/target/localfs/fs_tests/..file3.crc does not exist or the user running Flink ('gary') has insufficient permissions to access it.
 at org.apache.flink.util.WrappingRuntimeException.wrapIfNecessary(WrappingRuntimeException.java:65)
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.lambda$reportThrowable$0(MailboxProcessor.java:166)
 at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:66)
 ... 6 more
Caused by: java.io.FileNotFoundException: File file:/home/gary/code/flink/flink-tests/target/localfs/fs_tests/..file3.crc does not exist or the user running Flink ('gary') has insufficient permissions to access it.
 at org.apache.flink.core.fs.local.LocalFileSystem.getFileStatus(LocalFileSystem.java:115)
 at org.apache.flink.core.fs.local.LocalFileSystem.listStatus(LocalFileSystem.java:175)
 at org.apache.flink.api.common.io.FileInputFormat.addFilesInDir(FileInputFormat.java:707)
 at org.apache.flink.api.common.io.FileInputFormat.createInputSplits(FileInputFormat.java:591)
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.getInputSplitsSortedByModTime(ContinuousFileMonitoringFunction.java:270)
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.monitorDirAndForwardSplits(ContinuousFileMonitoringFunction.java:242)
 at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.run(ContinuousFileMonitoringFunction.java:206)
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:191)

 

java.lang.AssertionError: Test failed: Job failed (JobID: 73b2ee613731d76fb9ab20142c3ba52f)

at org.junit.Assert.fail(Assert.java:88)
 at org.apache.flink.test.checkpointing.StreamFaultToleranceTestBase.runCheckpointedProgram(StreamFaultToleranceTestBase.java:141)
 at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
 at org.junit.rules.RunRules.evaluate(RunRules.java:20)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
 at org.junit.runners.Suite.runChild(Suite.java:128)
 at org.junit.runners.Suite.runChild(Suite.java:27)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
 at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
 at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
 at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:54)
 at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
 at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
{noformat}",rev: c7ae2b8559c0fe4cf17613249db0c2857bb91d94,aljoscha,gjy,klion26,liyu,,,,,,,,,,"GJL commented on pull request #10533: [FLINK-14929][tests] Harden ContinuousFileProcessingCheckpointITCase 
URL: https://github.com/apache/flink/pull/10533
 
 
   ## What is the purpose of the change
   
   *This hardens the ContinuousFileProcessingCheckpointITCase.*
   
   
   ## Brief change log
   
     - *See commits*
   
   
   ## Verifying this change
   This change is already covered by existing tests, such as *ContinuousFileProcessingCheckpointITCase*.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Dec/19 13:21;githubbot;600","GJL commented on pull request #10533: [FLINK-14929][tests] Harden ContinuousFileProcessingCheckpointITCase 
URL: https://github.com/apache/flink/pull/10533
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 12:26;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-5125,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 10:48:24 UTC 2019,,,,,,,,,,"0|z08x4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/19 10:48;gjy;1.10: d430061d6c50527fa80569a697a8909f18f22e80
master: 8c78e1e6edeb60702b1fb6f810c3cbe6d5c95bb6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation links check nightly run failed on travis,FLINK-14928,13270049,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,klion26,liyu,liyu,22/Nov/19 08:23,25/Nov/19 02:55,13/Jul/23 08:10,25/Nov/19 02:55,1.10.0,,,,,,,,,1.10.0,,,,Documentation,,,,,0,pull-request-available,test-stability,,,"This test stage fails stably, with below error:
{noformat}
[2019-11-22 03:10:45] ERROR `/dev/table/udfs.html' not found.
[2019-11-22 03:10:46] ERROR `/dev/table/functions.html' not found.
[2019-11-22 03:10:49] ERROR `/zh/getting-started/tutorials/datastream_api.html' not found.
[2019-11-22 03:10:49] ERROR `/dev/table/functions/streaming/query_configuration.html' not found.
[2019-11-22 03:10:49] ERROR `/dev/table/functions/sql.html' not found.
[2019-11-22 03:10:49] ERROR `/dev/table/functions/tableApi.html' not found.
[2019-11-22 03:10:49] ERROR `/zh/dev/table/udfs.html' not found.
[2019-11-22 03:10:49] ERROR `/zh/dev/table/functions.html' not found.
[2019-11-22 03:10:49] ERROR `/zh/dev/table/functions/streaming/query_configuration.html' not found.
[2019-11-22 03:10:49] ERROR `/zh/dev/table/functions/sql.html' not found.
[2019-11-22 03:10:49] ERROR `/zh/dev/table/functions/tableApi.html' not found.
http://localhost:4000/dev/table/udfs.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/table/functions.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/getting-started/tutorials/datastream_api.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/table/functions/streaming/query_configuration.html:
Remote file does not exist -- broken link!!!
http://localhost:4000/dev/table/functions/sql.html:
Remote file does not exist -- broken link!!!
http://localhost:4000/dev/table/functions/tableApi.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/table/udfs.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/table/functions.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/table/functions/streaming/query_configuration.html:
Remote file does not exist -- broken link!!!
http://localhost:4000/zh/dev/table/functions/sql.html:
Remote file does not exist -- broken link!!!
http://localhost:4000/zh/dev/table/functions/tableApi.html:
Remote file does not exist -- broken link!!!
---------------------------------------------------------------------------
Found 11 broken links.
{noformat}

And here is the latest instance: https://api.travis-ci.org/v3/job/615032410/log.txt",,jark,klion26,liyu,,,,,,,,,,,"klion26 commented on pull request #10292: [FLINK-14928][docs] Fix the broken links in documentation of page systemFunctions
URL: https://github.com/apache/flink/pull/10292
 
 
   
   ## What is the purpose of the change
   
   FLINK-14638 moved some docs to another place, leave the links not change. FLINK-14866 fixed much of them, but there still a few broken links in page systemFunctions.html.
   This pr wants to fix this problem
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Nov/19 12:00;githubbot;600","wuchong commented on pull request #10292: [FLINK-14928][docs] Fix the broken links in documentation of page systemFunctions
URL: https://github.com/apache/flink/pull/10292
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Nov/19 02:55;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-14866,FLINK-14760,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 25 02:55:54 UTC 2019,,,,,,,,,,"0|z08wrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/19 08:27;klion26;I'll take a look at this issue.;;;","22/Nov/19 08:42;klion26;the reason here is FLINK-14638 moved some doc to another place, I'll file a pr for this.;;;","22/Nov/19 13:38;klion26;FLINK-14866 fixed much of them, but there still a few broken links in page systemFunctions.html;;;","25/Nov/19 02:55;jark;Fixed in 1.10.0: 3442189eb6d03f7c939fd684d255e45c056a9159;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
DisableAutoGeneratedUIDs fails on keyBy,FLINK-14910,13269912,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,wilcheng,wilcheng,21/Nov/19 17:55,24/Jan/20 01:12,13/Jul/23 08:10,04/Dec/19 18:05,1.9.0,,,,,,,,,1.10.0,1.9.2,,,API / DataStream,,,,,0,pull-request-available,,,,"There doesn't seem to be a way to add a UID to the Partition operator created by KeyBy, causing `disableAutoGeneratedUIDs` to fail.

 

Here's a simple test case that will reproduce the issue:
{noformat}
 @Test
public void testFailedUID() throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.getConfig().disableAutoGeneratedUIDs();

    DataStream<String> data = env.fromCollection(Arrays.asList(""1"", ""2"", ""3"")).uid(""source-uid"");
    data.keyBy(i -> i)
            .map(i -> i).uid(""map-uid"");

    env.execute();
}{noformat}
{noformat}
testFailedUID(twitch.creatoranalytics.sessions.StreamingJobTest)  Time elapsed: 0.008 sec  <<< ERROR!
java.lang.IllegalStateException: Auto generated UIDs have been disabled but no UID or hash has been assigned to operator Partition
 {noformat}
 

This passes if the keyBy is removed. ",,dwysakowicz,mzuehlke,wilcheng,,,,,,,,,,,"dawidwys commented on pull request #10417: [FLINK-14910][datastream] Checking for auto generated uids only for PhysicalStreamTransformations
URL: https://github.com/apache/flink/pull/10417
 
 
   ## What is the purpose of the change
   
   This PR enables the check for auto generated uids only for `PhysicalStreamTransformations`, as those are the only `StreamTransformations` that will produce a `StreamOperator` that can have a state.
   
   This solves the problem that it is not possible to assign the uid for some of the non-physical transformations (e.g `keyBy`, `split`).
   
   ## Verifying this change
   
   Added:
   `org.apache.flink.streaming.graph.StreamingJobGraphGeneratorNodeHashTest#testDisablingAutoUidsWorksWithKeyBy`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 09:22;githubbot;600","dawidwys commented on pull request #10417: [FLINK-14910][datastream] Checking for auto generated uids only for PhysicalStreamTransformations
URL: https://github.com/apache/flink/pull/10417
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 18:04;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-11653,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 18:05:50 UTC 2019,,,,,,,,,,"0|z08vww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/19 17:58;chesnay;[~sjwiesman] [~dwysakowicz] Looks like some special cases were not accounted for in FLINK-11653?;;;","28/Nov/19 18:05;dwysakowicz;Thanks for letting us now. Will have a look into it.;;;","28/Nov/19 18:21;dwysakowicz;I think an easy  fix would be to perform the check introduced in FLINK-11653 only for {{PhysicalTransformation}}. We create {{StreamOperators}} only for those. We can set uids for all of them.

Will prepare a fix shortly.;;;","04/Dec/19 18:05;dwysakowicz;Fixed in:
master: 4b7baf500b45d1602edd1ed3477a309f961cf1b7
1.9.2: cfef075d3c0738592a6e86569a91216dfc73bab6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
can not be translated to StreamExecDeduplicate when PROCTIME() is defined in query,FLINK-14899,13269857,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,21/Nov/19 13:56,17/Dec/19 12:16,13/Jul/23 08:10,17/Dec/19 12:16,1.9.0,1.9.1,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"CREATE TABLE user_log (
    user_id VARCHAR,
    item_id VARCHAR,
    category_id VARCHAR,
    behavior VARCHAR,
    ts TIMESTAMP
) WITH (
    'connector.type' = 'kafka',
    'connector.version' = 'universal',
    'connector.topic' = 'user_behavior',
    'connector.startup-mode' = 'earliest-offset',
    'connector.properties.0.key' = 'zookeeper.connect',
    'connector.properties.0.value' = 'localhost:2181',
    'connector.properties.1.key' = 'bootstrap.servers',
    'connector.properties.1.value' = 'localhost:9092',
    'update-mode' = 'append',
    'format.type' = 'json',
    'format.derive-schema' = 'true'
);

CREATE TABLE user_dist (
    dt VARCHAR,
    user_id VARCHAR,
    behavior VARCHAR
) WITH (
    'connector.type' = 'jdbc',
    'connector.url' = 'jdbc:mysql://localhost:3306/flink-test',
    'connector.table' = 'user_behavior_dup',
    'connector.username' = 'root',
    'connector.password' = ‘******',
    'connector.write.flush.max-rows' = '1'
);

INSERT INTO user_dist
SELECT
  dt,
  user_id,
  behavior
FROM (
   SELECT
      dt,
      user_id,
      behavior,
     ROW_NUMBER() OVER (PARTITION BY dt, user_id, behavior ORDER BY proc asc ) AS rownum
   FROM (select DATE_FORMAT(ts, 'yyyy-MM-dd HH:00') as dt,user_id,behavior,PROCTIME() as proc
            from user_log) )
WHERE rownum = 1;

Exception in thread ""main"" org.apache.flink.table.api.TableException: UpsertStreamTableSink requires that Table has a full primary keys if it is updated.
at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:114)
at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:50)
at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:54)
at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:50)
at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:61)
at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.Iterator$class.foreach(Iterator.scala:891)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.AbstractTraversable.map(Traversable.scala:104)
at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:60)
at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:149)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:439)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:348)",,aitozi,godfreyhe,jark,libenchao,Terry1897,yesorno,,,,,,,,"godfreyhe commented on pull request #10291: [FLINK-14899] [table-planner-blink] Fix unexpected plan when PROCTIME() is defined in query
URL: https://github.com/apache/flink/pull/10291
 
 
   
   ## What is the purpose of the change
   
   * currently,  `SELECT * (SELECT *, ROW_NUMBER() OVER (ORDER BY PROCTIME() ASC) as rowNum FROM MyTable) WHERE rowNum = 1` will be translated to StreamExecRank, while StreamExecDeduplicate is the expected operator. The reason is: PROCTIME type is materialized to Timestamp type in RelTimeIndicatorConverter. *
   
   
   ## Brief change log
   
     - *return original call when meeting PROCTIME instead of PROCTIME_MATERIALIZE in RexTimeIndicatorMaterializer*
     - *add related tests*
   
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
     - *Added related tests to verify the fix*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no)**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no)**
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no)**
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Nov/19 10:33;githubbot;600","wuchong commented on pull request #10291: [FLINK-14899] [table-planner-blink] Fix unexpected plan when PROCTIME() is defined in query
URL: https://github.com/apache/flink/pull/10291
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 12:13;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 12:16:06 UTC 2019,,,,,,,,,,"0|z08vko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/19 03:02;godfreyhe;the reason is:  {{PROCTIME}} type is materialized to {{Timestamp}} type in {{RelTimeIndicatorConverter}}, and that is an unintended behavior.;;;","22/Nov/19 03:02;godfreyhe;I would like to fix this.;;;","22/Nov/19 03:49;ykt836;assigned to you [~godfreyhe];;;","22/Nov/19 05:01;yesorno;I change DML to below, also get the same exception. just change _ORDER BY proc asc_ to _ORDER BY mytime asc_ _._
{code:java}
//代码占位符
INSERT INTO user_dist
SELECT
 dt,
 user_id,
 behavior
FROM (
 SELECT
 dt,
 user_id,
 behavior,
 ROW_NUMBER() OVER (PARTITION BY user_id, behavior ORDER BY mytime asc ) AS rownum
 FROM (select DATE_FORMAT(ts, 'yyyy-MM-dd HH:00') as dt, user_id, behavior, cast(ts as timestamp) as mytime
 from user_log) )
WHERE rownum = 1;
{code};;;","22/Nov/19 05:21;godfreyhe;hi [~yesorno], the following sql will be supported after this issue is fixed. (here requires {{PROCTIME()}} instead of {{Timestamp}} type)

{code:java}
SELECT
dt,
user_id,
behavior
FROM (
SELECT
dt,
user_id,
behavior,
ROW_NUMBER() OVER (PARTITION BY dt, user_id, behavior ORDER BY proc asc ) AS rownum
FROM (select DATE_FORMAT(ts, 'yyyy-MM-dd HH:00') as dt,user_id,behavior,PROCTIME() as proc
from user_log) )
WHERE rownum = 1;
{code}


;;;","17/Dec/19 12:16;jark;1.11.0: 15f7cdae533bdee5652ddfa2677d670c77eefa29
1.10.0: c19496a7fd3eee48e2e31784b6a77c314571ba16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis connector doesn't shade jackson dependency,FLINK-14896,13269836,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,dannycranmer,RustedBones,RustedBones,21/Nov/19 13:00,13/Jun/23 09:08,13/Jul/23 08:10,27/Sep/22 07:38,1.15.2,1.16.0,1.9.0,,,,,,,1.17.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,"flink-kinesis-connector depends on aws java sdk which is shaded to {{org.apache.flink.kinesis.shaded.com.amazonaws.}}

 

{{However, the aws sdk has a transitive dependency to jackson wich is not shaded in the artifact.}}

 

{{This creates problem when running flink on YARN: }}{{The aws sdk requires jackson-core v2.6 but hadoop pulls in 2.3. See [here|https://github.com/apache/flink/blob/e7c11ed672013512e5b159e7e892b27b1ef60a1b/flink-yarn/pom.xml#L133].}}

 

{{If YARN uses the loads wrong jackson version from classpath. Jod fails with}}
{code:java}
2019-11-20 17:23:11,563 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler    - Unhandled exception.org.apache.flink.client.program.ProgramInvocationException: The program caused an error:     at org.apache.flink.client.program.OptimizerPlanEnvironment.getOptimizedPlan(OptimizerPlanEnvironment.java:93)    at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:80)    at org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils$JarHandlerContext.toJobGraph(JarHandlerUtils.java:126)    at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$getJobGraphAsync$6(JarRunHandler.java:142)    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.enable([Lcom/fasterxml/jackson/core/JsonParser$Feature;)Lcom/fasterxml/jackson/databind/ObjectMapper;    at com.amazonaws.partitions.PartitionsLoader.<clinit>(PartitionsLoader.java:54)    at com.amazonaws.regions.RegionMetadataFactory.create(RegionMetadataFactory.java:30)    at com.amazonaws.regions.RegionUtils.initialize(RegionUtils.java:65)    at com.amazonaws.regions.RegionUtils.getRegionMetadata(RegionUtils.java:53)    at com.amazonaws.regions.RegionUtils.getRegion(RegionUtils.java:107)    at com.amazonaws.client.builder.AwsClientBuilder.getRegionObject(AwsClientBuilder.java:256)    at com.amazonaws.client.builder.AwsClientBuilder.setRegion(AwsClientBuilder.java:460)    at com.amazonaws.client.builder.AwsClientBuilder.configureMutableProperties(AwsClientBuilder.java:424)    at com.amazonaws.client.builder.AwsAsyncClientBuilder.build(AwsAsyncClientBuilder.java:80)
...
{code}
The flink-kinesis-connector should do as other connectors: shade jackson or use the flink-shaded-jackson core dependency",AWS EMR 5.28.0,afedulov,aljoscha,dannycranmer,knaufk,RustedBones,,,,,,,,,"RustedBones commented on pull request #10285: [FLINK-14896] [flink-kinesis-connector] Set jackson and guava dependency to flink-shaded
URL: https://github.com/apache/flink/pull/10285
 
 
   ## What is the purpose of the change
   
   - Avoid missing / conflict librabry error due to exclusion rule
   - Depends on [flink-shaded](https://github.com/apache/flink-shaded) as it is intended
   > The purpose of these dependencies is to provide a single instance of a shaded dependency in the Flink distribution, instead of each individual module shading the dependency.
   
   ## Brief change log
   
   - Add `flink-shaded-jackson` and `flink-shaded-guava` as provided dependency
   - Relocate jackson and guava to flink-shaded package
   - Fix wrong shading naming package (TBD since this this will break user code)
   
   ## Verifying this change
   
   This change is affecting packaging. 
   Can be tested by running flink with classpath containing old versions of jacskson/guava
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): yes
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no (except shaded imports)
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Nov/19 18:36;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 27 07:38:50 UTC 2022,,,,,,,,,,"0|z08vg0:",9223372036854775807,"Shade and relocate transitive Jackson dependencies of {{flink-connector-kinesis}}. If your Flink job was transitively relying on the these, you may need to include additional Jackson dependencies into your project.",,,,,,,,,,,,,,,,,,,"21/Nov/19 13:39;RustedBones;After digging deeper, this bug only happens if user make custom use of the shaded aws sdk.

In our case, we create a DynamoDbClient in our code, which leads to the issue. If user does not access the sdk, the problem does not trigger.

Not packaging the required {{jackson-databind}} dependency from the aws sdk is not intuitive and can easily lead to ClassNotFoundError or mismatched version.;;;","22/Nov/19 08:21;aljoscha;You mean the bug in the Flink framework code triggers when you access the SDK? Or there is a problem purely in user code when accessing the SDK (the aws SDK)?;;;","22/Nov/19 10:41;RustedBones;The bug triggers only when user code access part of the AWS SDK which relies on jackson (in my case  the PartitionsLoader).

In our case, we were simply using the SDK in the main function to figure out the DynamoDb stream ARN from the table name. Seeing that flink includes the aws-sdk-dynamodb, I though it was safe to use it. It actually worked for some time until the classpath ordering changed when updating to 1.9. At this point, the jackson lib used was the one pulled by hadoop which is not compatible with the SDK.

Even if this does not impact the flink framework itself, I'considering this as a *minor* bug since this problem can be avoided without costs (flink already contains a compatible shaded version of jackson);;;","14/Apr/21 22:46;flink-jira-bot;This issue and all of its Sub-Tasks have not been updated for 180 days. So, it has been labeled ""stale-minor"". If you are still affected by this bug or are still interested in this issue, please give an update and remove the label. In 7 days the issue will be closed automatically.;;;","22/Apr/21 22:56;flink-jira-bot;This issue has been labeled ""stale-minor"" for 7 days. It is closed now. If you are still affected by this or would like to raise the priority of this ticket please re-open, removing the label ""auto-closed"" and raise the ticket priority accordingly.;;;","29/Jul/21 07:31;knaufk;Re-opening in accordance with https://issues.apache.org/jira/browse/FLINK-23206.;;;","14/Sep/22 21:26;afedulov;[~danny.cranmer] could you address this one? It causes issues in the user code.;;;","15/Sep/22 21:47;dannycranmer;Copy from Slack (https://apache-flink.slack.com/archives/C03GV7L3G2C/p1663190927323999)

I have done some digging and cannot find anything to say why Jackson was not shaded. Here is the original commit to add the shade plugin: https://github.com/apache/flink/commit/fd3dba6e

I cannot see any public contracts that would be broken by using the shaded variant instead. That being said, it would be a breaking change since users could be relying on this transitive dependency. Therefore I would be inclined to fix in the next x.y.0 release. Thoughts?;;;","20/Sep/22 16:26;dannycranmer;There is a good discussion on the old PR https://github.com/apache/flink/pull/10285. I am inclined to shade Jackson since it is not exposed on the public interfaces, and the majority of dependencies are shaded for this connector, including AWS SDK. This means users cannot change Jackson without potentially breaking AWS SDK. Since AWS SDK is shaded, the user cannot override this version.

Looking forwards, the new Kinesis connector does not use Jackson directly, or shade the AWS SDK.;;;","27/Sep/22 07:38;dannycranmer;Merged commit [{{54f81c4}}|https://github.com/apache/flink/commit/54f81c4a9d1e29b887d4bccafc30df0392146b94] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,
HybridOffHeapUnsafeMemorySegmentTest#testByteBufferWrap failed on Travis,FLINK-14894,13269832,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,gjy,gjy,21/Nov/19 12:54,20/May/20 14:55,13/Jul/23 08:10,20/May/20 14:55,1.10.0,,,,,,,,,1.10.2,1.11.0,,,Runtime / Network,Tests,,,,0,pull-request-available,test-stability,,,"{noformat}
HybridOffHeapUnsafeMemorySegmentTest>MemorySegmentTestBase.testByteBufferWrapping:2465 expected:<992288337> but was:<196608>
{noformat}

https://api.travis-ci.com/v3/job/258950527/log.txt",,azagrebin,gjy,guoyangze,kezhuw,trohrmann,yunta,,,,,,,,"azagrebin commented on pull request #10940: [FLINK-14894][core][mem] Do not explicitly release unsafe memory when managed segment is freed
URL: https://github.com/apache/flink/pull/10940
 
 
   ## What is the purpose of the change
   
   The conclusion at the moment is that release unsafe memory, while potentially having link on it in Java code, is dangerous. We revert this to rely only on GC when there are no links in Java code. The problem can happen e.g. if task thread exits w/o joining with IO threads (e.g. spilling in batch job) then the unsafe memory is released but it can be written w/o segfault by IO thread. At the same time, other task can allocate interleaving memory which can be spoiled by that IO thread. We still keep it unsafe to allocate it outside of JVM direct memory limit to not interfere with direct allocations, also it does not make sense for RocksDB native memory (also accounted in MemoryManager) to be part of direct memory limit.
   
   The potential downside can be that over-allocating of unsafe memory will not hit the direct limit and will not cause GC immediately which will be the only way to release it. In this case, it can cause out-of-memory failures w/o triggering GC to release a lot of potentially already unused memory.
   
   If we see the delayed release as a problem then we can investigate further optimisations, like:
   - directly monitoring phantom reference queue of the cleaner (if JVM detects quickly that there are no more reference to the memory) and explicitly release memory ready for GC asap, e.g. after Task exit
   - monitor allocated memory amount and block allocation until GC releases occupied memory instead of failing with out-of-memory immediately
   
   ## Brief change log
   
   remove `cleaner` from `HybridMemorySegment` and `cleaner#run` from `HybridMemorySegment#free`
   
   ## Verifying this change
   
   existing tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (can be)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jan/20 15:20;githubbot;600","tillrohrmann commented on pull request #10940: [FLINK-14894][core][mem] Do not explicitly release unsafe memory when managed segment is freed
URL: https://github.com/apache/flink/pull/10940
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jan/20 17:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,FLINK-15758,,,,,,,,,,,,,,FLINK-13985,,,,FLINK-13980,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 14:55:07 UTC 2020,,,,,,,,,,"0|z08vf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/19 13:56;azagrebin;I think this is due to [FLINK-13985|https://jira.apache.org/jira/browse/FLINK-13985], where we decided that we will explicitly release underlying unsafe memory in HybridMemorySegment#free for new unsafe allocations. It looks like 

MemorySegmentTestBase#testByteBufferWrapping explicitly tests the contract that the underlying memory wrapped into a nio ByteBuffer is usable after the segment freeing. This is true for the nio DirectByteBuffer as the wrapping ByteBuffer keeps a strong link to it. So the original memory is not released both explicitly and during GC in this case but this does not hold for the new unsafe allocation after the explicit release.
[~sewen] do you remember why would we need the wraps to be valid after freeing the main segment? do we still need this?;;;","29/Nov/19 13:55;tison;another instance https://api.travis-ci.com/v3/job/261580360/log.txt;;;","06/Jan/20 07:57;yunta;Another instance [https://api.travis-ci.com/v3/job/272484106/log.txt];;;","24/Jan/20 15:17;azagrebin;[~sewen] [~trohrmann] and me had an offline discussion.

The conclusion at the moment is that release unsafe memory, while potentially having link on it in Java code, is dangerous. We revert this to rely only on GC when there are no links in Java code. The problem can happen e.g. if task thread exits w/o joining with IO threads (e.g. spilling in batch job) then the unsafe memory is released but it can be written w/o segfault by IO thread. At the same time, other task can allocate interleaving memory which can be spoiled by that IO thread. We still keep it unsafe to allocate it outside of JVM direct memory limit to not interfere with direct allocations, also it does not make sense for RocksDB native memory (also accounted in MemoryManager) to be part of direct memory limit.

The potential downside can be that over-allocating of unsafe memory will not hit the direct limit and will not cause GC immediately which will be the only way to release it. In this case, it can cause out-of-memory failures w/o triggering GC to release a lot of potentially already unused memory.

If we see the delayed release as a problem then we can investigate further optimisations (FLINK-15758), like:
 * directly monitoring phantom reference queue of the cleaner (if JVM detects quickly that there are no more reference to the memory) and explicitly release memory ready for GC asap, e.g. after Task exit
 * monitor allocated memory amount and block allocation until GC releases occupied memory instead of failing with out-of-memory immediately;;;","24/Jan/20 17:31;trohrmann;Fixed via

master: 6dcaae0e403a8d7322d5c63e82b01ed24340d984
1.10.0: 2c7dac62e10a8a2aa567281f10c18340303f4f17;;;","27/Jan/20 14:56;trohrmann;Reverting the commits of this issue because they are causing higher memory pressure. Due to this, our Travis builds are failing because we don't release the unsafely allocated memory blocks fast enough:

master: 676b53fe799374aa9e86ce65a12bc788a5b42f2f
1.10.0: 1c57e8ec31b0dd6fe873ee17f2437f091364744a

I guess this means that we should monitor the unsafe memory usage and try to recycle freed segments when requesting new segments. This will prevent the accumulation of unused memory segments which causes memory pressure.;;;","20/May/20 14:55;azagrebin;should be fixed by FLINK-15758;;;",,,,,,,,,,,,,,,,,,,,,,,,,
PythonScalarFunctionOperator should be chained with upstream operators by default,FLINK-14891,13269827,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,21/Nov/19 12:33,02/Dec/19 16:00,13/Jul/23 08:10,02/Dec/19 16:00,1.10.0,,,,,,,,,1.10.0,,,,API / Python,,,,,0,pull-request-available,,,,Currently the default chaining strategy for PythonScalarFunctionOperator is not set and it's HEAD by default. We should set the default value as ALWAYS.,,dian.fu,hequn8128,,,,,,,,,,,,"dianfu commented on pull request #10280: [FLINK-14891][python] Set the default chaining strategy to ALWAYS for Python operators
URL: https://github.com/apache/flink/pull/10280
 
 
   
   ## What is the purpose of the change
   
   *This pull request set the default chaining strategy to ALWAYS for Python operators.*
   
   ## Brief change log
   
     - *Set the default chaining strategy to ALWAYS for Python operators.*
   
   ## Verifying this change
   This change added tests and can be verified as follows:
   
     - *Added tests PythonScalarFunctionOperatorTestBase#testPythonScalarFunctionOperatorIsChainedByDefault*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Nov/19 12:38;githubbot;600","hequn8128 commented on pull request #10280: [FLINK-14891][python] Set the default chaining strategy to ALWAYS for Python operators
URL: https://github.com/apache/flink/pull/10280
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Dec/19 15:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 02 16:00:36 UTC 2019,,,,,,,,,,"0|z08ve0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/19 16:00;hequn8128;Fixed in 1.10.0 via b0fc92b4883270faec68bde70403fed8cc8bd15a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
(Partial fix) Potential deadlock for task reading from blocking ResultPartition.,FLINK-14872,13269528,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,20/Nov/19 08:04,07/Dec/19 19:45,13/Jul/23 08:10,07/Dec/19 19:45,1.9.1,,,,,,,,,1.10.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"Currently, the buffer pool size of InputGate reading from blocking ResultPartition is unbounded which have a potential of using too many buffers and may lead to ResultPartition of the same task can not acquire enough core buffers and finally lead to deadlock.

Considers the following case:

Core buffers are reserved for InputGate and ResultPartition -> InputGate consumes lots of Buffer (not including the buffer reserved for ResultPartition) -> Other tasks acquire exclusive buffer for InputGate and trigger redistribute of Buffers (Buffers taken by previous InputGate can not be released) -> The first task of which InputGate uses lots of buffers begin to emit records but can not acquire enough core Buffers (Some operators may not emit records out immediately or there is just nothing to emit) -> Deadlock.

 

I think we can fix this problem by limit the number of Buffers can be allocated by a InputGate which reads from blocking ResultPartition.",,fanrui,gaoyunhaii,kevin.cyj,klion26,lzljs3620320,maguowei,pnowojski,wind_ljy,,,,,,"wsry commented on pull request #10472: [FLINK-14872][runtime] Temporary fix for potential deadlock problem when tasks read from blocking ResultPartitions.
URL: https://github.com/apache/flink/pull/10472
 
 
   ## What is the purpose of the change
   
   This commit implements a temporary fix for the potential deadlock problem reported in FLINK-14872. The problem itself is not solved completely, however the possibility of deadlock is largely reduced. We leave the proper fix of this problem to the future version. Because the fix does not solve the deadlock problem completely and the fix is trivial, so no new test is attached.
   
   
   ## Brief change log
   
     - Reduce the max size of local buffer pool from ```Integer.MAX_VALUE``` to ```floatingNetworkBuffersPerGate``` for InputGates reading from blocking result partition.
   
   
   ## Verifying this change
   
   Because the fix does not solve the deadlock problem completely and the fix is trivial, so no new test is attached.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 16:34;githubbot;600","pnowojski commented on pull request #10472: [FLINK-14872][runtime] Temporary fix for potential deadlock problem when tasks read from blocking ResultPartitions.
URL: https://github.com/apache/flink/pull/10472
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Dec/19 15:50;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13203,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 07 16:03:34 UTC 2019,,,,,,,,,,"0|z08tjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/19 07:14;kevin.cyj;Only limit the number of Buffers can be allocated can only reduce the possibility of deadlock and gives users a chance to increase the number of network buffers by configuration. To totally solve the problem, we may also need to request core buffers directly from global NetworkBufferPool like the exclusive buffers for InputGate, we may add a method to local buffer pool like reserveRequiredBuffers and call it at result partition setup. Currently, core buffer request of the result partition is lazy, the buffer is not allocated from global pool until emitting records. Like the exclusive buffer request for InputGate, we can add a request time out and let the users know that the number of network buffer should be increased.

Another possible option is letting the InputGate release Buffer by spilling to disk, though the implementation is more complicated.

Letting the ResultPartition reserve core buffer at setup may lead to waste of Buffer, but I think that is not a big problem, most of the task will emit records sooner or latter.;;;","21/Nov/19 08:28;kevin.cyj;Theoretically, the problem is also exist with Pipelined mode. It is because the number extra Buffers a pipeline gate can use is very limited by default (8), the possibility is small enough. However, if the users increase the config value of taskmanager.network.memory.floating-buffers-per-gate, then the possibility of deadlock may increase.;;;","21/Nov/19 11:18;pnowojski;Is this a duplicate of https://issues.apache.org/jira/browse/FLINK-13203 ?;;;","21/Nov/19 11:51;kevin.cyj;[~pnowojski] They are relevant and the causes are similar though not exactly the same. Even with the timeout fix, there is still deadlock problem.

The issue reported in FLINK-12852 is because the down stream relies the upstream to release Buffer, but the upstream relies downstream to consume data before recycling Buffers.

The issue reported in this Jira is because the ResultPartition relies the InputGate to release Buffer, but the InputGate relies the data to be processed and emitted to ResultPartition.;;;","21/Nov/19 14:08;pnowojski;Ok, I see. Do you have a proposal how to actually change the buffers assignment? The number of exclusive/floating buffers?;;;","22/Nov/19 05:32;kevin.cyj;There are some choices we may consider:
 # Like Blink, each buffer pool has a finite min and max size which are calculate from number of channels and config options like buffer-per-channel, JM and RM will calculate according the max value a buffer pool can use, that is, the max value is guaranteed and no buffer pool will use more than the max value. The weak point of this option would be guaranteeing the max value and no floating may lead to waste of network memory.
 # As mentioned in FLINK-13203, no buffer floating, the downside is also waste  of memory, compared with 1, the good side would be we do not need to calculate the number of buffers at JM and RM.
 # As mentioned in FLINK-13203, make buffers always revokable by spilling. The problem we must consider would be when, how and which thread do the spill, which is not trivial. A choice may be allocating exclusive and core memory at setup and notify the buffer pool owner to release buffer if no enough buffer available. NetworkBufferPool may decide many buffers should be freed by each local pool. The problem of this option is some times we may not need to spill and only need to wait some more time. And another problem is that the Implementation is a little complicated.
 # The simplest way may be just like what we did for exclusive buffer, that is, allocating core (required) buffers at setup and giving a timeout. The advantage of this option is that it dose not change the behavior of the system and it's simple enough, though it dose not solves the deadlock problem directly.
 # Split the network memory into core and floating parts. The core buffers are not floatable, and we only reserve buffers from the core pool. The floating buffers are shared among all local pools. However this decrease the core buffer we can use and can lead to buffer insufficient problem, the users may need to reconfig the network buffer. Option 2 can be a special case of this one if we set the floating buffer to 0 (no floating). I would prefer this solution if the side impact on user is acceptable.

For a short term fix, I would prefer 4. For a long term solution, I think we can consider 3 and 5. What do you think? [~pnowojski];;;","22/Nov/19 07:07;lzljs3620320;Thanks [~kevin.cyj] and [~pnowojski], I provide some test background information, when we test TPC-DS with 10TB in batch mode and batch shuffle type:

It is very likely to bump into the deadlock situation. Try several times, and it is still possible to bump into it. Maybe the upstream data scale is too large, and it is very easy to fill all buffers (even if setting a large network buffer).;;;","22/Nov/19 08:38;pnowojski;[~kevin.cyj] let me think about it a bit more. 

Side note, please do not confuse floating buffers (credit based flow control) and optional buffers. Floating buffers can float within for example {{InputGate}} between channels. Optional buffers can be floating or exclusive, InputGate/ResultPartition will request required buffers (1 per channel) and optional (1 per channel + floating).
 
I personally do not like the spilling approach. If we had to implement something, I would be voting more toward ""assign required buffers immediately, and recommended only after a downstream consumers are guaranteed to make a progress"". 

For a quick fix, we might want to configure {{InputGate}} for {{BoundedBlockingSubpartition}} to request always obligatory ""1 exclusive buffer per channel + couple of floating"", without any ""optional"" buffers. Probably we could go away without any floating buffers, as performance will be bottlenecked by reading from files on the sender side.
 
There is one more dimension here. There is a semi known issue, that Flink allocates/requires way too many buffers in the network stack, which causes problems with checkpointing under backpressure and with general memory requirements. Currently we try requesting 2 exclusive per channel + 8 floating both on the input and output. I suspect we could cut it down to:
* on the input: 0 exclusive per channel + 10 (8? 20? 40?) floating
* on the output: 1 exclusive per channel + 10 (8? 20?) floating  

Without negative performance effects (10 floating buffers should be enough to saturate 1gbps network with 1 ms message round trip). However having 0 exclusive buffers per channel on the input, would require some changes/rethinking of credits assigning mechanism.

After cutting the exclusive buffers from 4 per channel (current input + output) down to 1, we could make all of them obligatory, which should solve dead lock issues, right?;;;","02/Dec/19 10:21;kevin.cyj;> If we had to implement something, I would be voting more toward ""assign required buffers immediately, and recommended only after a downstream consumers are guaranteed to make a progress"".

After reconsidering the problem, I think this should be the right direction. Maybe the easiest way to fix the problem is always using the optimal number of require buffer (numberOfSubpartitions * networkBuffersPerChannel + floatingNetworkBuffersPerGate) for BufferPool of ResultPartition and always assigning required buffer immediately (before assigning exclusive buffer for InputGate). This guarantees that no more buffers than the require one will be used by ResultPartition, which solves the problem reported in FLINK-12852. And assigning required buffers immediately before assigning exclusive buffers for InputGate can solve the problem reported in this JIRA. Then we can remove the previous timeout fix safely. There should be no performance problem because both the blocking and pipelined ResultPartition never use buffers more than the above optimal number when using default configuration. The only problem is that we may use more require buffer than before.

What do you think? [~pnowojski]

> After cutting the exclusive buffers from 4 per channel (current input + output) down to 1, we could make all of them obligatory, which should solve dead lock issues, right?

There are argument checks and we can not set the number of exclusive buffer per channel to 0 currently.;;;","03/Dec/19 15:17;pnowojski;{quote}
. The only problem is that we may use more require buffer than before.
{quote}
I think this is the blocker, which would cause quite a lot of deployments to start failing. As I wrote above, for the quick fix, I would vote for:
{quote}
For a quick fix, we might want to configure InputGate for BoundedBlockingSubpartition to request always obligatory ""1 exclusive buffer per channel + couple of floating"", without any ""optional"" buffers. Probably we could go away without any floating buffers, as performance will be bottlenecked by reading from files on the sender side.
{quote}
{quote}
There are argument checks and we can not set the number of exclusive buffer per channel to 0 currently.
{quote}
I think it's not only those checks, but currently code always assumes that it can send some exclusive buffers and this is used to propagate pending backlog lenght. With 0 exclusive buffers on the input gates, upstream producer, after completing a buffer, would have to pro-actively send a ""credit request message"", to acquire a credit.;;;","04/Dec/19 02:55;kevin.cyj;Thanks for the comment. [~pnowojski]

Making the BoundedBlockingSubpartition request always obligatory ""1 exclusive buffer per channel + couple of floating"" can solve the problem of BoundedBlockingSubpartition and is simple enough. I like this idea as a quick fix for version 1.10 and we can leave the final proper fix of both blocking and pipeline mode to future version.

Do you mind if I open a PR and fix the problem for blocking mode just in this way?;;;","04/Dec/19 16:09;pnowojski;Please do so :)

Am I right that the proper fix for this issue is the probably the same as for the FLINK-13203?;;;","05/Dec/19 13:30;kevin.cyj;[~pnowojski] You are right. We may add a related link to this Jira in FLINK-13203.

I will open a PR soon.;;;","06/Dec/19 12:39;kevin.cyj;Sadly, I find the number of exclusive buffer a channel requested is always the number of configured exclusive buffer per channel, we can not allocate only one exclusive buffer when exclusive buffer per channel is set to 2 or more except that we change the interface of NetworkBufferPool, which is what I don't want to do. Then what I can do is to reduce the number of optional buffers for InputGate to 0 and finally no buffer can float. What do you think? [~pnowojski];;;","06/Dec/19 13:29;kevin.cyj;By the way, I will test the change by running tpc-ds benchmark and guarantee that there is no performance regression.;;;","06/Dec/19 14:13;pnowojski;{quote}
Then what I can do is to reduce the number of optional buffers for InputGate to 0 and finally no buffer can float. 
{quote}
What do you mean by that?;;;","06/Dec/19 14:18;kevin.cyj;[~pnowojski] I mean creating a zero size buffer pool for InputGates that reading data from blocking ResultPartition. Just like this:
{code:java}
static SupplierWithException<BufferPool, IOException> createBufferPoolFactory(
      BufferPoolFactory bufferPoolFactory,
      int networkBuffersPerChannel,
      int floatingNetworkBuffersPerGate,
      int size,
      ResultPartitionType type) {
   int maxNumberOfMemorySegments = type.isBounded() ? floatingNetworkBuffersPerGate : 0;
   return () -> bufferPoolFactory.createBufferPool(0, maxNumberOfMemorySegments);
}
{code};;;","06/Dec/19 16:40;kevin.cyj;As discussed, we would implement a temporary fix for the problem for version 1.10 and leave the proper fix to the future version of Flink. I have opened a PR [https://github.com/apache/flink/pull/10472], could you please take a look? [~pnowojski];;;","07/Dec/19 15:52;pnowojski;merged commit 1863bb8 to master.

[~kevin.cyj] can you confirm/validate the fix on a cluster? ;;;","07/Dec/19 16:03;kevin.cyj;[~pnowojski] Thanks for the review. I have already tested the fix on our cluster and it works. The previous blocked tpc-ds query now can finish successfully. Sorry for the late update.;;;",,,,,,,,,,,,
A few documentation links are broken,FLINK-14866,13269510,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,phoenixjiangnan,dian.fu,dian.fu,20/Nov/19 05:56,22/Nov/19 13:40,13/Jul/23 08:10,21/Nov/19 18:31,1.10.0,,,,,,,,,1.10.0,,,,Documentation,,,,,0,pull-request-available,,,,"The links for udfs.html and functions.html doesn't work any more.

udfs.html and functions.html are referenced in a few places, i.e.:
[https://raw.githubusercontent.com/apache/flink/master/docs/dev/table/sql.md]",,dian.fu,liyu,phoenixjiangnan,,,,,,,,,,,"bowenli86 commented on pull request #10273: [FLINK-14866][doc] fix a few broken doc links
URL: https://github.com/apache/flink/pull/10273
 
 
   ## What is the purpose of the change
   
   fix a few broken doc links
   
   ## Brief change log
   
   fix a few broken doc links
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
   N/A
   
   ## Documentation
   
   N/A
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 22:53;githubbot;600","bowenli86 commented on pull request #10273: [FLINK-14866][doc] fix a few broken doc links
URL: https://github.com/apache/flink/pull/10273
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Nov/19 18:31;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14928,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 22 13:37:11 UTC 2019,,,,,,,,,,"0|z08tfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/19 06:06;dian.fu;Cc [~phoenixjiangnan] [~yunta];;;","21/Nov/19 18:31;phoenixjiangnan;master: e14bd50fed42e89674ba9d01a231d5c5e59f490c;;;","22/Nov/19 13:37;liyu;[~phoenixjiangnan] is this duplicated with FLINK-14760? If so, could you update FLINK-14760 to reflect the relationship of the two? Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable tests PyFlinkBlinkStreamUserDefinedFunctionTests#test_udf_in_join_condition_2,FLINK-14865,13269462,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,dian.fu,dian.fu,20/Nov/19 01:49,26/Nov/19 06:01,13/Jul/23 08:10,26/Nov/19 06:01,1.10.0,,,,,,,,,1.10.0,,,,API / Python,,,,,0,pull-request-available,,,,"{code:java}
Caused by: org.apache.beam.vendor.guava.v26_0_jre.com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: Cannot run program ""/tmp/32b29e73-3348-4326-bc06-69f6adda04ea_pyflink-udf-runner.sh"": error=26, Text file busy547E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4966)548E at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:211)549E at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:202)550E at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory.forStage(DefaultJobBundleFactory.java:185)551E at org.apache.flink.python.AbstractPythonFunctionRunner.open(AbstractPythonFunctionRunner.java:201)552E at org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator$ProjectUdfInputPythonScalarFunctionRunner.open(AbstractPythonScalarFunctionOperator.java:177)553E at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.open(AbstractPythonFunctionOperator.java:114)554E at org.apache.flink.table.runtime.operators.python.AbstractPythonScalarFunctionOperator.open(AbstractPythonScalarFunctionOperator.java:137)555E at org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator.open(BaseRowPythonScalarFunctionOperator.java:83)556E at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:585)557E at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:436)558E at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)559E at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)560E ... 1 more561E Caused by: java.io.IOException: Cannot run program ""/tmp/32b29e73-3348-4326-bc06-69f6adda04ea_pyflink-udf-runner.sh"": error=26, Text file busy562E at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)563E at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)564E at org.apache.beam.runners.fnexecution.environment.ProcessManager.startProcess(ProcessManager.java:133)565E at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:120)566E at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:178)567E at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:162)568E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528)569E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277)570E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)571E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044)572E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.get(LocalCache.java:3952)573E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)574E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)575E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4964)576E ... 13 more577E Suppressed: java.lang.NullPointerException: Process for id does not exist: 1578E at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)579E at org.apache.beam.runners.fnexecution.environment.ProcessManager.stopProcess(ProcessManager.java:147)580E at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:139)581E ... 23 more582E Caused by: java.io.IOException: error=26, Text file busy583E at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)584E at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)585E at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)586E at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)587E ... 26 more588589
{code}
[https://travis-ci.org/apache/flink/jobs/613990821]",,dian.fu,hequn8128,,,,,,,,,,,,"WeiZhong94 commented on pull request #10310: [FLINK-14865][python] fix unstable tests PyFlinkBlinkStreamUserDefinedFunctionTests#test_udf_in_join_condition_2
URL: https://github.com/apache/flink/pull/10310
 
 
   ## What is the purpose of the change
   
   *This pull request fixes the unstable tests PyFlinkBlinkStreamUserDefinedFunctionTests#test_udf_in_join_condition_2*
   
   ## Brief change log
   
     - *Create and write the shell script file in subprocess to avoid the ""text file is busy"" exception.*
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *test_udf.py*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Nov/19 09:21;githubbot;600","hequn8128 commented on pull request #10310: [FLINK-14865][python] fix unstable tests PyFlinkBlinkStreamUserDefinedFunctionTests#test_udf_in_join_condition_2
URL: https://github.com/apache/flink/pull/10310
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/19 06:00;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 26 06:00:51 UTC 2019,,,,,,,,,,"0|z08t4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/19 02:05;hequn8128;[~dian.fu] [~zhongwei] Not sure if this is a bug of JDK. see [https://bugs.openjdk.java.net/browse/JDK-8068370|https://bugs.openjdk.java.net/browse/JDK-8068370]. Could you help to check this? [~zhongwei];;;","26/Nov/19 06:00;hequn8128;Resolved in 1.10.0 via 8f665be6092d138a60c7318eaca5d47da1538283;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
parallelism.default in flink-conf.yaml do not work which is a bug imported by[FLINK-14745] ,FLINK-14861,13269329,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,leonard,leonard,leonard,19/Nov/19 14:56,20/Nov/19 13:51,13/Jul/23 08:10,20/Nov/19 13:50,1.10.0,,,,,,,,,1.10.0,,,,Client / Job Submission,,,,,0,pull-request-available,,,,"I set parameter ""parallelism.default"" in flink-conf.yaml, but it's do not work any more when I rebased my branch to master. I debug and find it's a bug imported  by FLINK-14745(https://issues.apache.org/jira/browse/FLINK-14745).

Detail: 
{code:java}
// ExecutionConfigAccessor#fromProgramOptions
public static ExecutionConfigAccessor fromProgramOptions(final ProgramOptions options, final List<URL> jobJars) {
   checkNotNull(options);
   checkNotNull(jobJars);

   final Configuration configuration = new Configuration();

   if (options.getParallelism() != ExecutionConfig.PARALLELISM_DEFAULT) {
      configuration.setInteger(CoreOptions.DEFAULT_PARALLELISM, options.getParallelism());
   }

   configuration.setBoolean(DeploymentOptions.ATTACHED, !options.getDetachedMode());
   configuration.setBoolean(DeploymentOptions.SHUTDOWN_IF_ATTACHED, options.isShutdownOnAttachedExit());

   ConfigUtils.encodeCollectionToConfig(configuration, PipelineOptions.CLASSPATHS, options.getClasspaths(), URL::toString);
   ConfigUtils.encodeCollectionToConfig(configuration, PipelineOptions.JARS, jobJars, URL::toString);

   SavepointRestoreSettings.toConfiguration(options.getSavepointRestoreSettings(), configuration);

   return new ExecutionConfigAccessor(configuration);
}{code}
 
 [1]. function executionConfigAccessor.getParallelism() will return 1 rather than -1 when options.getParallelism() == ExecutionConfig.PARALLELISM_DEFAULT because 
 when getParallelism() function will return the defaultValue of CoreOptions.DEFAULT_PARALLELISM.

 
{code:java}
// ExecutionConfigAccessor.java
public int getParallelism() {
 return configuration.getInteger(CoreOptions.DEFAULT_PARALLELISM);
} 
// Configuration.java
public int getInteger(ConfigOption<Integer> configOption) {
 return getOptional(configOption)
 .orElseGet(configOption::defaultValue);
}{code}
 

And function executionConfigAccessor.getParallelism()  still return 1 when options.getParallelism() == 1.

So， the following code in  CliFrontend.java will never reach if user not set parallelism in flink run command line.
{code:java}
// CliFrontend.java
int parallelism = executionParameters.getParallelism() == -1 ? defaultParallelism : executionParameters.getParallelism();{code}
[2]and another  position, I think we should keep three lines which deleted in FLINK-14745--. 
{code:java}
// 
int userParallelism = executionParameters.getParallelism();
LOG.debug(""User parallelism is set to {}"", userParallelism);
//if (ExecutionConfig.PARALLELISM_DEFAULT == userParallelism) {
//userParallelism = defaultParallelism;
// }
executeProgram(program, client, userParallelism, executionParameters.getDetachedMode());
 
{code}
 ",,kkl0u,leonard,,,,,,,,,,,,"kl0u commented on pull request #10254: [FLINK-14861][Client / Job Submission] parallelism.default in flink-conf.yaml do not work 
URL: https://github.com/apache/flink/pull/10254
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 13:51;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 20 13:50:54 UTC 2019,,,,,,,,,,"0|z08sbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/19 01:23;leonard;[~kkl0u]  Could you assignee this to me?  
 I submit a PR here,[FLINK-14861|https://github.com/apache/flink/pull/10254]

It's will be great if you can have a look on this。;;;","20/Nov/19 13:50;kkl0u;Merged with 948bab8db07b24d06ce2e2c41826e1c3cd47d1db;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Wrong ""if"" statement in SqlToOperationConverter",FLINK-14858,13269312,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,toutian,toutian,toutian,19/Nov/19 12:55,13/Apr/21 20:41,13/Jul/23 08:10,20/Nov/19 02:16,1.9.1,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"In class {{org.apache.flink.table.sqlexec.SqlToOperationConverter}}  
{code:java}
public static Optional<Operation> convert(
      FlinkPlannerImpl flinkPlanner,
      CatalogManager catalogManager,
      SqlNode sqlNode) {
   // validate the query
   final SqlNode validated = flinkPlanner.validate(sqlNode);
   SqlToOperationConverter converter = new SqlToOperationConverter(flinkPlanner, catalogManager);
   if (validated instanceof SqlCreateTable) {
      return Optional.of(converter.convertCreateTable((SqlCreateTable) validated));
   } if (validated instanceof SqlDropTable) {  // this should be ""else if"" not the ""if""
      return Optional.of(converter.convertDropTable((SqlDropTable) validated));
   } else if (validated instanceof RichSqlInsert) {
      SqlNodeList targetColumnList = ((RichSqlInsert) validated).getTargetColumnList();
      if (targetColumnList != null && targetColumnList.size() != 0) {
         throw new ValidationException(""Partial inserts are not supported"");
      }
      return Optional.of(converter.convertSqlInsert((RichSqlInsert) validated));
   } else if (validated.getKind().belongsTo(SqlKind.QUERY)) {
      return Optional.of(converter.convertSqlQuery(validated));
   } else {
      return Optional.empty();
   }
}{code}
 ",,dwysakowicz,jark,toutian,,,,,,,,,,,"KurtYoung commented on pull request #10253: [FLINK-14858][Table SQL / Planner] Wrong ""if"" statement in SqlToOperationConverter
URL: https://github.com/apache/flink/pull/10253
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 02:16;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 20 02:16:47 UTC 2019,,,,,,,,,,"0|z08s7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/19 12:58;toutian;This is a problem，and i want to fix it，please assign it for me，thank you~;;;","19/Nov/19 13:05;dwysakowicz;I assigned you [~toutian] to the ticket.;;;","20/Nov/19 02:16;ykt836;merged to 1.10.0: 0e4e60517231967b4caf6695e5a606595a9ca088;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix documentation about Hive dependencies,FLINK-14849,13269254,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lzljs3620320,lzljs3620320,19/Nov/19 07:53,06/Jan/20 16:07,13/Jul/23 08:10,27/Dec/19 18:15,,,,,,,,,,1.10.0,,,,Connectors / Hive,Documentation,,,,0,pull-request-available,,,,"{code:java}
With:
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-exec</artifactId>
    <version>3.1.1</version>
</dependency>

Caused by: java.lang.ClassCastException: org.codehaus.janino.CompilerFactory cannot be cast to org.codehaus.commons.compiler.ICompilerFactory
	at org.codehaus.commons.compiler.CompilerFactoryFactory.getCompilerFactory(CompilerFactoryFactory.java:129)
	at org.codehaus.commons.compiler.CompilerFactoryFactory.getDefaultCompilerFactory(CompilerFactoryFactory.java:79)
	at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.compile(JaninoRelMetadataProvider.java:432)
	... 68 more
{code}
After https://issues.apache.org/jira/browse/FLINK-13749 , flink-client will use default child-first resolve-order.

If user jar has some conflict dependents, there will be some problem.

Maybe we should update document to add some exclusions to hive dependents.",,jark,lirui,lzljs3620320,phoenixjiangnan,wind_ljy,,,,,,,,,"lirui-apache commented on pull request #10681: [FLINK-14849][hive][doc] Fix documentation about Hive dependencies
URL: https://github.com/apache/flink/pull/10681
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix the documentation about dependencies needed to use Hive connector.
   
   
   ## Brief change log
   
     - Update documentation about Hive dependencies
   
   
   ## Verifying this change
   
   NA
   
   ## Does this pull request potentially affect one of the following parts:
   
    NA
   
   ## Documentation
   
   NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 02:58;githubbot;600","bowenli86 commented on pull request #10681: [FLINK-14849][hive][doc] Fix documentation about Hive dependencies
URL: https://github.com/apache/flink/pull/10681
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 18:14;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 27 18:15:17 UTC 2019,,,,,,,,,,"0|z08ruo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/19 08:37;wind_ljy;I've come across the same issue and it took me really a lot of time to deal with the dependency conflicts between hive-metastore/exec repo and flink repo, which ends successfully like this (using hive 1.2.3):

{code:java}
<!-- Hive Metastore -->
<dependency>
        <groupId>org.apache.hive</groupId>
        <artifactId>hive-metastore</artifactId>
        <version>1.2.3</version>
        <exclusions>
                <exclusion>
                        <groupId>org.apache.hadoop</groupId>
                        <artifactId>*</artifactId>
                </exclusion>
                <exclusion>
                        <groupId>commons-cli</groupId>
                        <artifactId>*</artifactId>
                </exclusion>
        </exclusions>
</dependency>

<dependency>
        <groupId>org.apache.hive</groupId>
        <artifactId>hive-exec</artifactId>
        <version>1.2.3</version>
        <exclusions>
                <exclusion>
                        <groupId>commons-cli</groupId>
                        <artifactId>*</artifactId>
                </exclusion>
                <exclusion>
                        <groupId>com.google</groupId>
                        <artifactId>*</artifactId>
                </exclusion>
                <exclusion>
                        <groupId>org.apache.calcite</groupId>
                        <artifactId>*</artifactId>
                </exclusion>
        </exclusions>
</dependency>
{code}

Moreover, I have to deal with a special dependency called {{datanucleus}} when using a single fat jar for my job.
;;;","11/Dec/19 08:52;lzljs3620320;Thanks [~wind_ljy].

We should provide a util to help users or have a clear document.;;;","12/Dec/19 06:02;lzljs3620320;[~jark] Can you assign this to me?;;;","24/Dec/19 10:09;lirui;[~jark] You assigned the wrong Rui Li :D. My jira ID is lirui;;;","24/Dec/19 14:10;jark;Sorry. Re-assigned to you [~lirui].;;;","27/Dec/19 18:15;phoenixjiangnan;master: 28b6221aaaeb535afa728bb887e733b6576982a7
1.10: 7ada211863e02039b52088deb198611e8c762c47;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the default writerbuffer size documentation of RocksDB,FLINK-14846,13269238,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,19/Nov/19 06:28,20/Dec/19 13:16,13/Jul/23 08:10,26/Nov/19 10:05,,,,,,,,,,1.10.0,1.8.4,1.9.2,,Documentation,Runtime / State Backends,,,,0,pull-request-available,,,,"When introduce {{RocksDBConfigurableOptions}}, the default writer buffer size is referenced from RocksDB's javadoc. Unfortunately, RocksDB's official javadoc was described incorrectly as {{4MB}} for a long time until I create a [PR|https://github.com/facebook/rocksdb/pull/5670] to correct it. This also leads [our description|https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html#state-backend-rocksdb-writebuffer-size] of default write-buffer size not correct, we should fix this to avoid to mislead users.",,azagrebin,liyu,wind_ljy,yunta,,,,,,,,,,"Myasuka commented on pull request #10301: [FLINK-14846][doc] Correct the default writerbuffer size documentation of RocksDB
URL: https://github.com/apache/flink/pull/10301
 
 
   ## What is the purpose of the change
   
   Correct the default write buffer size of RocksDB to '64MB'.
   
   ## Brief change log
   Correct the default write buffer size of RocksDB to '64MB'
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Nov/19 18:05;githubbot;600","azagrebin commented on pull request #10301: [FLINK-14846][doc] Correct the default writerbuffer size documentation of RocksDB
URL: https://github.com/apache/flink/pull/10301
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/19 10:03;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 20 13:15:18 UTC 2019,,,,,,,,,,"0|z08rr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/19 06:31;yunta;I noticed that this has already mislead our training slides of Veverica at Flink-Forward-Europe. [~azagrebin], what do you think of this, please assign to me if possible.

 ;;;","23/Nov/19 07:09;liyu;Since the [rocksdb PR|https://github.com/facebook/rocksdb/pull/5670] on javadoc correction already accepted and merged by rocksdb community, I believe it's necessary to also update our documentation accordingly. Please open a PR for the change [~yunta], thanks.;;;","26/Nov/19 10:05;azagrebin;[~yunta] 
Should we also cherry pick your RocksDB PR to FRocksdb? I think we can merge it right away there as well.;;;","20/Dec/19 13:15;azagrebin;merged into master by 20f6976229ecd69eae933b5196a570808c088e51

merged into 1.9 by c22187205841b1bc7b9b1c769674b92c78a1244f

merged into 1.8 by c0f6f95353b5be284b0fd1cd53c5950ceeebbf56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming bucketing end-to-end test can fail with Output hash mismatch,FLINK-14843,13269166,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,banmoy,gjy,gjy,18/Nov/19 21:20,16/Jan/20 09:43,13/Jul/23 08:10,16/Jan/20 09:41,1.10.0,,,,,,,,,1.10.0,,,,Connectors / FileSystem,Tests,,,,0,pull-request-available,test-stability,,,"*Description*
Streaming bucketing end-to-end test ({{test_streaming_bucketing.sh}}) can fail with Output hash mismatch.

{noformat}
Number of running task managers has reached 4.
Job (e0b7a86e4d4111f3947baa3d004e083a) is running.
Waiting until all values have been produced
Truncating buckets
Number of produced values 26930/60000
Truncating buckets
Number of produced values 30890/60000
Truncating buckets
Number of produced values 37340/60000
Truncating buckets
Number of produced values 41290/60000
Truncating buckets
Number of produced values 46710/60000
Truncating buckets
Number of produced values 52120/60000
Truncating buckets
Number of produced values 57110/60000
Truncating buckets
Number of produced values 62530/60000
Cancelling job e0b7a86e4d4111f3947baa3d004e083a.
Cancelled job e0b7a86e4d4111f3947baa3d004e083a.
Waiting for job (e0b7a86e4d4111f3947baa3d004e083a) to reach terminal state CANCELED ...
Job (e0b7a86e4d4111f3947baa3d004e083a) reached terminal state CANCELED
Job e0b7a86e4d4111f3947baa3d004e083a was cancelled, time to verify
FAIL Bucketing Sink: Output hash mismatch.  Got 9e00429abfb30eea4f459eb812b470ad, expected 01aba5ff77a0ef5e5cf6a727c248bdc3.
head hexdump of actual:
0000000   (   2   ,   1   0   ,   0   ,   S   o   m   e       p   a   y
0000010   l   o   a   d   .   .   .   )  \n   (   2   ,   1   0   ,   1
0000020   ,   S   o   m   e       p   a   y   l   o   a   d   .   .   .
0000030   )  \n   (   2   ,   1   0   ,   2   ,   S   o   m   e       p
0000040   a   y   l   o   a   d   .   .   .   )  \n   (   2   ,   1   0
0000050   ,   3   ,   S   o   m   e       p   a   y   l   o   a   d   .
0000060   .   .   )  \n   (   2   ,   1   0   ,   4   ,   S   o   m   e
0000070       p   a   y   l   o   a   d   .   .   .   )  \n   (   2   ,
0000080   1   0   ,   5   ,   S   o   m   e       p   a   y   l   o   a
0000090   d   .   .   .   )  \n   (   2   ,   1   0   ,   6   ,   S   o
00000a0   m   e       p   a   y   l   o   a   d   .   .   .   )  \n   (
00000b0   2   ,   1   0   ,   7   ,   S   o   m   e       p   a   y   l
00000c0   o   a   d   .   .   .   )  \n   (   2   ,   1   0   ,   8   ,
00000d0   S   o   m   e       p   a   y   l   o   a   d   .   .   .   )
00000e0  \n   (   2   ,   1   0   ,   9   ,   S   o   m   e       p   a
00000f0   y   l   o   a   d   .   .   .   )  \n                        
00000fa
Stopping taskexecutor daemon (pid: 55164) on host gyao-desktop.
Stopping standalonesession daemon (pid: 51073) on host gyao-desktop.
Stopping taskexecutor daemon (pid: 51504) on host gyao-desktop.
Skipping taskexecutor daemon (pid: 52034), because it is not running anymore on gyao-desktop.
Skipping taskexecutor daemon (pid: 52472), because it is not running anymore on gyao-desktop.
Skipping taskexecutor daemon (pid: 52916), because it is not running anymore on gyao-desktop.
Stopping taskexecutor daemon (pid: 54121) on host gyao-desktop.
Stopping taskexecutor daemon (pid: 54726) on host gyao-desktop.
[FAIL] Test script contains errors.
Checking of logs skipped.

[FAIL] 'flink-end-to-end-tests/test-scripts/test_streaming_bucketing.sh' failed after 2 minutes and 3 seconds! Test exited with exit code 1
{noformat}


*How to reproduce*
Comment out the delay of 10s after the 1st TM is restarted to provoke the issue:

{code:bash}
echo ""Restarting 1 TM""
$FLINK_DIR/bin/taskmanager.sh start
wait_for_number_of_running_tms 4

#sleep 10

echo ""Killing 2 TMs""
kill_random_taskmanager
kill_random_taskmanager
wait_for_number_of_running_tms 2
{code}

Command to run the test:
{noformat}
FLINK_DIR=build-target/ flink-end-to-end-tests/run-single-test.sh skip flink-end-to-end-tests/test-scripts/test_streaming_bucketing.sh
{noformat}


",rev: dcc1330375826b779e4902176bb2473704dabb11,banmoy,gjy,kkl0u,klion26,liyu,,,,,,,,,"banmoy commented on pull request #10685: [FLINK-14843][e2e] Refactor bucketing sink test to make it more stable and comprehensible
URL: https://github.com/apache/flink/pull/10685
 
 
   ## What is the purpose of the change
   Refactor bucketing sink e2e test to make it more stable and comprehensible.
   
   ## Brief change log
   
   - For `BucketingSinkTestProgram`, set inactiveBucketCheckInterval and inactiveBucketThreshold to 2000 and 4000 respectively
   - only count records in part-* files to get total valid data
   - remove ""sleep 10"" between two failover to make it comprehensible
   
   ## Verifying this change
   
   Existing e2e test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 08:56;githubbot;600","kl0u commented on pull request #10685: [FLINK-14843][e2e] Refactor bucketing sink test to make it more stable and comprehensible
URL: https://github.com/apache/flink/pull/10685
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 09:43;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/19 06:45;gjy;complete_result;https://issues.apache.org/jira/secure/attachment/12986187/complete_result","19/Nov/19 06:44;gjy;flink-gary-standalonesession-0-gyao-desktop.log;https://issues.apache.org/jira/secure/attachment/12986195/flink-gary-standalonesession-0-gyao-desktop.log","19/Nov/19 06:44;gjy;flink-gary-taskexecutor-0-gyao-desktop.log;https://issues.apache.org/jira/secure/attachment/12986194/flink-gary-taskexecutor-0-gyao-desktop.log","19/Nov/19 06:44;gjy;flink-gary-taskexecutor-1-gyao-desktop.log;https://issues.apache.org/jira/secure/attachment/12986193/flink-gary-taskexecutor-1-gyao-desktop.log","19/Nov/19 06:44;gjy;flink-gary-taskexecutor-2-gyao-desktop.log;https://issues.apache.org/jira/secure/attachment/12986192/flink-gary-taskexecutor-2-gyao-desktop.log","19/Nov/19 06:44;gjy;flink-gary-taskexecutor-3-gyao-desktop.log;https://issues.apache.org/jira/secure/attachment/12986191/flink-gary-taskexecutor-3-gyao-desktop.log","19/Nov/19 06:44;gjy;flink-gary-taskexecutor-4-gyao-desktop.log;https://issues.apache.org/jira/secure/attachment/12986190/flink-gary-taskexecutor-4-gyao-desktop.log","19/Nov/19 06:44;gjy;flink-gary-taskexecutor-5-gyao-desktop.log;https://issues.apache.org/jira/secure/attachment/12986189/flink-gary-taskexecutor-5-gyao-desktop.log","19/Nov/19 06:44;gjy;flink-gary-taskexecutor-6-gyao-desktop.log;https://issues.apache.org/jira/secure/attachment/12986188/flink-gary-taskexecutor-6-gyao-desktop.log",,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 16 09:41:59 UTC 2020,,,,,,,,,,"0|z08rbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Dec/19 06:53;banmoy;I think it is not a bug, but how the test works. The output of test_streaming_bucketing.sh tells us that number of produced values is 62530, which is more than the expected 60000, so checksum fails. The duplicated data is from those pending files which isn't included in a checkpoint, and can't be truncated to remove duplicated data when job is restored. The meaning of ""sleep 10"" is waiting for at least one completed checkpoint before triggering another failover, so that pending files generated when job is closing are in the restored checkpoint. 10 seconds is enough because checkpoint interval is set to 4s in BucketingSinkTestProgram. Maybe we need to change the script to wait for a completed checkpoint explicitly . What do you think? [~gjy] [~kkl0u];;;","24/Dec/19 16:12;kkl0u;The fact that the duplicates can be the result of non-truncated data from before the failure sounds correct. {{Truncate}} is called upon recovery so we have to wait for all tasks to have successfully recovered before counting the valid records. 

I think the solution is to have *no* size limit for the pending files and to wait after recovery so that we have a high chance of having called {{truncate()}} after recovery. The no size requirement is due to the fact that pending files are never garbage collected, so if they are created and they do not belong to any checkpoint, then they will always affect the number of counted records.;;;","25/Dec/19 08:59;banmoy;I think an alternative solution is 
 # set inactiveBucketCheckInterval and inactiveBucketThreshold to a small value, such as 4 seconds
 # only count records in final state files with prefix of ""part-""

When source finish to send all records,  inactive bucket check will ensure in-progress files are transferred to pending files finally, and those pending files will be transferred to part-* files after completed checkpoints. So part-* files will finally contain all records and there is no duplication, which is just the exactly-once semantics of bucketing sink. In this way, there is no need to care about failover, and make the test more stable and comprehensible. I create a PR to make it more clear [https://github.com/apache/flink/pull/10685], and hope for your feedback [~kkl0u]. Thanks.;;;","26/Dec/19 09:22;banmoy;I think this is not a bug, but about the design of test, so I change the type and priority of this issue temporarily.;;;","15/Jan/20 04:38;liyu;Another instance which fails release-1.10 nightly build: https://api.travis-ci.org/v3/job/636999143/log.txt;;;","15/Jan/20 04:40;liyu;Upgrading to Critical since it reproduces in release-1.10 nightly build, and I suggest to fix it as soon as possible if the given PR looks good [~banmoy] [~gjy]. Thanks.;;;","15/Jan/20 04:46;liyu;Disclaimer: the latest release-1.10 nightly build fails at a different case: 'SQL Client end-to-end test for Kafka 0.10', but with the same error (Output hash mismatch). Please check it and feel free to open a new issue if turns out the root cause is different [~banmoy], thanks.

Another instance: https://api.travis-ci.org/v3/job/636999153/log.txt
in the same build: https://travis-ci.org/apache/flink/builds/636999105;;;","15/Jan/20 05:51;banmoy;[~liyu] Output hash mismatch for SQL Client end-to-end test for Kafka 0.10 is not the same issue for Streaming bucketing end-to-end test, and is tracked in [FLINK-14505|https://issues.apache.org/jira/browse/FLINK-14505];;;","15/Jan/20 06:04;liyu;Thanks for the investigation [~banmoy]. Downgrading priority here and will track FLINK-14505 instead.;;;","16/Jan/20 09:41;kkl0u;Merged on master with 9a58dedebef57cddd6e325ae40edc74aff6c6248
and on release-1.10 with 6ddcacd0abf9eac1fc5b6e2bc9dda94e453ec969;;;",,,,,,,,,,,,,,,,,,,,,,
Kerberized YARN on Docker test fails on Travis,FLINK-14834,13268942,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aljoscha,gjy,gjy,17/Nov/19 18:53,17/Dec/19 09:14,13/Jul/23 08:10,17/Dec/19 09:14,1.10.0,,,,,,,,,1.10.0,,,,Deployment / YARN,Runtime / Coordination,Tests,,,0,test-stability,,,,https://api.travis-ci.org/v3/job/612782888/log.txt,,aljoscha,gjy,klion26,liyu,pnowojski,trohrmann,wangyang0918,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14968,,,,,,,,,,FLINK-15007,FLINK-15013,,,,,,,,,,,,,,"28/Nov/19 17:56;aljoscha;run-with-3-slots.txt;https://issues.apache.org/jira/secure/attachment/12987089/run-with-3-slots.txt","28/Nov/19 17:56;aljoscha;run-with-4-slots.txt;https://issues.apache.org/jira/secure/attachment/12987090/run-with-4-slots.txt",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 09:14:26 UTC 2019,,,,,,,,,,"0|z08pxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/19 18:54;gjy;Another instance https://api.travis-ci.org/v3/job/612782882/log.txt;;;","18/Nov/19 09:53;gjy;Another instance https://api.travis-ci.org/v3/job/613079993/log.txt;;;","18/Nov/19 11:02;aljoscha;It seems it failed because of this:
{code}
The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: e69ae8462a5b75623d775928ec3ea1fc)
	at org.apache.flink.client.ClientUtils.submitJobAndWaitForResult(ClientUtils.java:144)
	at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:64)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1531)
	at org.apache.flink.streaming.examples.wordcount.WordCount.main(WordCount.java:96)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:333)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:217)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:183)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:747)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:282)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:219)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1012)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1085)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1085)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.ClientUtils.submitJobAndWaitForResult(ClientUtils.java:142)
	... 20 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate all requires slots within timeout of 120000 ms. Slots required: 14, slots allocated: 10, previous allocation IDs: [], execution status: completed: Attempt #0 (Source: Custom File Source (1/1)) @ org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@3f34f8de - [SCHEDULED], completed: Attempt #0 (Source: 
{code};;;","21/Nov/19 13:41;gjy;'Running Kerberized YARN on Docker test (default input)' also started failing
https://api.travis-ci.org/v3/job/614505046/log.txt;;;","22/Nov/19 08:13;liyu;Another instance for 'Running Kerberized YARN on Docker test (default input)': https://api.travis-ci.org/v3/job/615032422/log.txt;;;","22/Nov/19 08:15;aljoscha;The reason is also similar:
{code}
org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 517ed66541602083e10e5594216e9bfe)
	at org.apache.flink.client.ClientUtils.submitJobAndWaitForResult(ClientUtils.java:144)
	at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:64)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1531)
	at org.apache.flink.streaming.examples.wordcount.WordCount.main(WordCount.java:96)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:333)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:217)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:183)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:747)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:282)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:219)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1012)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1085)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1085)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.ClientUtils.submitJobAndWaitForResult(ClientUtils.java:142)
	... 20 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate all requires slots within timeout of 120000 ms. Slots required: 7, slots allocated: 3, previous allocation IDs: [], execution status: completed: Attempt #0 (Source: Collection Source (1/1)) @ org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@5bbe73e6 - [SCHEDULED], completed: Attempt #0 (Flat Map (1/3)) @ 
{code};;;","25/Nov/19 03:11;liyu;Another instance: https://api.travis-ci.org/v3/job/616253236/log.txt
With similar cause:
{noformat}
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$getJobExecutionResult$0(ClusterClientJobClientAdapter.java:81)
	... 19 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate all requires slots within timeout of 120000 ms.
Slots required: 7, slots allocated: 3, previous allocation IDs: [], execution status: completed: Attempt #0 (Source: Collection Source (1/1)) @ 
{noformat};;;","26/Nov/19 17:22;aljoscha;I think I found the culprit for the failure of the {{dummfs}} tests. It's this change: https://github.com/apache/flink/commit/749965348170e4608ff2a23c9617f67b8c341df5. This changes the job to have two sources instead of one which, under normal circumstances, requires too many slots to run and therefore the job will fail.

For the failure of the {{normal}} tests, I see in the logs that there is only two instances of
{code}
2019-11-20 21:32:48,107 INFO  org.apache.flink.yarn.YarnResourceManager                     - Requesting new TaskExecutor container with resources <memory:1000, vCores:1>. Number pending requests 1.
{code}
while the job requires three {{TaskExecutors}} to run. For a successful run of this test you will see three {{Requesting new TaskExecutor}} lines. I don't know why the {{JobMaster}} does not request more {{TaskExecutors}} but this could be a bug.;;;","28/Nov/19 17:56;aljoscha;Something very strange is going on. When I try this on a (dockerized) YARN cluster the job sometimes needs 3 slots to run and sometimes needs 4 slots. I run this job:

{code}
bin/flink run -m yarn-cluster -p 3 -yjm 2000 -ytm 2000 examples/streaming/WordCount.jar --input hdfs:///wc-in-1 --input hdfs:///wc-in-2 --output hdfs:///wc-out
{code}

The attached logs show the (DEBUG) jobmanager.log of two different runs. Try searching for ""Requesting new slot"".;;;","29/Nov/19 11:54;gjy;I also temporarily disabled the ""Kerberized YARN on Docker test (default input)"".

master: 85905f80e9711967711c2992612dccdd2cc211ac;;;","02/Dec/19 03:37;wangyang0918;I run the job on a real yarn cluster. It always need 3 slots and only 3 taskmanager are started.

It is very curious sometimes we need 4 slots. Maybe someone is familiar with scheduler could help.

[~zhuzh] What do you think?;;;","02/Dec/19 13:56;aljoscha;See the linked issues for analysis on which commit caused the issue.;;;","05/Dec/19 06:43;liyu;Correct me if I'm wrong, but from the comments in FLINK-15007 we could re-enable the ""Kerberized YARN on Docker"" tests now (FLINK-15013 is still open and necessary, but seems won't cause the test fail)? Thanks.;;;","05/Dec/19 10:22;aljoscha;FLINK-15013 is the reason why {{Running Kerberized YARN on Docker test (custom fs plugin)}} is failing, but I think {{Running Kerberized YARN on Docker test (default input)}} might work now.;;;","09/Dec/19 15:38;aljoscha;This issue can be clossed when FLINK-15013 is resolved.;;;","11/Dec/19 09:47;chesnay;Since [~trohrmann] is working on FLINK-15013 I'll assign him to this issue; just so it's marked as not requiring an assignee.;;;","11/Dec/19 14:08;aljoscha;The issue seems to be fixed on master. I'm currently bisecting to identify the ""culprit"".;;;","13/Dec/19 17:47;aljoscha;That comment was not correct, it was not fixed.;;;","17/Dec/19 09:14;aljoscha;Reactivated the tests on master in bd11e0541489cd59f03e671103997883c372e482

Reactivated the tests on release-1.10 in 68b4ebf33f8bff8b6a48a254ce417e6dbc52b1d8;;;",,,,,,,,,,,,,
Correct the link for chinese version stream_checkpointing page ,FLINK-14830,13268792,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,klion26,klion26,klion26,16/Nov/19 12:08,18/Nov/19 02:47,13/Jul/23 08:10,18/Nov/19 02:47,1.9.1,,,,,,,,,1.10.0,,,,Documentation,,,,,0,pull-request-available,,,,"Currently, in Chinese version of stream_checkpointing page, there are some links not correct set to the Chinese version, but set to the English version.

Such as 

{{[site.baseurl }}/dev/stream/state/index.html |https://github.com/apache/flink/blob/master/docs/internals/stream_checkpointing.zh.md]

_[state backend](\{{ site.baseurl }}/ops/state/state_backends.html)_.

[State Backends](\{{ site.baseurl }}/ops/state/state_backends.html)

[Restart Strategies](\{{ site.baseurl }}/dev/restart_strategies.html) 

 

This issue wants to fix the problem.",,jark,kkrugler,klion26,,,,,,,,,,,"klion26 commented on pull request #10233: [FLINK-14830][docs] Correct the external link for Chinese version stream_checkpointing page
URL: https://github.com/apache/flink/pull/10233
 
 
   ## What is the purpose of the change
   
   Correct the external link for Chinese version in stream_checkpointing page
   
   The page `stream_checkpointing` has not been translated yet, but the target page which the external link points to has been translated, currently, when we click the external link in `stream_checkpoint`, it will jump to the English version(not the Chinese version), this pr wants to fix this.
   
   
   ## Verifying this change
   
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Nov/19 02:40;githubbot;600","wuchong commented on pull request #10233: [FLINK-14830][docs] Correct the external link for Chinese version stream_checkpointing page
URL: https://github.com/apache/flink/pull/10233
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Nov/19 02:46;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 18 02:47:32 UTC 2019,,,,,,,,,,"0|z08p0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/19 12:08;klion26;[~jark] what do you think about this, please assign to me if this is valid. thanks.;;;","17/Nov/19 03:16;jark;Hi [~klion26], I think this is not a problem, because this is not translated yet. We should update the links to Chinese versions when translating. But before translation, I think it's fine to keep the content the same as English version, otherwise we have 200+ pages to update. What do you think?;;;","17/Nov/19 03:26;klion26;[~jark] I think this is a little tricky, because the target page has already been translated, but the link did not respect it yet.

The page[1] did not been translated yet, when we click the link of {{[Restart Strategies|https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/restart_strategies.html] }}  we'll redirect to English version currently, but the target page has already been translated[2]

I think this needs to be improved, and we need to update all the Chinese version even if they have not been translated yet to prevent this problem, what do you think?

 [1] [https://ci.apache.org/projects/flink/flink-docs-release-1.9/zh/internals/stream_checkpointing.html#recovery]

 [2] [https://ci.apache.org/projects/flink/flink-docs-release-1.9/zh/dev/task_failure_recovery.html];;;","17/Nov/19 03:33;jark;[~klion26], I agree with you this is bad for user experience. I think either update all the links in every {{.zh.md}} pages, or update links when translating. The latter one cost less when sync English changes to Chinese periodically. But I'm also fine the former one if you would like to do.;;;","17/Nov/19 03:45;klion26;[~jark], I agree that changing all the links in {{.zh.md}} pages has a lot of work to do. I think we can apply this to the new added {{.zh.md}} document, when adding a new {{.zh.md}} document we need to change the link to the Chinese version, for the exist {{.zh.md}} page, we can update the found wrong links for better user experience.;;;","17/Nov/19 04:00;jark;I assigned this issue to you [~klion26], feel free to open PR.;;;","18/Nov/19 02:47;jark;1.10.0: f6e39aa60d1ce3c020fcb49955912e559d5202e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,
StreamNetworkBenchmarkEnvironment incorrectly setups a receiving InputGate,FLINK-14818,13268579,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,pnowojski,pnowojski,15/Nov/19 13:56,27/Feb/20 07:13,13/Jul/23 08:10,27/Feb/20 07:13,1.7.2,1.8.2,1.9.1,,,,,,,1.11.0,,,,Benchmarks,,,,,0,pull-request-available,,,,"In network benchmark (for example 1000 channels benchmark with 4 record writers) {{StreamNetworkBenchmarkEnvironment#createInputGate}} creates 1000 input gates with 4 input channels each, which doesn't make much sense. I would expect either having 4 receivers with single input gate with 1000 channels each, or a single receiver with 4 input gates, with 1000 channels each.",,kevin.cyj,pnowojski,wind_ljy,,,,,,,,,,,"wsry commented on pull request #11155: [FLINK-14818] Fix receiving InputGate setup of StreamNetworkBenchmarkEnvironment.
URL: https://github.com/apache/flink/pull/11155
 
 
   ## What is the purpose of the change
   
   In network benchmark (for example 1000 channels benchmark with 4 record writers) StreamNetworkBenchmarkEnvironment#createInputGate creates 1000 input gates with 4 input channels each, which doesn't make much sense. It is expected that either having 4 receivers with single input gate with 1000 channels each, or a single receiver with 4 input gates, with 1000 channels each.
   
   
   ## Brief change log
   
     - The receiving InputGate setup logic of StreamNetworkBenchmarkEnvironment is changed
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as StreamNetworkThroughputBenchmarkTest.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Feb/20 10:22;githubbot;600","zhijiangW commented on pull request #11155: [FLINK-14818][benchmark] Fix receiving InputGate setup of StreamNetworkBenchmarkEnvironment.
URL: https://github.com/apache/flink/pull/11155
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 07:06;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 27 07:12:42 UTC 2020,,,,,,,,,,"0|z08np4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/19 12:03;kevin.cyj;I found there seems no very straightforward way to achieve our goal of 4 ResultPartition with 1000 subpartition each to 4 SingleInputGate which 1000 input channel each. The reason is that the subpartition index to be request is InputGate level not InputChannel level, each time when requesting a subpartition, the InputGate passes the subpartition index to InputChannel, so the requested subpartition indexes are all the same one for all InputChannels of a InputGate. If we want to change the behavior and without touching the non-test-code, we may need to mock a InputGate and relevant code to change the behavior. Do you think that is acceptable? [~pnowojski];;;","21/Nov/19 14:51;pnowojski;I'm not sure. It depends how complicated the mock would be? If it's overriding one simple method, then it's probably fine. If we have to mock hundreds lines of code, maybe it's not worth? Or at least not for now?;;;","21/Feb/20 06:52;kevin.cyj;I have opened a PR for this issue and Zhijiang is reviewing it. Could you please also take a look? [~pnowojski];;;","27/Feb/20 07:12;kevin.cyj;Fixed on master via 92253f2e15090f5dac8cfc68c49727b62da23b8c.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Streaming Aggregation"" document contains misleading code examples",FLINK-14817,13268549,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,victor-wong,victor-wong,victor-wong,15/Nov/19 11:17,26/Nov/19 15:39,13/Jul/23 08:10,26/Nov/19 15:39,1.9.1,,,,,,,,,1.10.0,1.9.2,,,Documentation,,,,,0,pull-request-available,,,,"In the document of [Streaming Aggregation |https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/tuning/streaming_aggregation_optimization.html] , there are some misleading code examples, e.g.
{code:java}
// instantiate table environment
TableEnvironment tEnv = ...tEnv.getConfig()        // access high-level configuration
  .getConfiguration()   // set low-level key-value options
  .setString(""table.exec.mini-batch.enabled"", ""true"")  // enable mini-batch optimization
  .setString(""table.exec.mini-batch.allow-latency"", ""5 s"") // use 5 seconds to buffer input records
  .setString(""table.exec.mini-batch.size"", ""5000""); // the maximum number of records can be buffered by each aggregate operator task
{code}
It seems `Configuration` supports method chaining, while it's not true since the return type of `Configuration#setString` is Void.

 

So what about making `Configuration` support method chaining, or updating the documentation?

 ",,jark,victor-wong,,,,,,,,,,,,"jiasheng55 commented on pull request #10323: [FLINK-14817][doc] Fix misleading documentation using method chaining…
URL: https://github.com/apache/flink/pull/10323
 
 
   … of Configuration
   
   ## What is the purpose of the change
   
   In the document, there are some misleading code examples, e.g.
   
   ```
   // instantiate table environment
   TableEnvironment tEnv = ...tEnv.getConfig()        // access high-level configuration
     .getConfiguration()   // set low-level key-value options
     .setString(""table.exec.mini-batch.enabled"", ""true"")  // enable mini-batch optimization
     .setString(""table.exec.mini-batch.allow-latency"", ""5 s"") // use 5 seconds to buffer input records
     .setString(""table.exec.mini-batch.size"", ""5000""); // the maximum number of records can be buffered by each aggregate operator task
   ```
   It seems `Configuration` supports method chaining, while it's not true since the return type of `Configuration#setString` is Void.
   
   This PR updates the documentation to fix this.
   
   ## Brief change log
   
     - Break the chaining call of `Documentation#setXXX`
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/19 09:27;githubbot;600","wuchong commented on pull request #10323: [FLINK-14817][doc] Fix misleading documentation using method chaining…
URL: https://github.com/apache/flink/pull/10323
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/19 15:04;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14835,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 26 15:39:37 UTC 2019,,,,,,,,,,"0|z08nig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/19 03:09;jark;Good catch! I would suggest to update the documentation first (both 1.9 and master). 
And create another issue to discuss method chain for {{Configuration}}. 
If you are interested in this , I can assign this issue to you.;;;","18/Nov/19 03:08;victor-wong;[~jark], thanks for your quick reply. Please assign this issue to me, and this is the issue discussing ""method chain for Configuration"": [ FLINK-14835 | https://issues.apache.org/jira/browse/FLINK-14835];;;","26/Nov/19 09:31;victor-wong;[~jark], the PR is ready, could you take a look when convenient.;;;","26/Nov/19 15:39;jark;1.10.0: e24681bf3cadd531bb0c6fd73049a67335008059

1.9.2: c1f2294cf27db77632c92ea821e30249d76db113;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataStreamAllroundTestProgram does not run because return types cannot be determined,FLINK-14809,13268509,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kkl0u,gjy,gjy,15/Nov/19 08:13,15/Nov/19 11:53,13/Jul/23 08:10,15/Nov/19 11:53,1.10.0,,,,,,,,,1.10.0,,,,API / DataStream,Tests,,,,0,,,,,"{noformat}
2019-11-14 19:34:55,185 ERROR org.apache.flink.client.cli.CliFrontend                       - Error while running the command.
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: The return type of function 'main(DataStreamAllroundTestProgram.java:182)' could not be determined automatically, due to type erasure. You can give type information hints by using the returns(...) method on the result of the transformation call, or by letting your function implement the 'ResultTypeQueryable' interface.
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:336)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:206)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:173)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:747)
        at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:282)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:219)
        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1011)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1084)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
        at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1084)
Caused by: org.apache.flink.api.common.functions.InvalidTypesException: The return type of function 'main(DataStreamAllroundTestProgram.java:182)' could not be determined automatically, due to type erasure. You can give type information hints by using the returns(...) method on the result of the transformation call, or by letting your function implement the 'ResultTypeQueryable' interface.
        at org.apache.flink.api.dag.Transformation.getOutputType(Transformation.java:412)
        at org.apache.flink.streaming.api.datastream.DataStream.addSink(DataStream.java:1296)
        at org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.main(DataStreamAllroundTestProgram.java:185)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:322)
        ... 12 more
Caused by: org.apache.flink.api.common.functions.InvalidTypesException: Input mismatch: Generic type 'org.apache.flink.streaming.tests.Event' or a subclass of it expected but was 'org.apache.flink.streaming.tests.Event'.
        at org.apache.flink.api.java.typeutils.TypeExtractor.validateInputType(TypeExtractor.java:1298)
        at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:585)
        at org.apache.flink.api.java.typeutils.TypeExtractor.getFlatMapReturnTypes(TypeExtractor.java:196)
        at org.apache.flink.streaming.api.datastream.DataStream.flatMap(DataStream.java:634)
        at org.apache.flink.streaming.tests.DataStreamAllroundTestProgram.main(DataStreamAllroundTestProgram.java:182)
        ... 17 more
Caused by: org.apache.flink.api.common.functions.InvalidTypesException: Generic type 'org.apache.flink.streaming.tests.Event' or a subclass of it expected but was 'org.apache.flink.streaming.tests.Event'.
        at org.apache.flink.api.java.typeutils.TypeExtractor.validateInfo(TypeExtractor.java:1481)
        at org.apache.flink.api.java.typeutils.TypeExtractor.validateInfo(TypeExtractor.java:1491)
        at org.apache.flink.api.java.typeutils.TypeExtractor.validateInputType(TypeExtractor.java:1295)
        ... 21 more
{noformat}

This happens in multiple nightlies and jepsen runs. Example
https://api.travis-ci.org/v3/job/611848582/log.txt",,aljoscha,gjy,kkl0u,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 15 11:53:00 UTC 2019,,,,,,,,,,"0|z08n9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/19 11:01;aljoscha;The reason seems to be FLINK-14808;;;","15/Nov/19 11:53;kkl0u;Merged with 1f28a24263971c5e9591e90b825a5b8e4a65e99b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClientUtils set thread ClassLoader that is different from the user cl of PackagedProgram,FLINK-14808,13268507,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kkl0u,lzljs3620320,lzljs3620320,15/Nov/19 08:09,18/Nov/19 02:01,13/Jul/23 08:10,15/Nov/19 11:52,1.10.0,,,,,,,,,1.10.0,,,,Client / Job Submission,,,,,0,,,,,"In ClientUtils.executeProgram, it create a new userCodeClassLoader and set it to thread context. (Name it cl1).

But in PackagedProgram, it use a different class loader to create user Class. (Name it cl2).

So in user's code,  if user use thread loader to create a class, this class is come from cl1, if user want to work this class with their own class, there should be a failure, because them come from different class loader.

Root cause: https://issues.apache.org/jira/browse/FLINK-14745",,aljoscha,kkl0u,lzljs3620320,tison,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 18 02:01:24 UTC 2019,,,,,,,,,,"0|z08n94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/19 08:14;lzljs3620320;CC: [~kkl0u] [~aljoscha] [~tison];;;","15/Nov/19 08:20;tison;IIRC there are at least two JIRA issues focus on this problem.

CC [~Paul Lin];;;","15/Nov/19 11:04;kkl0u;You are correct [~lzljs3620320] in your understanding. I will fix it.;;;","15/Nov/19 11:52;kkl0u;Merged with 1f28a24263971c5e9591e90b825a5b8e4a65e99b;;;","16/Nov/19 04:01;lzljs3620320;Thanks [~kkl0u];;;","16/Nov/19 04:08;lzljs3620320;[~kkl0u] Do you think we should add a test to cover this scenario?;;;","16/Nov/19 08:36;kkl0u;I think so, yes. I will open a subtask here. Should I assign it to you?;;;","16/Nov/19 08:40;kkl0u;[~tison] could you also point me to the other JIRAs you found that are related to this so that I can close them?;;;","17/Nov/19 00:34;tison;[~kkl0u] after looking into your fix I think I misunderstand the issue previously. It is a specific issue without duplicate.;;;","17/Nov/19 11:58;kkl0u;Thanks a lot [~tison]!;;;","18/Nov/19 02:01;lzljs3620320;[~kkl0u] Yes, please assign to me.;;;",,,,,,,,,,,,,,,,,,,,,
KeyedStream#transform not working with factories,FLINK-14794,13268347,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,arvid heise,14/Nov/19 15:11,22/Jun/21 14:07,13/Jul/23 08:10,15/Nov/19 12:12,1.10.0,,,,,,,,,1.10.0,,,,API / DataStream,,,,,0,pull-request-available,,,,"DataStream#transform got an overload that takes factories. However, this method does not set type information if invoked on a KeyedStream like the non-factory equivalent.",,aljoscha,arvid heise,wind_ljy,,,,,,,,,,,"AHeise commented on pull request #10198: [FLINK-14794] Fixing KeyedStream#transform to properly relay type information
URL: https://github.com/apache/flink/pull/10198
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fixing KeyedStream#transform to properly relay type information if invoked with factories.
   
   ## Brief change log
   
   - Overloading the correct function of DataStream in KeyedStream, such that the type information is passed to both operator and factory based transformation methods.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Nov/19 15:22;githubbot;600","aljoscha commented on pull request #10198: [FLINK-14794] Fixing KeyedStream#transform to properly relay type information
URL: https://github.com/apache/flink/pull/10198
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Nov/19 12:12;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 15 12:12:52 UTC 2019,,,,,,,,,,"0|z08m9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/19 12:12;aljoscha;Fixed on master in 749086daae433ec997a91b13aa1e50c4c931a012;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CsvTableSink miss delimiter when row start with null member,FLINK-14784,13268299,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,14/Nov/19 12:41,15/Nov/19 07:16,13/Jul/23 08:10,15/Nov/19 07:16,1.9.1,,,,,,,,,1.10.0,1.9.2,,,Table SQL / API,,,,,0,pull-request-available,,,,"{code:java}
//
public String map(Row row) {
   StringBuilder builder = new StringBuilder();
   Object o;
   for (int i = 0; i < row.getArity(); i++) {
      if (builder.length() != 0) {
         builder.append(fieldDelim);
      }
      if ((o = row.getField(i)) != null) {
         builder.append(o);
      }
   }
   return builder.toString();
}{code}
when row start with null member,  result string  will miss delimiter.",,jark,leonard,,,,,,,,,,,,"leonardBang commented on pull request #10199: [FLINK-14784][Table SQL / API] CsvTableSink miss delimiter when row s…
URL: https://github.com/apache/flink/pull/10199
 
 
   Fix CsvTableSink miss delimiter when row start with null member.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request Fix the bug that CsvTableSink miss delimiter when row start with null member.*
   
   
   ## Brief change log
   
    * fix bug in :
    flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/sinks/CsvTableSink.java
    * add related test in:                           
    flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/TableSinkITCase.scala  
    flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/stream/table/TableSinkITCase.scala   
   
   ## Verifying this change
   
   This change has two IT cases.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Nov/19 15:33;githubbot;600","KurtYoung commented on pull request #10199: [FLINK-14784][Table SQL / API] CsvTableSink miss delimiter when row s…
URL: https://github.com/apache/flink/pull/10199
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Nov/19 07:13;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 15 07:16:50 UTC 2019,,,,,,,,,,"0|z08lyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/19 15:37;leonard;[~ykt836] could you help assign this to me？

and I submit a PR here [https://github.com/apache/flink/pull/10199, |https://github.com/apache/flink/pull/10199]

It will be great if you  can have a look:);;;","15/Nov/19 01:14;ykt836;[~Leonard Xu] Thanks for reporting and fixing this bug, I will take a look shortly. ;;;","15/Nov/19 07:16;ykt836;fixed in 1.10.0: 65ecfab8d5a3f9ef7877ea35ff22fb72ec2db0a5

1.9.2: 3b97ec942852327c6845ac42175ba5dc7421fc46;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoreModule#getFunctionDefinition should return empty optional when function does not exist,FLINK-14782,13268283,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,phoenixjiangnan,dwysakowicz,dwysakowicz,14/Nov/19 11:13,15/Nov/19 16:45,13/Jul/23 08:10,15/Nov/19 16:43,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"As in the subject CoreModule#getFunctionDefinition throws exception when function does not exist, while it should just return an Optional.empty(). 
{code}
	@Test
	public void testGetFunction() {
		Optional<FunctionDefinition> optional = CoreModule.INSTANCE.getFunctionDefinition(""AAA"");
	}
{code}

An easy fix would be to change the {{CoreModule#getFunctionDefinition}} to
{code}
	@Override
	public Optional<FunctionDefinition> getFunctionDefinition(String name) {
		return BuiltInFunctionDefinitions.getDefinitions().stream()
			.filter(f -> f.getName().equalsIgnoreCase(name))
			.findFirst()
			.map(Function.identity());
	}
{code}",,dwysakowicz,phoenixjiangnan,,,,,,,,,,,,"bowenli86 commented on pull request #10203: [FLINK-14782][table] CoreModule#getFunctionDefinition should return empty optional when function does not exist
URL: https://github.com/apache/flink/pull/10203
 
 
   ## What is the purpose of the change
   
   CoreModule#getFunctionDefinition should return empty optional when function does not exist.
   
   ## Brief change log
   
   - fixed the bug
   - added UT
   
   ## Verifying this change
   
   This change added tests and can be verified as `CoreModuleTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
   n/a
   
   ## Documentation
   
   n/a
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Nov/19 21:23;githubbot;600","bowenli86 commented on pull request #10203: [FLINK-14782][table] CoreModule#getFunctionDefinition should return empty optional when function does not exist
URL: https://github.com/apache/flink/pull/10203
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Nov/19 16:45;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 15 16:43:35 UTC 2019,,,,,,,,,,"0|z08lvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/19 11:14;dwysakowicz;cc [~phoenixjiangnan];;;","15/Nov/19 16:43;phoenixjiangnan;master: 71142816113a4f1d2160b73cdaed71c4bbfb575f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[ZH] clarify that a RocksDB dependency in pom.xml may not be needed,FLINK-14781,13268277,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,catkint,nkruber,nkruber,14/Nov/19 10:50,05/Dec/19 09:38,13/Jul/23 08:10,05/Dec/19 09:37,1.10.0,,,,,,,,,1.10.0,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,The English version was clarified with respect when and how to add the maven dependencies via https://github.com/apache/flink/commit/d36ce5ff77fae2b01b8fbe8e5c15d610de8ed9f5. The Chinese version still needs that update,,catkint,jark,klion26,nkruber,,,,,,,,,,"catkint commented on pull request #10206: [FLINK-14781] [docs-zh] clarify that a RocksDB dependency in pom.xml may not be needed
URL: https://github.com/apache/flink/pull/10206
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   clarify that a RocksDB dependency in pom.xml may not be needed
   
   
   ## Brief change log
   
   clarify that a RocksDB dependency in pom.xml may not be needed
   
   
   ## Verifying this change
   
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Nov/19 03:56;githubbot;600","wuchong commented on pull request #10206: [FLINK-14781] [docs-zh] correct the state_backends.zh external link and some other things
URL: https://github.com/apache/flink/pull/10206
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 09:37;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 05 09:37:59 UTC 2019,,,,,,,,,,"0|z08lu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/19 13:11;catkint;Hi, I’d like to do this issue, would you please assign it to me ? thx.;;;","05/Dec/19 09:37;jark;Fixed in 1.10.0: 4b95648975056db58c6d5a669aad9d6fbd6e9f99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression with latest changes to Mailbox,FLINK-14747,13268051,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,arvid,arvid,arvid heise,13/Nov/19 15:15,22/Jun/21 14:07,13/Jul/23 08:10,21/Nov/19 14:35,,,,,,,,,,1.10.0,,,,Runtime / Task,,,,,0,pull-request-available,,,,"The commits [edeec8d7420185d1c49b2739827bd921d2c2d485|https://github.com/apache/flink/commit/edeec8d7420185d1c49b2739827bd921d2c2d485] .. [809533e5b5c686e2d21b64361d22178ccb92ec26|https://github.com/apache/flink/commit/809533e5b5c686e2d21b64361d22178ccb92ec26] introduced a performance regression in the course of FLINK-14304 .

The root cause seems to be the removal of the volatile variable for speed up the hotpath in case of an empty mailbox queue resulting in unnecessary lock acquisitions.",,arvid heise,klion26,pnowojski,wind_ljy,,,,,,,,,,"AHeise commented on pull request #10177: [FLINK-14747] Fix performance regression with latest changes to Mailbox
URL: https://github.com/apache/flink/pull/10177
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   * Fixes performance regression introduced with FLINK-14304
   
   ## Brief change log
   
   * Reintroduce volatile variable to avoid lock acquisition on empty queue.
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   * Correctness covered by existing tests.
   * Performance visible in benchmarks.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (yes)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Nov/19 15:34;githubbot;600","pnowojski commented on pull request #10177: [FLINK-14747] Fix performance regression with latest changes to Mailbox
URL: https://github.com/apache/flink/pull/10177
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Nov/19 14:34;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 21 14:35:18 UTC 2019,,,,,,,,,,"0|z08kfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/19 14:35;pnowojski;Fixed in master by {{ca0e7b75d3f724604eccf9ddce1d0fdaea130783}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History server does not handle uncaught exceptions in archive fetcher.,FLINK-14746,13268044,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,13/Nov/19 14:46,21/Nov/19 12:31,13/Jul/23 08:10,21/Nov/19 12:31,1.8.2,1.9.1,,,,,,,,1.10.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"In case archive fetcher fails with an error - eg. OOM while parsing json archives, the error is swallowed by ScheduledExectutorService and the submitted runnable is never rescheduled.",,aljoscha,dmvk,,,,,,,,,,,,"dmvk commented on pull request #10175: [FLINK-14746][web] Handle uncaught exceptions in HistoryServerArchive…
URL: https://github.com/apache/flink/pull/10175
 
 
   ## What is the purpose of the change
   
   Handle uncaught exceptions in history server archive fetcher.
   
   ## Brief change log
   
   - Introduced `ScheduledFutures` utility class, that can attach UncaughtExceptionHandler to `ScheduledFuture`.
   - Attach UEH to the future created by archive fetcher submission.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - ScheduledFutures test suite.
   - Manually verified on a cluster with large json archive, that results in OOM.
   
   ```
   2019-11-13 15:52:13,293 INFO  org.apache.flink.runtime.webmonitor.history.HistoryServer     - Web frontend listening at localhost:8082
   2019-11-13 15:52:25,385 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler       - FATAL: Thread 'Flink-Scheduled-Future-SafeGuard' produced an uncaught exception. Stopping the process...
   java.lang.OutOfMemoryError: Java heap space
   	at java.util.Arrays.copyOfRange(Arrays.java:3664)
   	at java.lang.String.<init>(String.java:207)
   	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.util.TextBuffer.contentsAsString(TextBuffer.java:392)
   	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishAndReturnString(UTF8StreamJsonParser.java:2414
   ```
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Nov/19 14:53;githubbot;600","aljoscha commented on pull request #10175: [FLINK-14746][web] Handle uncaught exceptions in HistoryServerArchive…
URL: https://github.com/apache/flink/pull/10175
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Nov/19 12:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 21 12:31:32 UTC 2019,,,,,,,,,,"0|z08ke8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/19 12:31;aljoscha;Merged on master in e7c11ed672013512e5b159e7e892b27b1ef60a1b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable tests TaskExecutorTest#testSubmitTaskBeforeAcceptSlot,FLINK-14742,13268006,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,tison,tison,13/Nov/19 11:57,23/Jan/20 13:30,13/Jul/23 08:10,23/Jan/20 13:30,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"deadlock.


{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f1f8800b800 nid=0x356 waiting on condition [0x00007f1f8e65c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000086e9e9c0> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1693)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1729)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.runtime.taskexecutor.TaskExecutorTest.testSubmitTaskBeforeAcceptSlot(TaskExecutorTest.java:1108)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
{code}


full log https://api.travis-ci.org/v3/job/611275566/log.txt",,aljoscha,azagrebin,kkl0u,klion26,SleePy,tison,trohrmann,wind_ljy,,,,,,"tillrohrmann commented on pull request #10927: [FLINK-14742][test] Harden TaskExecutorTest#testSubmitTaskBeforeAcceptSlot
URL: https://github.com/apache/flink/pull/10927
 
 
   ## What is the purpose of the change
   
   Remove the access to TaskExecutor's internal state from the test thread. Instead we rely
   on the public API of the TaskExecutorGateway and its collaborators to verify the test.
   
   cc @kl0u 
   
   ## Verifying this change
   
   This PR hardens the above-mentioned test case. You can try this fix by running the test in a loop to see that it no longer fails.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/20 16:11;githubbot;600","tillrohrmann commented on pull request #10927: [FLINK-14742][test] Harden TaskExecutorTest#testSubmitTaskBeforeAcceptSlot
URL: https://github.com/apache/flink/pull/10927
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jan/20 13:29;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 23 13:30:19 UTC 2020,,,,,,,,,,"0|z08k5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/19 16:27;azagrebin;[~tison] 
are you working on this? did you manage to reproduce the failure?;;;","19/Nov/19 23:00;tison;[~azagrebin] I'm not working on this. Here is the change set where I notice this test fails https://github.com/apache/flink/compare/965e4e58a354...79e3b32b4ae9 . IMO the change is safe so that I file a JIRA to report unstable test. I don't have a patch to reproduce it also.;;;","16/Jan/20 13:19;kkl0u;If you run it in a loop until failure (configurable in Intellij), you end up having this exception:


{code:java}
 ERROR org.apache.flink.runtime.rpc.akka.AkkaRpcActor  - Caught exception while executing runnable in main thread.
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1442)
	at java.util.HashMap$ValueIterator.next(HashMap.java:1471)
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable.createSlotReport(TaskSlotTable.java:213)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.establishResourceManagerConnection(TaskExecutor.java:1030)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$1700(TaskExecutor.java:155)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$ResourceManagerRegistrationListener.lambda$onRegistrationSuccess$0(TaskExecutor.java:1725)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}
 ;;;","16/Jan/20 13:38;kkl0u;Or this:


{code:java}
Caught exception while executing runnable in main thread.
java.lang.NullPointerException
	at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable.createSlotReport(TaskSlotTable.java:199)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.establishResourceManagerConnection(TaskExecutor.java:1030)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$1700(TaskExecutor.java:155)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$ResourceManagerRegistrationListener.lambda$onRegistrationSuccess$0(TaskExecutor.java:1725)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

which seems to be also related to concurrent modification of one of the slot data structures in the {{TaskSlotTable}}.;;;","16/Jan/20 14:06;kkl0u;I upgraded it to a blocker until we know what happens. Please downgrade it if you think it should not be.;;;","16/Jan/20 15:58;kkl0u;Ok it seems to be a test instability that has to do with the test using directly the {{TaskSlotTable}} to allocate slots and then executing the job submission through RPCs which run on a separate thread (the main thread of the endpoint).

Given this, I am downgrading it to ""Critical"" and I will open a PR soon.;;;","17/Jan/20 07:56;SleePy;Such a subtle case if it couldn't be reproduced :)
I have checked the test case, but didn't find the clue. Nice work [~kkl0u]!;;;","21/Jan/20 18:22;trohrmann;Let me know once you have the PR ready [~kkl0u]. I can help with reviewing it.;;;","23/Jan/20 13:30;trohrmann;Fixed via

master: 8b485eb24553ca7db0ae7bb66826c9662020c9eb
1.10.0: 0fda1e1173d825f9840b0f7efea5ac61d6e648dd;;;",,,,,,,,,,,,,,,,,,,,,,,
LogicalWatermarkAssigner should use specified trait set when doing copy,FLINK-14731,13267890,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fan_li_ya,fan_li_ya,fan_li_ya,13/Nov/19 02:52,13/Apr/21 20:41,13/Jul/23 08:10,13/Nov/19 05:40,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"In LogicalWatermarkAssigner#copy method, creating the new LogicalWatermarkAssigner object should use the trait set from the input parameter, instead of the trait set of the current object. ",,fan_li_ya,jark,openinx,,,,,,,,,,,"liyafan82 commented on pull request #10167: [FLINK-14731] LogicalWatermarkAssigner should use specified trait set when doing copy
URL: https://github.com/apache/flink/pull/10167
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   In LogicalWatermarkAssigner#copy method, creating the new LogicalWatermarkAssigner object should use the trait set from the input parameter, instead of the trait set of the current object.
   
   ## Brief change log
   
   change method LogicalWatermarkAssigner#copy directly
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Nov/19 03:01;githubbot;600","wuchong commented on pull request #10167: [FLINK-14731] LogicalWatermarkAssigner should use specified trait set when doing copy
URL: https://github.com/apache/flink/pull/10167
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Nov/19 05:39;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 13 05:40:11 UTC 2019,,,,,,,,,,"0|z08jg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/19 05:40;jark;1.10.0: 276e850aaee78d30a5a6e5eec1f8dbd0b167a9c7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slot leaks if SharedSlotOversubscribedException happens,FLINK-14701,13267414,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,zhuzh,zhuzh,11/Nov/19 07:29,24/Jan/20 10:39,13/Jul/23 08:10,24/Jan/20 10:39,1.9.2,,,,,,,,,1.10.0,1.9.2,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"If a {{SharedSlotOversubscribedException}} happens, the {{MultiTaskSlot}} will release some of its child {{SingleTaskSlot}}. The triggered releasing will trigger a re-allocation of the task slot right inside {{SingleTaskSlot#release(...)}}. So that a previous allocation in {{SloSharingManager#allTaskSlots}} will be replaced by the new allocation because they share the same {{slotRequestId}}.
However, the {{SingleTaskSlot#release(...)}} will then invoke {{MultiTaskSlot#releaseChild}} to release the previous allocation with the {{slotRequestId}}, which will unexpectedly remove the new allocation from the {{SloSharingManager}}.
In this way, slot leak happens because the pending slot request is not tracked by the {{SloSharingManager}} and cannot be released when its payload terminates.

A test case {{testNoSlotLeakOnSharedSlotOversubscribedException}} which exhibits this issue can be found in this [commit|https://github.com/zhuzhurk/flink/commit/9024e2e9eb4bd17f371896d6dbc745bc9e585e14].

The slot leak blocks the TPC-DS queries on flink 1.10, see FLINK-14674.

To solve it, I'd propose to strengthen the {{MultiTaskSlot#releaseChild}} to only remove its true child task slot from the {{SloSharingManager}}, i.e. add a check {{if (child == allTaskSlots.get(child.getSlotRequestId()))}} before invoking {{allTaskSlots.remove(child.getSlotRequestId())}}.
",,gjy,hequn8128,leonard,trohrmann,wind_ljy,zhuzh,,,,,,,,"zhuzhurk commented on pull request #10867: [FLINK-14701][runtime] Fix MultiTaskSlot to not remove slots which are not its children
URL: https://github.com/apache/flink/pull/10867
 
 
   ## What is the purpose of the change
   
   If a SharedSlotOversubscribedException happens, the MultiTaskSlot will release some of its child SingleTaskSlot. The triggered releasing will trigger a re-allocation of the task slot right inside SingleTaskSlot#release(...). So that a previous allocation in SloSharingManager#allTaskSlots will be replaced by the new allocation because they share the same slotRequestId.
   However, the SingleTaskSlot#release(...) will then invoke MultiTaskSlot#releaseChild to release the previous allocation with the slotRequestId, which will unexpectedly remove the new allocation from the SloSharingManager.
   In this way, slot leak happens because the pending slot request is not tracked by the SloSharingManager and cannot be released when its payload terminates.
   
   Note that the it's not a problem in 1.10/master now since SharedSlotOversubscribedException is removed in FLINK-14314. However, it's still an issue in 1.9.
   However, the fix would still help in master to avoid similar issue to happen in the future.
   
   A test case testNoSlotLeakOnSharedSlotOversubscribedException which exhibits this issue can be found at https://github.com/zhuzhurk/flink/commit/9024e2e9eb4bd17f371896d6dbc745bc9e585e14.
   
   ## Brief change log
   
     - *See the code change.*
   
   
   ## Verifying this change
   
   The change is verified on 1.9 with the test in https://github.com/zhuzhurk/flink/commit/9024e2e9eb4bd17f371896d6dbc745bc9e585e14.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**yes** / no / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 04:23;githubbot;600","tillrohrmann commented on pull request #10867: [FLINK-14701][runtime] Fix MultiTaskSlot to not remove slots which are not its children
URL: https://github.com/apache/flink/pull/10867
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jan/20 10:38;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,FLINK-14674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 24 10:39:04 UTC 2020,,,,,,,,,,"0|z08gi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/19 07:35;zhuzh;[~chesnay], what do you think of the issue and the proposed solution?
This issue also happens in 1.9 so I think we also need the fix there.;;;","24/Nov/19 16:02;zhuzh;This issue is not critical now since {{SharedSlotOversubscribedException}} is removed in FLINK-14314.
However, the proposed change is still valid to avoid similar issue to happen in the future.
The proposed change is small, at no cost nor risk. So I'd still like to have the proposed change for {{MultiTaskSlot#releaseChild}}.;;;","16/Jan/20 04:08;zhuzh;Reopen this issue since it's still a issue in 1.9 and we should fix it.;;;","23/Jan/20 01:27;hequn8128;[~zhuzh] Hi, could I move this issue to 1.9.3 as it is not critical? The release of 1.9.2 is very close. :);;;","23/Jan/20 03:51;zhuzh;Hi [~hequn8128], it is still a critical issue (slot leak) for 1.9. It is not critical for 1.10 because some feature changes fixed it in a different way, however the issue still exists in 1.9.;;;","23/Jan/20 13:42;trohrmann;I'm looking forward to the days where we finally get rid of the {{SlotSharingManager}} and of all other related issues caused by the dynamic slot sharing model.;;;","24/Jan/20 10:39;trohrmann;Fixed via

master: bd5901f3351d1f70a8b0edcfee8015f91b099f3a
1.10.0: 27d4cc4b66bf7485d5d26660f9640a1e8862a9c2
1.9.2: 0bdd21a36136538b554d7e4bd7ce80bc3c461596;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Most tests from package o.a.f.table.planner.functions.aggfunctions are not executed during mvn test,FLINK-14694,13267340,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,ykt836,ykt836,10/Nov/19 10:33,02/Dec/19 08:23,13/Jul/23 08:10,02/Dec/19 08:23,,,,,,,,,,1.10.0,,,,Table SQL / Planner,Tests,,,,0,pull-request-available,,,,"Only `ListAggWsWithRetractAggFunctionTest` and `ListAggWithRetractAggFunctionTest` are executed. 

And if we run the ignored tests from IDE, some of them will fail. 

 ",,godfreyhe,jark,,,,,,,,,,,,"godfreyhe commented on pull request #10158: [FLINK-14694] [table-planner-blink] Most tests from package o.a.f.table.planner.functions.aggfunctions are not executed during mvn test and fix BinaryGeneric comparison failure
URL: https://github.com/apache/flink/pull/10158
 
 
   
   
   ## What is the purpose of the change
   
   *Most tests from package o.a.f.table.planner.functions.aggfunctions are not executed during mvn test because those test case classed are abstract class and will be ignored when running `mvn test`. In [FLINK-13702](https://issues.apache.org/jira/browse/FLINK-14694), `BinaryGeneric` can't be compared directly, so those failures are also ignored. This pr aims to fix both. The first one's solution is introducing [Enclosed](https://junit.org/junit4/javadoc/4.12/org/junit/experimental/runners/Enclosed.html) runner, and moving the abstract class as an inner class, and making sure the outer class is a non-abstract class. The second one's solution is verifying `BinaryGeneric` using `BinaryGenericAsserter.equivalent` (should also consider that `BinaryGeneric` is in `GenericRow`) *
   
   ## Brief change log
   
     - *introducing `Enclosed` runner and make sure the outer class is a non-abstract class*
     - *comparing `BinaryGeneric` using `BinaryGenericAsserter.equivalent` *
   
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Nov/19 08:26;githubbot;600","dawidwys commented on pull request #10158: [FLINK-14694] [table-planner-blink] Most tests from package o.a.f.table.planner.functions.aggfunctions are not executed during mvn test and fix BinaryGeneric comparison failure
URL: https://github.com/apache/flink/pull/10158
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Nov/19 15:48;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 02 08:23:56 UTC 2019,,,,,,,,,,"0|z08g1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/19 10:33;ykt836;cc [~godfreyhe];;;","10/Nov/19 11:12;godfreyhe;i would like to take this ticket;;;","10/Nov/19 15:09;godfreyhe;The reason is test classes (excluding `ListAggWsWithRetractAggFunctionTest ` and `ListAggWithRetractAggFunctionTest `) in o.a.f.table.planner.functions.aggfunctions  package  are abstract class, and will be ignored. So only `ListAggWsWithRetractAggFunctionTest` and `ListAggWithRetractAggFunctionTest` are executed.

My solution is introducing [Enclosed|https://junit.org/junit4/javadoc/4.12/org/junit/experimental/runners/Enclosed.html] runner, and moving the abstract class as an inner class, and making sure the outer class is a non-abstract class.

The original class pattern is:
{code:java}
public abstract class MaxWithRetractAggFunctionTest<T> extends AggFunctionTestBase<T, MaxWithRetractAccumulator<T>> {

	@Override
	protected Class<?> getAccClass() {
		return MaxWithRetractAccumulator.class;
	}

	@Override
	protected Method getRetractFunc() throws NoSuchMethodException {
		return getAggregator().getClass().getMethod(""retract"", getAccClass(), Object.class);
	}

       public static class StringMaxWithRetractAggFunctionTest extends MaxWithRetractAggFunctionTest<BinaryString> { ... }

       public static class TimestampMaxWithRetractAggFunctionTest extends MaxWithRetractAggFunctionTest<Timestamp> { ... }

       ...
}
{code}
new class pattern is:
{code:java}
@RunWith(Enclosed.class)
public class MaxWithRetractAggFunctionTest {

        /**
         * The base test class for MaxWithRetractAggFunction.
         */
        public abstract static class MaxWithRetractAggFunctionTestBase<T> extends AggFunctionTestBase<T, MaxWithRetractAccumulator<T>> {

	        @Override
	        protected Class<?> getAccClass() {
		        return MaxWithRetractAccumulator.class;
	        }

	        @Override
	        protected Method getRetractFunc() throws NoSuchMethodException {
		        return getAggregator().getClass().getMethod(""retract"", getAccClass(), Object.class);
	        }

       public static class StringMaxWithRetractAggFunctionTest extends MaxWithRetractAggFunctionTestBase<BinaryString> { ... }

       public static class TimestampMaxWithRetractAggFunctionTest extends MaxWithRetractAggFunctionTestBase<Timestamp> { ... }

       ...
}
{code};;;","12/Nov/19 02:15;jark;Thanks [~godfreyhe] for looking into it. The resolution looks good to me. Feel free to open pull request. 
I'm just wondering how to avoid or detect such case in the future. At least we should keep in mind that inner test classes are not executed by maven by default.;;;","13/Nov/19 09:56;docete;What about removing all default exclusions of Surefire since it excludes _nested_ classes by default?

[https://github.com/junit-team/junit5/issues/1377#issue-315009606]

[|#issuecomment-381964988]];;;","02/Dec/19 08:23;ykt836;merged to master: dcc1330375826b779e4902176bb2473704dabb11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
RemoteStreamEnvironment's construction function has a wrong method,FLINK-14683,13267143,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kkl0u,forideal,forideal,08/Nov/19 16:20,19/Dec/19 18:40,13/Jul/23 08:10,19/Dec/19 18:40,1.9.0,,,,,,,,,1.10.0,1.9.2,,,Table SQL / API,,,,,0,pull-request-available,,,,"Hi:
   I think, the following code has a bug.The parameter of globalClasspaths is shouldn't set a null value.
{code:java}
   public RemoteStreamEnvironment(String host, int port, Configuration clientConfiguration, String[] jarFiles, URL[] globalClasspaths) {
        this(host, port, clientConfiguration, jarFiles, (URL[])null, (SavepointRestoreSettings)null);
    }
{code}

",,aljoscha,forideal,kkl0u,,,,,,,,,,,"kl0u commented on pull request #10633: [FLINK-14683] Fix RemoteStreamEnvironment's constructor
URL: https://github.com/apache/flink/pull/10633
 
 
   This is a trivial fix.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Dec/19 12:43;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 19 18:40:44 UTC 2019,,,,,,,,,,"0|z08eu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/19 08:34;aljoscha;Seems legit.;;;","13/Dec/19 11:04;forideal;[~aljoscha] thank you for your reply.
i think, the globalClasspaths maybe make me confuse.
the fellowing code is look good for me.
{code:java}
 public RemoteStreamEnvironment(String host, int port, Configuration clientConfiguration, String[] jarFiles, URL[] globalClasspaths) {
        this(host, port, clientConfiguration, jarFiles, globalClasspaths, (SavepointRestoreSettings)null);
    }
{code}

;;;","19/Dec/19 18:40;kkl0u;Merged on master with a27d73a4c2c52ad120c88fc7947d743c7d8abd6e
release-1.10 with 329fa855757bbd1f5bbfa8b6065f77e53179f863
and release-1.9 with ff6b350e39c1f3d24dddd469e627d066a2d7d22f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shouldn't expect HMS client to throw NoSuchObjectException for non-existing function,FLINK-14673,13267068,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,08/Nov/19 10:11,12/Nov/19 20:10,13/Jul/23 08:10,08/Nov/19 23:16,,,,,,,,,,1.10.0,1.9.2,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,lirui,phoenixjiangnan,,,,,,,,,,,,"lirui-apache commented on pull request #10133: [FLINK-14673][hive] Shouldn't expect HMS client to throw NoSuchObject…
URL: https://github.com/apache/flink/pull/10133
 
 
   …Exception for non-existing function
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Always to check MetaException when getting function with HMS client.
   
   
   ## Brief change log
   
     - Removed shim method `getFunction`.
     - Moved the logic to `HiveMetastoreClientWrapper`.
   
   
   ## Verifying this change
   
   Existing test cases.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 11:26;githubbot;600","bowenli86 commented on pull request #10133: [FLINK-14673][hive] Shouldn't expect HMS client to throw NoSuchObject…
URL: https://github.com/apache/flink/pull/10133
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 23:15;githubbot;600","lirui-apache commented on pull request #10145: [FLINK-14673][hive] Shouldn't expect HMS client to throw NoSuchObject…
URL: https://github.com/apache/flink/pull/10145
 
 
   …Exception for non-existing function
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Always to check MetaException when getting function with HMS client.
   
   
   ## Brief change log
   
     - Removed shim method getFunction.
     - Moved the logic to HiveMetastoreClientWrapper.
   
   
   ## Verifying this change
   
   Existing test cases.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 03:51;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 12 20:10:32 UTC 2019,,,,,,,,,,"0|z08edc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/19 10:19;lirui;When getting functions, we expect HMS client to throw NoSuchObjectException if the function doesn't exist (in {{HiveShimV230}}). This is wrong because NoSuchObjectException is not part of the signature of {{IMetaStoreClient::getFunction}}. Therefore let's always handle it like in {{HiveShimV100}}.;;;","08/Nov/19 23:16;phoenixjiangnan;master: 1806a373fcec1e981601bd755d3d3652dc61219a;;;","12/Nov/19 20:10;phoenixjiangnan;1.9: aaae2d51d21c2e93c07a03125592019c8a74746f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distinguish unknown column stats and zero,FLINK-14663,13266998,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Terry1897,ykt836,ykt836,08/Nov/19 01:56,04/Dec/19 21:32,13/Jul/23 08:10,04/Dec/19 21:32,,,,,,,,,,1.10.0,,,,Connectors / Hive,Table SQL / API,,,,0,pull-request-available,,,,"When converting from hive stats to flink's column stats, we didn't check whether some columns stats is really set or just an initial value. For example:
{code:java}
// code placeholder
LongColumnStatsData longColStats = stats.getLongStats();
return new CatalogColumnStatisticsDataLong(
      longColStats.getLowValue(),
      longColStats.getHighValue(),
      longColStats.getNumDVs(),
      longColStats.getNumNulls());
{code}
 Hive `LongColumnStatsData` actually has information whether some stats is set through APIs like `isSetNumDVs()`. And the initial values are all 0, it will confuse us is it really 0 or just an initial value. 

 

We can use -1 to represent UNKNOWN value for column stats. ",,jark,phoenixjiangnan,Terry1897,,,,,,,,,,,"zjuwangg commented on pull request #10394: [FLINK-14663]Distinguish catalogColumnStats' unknown  value and real values
URL: https://github.com/apache/flink/pull/10394
 
 
   ## What is the purpose of the change
   
   *When converting from hive stats to Flink's column stats, we didn't check whether some columns stats is really set or just an initial value. This PR we aim to change the CatalogColumnStatisticsDataBase and its subclass's stats from primitive types to corresponding boxed-type.*
   
   
   ## Brief change log
   
     - *[68e7752](https://github.com/apache/flink/commit/68e77528300eadcbeaa62372d676b01e268c39a8) Distinguish catalogColumnStats' unknown  value and real values*
   
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *HiveCatalogHiveMetadataTest#testAlterTableColumnStatistics* and so on.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/19 09:50;githubbot;600","bowenli86 commented on pull request #10394: [FLINK-14663]Distinguish catalogColumnStats' unknown  value and real values
URL: https://github.com/apache/flink/pull/10394
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 21:31;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 21:32:14 UTC 2019,,,,,,,,,,"0|z08dxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/19 21:32;phoenixjiangnan;master: c79ca44ac40fec0781cde2e0b4bdd394707cf9e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distinguish unknown CatalogTableStatistics and zero,FLINK-14662,13266994,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Terry1897,ykt836,ykt836,08/Nov/19 01:51,03/Dec/19 02:49,13/Jul/23 08:10,03/Dec/19 02:49,,,,,,,,,,1.10.0,,,,Connectors / Hive,Table SQL / API,,,,0,pull-request-available,,,,"Currently UNKNOWN table stats is represented with zeros, which might confuse with KNOWN table stats with exactly 0 row count. 

 

We can use -1 to represent UNKNOWN instead. ",,jark,lzljs3620320,Terry1897,,,,,,,,,,,"zjuwangg commented on pull request #10380: [FLINK-14662]Distinguish unknown CatalogTableStatistics and zero
URL: https://github.com/apache/flink/pull/10380
 
 
   ## What is the purpose of the change
   
   * Currently UNKNOWN table stats is represented with zeros, which might confuse with UNKNOWN table stats with exactly 0 row count. This PR we use -1 to represent UNKNOWN instead. *
   
   
   ## Brief change log
   
     - [6d8839c](https://github.com/apache/flink/commit/6d8839c83151f0ec062e618c2429b7515020acab) Distinguish unknown CatalogTableStatistics and zero
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *CatalogTest*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Dec/19 11:28;githubbot;600","KurtYoung commented on pull request #10380: [FLINK-14662]Distinguish unknown CatalogTableStatistics and zero
URL: https://github.com/apache/flink/pull/10380
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/19 02:48;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-15005,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 03 02:49:21 UTC 2019,,,,,,,,,,"0|z08dww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/19 02:07;lzljs3620320;I think we have already used -1 to UNKNOWN:
{code:java}
public static final TableStats UNKNOWN = new TableStats(-1, new HashMap<>());
{code}
Where do you mean?;;;","08/Nov/19 02:12;ykt836;Sorry, I meant `CatalogTableStatistics`;;;","02/Dec/19 06:40;Terry1897;+1 for this issue. I'd like to fix it.;;;","03/Dec/19 02:49;ykt836;master: ed96feacfeaa7a88782527a6b52451b52f7a4336;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple logging statements use incorrect placeholders,FLINK-14654,13266802,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,yunta,yunta,yunta,07/Nov/19 10:46,04/Dec/19 12:31,13/Jul/23 08:10,04/Dec/19 12:30,1.9.1,,,,,,,,,1.10.0,,,,,,,,,0,pull-request-available,,,,"As official Flink [java code style|https://flink.apache.org/contributing/code-style-and-quality-java.html#preconditions-and-log-statements] suggested, we should use correct log statement format. However, there existed 13 files within current master branch that the arguments number mismatch with placeholders in log statements.

The error looks like:
{code:java}
LOG.warn(""Failed to read native metric %s from RocksDB"", property, e);
{code}
and the correct format should be
{code:java}
LOG.warn(""Failed to read native metric {} from RocksDB."", property, e);
{code}
The other errors look like
{code:java}
LOG.warn(""Could not find method implementations in the shaded jar. Exception: {}"", e);
{code}
and the correct format should be
{code:java}
LOG.warn(""Could not find method implementations in the shaded jar."", e);
{code}
Below is the full list of files have problems in log statements.
{code:java}
flink-contrib/flink-connector-wikiedits/src/main/java/org/apache/flink/streaming/connectors/wikiedits/WikipediaEditEventIrcStream.java
flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisProducer.java
flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/PipelineErrorHandler.java
flink-runtime/src/main/java/org/apache/flink/runtime/security/modules/HadoopModule.java
flink-metrics/flink-metrics-datadog/src/main/java/org/apache/flink/metrics/datadog/DatadogHttpReporter.java
flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetPojoInputFormat.java
flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetTableSource.java
flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/functions/SqlFunctionUtils.java
flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/values/ValuesInputFormat.java
flink-end-to-end-tests/flink-connector-gcp-pubsub-emulator-tests/src/test/java/org/apache/flink/streaming/connectors/gcp/pubsub/emulator/GCloudEmulatorManager.java
flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaTestEnvironmentImpl.java
flink-runtime/src/main/java/org/apache/flink/runtime/metrics/ReporterSetup.java
flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBNativeMetricMonitor.java{code}",,sewen,yunta,,,,,,,,,,,,"Myasuka commented on pull request #10127: [FLINK-14654] Fix the arguments number mismatching with placeholders in log statements
URL: https://github.com/apache/flink/pull/10127
 
 
   ## What is the purpose of the change
   As official Flink [java code style](https://flink.apache.org/contributing/code-style-and-quality-java.html#preconditions-and-log-statements) suggested, we should use correct log statement format. However, there existed 13 files within current master branch that the arguments number mismatch with placeholders in log statements.
   
   ## Brief change log
   
     - Correct the log format in all 13 files.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   You could verify whether there exists any more files with log formatting problem [within Intellij](https://www.jetbrains.com/help/idea/running-inspections.html) by ""Number of placeholders does not match arguments in log calling"" inspection.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 06:38;githubbot;600","zentol commented on pull request #10127: [FLINK-14654] Fix the arguments number mismatching with placeholders in log statements
URL: https://github.com/apache/flink/pull/10127
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 12:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 12:30:17 UTC 2019,,,,,,,,,,"0|z08cq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/19 10:51;yunta;[~chesnay], what do you think of this issue, please assign to me if possible.

 

BTW, I cannot find a good place to tag this issue to which component.;;;","07/Nov/19 14:28;sewen;Makes sense to fix those, thanks for looking at this.;;;","04/Dec/19 12:30;chesnay;master: 64e2f27640946bf3b1608d4d85585fe18891dcee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink TupleSerializer and CaseClassSerializer shoud support copy NULL values,FLINK-14642,13266720,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,victor-wong,victor-wong,07/Nov/19 04:19,01/Apr/21 19:29,13/Jul/23 08:10,14/Nov/19 14:10,1.9.1,,,,,,,,,1.10.0,,,,API / Type Serialization System,,,,,0,pull-request-available,,,,"Currently, TupleSerializer and CaseCassSerializer do not support serialize NULL values, which I think is acceptable. But not supporting copy NULL values will cause the following codes to throw an exception, which I think is not matched with users' expectations and prone to error.

*codes:*
{code:java}
stream.map(xxx).filter(_ != null).xxx //the return type of the map function is Tuple and it may return null{code}
 

*exception info:*

 
{code:java}
Caused by: java.lang.NullPointerException 
  at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92) 
  at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32) 
  at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:635)
{code}
 

*suggestion:*

Can we make the `copy` method of TupleSerializer/CaseClassSerializer to handle NULL values? e.g.
{code:java}
// org.apache.flink.api.scala.typeutils.CaseClassSerializer#copy
def copy(from: T): T = {
  // handle NULL values.
  if(from == null) {
    return from
  }
  initArray()
  var i = 0
  while (i < arity) {
    fields(i) = fieldSerializers(i).copy(from.productElement(i).asInstanceOf[AnyRef])
    i += 1
  }
  createInstance(fields)
}
{code}
 

 

 

 

 ",,aljoscha,liuyufei,maver1ck,sewen,victor-wong,,,,,,,,,"zhuzhurk commented on pull request #10129: [FLINK-14642][runtime] Deprecate metric `fullRestarts`
URL: https://github.com/apache/flink/pull/10129
 
 
   
   ## What is the purpose of the change
   
   FLINK-14164 introduces a metric 'numberOfRestarts' that counts all kinds of restarts.
   The metric 'fullRestarts' is superseded and this PR is to deprecate it for future removal.
   
   ## Brief change log
   
     - *Marked NumberOfFullRestartsGauge as deprecated*
     - *Adjust description doc of 'numberOfRestarts'*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   Manually verified locally.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 06:51;githubbot;600","jiasheng55 commented on pull request #10130: [FLINK-14642] [types] Add support for copying null values to the TupleSeriali…
URL: https://github.com/apache/flink/pull/10130
 
 
   …zer and CaseClassSerializer
   
   ## What is the purpose of the change
   
   This PR makes the `copy` method of the TupleSerializer and CaseClassSerializer to handle NULL values properly, so elements of these types can be passed between chained operators.   
   Without this PR, the following code may throw NPE in case of `map` function returns null, which may confuse the user.
   
   ```
   stream.map(xxx).filter(_ != null).xxx //the return type of the map function is Tuple and it may return null
   ```
   
   ## Brief change log
   
     - return **null** if the element itself is null.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (**yes**)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 07:55;githubbot;600","aljoscha commented on pull request #10130: [FLINK-14642] [types] Add support for copying null values to the TupleSeriali…
URL: https://github.com/apache/flink/pull/10130
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Nov/19 14:10;githubbot;600","zentol commented on pull request #10129: [FLINK-14642][runtime] Deprecate metric `fullRestarts`
URL: https://github.com/apache/flink/pull/10129
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 14:18;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 19:29:13 UTC 2021,,,,,,,,,,"0|z08c80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/19 14:34;sewen;Seems reasonable to me. Do you want to prepare a patch for this, per your suggestion?;;;","07/Nov/19 16:06;victor-wong;[~sewen], sure I will ping you once PR is ready:);;;","08/Nov/19 08:01;victor-wong;[~sewen], here is the PR, [https://github.com/apache/flink/pull/10130]

I updated the TupleSerializer and CaseClassSerializer, but I'm not sure if we should make the same change to **OptionSerializer** and **RowSerializer**,  any suggestion?;;;","14/Nov/19 14:10;aljoscha;Merged on master in 84c96b32bf8384e651c949d38b11427a9f20fdd0;;;","20/Jan/20 09:22;liuyufei;[~victor-wong] Didn't support `serialize` and `deserialize` function? It will still throw NPE in network shuffle.;;;","23/Jan/20 08:36;victor-wong;[~liuyufei] if I understand correctly, the Tuple and CaseClass themself should not be NULL, but their fields could be NULL.;;;","01/Apr/21 19:29;maver1ck;[~sewen] 
Is there any reasons those serializers doesn't handle nulls ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Fix description of metric `fullRestarts`,FLINK-14641,13266719,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,zhuzh,zhuzh,07/Nov/19 04:17,20/Nov/19 14:14,13/Jul/23 08:10,20/Nov/19 14:13,1.10.0,1.9.2,,,,,,,,1.10.0,1.9.2,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The metric `fullRestarts` counts both full restarts and fine grained restarts since 1.9.2.
We should update the metric description doc accordingly.

We need to pointing out the the metric counts full restarts in 1.9.1 or earlier versions, and turned to count all kinds of restarts since 1.9.2.",,zhuzh,,,,,,,,,,,,,"zhuzhurk commented on pull request #10128: [FLINK-14641][docs] Fix description of metric `fullRestarts`
URL: https://github.com/apache/flink/pull/10128
 
 
   
   ## What is the purpose of the change
   
   The metric `fullRestarts` counts both full restarts and fine grained restarts since 1.9.2.
   This PR is to update the metric description doc accordingly.
   
   ## Brief change log
   
     - *Fixed descriptions doc of metric `fullRestarts`*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 06:47;githubbot;600","zentol commented on pull request #10128: [FLINK-14641][docs] Fix description of metric `fullRestarts`
URL: https://github.com/apache/flink/pull/10128
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 14:11;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 20 14:14:53 UTC 2019,,,,,,,,,,"0|z08c7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/19 13:43;chesnay;Sounds good, do you want to open a PR [~zhuzh]?;;;","07/Nov/19 14:05;zhuzh;Yes. Would you assign it to me [~chesnay]?;;;","08/Nov/19 06:39;zhuzh;[~chesnay] not pretty sure whether we can apply this fix to 1.9.2 only? Or should we also do it in master and base FLINK-14643 on it?;;;","20/Nov/19 14:13;chesnay;master: 040bdb4429faebf3a7b75c1371bdd9b9e54e544e
1.9: 1c97e1d380e776ee2a3d32981f0e9a4cedd41a64 ;;;","20/Nov/19 14:14;chesnay;Yep you're right, I should've only merged this to 1.9 .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't use relocated imports in Kinesis End-2-End Tests,FLINK-14635,13266581,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,sewen,sewen,06/Nov/19 13:09,15/Nov/19 17:47,13/Jul/23 08:10,15/Nov/19 17:46,1.9.1,,,,,,,,,1.10.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,Using relocated imports in {{KinesisPubsubClient}} makes it not possible to build the code in the IDE any more.,,sewen,,,,,,,,,,,,,"StephanEwen commented on pull request #10138: [FLINK-14635][e2e tests] Use non-relocated imports for AWS SDK Kinesis classes in tests.
URL: https://github.com/apache/flink/pull/10138
 
 
   ## What is the purpose of the change
   
   This allows compiling the code base fully in the IDE again by changing the way that the AWS SDK classes are used in the `KinesisPubsubClient` in the Kineses end-2-end test.
   
   Some imports were referring to relocated classes from the connector, which cannot be resolved in the IDE because no shading happens during compilation.
   
   ## Brief change log
   
     - Instead of using relocated imports, use vanilla imports in `KinesisPubsubClient`.
     - We add a provided AWS SDK dependency for the non-relocated classes. This follows the pattern to not directly reference transitive dependencies, especially shaded ones. That way, we supports IDE and unit test compilation / execution.
     - For end-2-end test execution, we relocate the AWS SDK classes in the shade phase following the exact same pattern as the original relocation in the Kinesis connector.
     - We need to make sure we don't operate directly on relocated classes when simultaneously using non relocated classes (class cast exceptions). We use a utility in the Kinesis module to directly obtain stringified or byte[]-ified versions of the records, rather than objects backed by relocated classes.
   
   ## Verifying this change
   
   This changes a test which still runs and works.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **yes** (in a test)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 17:52;githubbot;600","StephanEwen commented on pull request #10138: [FLINK-14635][e2e tests] Use non-relocated imports for AWS SDK Kinesis classes in tests.
URL: https://github.com/apache/flink/pull/10138
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 09:07;githubbot;600","StephanEwen commented on pull request #10162: [FLINK-14635][e2e tests] Use non-relocated imports for AWS SDK Kinesis classes in tests.
URL: https://github.com/apache/flink/pull/10162
 
 
   ## What is the purpose of the change
   
   This allows compiling the code base fully in the IDE again by changing the way that the AWS SDK classes are used in the `KinesisPubsubClient in the Kineses end-2-end test.
   
   Some imports were referring to relocated classes from the connector, which cannot be resolved in the IDE because no shading happens during compilation.
   
   ## Brief change log
   
     - Move the `KinesisPubsubClient` to `flink-connector-kinesis`
     - Export a test-jar with the test utils from `flink-connector-kinesis`
     - Add the test-jar to the Kinesis end-2-end test
     - There is a separate cleanup commit for some compiler warning and code inspections
   
   ## Verifying this change
   
   This changes a test which still runs and works.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **yes** (in a test)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Nov/19 12:20;githubbot;600","StephanEwen commented on pull request #10162: [FLINK-14635][e2e tests] Use non-relocated imports for AWS SDK Kinesis classes in tests.
URL: https://github.com/apache/flink/pull/10162
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Nov/19 17:47;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 15 17:46:44 UTC 2019,,,,,,,,,,"0|z08bd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/19 16:46;sewen;The main issue is that records coming from the Flink Kinesis connector are of the relocated {{GetRecordsResult}} type, which means one cannot work with them unless referring to the relocated type which does however not work in the IDE.

The solution would be to have a utility in the Kinesis Connector that collects the records as Strings to avoid dealing with the relocated types.

[~thw@apache.org] what do you think about this?
;;;","15/Nov/19 17:46;sewen;Fixed via 2b0a8ceeb131c938d2e41dfee66099bfa5f366ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metric Option of 'metrics.scope.delimiter' won't work probably.,FLINK-14634,13266561,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Eric Lee,Eric Lee,06/Nov/19 11:26,07/Nov/19 15:26,13/Jul/23 08:10,07/Nov/19 15:26,,,,,,,,,,,,,,Runtime / Metrics,,,,,0,,,,,"In the `master` branch, when initializing the metricRegistryConfiguration, the configuration of `metrics.scope.delimiter` doesn't work probably because the user-defined delimiter is not used in the process of constructing `ScopeFormat` which uses the fixed character - `.` as the delimiter.

1. Attachment 1 shows that in the process of constructing `ScopeFormat`, `.` is used as the default delimiter rather than user-defined delimiter.

2. Attachment 2 illustrates that the wrong order of using user-defined delimiter.","Flink 1.9.0

JDK 1.8",Eric Lee,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/19 11:21;Eric Lee;5506F8C7-ACCA-44E8-9BA9-D6023E02B633.png;https://issues.apache.org/jira/secure/attachment/12985041/5506F8C7-ACCA-44E8-9BA9-D6023E02B633.png","06/Nov/19 11:22;Eric Lee;C99D8A02-9C7A-4809-B151-93CE166E92DC.png;https://issues.apache.org/jira/secure/attachment/12985040/C99D8A02-9C7A-4809-B151-93CE166E92DC.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 07 15:26:02 UTC 2019,,,,,,,,,,"0|z08b8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/19 11:37;Eric Lee;Would like to fix the problems. Can someone assign me this issue?;;;","06/Nov/19 12:11;chesnay;This part of the code evaluate the [scope formats|https://ci.apache.org/projects/flink/flink-docs-master/monitoring/metrics.html#system-scope], which are _always _separated by periods, regardless of what is configured as a global delimiter.

The only case where this is problematic is if a constant part of a scope format is supposed to contain a period; is that your goal?;;;","07/Nov/19 03:12;Eric Lee;Yes, under certain circumstances, it will happen.

Also, I think we can't always make an assumption that people always use a period as default delimiter. It's also possible that users set '|' or other characters as the default delimiter if they have set the 'metrics.scope.jm
' with value '<host>|jobmanager'. There are some logical errors here.;;;","07/Nov/19 09:41;chesnay;The delimiter used in the scope-format is is no way connected to the actual delimiter used at runtime.

If you configure {{|}} as the global delimiter, and {{<host>.jobmanager}} as the scope format, then the final scope at runtime will be something like {{localhost|jobmanager}}.
The period in the scope format only separates components of the format, it does not control which delimiter is used at runtime.;;;","07/Nov/19 15:26;Eric Lee;Got your point. Thanks for your help.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn tests no longer create jobmanager log files,FLINK-14630,13266545,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kkl0u,trohrmann,trohrmann,06/Nov/19 10:18,04/Dec/19 14:26,13/Jul/23 08:10,08/Nov/19 08:50,1.10.0,,,,,,,,,1.10.0,,,,Deployment / YARN,Tests,,,,0,pull-request-available,,,,"With FLINK-14502 we changed how log property files are being discovered by the {{YarnClusterDescriptor}}. In fact we moved the responsibility to the {{FlinkYarnSessionCli}}. This is problematic because the {{YarnTestBase}} creates for Yarn tests a configuration directory which contains the Flink configuration and the log property files. These files are no longer respected because the respective code path has been removed.

I think it is important to fix this problem so that our Yarn tests start to log again. Otherwise it becomes harder to debug Yarn problems and the tests can no longer check for the absence of exceptions in the logs.",,kkl0u,tison,trohrmann,wind_ljy,,,,,,,,,,"kl0u commented on pull request #10115: [FLINK-14630] Re-enable Yarn tests to write job manager logs
URL: https://github.com/apache/flink/pull/10115
 
 
   ## What is the purpose of the change
   
   With [FLINK-14502](https://issues.apache.org/jira/browse/FLINK-14502) we changed how log property files are being discovered by the `YarnClusterDescriptor` and this resulted in Yarn tests not writing their job manager log files anymore. This made debugging a lot harder and next to impossible. 
   
   This commit fixes this by introducing and exposing the utility method `Configuration setLogConfigFileInConfig(...)` in the `FlinkYarnSessionCli` which is responsible for discovering the log config file. This method is now used directly by the tests to simulate what the CLI does based on user-provided input.
   
   ## Verifying this change
   
   Running a test like the `YarnITCase` and seeing the log files being created in the `target` dir is sufficient to verify the validity of the change.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 11:55;githubbot;600","kl0u commented on pull request #10115: [FLINK-14630] Re-enable logging for Yarn tests
URL: https://github.com/apache/flink/pull/10115
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 08:49;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-14502,,,,FLINK-15055,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 08 08:50:59 UTC 2019,,,,,,,,,,"0|z08b54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/19 08:50;kkl0u;Merged on master with 18f101acef33c4375c1e21d133a9a16c9d333e03;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add validation check when applying UDF to  tempral table key in  Temporal Table Join condition ,FLINK-14613,13266316,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hailong wang,hailong wang,hailong wang,05/Nov/19 11:49,03/Jan/20 02:51,13/Jul/23 08:10,03/Jan/20 02:02,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"In Temporal Table Join, We don't support using  UDF in tempral table join key. For we can't analyze LookupKeys  when call is an expression. When users use like this, the program run normally,  and the result will be wrong. So we should add validation to prevent it.

The SQL as following:
{code:java}
INSERT INTO A
SELECT B.amount, B.currency, C.amount, C.product 
FROM B join C FOR SYSTEM_TIME AS OF B.proctime 
on B.amount = cancat(C.amount, 'r') and C.product = '1'
{code}",,hailong wang,jark,leonard,libenchao,Terry1897,,,,,,,,,"wangxlong commented on pull request #10144: [FLINK-14613][table-planner-blink] Fix temporal table join when containing multiple keyfields with UDF
URL: https://github.com/apache/flink/pull/10144
 
 
   
   ## What is the purpose of the change
   
   Fix temporal table join when containing multiple keyfields with UDF.
   The issue link is https://issues.apache.org/jira/browse/FLINK-14613
   
   
   ## Brief change log
   
   Add  keyfields with UDF condition to remainingJoinCondition in CommonLookupJoin.
   
   
   ## Verifying this change
   This change added tests and can be verified as follows:
   
   (1) LookupJoinITCase#testJoinTemporalTableOnMultiKeyFieldsWithUDF
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Nov/19 09:03;githubbot;600","wuchong commented on pull request #10144: [FLINK-14613][table-planner-blink] Fix temporal table join when containing multiple keyfields with UDF
URL: https://github.com/apache/flink/pull/10144
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 02:01;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-14961,,,,,,,,,,FLINK-14961,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 03 02:02:47 UTC 2020,,,,,,,,,,"0|z089q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/19 11:53;hailong wang;cc [~jark]. What do you think,  Can I go ahead on it. Thanks;;;","06/Nov/19 07:45;jark;Hi [~hailong wang], I don't fully understand it. The above exmaple doesn't contain UDF in join key. ;;;","06/Nov/19 08:25;hailong wang;Hi [~jark], I sorry for giving a wrong example. I have updated. In short, when using UDF in tempral table join key, the result is error. For the join condition of ' B.amount = cancat(C.amount, 'r')' is ignored, and the final condition is 'C.product = '1'.

So when used like this, we should throw error when validation?;;;","06/Nov/19 09:15;jark;For temporary solution, adding an exception is fine. However, even there is an UDF in the join key, if we still have other equi-conditions on field reference (above example), it should still work because we have lookup key.

I think if the proper fixing is not a big job, we should fix the root problem. ;;;","07/Nov/19 11:50;hailong wang;Hi [~jark], you are right. I have some misunderstanding before. There are two situation:

(1) Join condition has  an UDF in temporal join key and other equi-conditions. Example as following:
{code:java}
INSERT INTO A
SELECT B.amount, B.currency, C.amount, C.product 
FROM B join C FOR SYSTEM_TIME AS OF B.proctime 
on B.amount = cancat(C.amount, 'r') and C.product = '1'
{code}
For now, the program runs normally. But the join condition of ' B.amount = cancat(C.amount, 'r')' is ignored, and the final condition is 'C.product = '1'. So the result is error. So we should add 'B.amount = cancat(C.amount, 'r')'  to remainingJoinCondition.

(2) Join condition only has  an UDF in temporal join key. Example as following:
{code:java}
INSERT INTO A SELECT B.amount, B.currency, C.amount, C.product FROM B join C FOR SYSTEM_TIME AS OF B.proctime on B.amount = cancat(C.amount, 'r')
{code}
It will throw error:
{code:java}
 Temporal table join requires an equality condition on fields of table ……
{code}
I think it's reasonable. 

If we want to fix this, I think we should lookup the all rows from Temporal Table, and add remainingJoinCondition in JoinCollector.

In conclusion, we should fix the first situation by adding 'B.amount = cancat(C.amount, 'r')' to remainingJoinCondition.

Thanks you.

 ;;;","08/Nov/19 08:32;jark;Hi [~hailong wang], you are right. I assigned this issue to you. Feel free to open pull request. ;;;","03/Jan/20 02:02;jark;1.11.0: 9637af38b415e08888e45dde06a220507c4f53ae
1.10.0: 33cd82238cdd0a590aa83a8b9afcdb4f639da5d6;;;",,,,,,,,,,,,,,,,,,,,,,,,,
NetworkBufferPoolTest.testBlockingRequestFromMultiLocalBufferPool timeout in travis,FLINK-14603,13266243,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kevin.cyj,ykt836,ykt836,05/Nov/19 03:12,07/Nov/19 02:24,13/Jul/23 08:10,06/Nov/19 11:14,,,,,,,,,,1.10.0,,,,Runtime / Network,Tests,,,,0,pull-request-available,test-stability,,,"21:50:11.821 [ERROR] Tests run: 14, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.538 s <<< FAILURE! - in org.apache.flink.runtime.io.network.buffer.NetworkBufferPoolTest 21:50:11.828 [ERROR] testBlockingRequestFromMultiLocalBufferPool(org.apache.flink.runtime.io.network.buffer.NetworkBufferPoolTest) Time elapsed: 10.01 s <<< ERROR! org.junit.runners.model.TestTimedOutException: test timed out after 10000 milliseconds at org.apache.flink.runtime.io.network.buffer.NetworkBufferPoolTest.testBlockingRequestFromMultiLocalBufferPool(NetworkBufferPoolTest.java:713)

 

see: [https://api.travis-ci.org/v3/job/607303537/log.txt]",,gjy,kevin.cyj,pnowojski,trohrmann,zjwang,,,,,,,,,"wsry commented on pull request #10097: [FLINK-14603][runtime]Notify the potential buffer consumers if the size of LocalBufferPool has been expanded.
URL: https://github.com/apache/flink/pull/10097
 
 
   ## What is the purpose of the change
   Currently, when the size of ```LocalBufferPool``` is expended by ```LocalBufferPool#setNumBuffers``` and there are segments available in the global ```NetworkBufferPool```, we may failed to notify the potential buffer consumers which are waiting for the ``LocalBufferPool`` to be available. This is a potential regression compared to the previous implementation in which the blocking request thread will wake itself up actively and request available buffers form the global pool.
   
   The purpose of this PR is to fix the regression by notify the potential buffer consumers when the size of ```LocalBufferPool``` is expended.
   
   ## Brief change log
   
     - The potential buffer consumers are notified when the size of ```LocalBufferPool``` is expended.
     - Test case ```LocalBufferPoolTest#testIsAvailableOrNot``` is enhanced to cover the scenario.
   
   
   ## Verifying this change
   
   This change is verified by enhancing the existing test case ```LocalBufferPoolTest#testIsAvailableOrNot```.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/19 06:00;githubbot;600","zhijiangW commented on pull request #10097: [FLINK-14603][runtime]Notify the potential buffer consumers if the size of LocalBufferPool has been expanded.
URL: https://github.com/apache/flink/pull/10097
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/19 11:12;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 06 11:14:49 UTC 2019,,,,,,,,,,"0|z089a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/19 09:21;trohrmann;cc [~kevin.cyj];;;","05/Nov/19 09:24;trohrmann;Another instance: https://api.travis-ci.org/v3/job/607109379/log.txt;;;","05/Nov/19 09:32;kevin.cyj;Thanks, [~trohrmann] and [~ykt836]. I will try to find out why the case is not stable and fix it as soon as possible.;;;","05/Nov/19 13:23;gjy;Another instance https://api.travis-ci.org/v3/job/607574361/log.txt;;;","06/Nov/19 02:53;zjwang;We already found the root cause and would submit a fix for it today.;;;","06/Nov/19 11:14;zjwang;Merged in master : 9b43f13a50848382fbd634081b82509f464e62ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
CLI documentation for list is missing '-a',FLINK-14601,13266202,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xiaodao,turbocon,turbocon,04/Nov/19 22:08,11/Nov/19 13:49,13/Jul/23 08:10,11/Nov/19 13:49,,,,,,,,,,1.10.0,,,,,,,,,0,pull-request-available,,,,"It is shown in the examples here [https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/cli.html#job-management-examples] however it is not in the help docs or any other documentation I can find 

 
{code:java}
$ ./bin/flink list --help
Action ""list"" lists running and scheduled programs.  Syntax: list [OPTIONS]
  ""list"" action options:
     -r,--running     Show only running programs and their JobIDs
     -s,--scheduled   Show only scheduled programs and their JobIDs
  Options for default mode:
     -m,--jobmanager <arg>           Address of the JobManager (master) to which
                                     to connect. Use this flag to connect to a
                                     different JobManager than the one specified
                                     in the configuration.
     -z,--zookeeperNamespace <arg>   Namespace to create the Zookeeper sub-paths
                                     for high availability mode

{code}",,aljoscha,tison,turbocon,xiaodao,ysqwhiletrue,,,,,,,,,"zoudaokoulife commented on pull request #10095: [FLINK-14601] [client] CLI documentation for list is missing '-a'
URL: https://github.com/apache/flink/pull/10095
 
 
   ## What is the purpose of the change
   
   resolve the issue CLI documentation for list is missing '-a'
     described in [FLINK-14601](https://issues.apache.org/jira/browse/FLINK-14601)
   
   
   ## Brief change log
     - add ALL_OPTION to the options of CliFrontend list
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable )
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/19 05:17;githubbot;600","aljoscha commented on pull request #10095: [FLINK-14601] [client] CLI documentation for list is missing '-a'
URL: https://github.com/apache/flink/pull/10095
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 13:48;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 11 13:49:00 UTC 2019,,,,,,,,,,"0|z0890w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/19 09:40;tison;Thanks for reporting this. I think with the patch below we can solve this issue. Could you verify it?


{code}
diff --git a/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontendParser.java b/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontendParser.java
index c0295c7073..4905a374a7 100644
--- a/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontendParser.java
+++ b/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontendParser.java
@@ -283,6 +283,7 @@ public class CliFrontendParser {
        }
 
        private static Options getListOptionsWithoutDeprecatedOptions(Options options) {
+               options.addOption(ALL_OPTION);
                options.addOption(RUNNING_OPTION);
                return options.addOption(SCHEDULED_OPTION);
        }
{code}
;;;","05/Nov/19 09:41;tison;Maybe we can refactor the parser a bit for unify the code path for retrieving options so that we don't meet the problem update one without the other. But the diff above is a quick hotfix. If you're willing to send a pull request or even investigate a bit, please let me know so that I can assign the issue to you.;;;","05/Nov/19 11:38;xiaodao;[~tison] can assign the issue to me, just fix the bug,but not refactor the parser of CliFrontendParser.;;;","06/Nov/19 02:03;tison;[~xiaodao] I've assigned the issue to you.;;;","11/Nov/19 13:49;aljoscha;Fixed on master in a43175c8b03e2dc3fcfc2a42a71be37cf05c92cd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Move flink-orc to flink-formats from flink-connectors,FLINK-14595,13266016,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,04/Nov/19 01:53,25/Nov/19 03:32,13/Jul/23 08:10,25/Nov/19 03:32,,,,,,,,,,1.10.0,,,,Connectors / ORC,,,,,0,pull-request-available,,,,"We already have the parent model of formats. we have put other formats(flink-avro, flink-json, flink-parquet, flink-json, flink-csv, flink-sequence-file) to flink-formats. flink-orc is a format too. So we can move it to flink-formats.
 
In theory, there should be no compatibility problem, only the parent model needs to be changed, and no other changes are needed. 
 
Discuss thread: [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Move-flink-orc-to-flink-formats-from-flink-connectors-td34438.html]
Vote thread: [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Move-flink-orc-to-flink-formats-from-flink-connectors-td34496.html]",,jark,lzljs3620320,wind_ljy,,,,,,,,,,,"JingsongLi commented on pull request #10277: [FLINK-14595][orc] Move flink-orc to flink-formats from flink-connectors
URL: https://github.com/apache/flink/pull/10277
 
 
   
   ## What is the purpose of the change
   
   We already have the parent model of formats. we have put other formats(flink-avro, flink-json, flink-parquet, flink-json, flink-csv, flink-sequence-file) to flink-formats. flink-orc is a format too. So we can move it to flink-formats.
    
   In theory, there should be no compatibility problem, only the parent model needs to be changed, and no other changes are needed. 
    
   Discuss thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Move-flink-orc-to-flink-formats-from-flink-connectors-td34438.html
   Vote thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Move-flink-orc-to-flink-formats-from-flink-connectors-td34496.html
   
   ## Brief change log
   
   - Move flink-orc to flink-formats from flink-connectors
   - modify stage.sh
   
   ## Verifying this change
   
   This change is a code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Nov/19 10:56;githubbot;600","wuchong commented on pull request #10277: [FLINK-14595][orc] Move flink-orc to flink-formats from flink-connectors
URL: https://github.com/apache/flink/pull/10277
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Nov/19 03:17;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 25 03:32:06 UTC 2019,,,,,,,,,,"0|z087vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/19 02:19;jark;Thanks [~lzljs3620320] for driving this. I assigned this issue to you if you have time to do it. ;;;","25/Nov/19 03:32;jark;1.10.0: 320240e2c412c15c7fa91649a0faa78018e74d86;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redundant slot requests with the same AllocationID leads to inconsistent slot table,FLINK-14589,13265602,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hwanju,hwanju,hwanju,31/Oct/19 18:04,06/Nov/19 22:56,13/Jul/23 08:10,06/Nov/19 22:56,1.6.3,,,,,,,,,1.10.0,1.8.3,1.9.2,,Runtime / Coordination,,,,,0,pull-request-available,,,,"_NOTE: We found this issue in 1.6.2, but I checked the relevant code is still in the mainline. What I am not sure, however, is what other slot-related fixes after 1.6.2 (such as FLINK-11059 and FLINK-12863, etc) would prevent the initial cause of this issue from happening. So far, I have not found the related fix to the issue I am describing here, so opening this issue. Please feel free to deduplicate this if another one already covers it. Please note that we have already picked FLINK-9912, which turned out to be a major fix to slot allocation failure issue. I will note the ramification to that issue just in case others experience the same problem)._
h2. Summary

When *requestSlot* is called from ResourceManager (RM) to TaskManager (TM), TM firstly reserves the requested slot marking it as ALLOCATED, offers the slot to JM, and marks the slot as ACTIVE once getting acknowledgement from JM. This three-way communication for slot allocation is identified by AllocationID, which is generated by JM initially. The way TM reserves a slot is by calling *TaskSlotTable.allocateSlot* if the requested slot number (i.e., slot index) is free to use. The major data structure is *TaskSlot* indexed by slot index. Once the slot is marked as ALLOCATED with a given AllocationID, it tries to update other maps such as *allocationIDTaskSlotMap* keyed by AllocationID and *slotsPerJob* keyed by JobID. Here when updating *allocationIDTaskSlotMap*, it's directly using *allocationIDTaskSlotMap.put(allocationId, taskSlot)*, which may overwrite existing entry, if one is already there with the same AllocationID. This would render inconsistency between *TaskSlot* and *allocationIDTaskSlotMap*, where the former says two slots are allocated by the same AllocationID and the latter says the AllocationID only has the latest task slot. With this state, once the slot is freed, *freeSlot* is driven by AllocationID, so it fetches slot index (i.e., the latter one that has arrived later) from *allocationIDTaskSlotMap*, marks the slot free, and removes it from *allocationIDTaskSlotMap*. But still the old task slot is marked as allocated. This old task slot becomes zombie and can never be freed. This can cause permanent slot allocation failure if TM slots are statically and tightly provisioned and resource manager is not actively spawning new TMs where unavailable (e.g., Kubernetes without active mode integration, which is not yet available).
h2. Scenario

From my observation, the redundant slot requests with the same AllocationID and different slot indices should be rare but can happen with race condition especially when repeated fail-over and heartbeat timeout (primarily caused by transient resource overload, not permanent network partition/node outage) are taking place. The following is a detailed scenario, which could lead to this issue (AID is AllocationID):
 # AID1 is requested from JM and put in the pending request queue in RM.
 # RM picks up slot number 1 (Slot1) from freeSlots and performs requestSlot with Slot1 and AID1. Here this slot request is on the fly.
 # In the meantime, Slot1 is occupied by AID2 in TM for a delayed slot request and TM sends slot report via heartbeat to RM saying Slot1 is already allocated with AID2.
 # RM's heartbeat handler identifies that Slot1 is occupied with a different AID (AID2) so that it should reject the pending request sent from step 2.
 # handleFailedSlotRequest puts the rejected AID1 to pending request again by retrying the slot request. Now it picks up another available slot, say Slot2. So, the retried slot request with Slot 2 and AID1 is on the fly.
 # In the meantime, Slot1 occupied by AID2 is freed (by any disconnection with JM, or releasing all the tasks in the slot on cancellation/failure - the latter was observed).
 # The in-flight slot request (Slot1, AID1) from step 2 arrives at TM, and it's succeeded as Slot1 is free to allocate. TM offers the Slot1 to JM, which acknowledges it so that TM marks Slot1 ACTIVE with AID1. As this point, allocationIDTaskSlotMap[AID1] = Slot1 in TM. JM's allocatedSlots[AID1] = Slot1.
 # The next in-flight slot request (Slot2, AID1) from step 5 arrives at TM. As Slot2 is still free, TM marks it ALLOCATED and offers Slot2 to JM and *""overwrite allocationIDTaskSlotMap[AID1] to Slot2""*
 # In step 7, JM has allocatedSlots[AID1] = Slot1, which leads JM to reject the offer as the same AID is already occupied by another slot.
 # TM gets the rejected offer for (Slot2, AID1) and frees Slot2. As part of that, it removes allocationIDTaskSlotMap[AID1]. *Here Slot1 is still marked as ALLOCATED with AID1 but allocationIDTaskSlotMap contains nothing for AID1.*
 # From this point on, RM believes that Slot1 is allocated for AID1, so is JM, proceeding task deployment with AID1. In TM, AID1 is not allocated at all due to allocationIDTaskSlotMap[AID1] = null. Task deployment is failed with *TaskSubmissionException(""No task slot allocated for job ID"")*.
 # Any slot release from JM (by another heartbeat timeout) removes the allocated slot (Slot1, AID1) from allocatedSlots and availableSlots, where freeing slots with AID1 in TM is no-op due to allocationIDTaskSlotMap[AID1] = null.
 # Any further scheduling is failed with *NoResourceAvailableException(""Could not allocate all requires slots within timeout"")*, unless active resource manager is used.

h2. Repro method

The repro I have is a little stressed way than deterministic one, by having constantly failing app (also with not reacting to cancellation to prolong fail-over), with short heartbeat timeout (<=1s) to emulate resource overload without imposing arbitrary resource overloads (I enabled DEBUG log, which could also add a bit more load). Apart from repro test, the real applications that run into this issue are heavily loaded (high slot oversubscription) doing continuous fail-overs (by code error).
h2. Experiment

I've tested the simple solution like having *TaskSlotTable.allocateSlot* check if *allocationIDTaskSlotMap* has an existing task slot entry for a requested AID, it can reject allocation up front, so that the redundant slot request can fail fast not reaching reservation and slot offer, preventing allocationIDTaskSlotMap overwrite and inconsistent allocation tables. In the above example, the fix can replace step 8, 9, and 10 with RM getting *SlotAllocationException* on handling failed slot request. It may retry the pending request via handleFailedSlotRequest, but it would stop retrying once next heartbeat saying the pending slot was allocated by another AID or eventually slot request timeout. My test with the fix has run for a while hitting this race case multiple times and survived without slot allocation failure.

As my experience may be in a bit old version (1.6.2), It would be good to discuss what other unexpected things happened and what would be other related issues that have been incorporated in mainline.",,dchmelev,dwysakowicz,gjy,hwanju,praveeng,rmetzger,trohrmann,txhsj,uce,,,,,"hwanju commented on pull request #10099: [FLINK-14589] [Runtime/Coordination] Redundant slot requests with the same AllocationID lead…
URL: https://github.com/apache/flink/pull/10099
 
 
   …s to inconsistent slot table
   
   ## What is the purpose of the change
   
   When a slot request is redundantly made with the same AllocationID to a
   slot index other than the already allocated one, slot table becomes
   inconsistent having two slot indices allocated but one AllocationID
   assigned to only the latest slot index. This can lead to slot leakage.
   This patch prevents such redundent slot request from rendering
   inconsistent slot allocation state by rejecting the request.
   
   ## Brief change log
   
     - Let `TaskSlotTable.allocateSlot` disallow slot allocation request if a requested allocation ID is already occupied by any slot.
     - Added a unit test to `TaskSlotTableTest`
   
   ## Verifying this change
    - Existing tests should pass
    - A newly added `testRedundantSlotAllocation` is succeeded with the fix.
    - Manually verified the change by running a constantly failing app (by throwing exception in UDF to trigger fail-over and swallowing interrupts on source to make cancellation stuck) with 64 parallelism with 8 slots per taskmanager and 600ms heartbeat timeout (100ms heartbeat interval) for both continuous fail-over and heartbeat timeout. Without fix, this stress test hits this bug and then keeps getting slot allocation failure exception. With the fix, it does survive without slot allocation failure for days (with log `Allocation ID {} is already allocated in {}.` printed, indicating it exercise the bug).
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/19 07:44;githubbot;600","tillrohrmann commented on pull request #10099: [FLINK-14589] [Runtime/Coordination] Redundant slot requests with the same AllocationID lead…
URL: https://github.com/apache/flink/pull/10099
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/19 22:54;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 06 22:56:09 UTC 2019,,,,,,,,,,"0|z085bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/19 08:30;trohrmann;Thanks for reporting this issue [~hwanju]. I think your analysis is spot on and it should be still a problem in Flink > 1.6. I think your solution approach is good and should solve the problem. Would you have time to open a PR against the master for it? I've assigned you to the issue.;;;","04/Nov/19 20:43;hwanju;[~trohrmann], sure I will ping you once PR can be ready.;;;","06/Nov/19 22:56;trohrmann;Fixed via

1.10.0: 155885cc6d182bf0f6519a9992b21b957444b4fe
1.9.2: 6904fc6dc55d43ead2bf95f0cf495b3e98a53337
1.8.3: d25246920bddfd045528a8e683fc0b3ecca8a6a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 flink-s3-fs-hadoop doesn't work with plugins mechanism,FLINK-14574,13265300,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,pnowojski,pnowojski,30/Oct/19 13:25,22/Jun/21 14:05,13/Jul/23 08:10,13/Dec/19 12:57,1.9.0,,,,,,,,,1.10.0,1.9.2,,,FileSystems,,,,,0,pull-request-available,,,,"As reported by a user via [mailing list|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/No-FileSystem-for-scheme-quot-file-quot-for-S3A-in-and-state-processor-api-in-1-9-td30704.html]:
{noformat}
We've added flink-s3-fs-hadoop library to plugins folder and trying to
bootstrap state to S3 using S3A protocol. The following exception happens
(unless hadoop library is put to lib folder instead of plugins). Looks like
S3A filesystem is trying to use ""local"" filesystem for temporary files and
fails:

java.lang.Exception: Could not write timer service of MapPartition
(d2976134f80849779b7a94b7e6218476) (4/4) to checkpoint state stream.
	at
org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:466)
	at
org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:89)
	at
org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:399)
	at
org.apache.flink.state.api.output.SnapshotUtils.snapshot(SnapshotUtils.java:59)
	at
org.apache.flink.state.api.output.operators.KeyedStateBootstrapOperator.endInput(KeyedStateBootstrapOperator.java:84)
	at
org.apache.flink.state.api.output.BoundedStreamTask.performDefaultAction(BoundedStreamTask.java:85)
	at
org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:298)
	at
org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:403)
	at
org.apache.flink.state.api.output.BoundedOneInputStreamTaskRunner.mapPartition(BoundedOneInputStreamTaskRunner.java:76)
	at
org.apache.flink.runtime.operators.MapPartitionDriver.run(MapPartitionDriver.java:103)
	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:504)
	at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:369)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Could not open output stream for state
backend
	at
org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.createStream(FsCheckpointStreamFactory.java:367)
	at
org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.flush(FsCheckpointStreamFactory.java:234)
	at
org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.write(FsCheckpointStreamFactory.java:209)
	at
org.apache.flink.runtime.state.NonClosingCheckpointOutputStream.write(NonClosingCheckpointOutputStream.java:61)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:401)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at
org.apache.flink.util.LinkedOptionalMapSerializer.lambda$writeOptionalMap$0(LinkedOptionalMapSerializer.java:58)
	at
org.apache.flink.util.LinkedOptionalMap.forEach(LinkedOptionalMap.java:163)
	at
org.apache.flink.util.LinkedOptionalMapSerializer.writeOptionalMap(LinkedOptionalMapSerializer.java:57)
	at
org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshotData.writeKryoRegistrations(KryoSerializerSnapshotData.java:141)
	at
org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshotData.writeSnapshotData(KryoSerializerSnapshotData.java:128)
	at
org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.writeSnapshot(KryoSerializerSnapshot.java:72)
	at
org.apache.flink.api.common.typeutils.TypeSerializerSnapshot.writeVersionedSnapshot(TypeSerializerSnapshot.java:153)
	at
org.apache.flink.streaming.api.operators.InternalTimersSnapshotReaderWriters$InternalTimersSnapshotWriterV2.writeKeyAndNamespaceSerializers(InternalTimersSnapshotReaderWriters.java:199)
	at
org.apache.flink.streaming.api.operators.InternalTimersSnapshotReaderWriters$AbstractInternalTimersSnapshotWriter.writeTimersSnapshot(InternalTimersSnapshotReaderWriters.java:117)
	at
org.apache.flink.streaming.api.operators.InternalTimerServiceSerializationProxy.write(InternalTimerServiceSerializationProxy.java:101)
	at
org.apache.flink.streaming.api.operators.InternalTimeServiceManager.snapshotStateForKeyGroup(InternalTimeServiceManager.java:139)
	at
org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:462)
	... 14 common frames omitted
Caused by:
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.UnsupportedFileSystemException:
No FileSystem for scheme ""file""
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3332)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3352)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3403)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3371)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem.get(FileSystem.java:477)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:433)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:301)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:378)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:456)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:200)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:572)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:811)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:190)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3ABlockOutputStream.<init>(S3ABlockOutputStream.java:168)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:778)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)
	at
org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)
	at
org.apache.flink.fs.s3.common.hadoop.HadoopFileSystem.create(HadoopFileSystem.java:141)
	at
org.apache.flink.fs.s3.common.hadoop.HadoopFileSystem.create(HadoopFileSystem.java:37)
	at
org.apache.flink.core.fs.SafetyNetWrapperFileSystem.create(SafetyNetWrapperFileSystem.java:126)
	at
org.apache.flink.core.fs.EntropyInjector.createEntropyAware(EntropyInjector.java:61)
	at
org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.createStream(FsCheckpointStreamFactory.java:356)
	... 32 common frames omitted
{noformat}
I think the problem is caused by {{org.apache.hadoop.fs.FileSystem#loadFileSystems}} method inside {{flink-s3-fs-hadoop}}, which is using {{ServiceLoader.load(FileSystem.class);}} to load a FileSystem via {{Thread.currentThread().getContextClassLoader();}}. At this point of time {{getContextClassLoader()}} is probably already the user class loader instead of plugin's.

We should investigate why is this {{loadFileSystems}} method called so late (after we have already restored user's class loader) and how can we workaround this.",,aljoscha,arvid heise,lukesun,pnowojski,spoganshev,wangyang0918,,,,,,,,"AHeise commented on pull request #10460: [FLINK-14574] Fixing writing with plugin s3 filesystem. 
URL: https://github.com/apache/flink/pull/10460
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Writing with the flink-s3-fs-hadoop plugin currently fails because the filesystem internally lazily loads other services when the context classloader has been replaced by the user code classloader.
   
   ## Brief change log
   
   - Changing e2e tests to actually reproduce the error.
   - Adding minio-based tests that allow local development without AWS secrets.
   - Wrapping filesystems coming from plugins to switch context classloader for all filesystem operations.
   
   ## Verifying this change
   
     - Added minio-based e2e tests.
     - Extended existing s3 batch tests to write to s3.
     - Added s3 streaming test to checkpoint to s3.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (**yes** / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 09:26;githubbot;600","zentol commented on pull request #10460: [FLINK-14574] Fixing writing with plugin s3 filesystem. 
URL: https://github.com/apache/flink/pull/10460
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Dec/19 16:21;githubbot;600","zentol commented on pull request #10552: [FLINK-14574][1.9] Fixing writing with plugin s3 filesystem. 
URL: https://github.com/apache/flink/pull/10552
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 12:56;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,FLINK-15216,,,,,,,,,,,,,FLINK-11952,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 13 12:57:21 UTC 2019,,,,,,,,,,"0|z07jv0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/19 16:22;chesnay;master:
b0d2ed55c44105e6192e32b391a4b6ef54aa2879
418e78bd8d9d6d207009fc6697815a59f7086864
713cbf8a668aedaf23a6a5d160ebc0dcd76dbe73;;;","12/Dec/19 09:13;chesnay;[~AHeise] can we backport this to 1.9?;;;","12/Dec/19 09:47;arvid;Backporting and open PR, once Travis passes.;;;","13/Dec/19 12:57;chesnay;1.9:

d8fec10a6627049645522cbd7f5bdec003cc00d1

9219b238eb3d897f74fa7e68f8e4a258e6acee83

f702cfba6d12bfc7488808f966bf9ed92d5f8b5e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggregate query with more than two group fields can't be write into HBase sink,FLINK-14567,13265216,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,jark,jark,30/Oct/19 07:17,30/Jun/20 10:33,13/Jul/23 08:10,30/Jun/20 10:33,,,,,,,,,,1.11.0,,,,Connectors / HBase,Table SQL / Legacy Planner,Table SQL / Planner,,,0,,,,,"If we have a hbase table sink with rowkey of varchar (also primary key) and a column of bigint, we want to write the result of the following query into the sink using upsert mode. However, it will fail when primary key check with the exception ""UpsertStreamTableSink requires that Table has a full primary keys if it is updated.""

{code:sql}
select concat(f0, '-', f1) as key, sum(f2)
from T1
group by f0, f1
{code}

This happens in both blink planner and old planner. That is because if the query works in update mode, then there must be a primary key exist to be extracted and set to {{UpsertStreamTableSink#setKeyFields}}. 

That's why we want to derive primary key for concat in FLINK-14539, however, we found that the primary key is not preserved after concating. For example, if we have a primary key (f0, f1, f2) which are all varchar type, say we have two unique records ('a', 'b', 'c') and ('ab', '', 'c'), but the results of concat(f0, f1, f2) are the same, which means the concat result is not primary key anymore.

So here comes the problem, how can we proper support HBase sink or such use case? ",,jackylau,jark,KevinZwx,leonard,openinx,twalthr,yesorno,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14977,FLINK-15407,,FLINK-14539,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 10:32:55 UTC 2020,,,,,,,,,,"0|z082xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/19 09:27;jackylau;Is it proper to deliver to the user who knows that the data format which seperator to use when we use concat_ws udf instead of framework?;;;","04/Nov/19 13:01;jark;[~jackylau] the problem is how. I don't have a clear idea right now. ;;;","06/Nov/19 09:30;jackylau;i means that the framework can not solve it. .User use concat_ws, and he/she choses seperator , because he know whether it can conform primary key is unique

Si it may need a ignore flag 

1) false, framework do it, but will cost lots of time. it will first get to know hbase whether has the pk before. If yes, throw expression and exit

2) true, framework don't do it. it will be overwride.;;;","15/Nov/19 10:27;KevinZwx;I basically agree with [~jackylau]. In the proposed scenario flink may be not able to figure out whether concating two unique key fields with a specific separator can preserve the uniqueness. However, most of the time the users understand their data and know using which separtor won't break the uniqueness, at this time we should let the users to determine whether the query result can be inserted into hbase sink.  The solution may be providing another concat_ws-like function with an additional parameter to indicate if to ignore the potential risks, and document the reason properly.;;;","17/Nov/19 03:06;jark;Another soution is letting sink concats keys. For example, an HBase sink can have more than one key fields, say k1, k2, k3, then a {{key-delimiter}} option is required to concat key fields to a rowkey, the concated rowkey will always be varchar type. The HBase sink will insert the concated rowkey into HBase table. 


{code:java}
create table my_table (
  k1 int,
  k2 varchar,
  k3 timestamp(3),
  f1 row<q1 bigint, q2 bigint>
) with (
  'connector.type' = 'hbase',
  'connector.key-delimiter' = '-'
);

insert into my_table 
select k1, k2, k3, ROW(count(*), count(distinct user))
group by k1, k2, k3
{code}

This is very similar to the [ElasticSearch Connector|https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connect.html#elasticsearch-connector]. In this way, {{UpsertStreamTableSink#setKeyFields}} still work for the sink, because the sink pretends it has a composite key with 3 fields. 


However, this can't solve all the problems. For example, if one of the 3 fields is transformed from the group key, but the transformation will lose key information.;;;","17/Nov/19 06:48;ykt836;Even if the framework can't derive primary key information of the query, we can still add an operator to convert the stream into an upsert stream. ;;;","18/Nov/19 02:34;jark;But adding an operator to convert into upsert stream is an unnecessary performance cost for users. It's a temporary work around, but we should have a better solution. ;;;","18/Nov/19 05:40;ykt836;It depends on how we see this problem. IMO if we want to emit the output stream with upsert fashion, it is required for framework to convert all outputs into an upsert stream based on some keys. It's not different with requiring hash distribution for some operators like HashJoin, GroupAggregate. Adding an operator to convert the stream is actually the baseline for this situation, just like we will add a keyBy shuffle when hash distribution is required. But in some cases we can do some optimizations, like if we can derive the primary key information of the query, and we can optimize the plan to get rid of the operator. Again, just the same as hash distribution, if we can derive this and can reduce the effort to add a dedicated keyBy shuffle. 

Back to this case, it looks to me there are some certain cases where we can't apply the optimization, that's all. ;;;","04/Dec/19 08:08;twalthr;In general, I think the key information should come from the \{{CREATE TABLE}} definition and not the query. And as Kurt said if the query keys match with the create table keys we can use this information for optimization. ;;;","04/Dec/19 08:22;jark;Hi [~twalthr] [~ykt836], do you mean if the key information of sink is not matched with query keys, then we can add a keyBy shuffle between them?  Otherwise, they can be chained. 

We already added primary key constraint to {{TableSchema}}, so we only need to add a keyBy shuffle in framework if they are not matched. ;;;","04/Dec/19 09:15;ykt836;Basically yes. But this would involve the discussion about how to deal with primary key in source and sink, which would be the following work of FLIP-87.;;;","04/Dec/19 09:54;jark;[~ykt836], could we just use the key constraint in {{TableSink#getTableSchema()}}, if this is still needed discussion, could we just support multiple keys for hbase sink? This can be a workaround and can reduce the keyBy shuffle. 


{code:sql}
create table my_table (
  k1 int,
  k2 varchar,
  k3 timestamp(3),
  f1 row<q1 bigint, q2 bigint>
) with (
  'connector.type' = 'hbase',
  'connector.key-delimiter' = '-'
);

insert into my_table 
select k1, k2, k3, ROW(count(*), count(distinct user))
group by k1, k2, k3
{code}
;;;","06/Dec/19 14:08;twalthr;I'm not very familiar with HBase. Is the key always a string or can it also be byte[]?;;;","09/Dec/19 02:39;jark;[~twalthr], in the storage, the HBase rowkey is always stored in bytes. But we can support String/Int as the rowkey logical type, because we can convert it to bytes using HBase's {{Bytes}} util.;;;","30/Jun/20 10:32;leonard;I close this issue because defining primary keys in sink table after FLIP-87  has resolved this problem.

feel free to reopen if you have any concern.;;;",,,,,,,,,,,,,,,,,
RMQSource leaves idle consumer after closing,FLINK-14562,13265061,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ndeslandes,ndeslandes,ndeslandes,29/Oct/19 14:49,29/Mar/21 14:08,13/Jul/23 08:10,03/Nov/19 20:13,1.7.2,1.8.0,1.8.2,,,,,,,1.10.0,1.8.3,1.9.2,,Connectors/ RabbitMQ,,,,,0,pull-request-available,,,,"RabbitMQ connector do not close consumers and channel on closing

This potentially leaves idle consumer on the queue that prevent any other consumer on the same queue to get message, this happens the most when a job is stop/cancel and redeploy.",,ndeslandes,trohrmann,,,,,,,,,,,,"ndeslandes commented on pull request #10036: [FLINK-14562][Connectors/ RabbitMQ]Idle consumer are left behind on closing
URL: https://github.com/apache/flink/pull/10036
 
 
   ## What is the purpose of the change
   
   *This pull request fix the close() method on the RMQSource class to clean any consumer that was used in it. This is needed to prevent leaving idle consumer of the RabbitMQ cluster*
   
   
   ## Brief change log
   
     - *Use the channel to cancel the active consumer*
     - *Close the channel itself*
     - *Close the connection (was already there)*
   
   
   ## Verifying this change
   
   The close method does not seem to be tested right now and I am not sure how to add a test for that.
   This fix clearly fix that bug for us in all of our environment  and I don’t think it can break anything else.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? No
     - If yes, how is the feature documented? not applicable
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Oct/19 15:07;githubbot;600","tillrohrmann commented on pull request #10036: [FLINK-14562][Connectors/ RabbitMQ]Idle consumer are left behind on closing
URL: https://github.com/apache/flink/pull/10036
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Nov/19 20:11;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 03 20:13:40 UTC 2019,,,,,,,,,,"0|z081zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/19 20:13;trohrmann;Fixed via

1.10.0: 0828e2942af41cdfe8356ce46cd7f1540f981a5d
1.9.2: 93f2c68c02080d30044924640db9643fdeb4dba8 
1.8.3: 9296ba5187a36af4115b1d493efae20d6cedfacc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't write FLINK_PLUGINS_DIR ENV variable to Flink configuration,FLINK-14561,13265053,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,29/Oct/19 14:21,31/Oct/19 13:02,13/Jul/23 08:10,31/Oct/19 13:02,1.10.0,1.9.1,,,,,,,,1.10.0,1.9.2,,,Runtime / Coordination,,,,,1,pull-request-available,,,,"With FLINK-12143 we introduced the plugin mechanism. As part of this feature, we now write the {{FLINK_PLUGINS_DIR}} environment variable to the Flink {{Configuration}} we use for the cluster components. This is problematic, because we also use this {{Configuration}} to start new processes (Yarn and Mesos {{TaskExecutors}}). If the {{Configuration}} contains a configured {{FLINK_PLUGINS_DIR}} which differs from the one used by the newly created process, then this leads to problems.

In order to solve this problem, I suggest to not write env variables which are intended for local usage within the started process into the {{Configuration}}. Instead we should directly read the environment variable at the required site similar to what we do with the env variable {{FLINK_LIB_DIR}}.",,trohrmann,wind_ljy,,,,,,,,,,,,"tillrohrmann commented on pull request #10037: [FLINK-14561] Don't write FLINK_PLUGINS_DIR env variable to Configuration
URL: https://github.com/apache/flink/pull/10037
 
 
   ## What is the purpose of the change
   
   This commit reads the FLINK_PLUGINS_DIR env variable directly instead of first
   writing it first to the Flink Configuration and then reading it from there. This
   has the advantage that the plugins env variable which is intended for the local
   process only is not being leaked to other processes. The latter can happen if
   we start a new Flink process based on the read Flink Configuration (e.g. Yarn,
   Mesos TaskExecutor).
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Oct/19 15:29;githubbot;600","tillrohrmann commented on pull request #10037: [FLINK-14561] Don't write FLINK_PLUGINS_DIR env variable to Configuration
URL: https://github.com/apache/flink/pull/10037
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Oct/19 13:00;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-12143,,,,FLINK-14382,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 13:02:07 UTC 2019,,,,,,,,,,"0|z081xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/19 13:02;trohrmann;Fixed via

1.10.0: f533694eca8d648885ab72d1143bb992f8d0cf37
1.9.2: af5f3a4232213e3763ee05d7be489b416c28939c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The value of taskmanager.memory.size in flink-conf.yaml is set to zero will cause taskmanager not to work ,FLINK-14560,13265024,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,faaronzheng,faaronzheng,faaronzheng,29/Oct/19 12:39,12/Mar/20 11:46,13/Jul/23 08:10,02/Mar/20 15:09,1.9.0,1.9.1,,,,,,,,1.9.3,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"If you accidentally  set taskmanager.memory.size: 0 in flink-conf.yaml, flink should take a fixed ratio with respect to the size of the task manager JVM. The relateted codes are in TaskManagerServicesConfiguration.fromConfiguration
{code:java}
//代码占位符
// extract memory settings
long configuredMemory;
String managedMemorySizeDefaultVal = TaskManagerOptions.MANAGED_MEMORY_SIZE.defaultValue();
if (!configuration.getString(TaskManagerOptions.MANAGED_MEMORY_SIZE).equals(managedMemorySizeDefaultVal)) {
   try {
      configuredMemory = MemorySize.parse(configuration.getString(TaskManagerOptions.MANAGED_MEMORY_SIZE), MEGA_BYTES).getMebiBytes();
   } catch (IllegalArgumentException e) {
      throw new IllegalConfigurationException(
         ""Could not read "" + TaskManagerOptions.MANAGED_MEMORY_SIZE.key(), e);
   }
} else {
   configuredMemory = Long.valueOf(managedMemorySizeDefaultVal);
}{code}
However, in ActiveResourceManagerFactory.java, flink will translate the value to byte.
{code:java}
//代码占位符
public static Configuration createActiveResourceManagerConfiguration(Configuration originalConfiguration) {
   final int taskManagerMemoryMB = ConfigurationUtils.getTaskManagerHeapMemory(originalConfiguration).getMebiBytes();
   final long cutoffMB = ContaineredTaskManagerParameters.calculateCutoffMB(originalConfiguration, taskManagerMemoryMB);
   final long processMemoryBytes = (taskManagerMemoryMB - cutoffMB) << 20; // megabytes to bytes
   final long managedMemoryBytes = TaskManagerServices.getManagedMemoryFromProcessMemory(originalConfiguration, processMemoryBytes);

   final Configuration resourceManagerConfig = new Configuration(originalConfiguration);
   resourceManagerConfig.setString(TaskManagerOptions.MANAGED_MEMORY_SIZE, managedMemoryBytes + ""b"");

   return resourceManagerConfig;
}
{code}
 

As a result, 0 will translate to 0 b and is different from default value.  0 b will cause a error in following check code
{code:java}
//代码占位符
checkConfigParameter(
   configuration.getString(TaskManagerOptions.MANAGED_MEMORY_SIZE).equals(TaskManagerOptions.MANAGED_MEMORY_SIZE.defaultValue()) ||
      configuredMemory > 0, configuredMemory,
   TaskManagerOptions.MANAGED_MEMORY_SIZE.key(),
   ""MemoryManager needs at least one MB of memory. "" +
      ""If you leave this config parameter empty, the system automatically "" +
      ""pick a fraction of the available memory."");
{code}
 

 ",,faaronzheng,trohrmann,,,,,,,,,,,,"faaronzheng commented on pull request #10073: [FLINK-14560] [runtime] The value of taskmanager.memory.size in flink-conf.yaml is set to zero will cause taskmanager not to work
URL: https://github.com/apache/flink/pull/10073
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   In order to fix the issue FLINK-14560
   
   ## Brief change log
   ConfigurationParserUtils.java : I modify the logical in getManagedMemorySize() to obtain a correct result when TaskManagerOptions.LEGACY_MANAGED_MEMORY_SIZE is set to zero.
   
   TaskManagerServices.java : I modify the condition in getManagedMemoryFromHeapAndManaged() to calculate the taskmanager.memory.size according to fraction  even when taskmanager.memory.size is set to zero . 
   
   TaskManagerHeapSizeCalculationJavaBashTest.java : Add a scenario with taskmanager.memory.size equal to 0
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   TaskManagerHeapSizeCalculationJavaBashTest.compareHeapSizeShellScriptWithJava()
   NettyShuffleEnvironmentConfigurationTest.calculateHeapSizeMB()
   NetworkBufferCalculationTest.calculateNetworkBufFromHeapSize()
   YarnConfigurationITCase.testFlinkContainerMemory()
   
   This change added tests and can be verified as follows:
   
   By setting taskmanager.memory.size : 0
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Nov/19 14:29;githubbot;600","faaronzheng commented on pull request #10090: [FLINK-14560] [runtime] The value of taskmanager.memory.size in flink…
URL: https://github.com/apache/flink/pull/10090
 
 
   …-conf.yaml is set to zero will cause taskmanager not to work
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   In order to fix the issue FLINK-14560
   
   
   ## Brief change log
   
   ConfigurationParserUtils.java : The comparison with the default value is removed, and only the existence is judged
   
   TaskManagerOptions.java : the default value of MANAGED_MEMORY_SIZE is removed
   
   LocalStreamEnvironment.java ： Delete assignment statement of MANAGED_MEMORY_SIZE
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as (please describe tests).
   TaskManagerHeapSizeCalculationJavaBashTest.compareHeapSizeShellScriptWithJava()
   NettyShuffleEnvironmentConfigurationTest.calculateHeapSizeMB()
   NetworkBufferCalculationTest.calculateNetworkBufFromHeapSize()
   YarnConfigurationITCase.testFlinkContainerMemory()
   
   
   ## Does this pull request potentially affect one of the following parts:
   
   Dependencies (does it add or upgrade a dependency): (no)
   The public API, i.e., is any changed class annotated with @Public(Evolving): (no)
   The serializers: (no)
   The runtime per-record code paths (performance sensitive): ( no)
   Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
   The S3 file system connector: (no)
   
   ## Documentation
   
   Does this pull request introduce a new feature? (no)
   If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Nov/19 15:43;githubbot;600","faaronzheng commented on pull request #10090: [FLINK-14560] [runtime] The value of taskmanager.memory.size in flink…
URL: https://github.com/apache/flink/pull/10090
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Nov/19 10:23;githubbot;600","faaronzheng commented on pull request #10073: [FLINK-14560] [runtime] The value of taskmanager.memory.size in flink-conf.yaml is set to zero will cause taskmanager not to work
URL: https://github.com/apache/flink/pull/10073
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Nov/19 10:23;githubbot;600","faaronzheng commented on pull request #10299: [FLINK-14560] [runtime] The value of taskmanager.memory.size in flink…
URL: https://github.com/apache/flink/pull/10299
 
 
   …-conf.yaml is set to zero will cause taskmanager not to work
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   In order to fix the issue FLINK-14560
   
   
   ## Brief change log
   TaskManagerOptions.java ：remove the default value
   ResourceManager.java: modify the code to suit none value
   ConfigurationParserUtils.java: modify the code  to suit none value
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
   Dependencies (does it add or upgrade a dependency): (no)
   The public API, i.e., is any changed class annotated with @public(Evolving): (no)
   The serializers: (no)
   The runtime per-record code paths (performance sensitive): ( no)
   Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
   The S3 file system connector: (no)
   
   ## Documentation
   
   Does this pull request introduce a new feature? (no)
   If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Nov/19 11:41;githubbot;600","zentol commented on pull request #10299: [FLINK-14560] [runtime] The value of taskmanager.memory.size in flink…
URL: https://github.com/apache/flink/pull/10299
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/20 11:46;githubbot;600",,,,,,,,,259200,255600,3600,1%,259200,255600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 15:09:48 UTC 2020,,,,,,,,,,"0|z081r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/19 12:43;faaronzheng;who can assign this issue to me?;;;","30/Oct/19 03:36;jark;Hi [~faaronzheng], I'm wondering it maybe not a bug. As the exception says, if you leave the config empty, Flink will automatically pick a ratio. 
However, the config is set to 0 now, and 0 is an invalid value. So I think the exception is as expected. ;;;","30/Oct/19 04:18;faaronzheng;[~jark], i think it's a little unreasonable.  zero is same as your default value in flink, but cause an error.  We find this issue because our hadoop system provide a  web to config some important settings when installing flink service. taskmanager.memory.size will be set to zero as default according to your default value. I think it will be better if we unify the meanning of 0 and 0b . ;;;","30/Oct/19 06:24;jark;[~faaronzheng], I see what's your problem. {{taskmanager.memory.size}} is a deprecated config option which has a default value 0. However, the new config option is {{taskmanager.memory.managed.size}} which doesn't have a default value and the logic is handled by the new config option.

See https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/configuration/TaskManagerOptions.java#L305

I'm not sure whether/how to fix this. I cc'ed guys who are familiar with this.  [~trohrmann] [~xintongsong];;;","30/Oct/19 14:21;trohrmann;I think this is a valid problem for Flink 1.9 at least. With 1.10 this logic will most likely change. But we should fix it for 1.9 and it doesn't hurt to also fix it in the master branch as long as we are still using {{TaskManagerOptions.LEGACY_MANAGED_MEMORY_SIZE}}.

I would suggest to not compare strings but {{MemorySize}} instead. [~faaronzheng] do you wanna work on this?;;;","31/Oct/19 01:17;faaronzheng;[~trohrmann], sure, i want to have a try.;;;","02/Mar/20 15:09;trohrmann;Fixed via 272fccf44fd92fc9a2c7fa7465b2fcc430842427;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Fix the ClassNotFoundException issue for run python job in standalone mode,FLINK-14558,13265008,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,sunjincheng121,sunjincheng121,29/Oct/19 10:58,31/Oct/19 14:04,13/Jul/23 08:10,31/Oct/19 14:04,,,,,,,,,,1.10.0,,,,API / Python,,,,,0,pull-request-available,,,,java.lang.ClassNotFoundException: org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperator will be thrown when running a Python UDF job in a standalone cluster.,,dian.fu,hequn8128,sunjincheng121,,,,,,,,,,,"dianfu commented on pull request #10045: [FLINK-14558][python] Fix ClassNotFoundException of PythonScalarFunctionOperator
URL: https://github.com/apache/flink/pull/10045
 
 
   
   ## What is the purpose of the change
   
   *This pull request fixes the ClassNotFoundException of PythonScalarFunctionOperator. Currently the jar of flink-python is loaded with user classloader and could not be found with the system classloader.*
   
   ## Brief change log
   
     - *Uses ContextClassLoader to load the classes located in flink-python module*
   
   ## Verifying this change
   
   Test manually in a standalone cluster.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 11:15;githubbot;600","hequn8128 commented on pull request #10045: [FLINK-14558][python] Fix ClassNotFoundException of PythonScalarFunctionOperator
URL: https://github.com/apache/flink/pull/10045
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Oct/19 14:03;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 14:04:29 UTC 2019,,,,,,,,,,"0|z081nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/19 01:28;dian.fu;[~sunjincheng121] Good catch! I think we should use the ContextClassLoader to load PythonScalarFunctionOperator as currently flink-python-*.jar is loaded with user classloader. I'd like to take this issue, could you assign it to me? Thanks in advance!;;;","30/Oct/19 01:53;sunjincheng121;Thanks for taking this Dian!;;;","31/Oct/19 14:04;hequn8128;Fixed in 1.10.0 via 3a69d5b8af7fd131670fbf4479653b8700c8a3c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the package of cloud pickle,FLINK-14556,13265006,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,sunjincheng121,sunjincheng121,29/Oct/19 10:56,31/Oct/19 06:50,13/Jul/23 08:10,31/Oct/19 06:50,,,,,,,,,,1.10.0,,,,API / Python,,,,,0,pull-request-available,,,,"Currently the package structure of cloud pickle is as following:
{code:java}
cloudpickle-1.2.2/
cloudpickle-1.2.2/cloudpickle/
cloudpickle-1.2.2/cloudpickle/__init__.py 
cloudpickle-1.2.2/cloudpickle/cloudpickle.py 
cloudpickle-1.2.2/cloudpickle/cloudpickle_fast.py 
cloudpickle-1.2.2/LICENSE
{code}
It should be:
{code:java}
cloudpickle/ 
cloudpickle/__init__.py  
cloudpickle/cloudpickle.py  
cloudpickle/cloudpickle_fast.py  
cloudpickle/LICENSE
{code}
Otherwise, the following error will be thrown when running in a standalone cluster :""ImportError: No module named cloudpickle"".

 ",,dian.fu,hequn8128,sunjincheng121,,,,,,,,,,,"dianfu commented on pull request #10046: [FLINK-14556][python] Correct the package of cloudpickle
URL: https://github.com/apache/flink/pull/10046
 
 
   
   ## What is the purpose of the change
   
   *This pull request corrects the cloudpickle package.*
   
   ## Brief change log
   
     - *Corrects the cloudpickle package*
   
   ## Verifying this change
   
   Test manually in a standalone cluster.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 11:19;githubbot;600","hequn8128 commented on pull request #10046: [FLINK-14556][python] Correct the package of cloudpickle
URL: https://github.com/apache/flink/pull/10046
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Oct/19 06:47;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 06:50:22 UTC 2019,,,,,,,,,,"0|z081n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/19 01:25;dian.fu;[~sunjincheng121] Thanks a lot for reporting this issue. Good catch! I'd like to take this issue, could you assign it to me? Thanks in advance!;;;","30/Oct/19 01:52;sunjincheng121;Thanks for taking this Dian!;;;","31/Oct/19 06:50;hequn8128;Fix in 1.10.0 via 6c6ada5b36577490fde74fc8d34b784a6e0dc25a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF cannot be in the join condition in blink planner,FLINK-14547,13264789,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,28/Oct/19 10:16,31/Oct/19 05:28,13/Jul/23 08:10,31/Oct/19 05:28,1.9.0,1.9.1,,,,,,,,1.10.0,1.9.2,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Currently, UDF cannot be in the join condition in blink planner, for the following example:

val util = batchTestUtil()
val left = util.addTableSource[(Int, Int, String)](""Table3"",'a, 'b, 'c)
val right = util.addTableSource[(Int, Int, Int, String, Long)](""Table5"", 'd, 'e, 'f, 'g, 'h)
util.addFunction(""Func"", Func0)
val result = left
 .leftOuterJoin(right, ""a === d && Func(a) === a + d"")
 .select(""a, d"")
util.verifyExplain(result)

The following exception will be thrown:

java.util.NoSuchElementException: No value presentjava.util.NoSuchElementException: No value present
 at java.util.Optional.get(Optional.java:135) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:475) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:463) at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:121) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.lambda$visit$0(QueryOperationConverter.java:470) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:472) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:463) at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:121) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.lambda$visit$0(QueryOperationConverter.java:470) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:472) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:463) at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:121) at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:228) at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:139) at org.apache.flink.table.operations.JoinQueryOperation.accept(JoinQueryOperation.java:128) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:136) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:116) at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:61) at org.apache.flink.table.operations.JoinQueryOperation.accept(JoinQueryOperation.java:128) at org.apache.flink.table.planner.plan.QueryOperationConverter.lambda$defaultMethod$0(QueryOperationConverter.java:135) at java.util.Collections$SingletonList.forEach(Collections.java:4822) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:135) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:116) at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:46) at org.apache.flink.table.operations.ProjectQueryOperation.accept(ProjectQueryOperation.java:75) at org.apache.flink.table.planner.calcite.FlinkRelBuilder.queryOperation(FlinkRelBuilder.scala:151) at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$1.apply(BatchPlanner.scala:81) at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$1.apply(BatchPlanner.scala:79) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.BatchPlanner.explain(BatchPlanner.scala:79) at org.apache.flink.table.api.internal.TableEnvironmentImpl.explain(TableEnvironmentImpl.java:296) at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyExplain(TableTestBase.scala:404) at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExplain(TableTestBase.scala:304) at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExplain(TableTestBase.scala:301) at org.apache.flink.table.planner.plan.batch.table.validation.JoinValidationTest.testOuterJoinWithPythonFunctionInCondition(JoinValidationTest.scala:130) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)",,hequn8128,hxbks2ks,jark,,,,,,,,,,,"HuangXingBo commented on pull request #10016: [FLINK-14547][table-planner-blink] Fix UDF cannot be in the join condition in blink planner
URL: https://github.com/apache/flink/pull/10016
 
 
   ## What is the purpose of the change
   
   * This pr fix UDF cannot be in the join condition in blink planner*
    
   ## Brief change log
   
     - *In the method visit(CallExpression callExpression) of JoinExpressionVisitor, When the objectIdentifier field of CallExpression is null, it will create a unresolved CallExpression.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Oct/19 11:20;githubbot;600","hequn8128 commented on pull request #10016: [FLINK-14547][table-planner-blink] Fix UDF cannot in the join condition in blink planner for Table API
URL: https://github.com/apache/flink/pull/10016
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Oct/19 01:45;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 05:28:06 UTC 2019,,,,,,,,,,"0|z080aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/19 05:28;hequn8128;Fixed in
1.9.2 via 4368f1da8598f993523dd18edef4b90f88eab4a1
1.10.0 via b4dfb3d6120762c3103d69e25ff05c344825e9ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support map type in flink-json,FLINK-14546,13264780,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,libenchao,libenchao,libenchao,28/Oct/19 09:44,06/Nov/19 00:49,13/Jul/23 08:10,06/Nov/19 00:49,1.8.2,1.9.1,,,,,,,,1.10.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,1,pull-request-available,,,,"Currently in flink-json, we don't treat map type as a special type, which results that the type of map is deduced from fastjson.

For example, when we set `map<varchar, int>` in SQL DDL，and get a value large than int_max, which will result in `Long` from fastjson, and will cause a runtime error.",,leonard,libenchao,mserranom,Terry1897,wind_ljy,,,,,,,,,"libenchao commented on pull request #10060: [FLINK-14546] [flink-json] Support map type in flink-json
URL: https://github.com/apache/flink/pull/10060
 
 
   ## What is the purpose of the change
   
   This pr fix flink-json map type resolution bug.
   
   
   ## Brief change log
   
   Add map type converter in flink-json.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no )
     - The runtime per-record code paths (performance sensitive): ( no )
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Oct/19 10:11;githubbot;600","KurtYoung commented on pull request #10060: [FLINK-14546] [flink-json] Support map type in flink-json
URL: https://github.com/apache/flink/pull/10060
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/19 00:49;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 06 00:49:57 UTC 2019,,,,,,,,,,"0|z0808w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/19 00:49;ykt836;merged in 1.10.0: 2ea14169a1997434d45d6f1da6dfe9acd6bd8da3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resuming Externalized Checkpoint (file, async, scale down) end-to-end test fails on travis",FLINK-14545,13264777,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,liyu,liyu,28/Oct/19 09:38,30/Oct/19 18:22,13/Jul/23 08:10,30/Oct/19 18:22,1.10.0,,,,,,,,,,,,,Runtime / Checkpointing,Tests,,,,0,test-stability,,,,"The job keeps running until exceeded the maximum log length, and from the log message there're no special error but only the below warning:
{code}
2019-10-27 19:00:22,345 WARN  org.apache.flink.client.cli.CliFrontend                       - Could not load CLI class org.apache.flink.yarn.cli.FlinkYarnSessionCli.
java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/exceptions/YarnException
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.flink.client.cli.CliFrontend.loadCustomCommandLine(CliFrontend.java:1169)
	at org.apache.flink.client.cli.CliFrontend.loadCustomCommandLines(CliFrontend.java:1129)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1054)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.exceptions.YarnException
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 5 more
{code}

https://api.travis-ci.org/v3/job/603503569/log.txt",,gjy,klion26,liyu,trohrmann,wind_ljy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 30 18:22:10 UTC 2019,,,,,,,,,,"0|z08088:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/19 09:57;trohrmann;The logging statement just says that there is no Hadoop binding on the classpath. If the job does not need to interact with Yarn, then this is not the root cause for the failing test case.;;;","28/Oct/19 10:54;gjy;The exception should be even excluded: https://github.com/apache/flink/blob/96640cad3d770756cb6e70c73b25bd4269065775/flink-end-to-end-tests/test-scripts/common.sh#L327
It's not clear what caused the test failure. There are no other errors.;;;","28/Oct/19 10:56;gjy;Because the output is truncated, there is probably an error that was not printed.;;;","28/Oct/19 10:57;gjy;Might be related to FLINK-14544 after all.;;;","30/Oct/19 18:22;gjy;Closing this because there is currently no evidence that it is a different issue than  FLINK-14544.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resuming Externalized Checkpoint after terminal failure (file, async) end-to-end test fails on travis",FLINK-14544,13264772,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,liyu,liyu,28/Oct/19 09:30,22/Jun/21 14:05,13/Jul/23 08:10,31/Oct/19 15:10,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Checkpointing,Tests,,,,0,pull-request-available,test-stability,,,"From the log we could see below error message and then the job was terminated due to job exceeded the maximum log length. 
{code}
2019-10-25 20:46:09,361 ERROR org.apache.flink.runtime.taskmanager.Task                     - Error while canceling task FailureMapper (1/1).
java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.MailboxStateException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorImpl.executeFirst(MailboxExecutorImpl.java:75)
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.sendPriorityLetter(MailboxProcessor.java:176)
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.allActionsCompleted(MailboxProcessor.java:172)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cancel(StreamTask.java:540)
	at org.apache.flink.runtime.taskmanager.Task.cancelInvokable(Task.java:1191)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:764)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:521)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.streaming.runtime.tasks.mailbox.MailboxStateException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkPutStateConditions(TaskMailboxImpl.java:199)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.putHeadInternal(TaskMailboxImpl.java:141)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.putFirst(TaskMailboxImpl.java:131)
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorImpl.executeFirst(MailboxExecutorImpl.java:73)
	... 7 more
{code}

https://api.travis-ci.org/v3/job/602788586/log.txt",,arvid.heise@gmail.com,gjy,kkl0u,klion26,liyu,pnowojski,rmetzger,trohrmann,wangxiyuan,wind_ljy,,,,"AHeise commented on pull request #10049: [FLINK-14544][runtime] Fixing race condition during task cancellation
URL: https://github.com/apache/flink/pull/10049
 
 
   when closing mailbox / enqueuing poison letter.
   
   Added a convenience method to TaskMailbox to run code under the internal
   lock, such that MailboxProcessor can atomically check if mailbox is
   still open and enqueuing poison letter.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fixes race condition during task cancellation.
   
   ## Brief change log
   
   *(for example:)*
     - Adds convenience method in TaskMailbox to execute code under the internal lock.
    - MailboxProcessor uses that method to perform an atomic operation that checks for the mailbox state and enqueues poison letter when open.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as the failing e2e tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 13:52;githubbot;600","pnowojski commented on pull request #10049: [FLINK-14544][runtime] Fixing race condition during task cancellation
URL: https://github.com/apache/flink/pull/10049
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Oct/19 15:10;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-14548,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 15:10:42 UTC 2019,,,,,,,,,,"0|z08074:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/19 12:44;gjy;Different test ({{'Resuming Externalized Checkpoint (file, sync, no parallelism change) end-to-end test'}}) but same stacktrace

[https://api.travis-ci.org/v3/job/603503557/log.txt]
{noformat}
java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.MailboxStateException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorImpl.executeFirst(MailboxExecutorImpl.java:75)
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.sendPriorityLetter(MailboxProcessor.java:176)
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.allActionsCompleted(MailboxProcessor.java:172)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cancel(StreamTask.java:540)
	at org.apache.flink.runtime.taskmanager.Task.cancelInvokable(Task.java:1191)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:756)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:521)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.streaming.runtime.tasks.mailbox.MailboxStateException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkPutStateConditions(TaskMailboxImpl.java:199)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.putHeadInternal(TaskMailboxImpl.java:141)
	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.putFirst(TaskMailboxImpl.java:131)
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorImpl.executeFirst(MailboxExecutorImpl.java:73)
	... 7 more
{noformat};;;","28/Oct/19 12:56;kkl0u;Also this one https://issues.apache.org/jira/browse/FLINK-14548

 

I will close it as duplicate and post the link here.  [https://travis-ci.org/apache/flink/builds/603215648?utm_source=slack&utm_medium=notification];;;","28/Oct/19 16:14;klion26;From the given log,  If I understand right, this test failed because the log contains exception of {{MailboxStateException. we can filter out the ""}}{{MailboxStateException"" before counting the exception counts. and need to figure out why the MailboxStateException throws out.}}

 
{code:java}
Running externalized checkpoints test, with ORIGINAL_DOP=2 NEW_DOP=2 and STATE_BACKEND_TYPE=file STATE_BACKEND_FILE_ASYNC=true STATE_BACKEND_ROCKSDB_INC      REMENTAL=false SIMULATE_FAILURE=true ...^M
19106 Waiting for job (2d7c274d4561078c592df0bbb1dfad52) to reach terminal state FAILED ...^M
19107 Job (2d7c274d4561078c592df0bbb1dfad52) reached terminal state FAILED^M
19108 Restoring job with externalized checkpoint at /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-50753031208/extern      alized-chckpt-e2e-backend-dir/2d7c274d4561078c592df0bbb1dfad52/chk-2 ...^M
19109 Job (824b849f432dcffdeb0d18ab6b1f7d6c) is running.^M
19110 Checking for errors...^M
19111 Found error in log files:^M
......
25083 2019-10-25 20:46:09,361 ERROR org.apache.flink.runtime.taskmanager.Task                     - Error while canceling task FailureMapper (1/1).^M
25084 java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.MailboxStateException: Mailbox is in state CLOSED, but       is required to be in state OPEN for put operations.^M
25085         at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorImpl.executeFirst(MailboxExecutorImpl.java:75)^M
25086         at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.sendPriorityLetter(MailboxProcessor.java:176)^M
25087         at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.allActionsCompleted(MailboxProcessor.java:172)^M
25088         at org.apache.flink.streaming.runtime.tasks.StreamTask.cancel(StreamTask.java:540)^M
25089         at org.apache.flink.runtime.taskmanager.Task.cancelInvokable(Task.java:1191)^M
25090         at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:764)^M
25091         at org.apache.flink.runtime.taskmanager.Task.run(Task.java:521)^M
25092         at java.lang.Thread.run(Thread.java:748)^M
25093 Caused by: org.apache.flink.streaming.runtime.tasks.mailbox.MailboxStateException: Mailbox is in state CLOSED, but is required to be in state OPEN for p      ut operations.^M
25094         at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkPutStateConditions(TaskMailboxImpl.java:199)^M
25095         at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.putHeadInternal(TaskMailboxImpl.java:141)^M
25096         at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.putFirst(TaskMailboxImpl.java:131)^M
25097         at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorImpl.executeFirst(MailboxExecutorImpl.java:73)^M
25098         ... 7 more^M
{code};;;","29/Oct/19 03:34;klion26;from the log, found that the {{MailboxStateException}} throws out after {{Artificial failure in the second job.}}
{code:java}
2019-10-25 20:46:09,345 INFO  org.apache.flink.runtime.taskmanager.Task                     - FailureMapper (1/1) (466747dfea13738afd021da649dc53f4) switched from RUNNING to FAILED.
java.lang.Exception: Artificial failure.
        at org.apache.flink.streaming.tests.FailureMapper.map(FailureMapper.java:59)
        at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41)
        at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:280)
        at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:152)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:423)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:696)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:521)
        at java.lang.Thread.run(Thread.java:748)
2019-10-25 20:46:09,361 ERROR org.apache.flink.runtime.taskmanager.Task                     - Error while canceling task FailureMapper (1/1).
java.util.concurrent.RejectedExecutionException: org.apache.flink.streaming.runtime.tasks.mailbox.MailboxStateException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
        at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorImpl.executeFirst(MailboxExecutorImpl.java:75)
        at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.sendPriorityLetter(MailboxProcessor.java:176)
        at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.allActionsCompleted(MailboxProcessor.java:172)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.cancel(StreamTask.java:540)
        at org.apache.flink.runtime.taskmanager.Task.cancelInvokable(Task.java:1191)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:764)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:521)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.streaming.runtime.tasks.mailbox.MailboxStateException: Mailbox is in state CLOSED, but is required to be in state OPEN for put operations.
        at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkPutStateConditions(TaskMailboxImpl.java:199)
        at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.putHeadInternal(TaskMailboxImpl.java:141)
        at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.putFirst(TaskMailboxImpl.java:131)
        at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxExecutorImpl.executeFirst(MailboxExecutorImpl.java:73)
        ... 7 more
{code}
 

so from currently analysis, the test `Resuming Externalized Checkpoint after terminal failure (file, async) ` complete checkpoint in job 1, restore from checkpoint completed in job 1, and complete more checkpoint in job2 , but the log contains {{MailboxStateException}}, so we see the test failed.
 * the first job complete checkpoints 
 ** {{2019-10-25 20:46:08,614 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Completed checkpoint 2 for job 2d7c274d4561078c592df0bbb1dfad52 (156791 bytes in 367 ms).}}
 * trigger artifical exception
 * retore from the checkpoint completed by the previous job
 ** 2019-10-25 20:46:13,358 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Starting job 824b849f432dcffdeb0d18ab6b1f7d6c from savepoint [file:///home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-50753031208/externalized-chckpt-e2e-backend-dir/2d7c274d4561078c592df0bbb1dfad52/chk-2] ()
 2019-10-25 20:46:13,378 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Reset the checkpoint ID of job 824b849f432dcffdeb0d18ab6b1f7d6c to 3.
 2019-10-25 20:46:13,378 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Restoring job 824b849f432dcffdeb0d18ab6b1f7d6c from latest valid checkpoint: Checkpoint 2 @ 0 for 824b849f432dcffdeb0d18ab6b1f7d6c.
 *    complete more new checkpoints in new job
 ** 2019-10-25 20:46:15,608 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Completed checkpoint 3 for job 824b849f432dcffdeb0d18ab6b1f7d6c (160262 bytes in 1154 ms).
 2019-10-25 20:46:15,655 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Triggering checkpoint 4 @ 1572036375608 for job 824b849f432dcffdeb0d18ab6b1f7d6c.
 2019-10-25 20:46:15,831 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Completed checkpoint 4 for job 824b849f432dcffdeb0d18ab6b1f7d6c (226751 bytes in 221 ms).
 * trigger {{Artificial failure}} and fail the job(the log above)

As the analysis, I think we can change the test as
 * do not trigger artifical exception in the second job
 * ignore the {{MailboxStateException}} when finding exceptions in log, I think if there indeed an {{MailboxStateException}} need we attention, the test will be failed in previous stages before finding exception/error in logs.;;;","29/Oct/19 09:27;wangxiyuan;I hit the similar error on ARM arch as well. The first time it occured is 10/24

http://status.openlabtesting.org/builds?project=apache%2Fflink (see: flink-end-to-end-test-checkpoints-and-sticky)

https://logs.openlabtesting.org/logs/periodic-20/github.com/apache/flink/master/flink-end-to-end-test-checkpoints-and-sticky/98831e3/job-output.txt.gz;;;","29/Oct/19 09:30;gjy;Another instance https://api.travis-ci.org/v3/job/603882580/log.txt;;;","29/Oct/19 10:08;arvid;Can someone assign that ticket to me? I will take a look.;;;","30/Oct/19 09:31;arvid;Small update: Found the issue and will provide a fix later.;;;","31/Oct/19 15:10;pnowojski;merged to master as 8522f3b ;;;",,,,,,,,,,,,,,,,,,,,,,,
PushFilterIntoTableSourceScanRule misses predicate pushdowns,FLINK-14533,13264485,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yuval.Itzchakov,Yuval.Itzchakov,Yuval.Itzchakov,25/Oct/19 13:37,19/Feb/20 13:16,13/Jul/23 08:10,19/Feb/20 13:14,1.8.1,1.8.2,1.9.0,1.9.1,,,,,,1.11.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When Flink attempts to perform predicate pushdown via `PushFilterIntoTableSourceScanRule`, it first checks the RexNodes to see if they can actually be pushed down to the source. It does that via `RexNodeToExpressionConverter.visitCall`, which traverses the nodes and eventually checks to see if it's a condition it knows:
{code:scala}
call.getOperator match { 
  case SqlStdOperatorTable.OR => Option(operands.reduceLeft { (l, r) => Or(l.asInstanceOf[PlannerExpression], r.asInstanceOf[PlannerExpression]) }) 
  case SqlStdOperatorTable.AND => Option(operands.reduceLeft { (l, r) => And(l.asInstanceOf[PlannerExpression], r.asInstanceOf[PlannerExpression]) }) 
  case function: SqlFunction => lookupFunction(replace(function.getName), operands)  
  case postfix: SqlPostfixOperator => lookupFunction(replace(postfix.getName), operands) 
  case operator@_ => lookupFunction(replace(s""${operator.getKind}""), operands) 
}
{code}
If we take as an example the following query:
{code:sql}
SELECT a, b, c 
FROM d 
WHERE LOWER(a) LIKE '%%foo%%' AND LOWER(b) LIKE '%%python%%'
{code}
When we hit the above pattern match, we fall to the case matching `SqlFunction`, as `LOWER` is of that type. Inside `lookupFunction`, we have a call to `functionCatalog.lookupFunction(name)` which looks up the given function in the function catalog. Eventually, we reach a static class called `BuiltInFunctionDefinitions`, which defines all of Flink's built in functions. When we iterate the list of built in functions as follows:
{code:java}
foundDefinition = BuiltInFunctionDefinitions.getDefinitions()
				.stream()
				.filter(f -> functionName.equals(normalizeName(f.getName())))
				.findFirst()
				.map(Function.identity());
{code}
 This doesn't yield a result, because `LOWER`, inside `BuiltInFunctionDefinitions`, is defined as follows:


{code:java}
public static final BuiltInFunctionDefinition LOWER =
		new BuiltInFunctionDefinition.Builder()
			.name(""lowerCase"")
			.kind(SCALAR)
			.outputTypeStrategy(TypeStrategies.MISSING)
			.build();
{code}
 
And since we're using String to do the lookups, this fails to match, hence returning `null` and causing the entire pushdown to fail.

 ",,godfreyhe,jark,rmetzger,Yuval.Itzchakov,,,,,,,,,,"YuvalItzchakov commented on pull request #10004: [FLINK-14533][flink-table-planner] Fix LOWER/UPPER not being pushed down to TableSource
URL: https://github.com/apache/flink/pull/10004
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix issue 14533 where using `LOWER/UPPER` in predicate would cause predicate pushdown to fail
   
   ## Brief change log
   
   Change `BuiltInFunctionDefinition` to match the `SqlFunction` string.
   
   ## Verifying this change
   
   Added tests
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/19 13:19;githubbot;600","YuvalItzchakov commented on pull request #10005: [FLINK-14533][flink-table-planner] Fix LOWER/UPPER not being pushed down to TableSource
URL: https://github.com/apache/flink/pull/10005
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix issue [FLINK-14533](https://issues.apache.org/jira/browse/FLINK-14533) where using `LOWER/UPPER` in predicate would cause predicate pushdown to fail
   
   ## Brief change log
   
   Change `BuiltInFunctionDefinition` to match the `SqlFunction` string.
   
   ## Verifying this change
   
   Added tests
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/19 14:05;githubbot;600","YuvalItzchakov commented on pull request #10004: [FLINK-14533][flink-table-planner] Fix LOWER/UPPER not being pushed down to TableSource
URL: https://github.com/apache/flink/pull/10004
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/19 14:05;githubbot;600","wuchong commented on pull request #10005: [FLINK-14533][flink-table-planner] Fix LOWER/UPPER not being pushed down to TableSource
URL: https://github.com/apache/flink/pull/10005
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/20 12:51;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 19 13:15:43 UTC 2020,,,,,,,,,,"0|z07yfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/20 13:14;jark;Fixed in master(1.11.0): 296a8af2250022b03c391bab9e8d17c98180aedd;;;","19/Feb/20 13:15;jark;Hi [~Yuval.Itzchakov], I didn't check-pick it back to 1.10 and 1.9, because it may change the plan. The community gives the guarantee that the SQL job state should be reused across minor releases. Hope it makes sense to you. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PostgreSQL JDBC sink generates invalid SQL in upsert mode,FLINK-14524,13264391,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fawad,fawad,fawad,25/Oct/19 01:39,25/Oct/19 06:45,13/Jul/23 08:10,25/Oct/19 06:45,1.10.0,1.9.1,,,,,,,,1.10.0,1.9.2,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"The ""upsert"" query generated for the PostgreSQL dialect is missing a closing parenthesis in the ON CONFLICT clause, causing the INSERT statement to error out with the error

 

{{ERROR o.a.f.s.runtime.tasks.StreamTask - Error during disposal of stream operator.}}
{{java.lang.RuntimeException: Writing records to JDBC failed.}}
{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.checkFlushException(JDBCUpsertOutputFormat.java:135)}}
{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.close(JDBCUpsertOutputFormat.java:184)}}
{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertSinkFunction.close(JDBCUpsertSinkFunction.java:61)}}
{{ at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43)}}
{{ at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)}}
{{ at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:585)}}
{{ at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:484)}}
{{ at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)}}
{{ at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)}}
{{ at java.lang.Thread.run(Thread.java:748)}}
{{Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO ""public.temperature""(""id"", ""timestamp"", ""temperature"") VALUES ('sensor_17', '2019-10-25 00:39:10-05', 20.27573964210997) ON CONFLICT (""id"", ""timestamp"" DO UPDATE SET ""id""=EXCLUDED.""id"", ""timestamp""=EXCLUDED.""timestamp"", ""temperature""=EXCLUDED.""temperature"" was aborted: ERROR: syntax error at or near ""DO""}}
{{ Position: 119 Call getNextException to see other errors in the batch.}}
{{ at org.postgresql.jdbc.BatchResultHandler.handleCompletion(BatchResultHandler.java:163)}}
{{ at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:838)}}
{{ at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)}}
{{ at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter$UpsertWriterUsingUpsertStatement.internalExecuteBatch(UpsertWriter.java:177)}}
{{ at org.apache.flink.api.java.io.jdbc.writer.UpsertWriter.executeBatch(UpsertWriter.java:117)}}
{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.flush(JDBCUpsertOutputFormat.java:159)}}
{{ at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.lambda$open$0(JDBCUpsertOutputFormat.java:124)}}
{{ at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)}}
{{ at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)}}
{{ at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)}}
{{ at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)}}
{{ at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}
{{ at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}
{{ ... 1 common frames omitted}}
{{Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""DO""}}
{{ Position: 119}}
{{ at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)}}
{{ at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)}}
{{ at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)}}
{{ at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:834)}}
{{ ... 12 common frames omitted}}",,fawad,jark,,,,,,,,,,,,"fhalim commented on pull request #9990: [FLINK-14524] [flink-jdbc] Correct syntax for PostgreSQL dialect ""upsert"" statement
URL: https://github.com/apache/flink/pull/9990
 
 
   ## What is the purpose of the change
   
   * This pull request adds a missing closing parenthesis in the INSERT statement generated for PostgreSQL in upsert mode.
   
   ## Brief change log
   - *Added missing parenthesis in generated SQL statement*
   
   ## Verifying this change
   
   The effect of this change was verified against a local postgresql database where the sink was erroring out without the fix.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Oct/19 01:43;githubbot;600","wuchong commented on pull request #9990: [FLINK-14524] [flink-jdbc] Correct syntax for PostgreSQL dialect ""upsert"" statement
URL: https://github.com/apache/flink/pull/9990
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Oct/19 06:40;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 25 06:45:51 UTC 2019,,,,,,,,,,"0|z07xug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/19 06:45;jark;1.10.0: 1cefbb8b5d0ba0e687635441c04d1790e33ef76c
1.9.2: 7086a8958a40804a604171483d2f374de236cfdf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Checking YARN queues should add ""root"" prefix",FLINK-14511,13264062,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Dillon.,Dillon.,Dillon.,23/Oct/19 16:13,17/Jun/20 09:09,13/Jul/23 08:10,17/Jun/20 09:09,,,,,,,,,,1.12.0,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"As we all know, all queues in the YARN cluster are children of the ""root"" queue. While submitting an application to ""root.product"" queue with -qu product parameter, the client logs that ""The specified queue 'product' does not exist. Available queues...."". But this queue is exist and we can still submit application to YARN cluster, which is confusing for users. So I think that when checking queues should add ""root."" prefix to the queue name.


{code:java}
List<QueueInfo> queues = yarnClient.getAllQueues();
if (queues.size() > 0 && this.yarnQueue != null) { // check only if there are queues configured in yarn and for this session.
	boolean queueFound = false;
	for (QueueInfo queue : queues) {
		if (queue.getQueueName().equals(this.yarnQueue) {
			queueFound = true;
			break;
		}
	}
	if (!queueFound) {
		String queueNames = """";
		for (QueueInfo queue : queues) {
			queueNames += queue.getQueueName() + "", "";
		}
		LOG.warn(""The specified queue '"" + this.yarnQueue + ""' does not exist. "" +
				""Available queues: "" + queueNames);
	}

{code}
",,Dillon.,trohrmann,wangyang0918,,,,,,,,,,,"zzchun commented on pull request #9978: [FLINK-14511][Deployment / YARN] Checking YARN queues with  ""root."" prefix
URL: https://github.com/apache/flink/pull/9978
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   All the queues in the YARN cluster are start with ""root"", while submitting applications to a queue with the queue name not starting with ""root"". will logs the queue does not exist. But the queue is exist and can submit applications to the queue, which is confusing for users. So,  while checking YARN queues should add  ""root."" prefix. 
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Oct/19 16:30;githubbot;600","zzchun commented on pull request #9978: [FLINK-14511][Deployment / YARN] Checking YARN queues with  ""root."" prefix
URL: https://github.com/apache/flink/pull/9978
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Oct/19 12:35;githubbot;600","zzchun commented on pull request #9978: [FLINK-14511][Deployment / YARN] Checking YARN queues with  ""root."" prefix
URL: https://github.com/apache/flink/pull/9978
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   All the queues in the YARN cluster are start with ""root"", while submitting applications to a queue with the queue name not starting with ""root"". will logs the queue does not exist. But the queue is exist and can submit applications to the queue, which is confusing for users. So,  while checking YARN queues should add  ""root."" prefix. 
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 08:07;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 09:09:48 UTC 2020,,,,,,,,,,"0|z07vtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/19 02:19;wangyang0918;Hi [~Dillon.], thanks for this issue.

I think we need more discussion here. As i know, there are two schedulers in Yarn. And they have difference policies about queue name.

For fair scheduler, the queue is an absolute path like 'root.a.x'. And the leaf node could be same. That means we could have two queues 'root.a.x' and 'root.b.x'. So when we submit a flink application to fair scheduler, `-yqu` need to be an absolute path.

For capacity scheduler, the leaf node is unique. And we could only use leaf node name to submit application.

 

If you are using capacity scheduler, i think you should specify the `-yqu` to 'product' not 'root.product'. And use 'root.product' for fair scheduler.;;;","24/Oct/19 13:07;Dillon.;Hi [~fly_in_gis], thanks for your reply.

You are right. For capacity scheduler, we could only use a unique leaf queue to submit applications, such as: 'product'. For Fair scheduler, we could not only use a full queue name 'root.product', but also 'product' to submit applications, as Fair scheduler will add 'root' prefix while scheduling appliations.

The problem is that, if we submit applications with queue name 'product' to a YARN cluster with Fair scheduler, YarnClusterDescriptor#checkYarnQueues method will give a warning information, informs that the 'product' queue does exist. But we still can submit applications to YARN successfully, which is confusing for users.

As we don't know what kind of scheduler users use, I think that we should check queues for both schedulers. YarnClusterDescriptor#checkYarnQueues works well for capacity scheduler, but not works fine for fair scheduler. If we have a queue named 'root.product' in YARN with fair scheduler, we can use `-yqu root.product` or `-yqu product` to submit applications to this queue. So, I think we should add a ’root‘ prefix before checking queues for Fair scheduler.;;;","25/Oct/19 02:27;wangyang0918;So we are trying to get the conclusion.

For capacity scheduler, `YarnClusterDescriptor#checkYarnQueues` works well.

For fair scheduler,  if we use absolute queue path `-yqu root.a.product`, it also works well. The only case is using short queue name `-yqu product`, the warning log will show. I suggest to use absolute queue path in fair scheduler, because there is no guarantee that short queue name is unique and flink application could be submitted successfully.

No matter what is the scheduler, we could submit a flink application successfully when the queue does not exist. It is based on the policy of yarn scheduler. So i think the warning log is reasonable if ""Available queues: xxx, yyy"" could be found at the same time. [~Dillon.] could you please help to check the warning log in fair scheduler?

 

[~trohrmann] How do you think?;;;","25/Oct/19 04:07;Dillon.;Hi [~fly_in_gis]. For fair scheduler, we can use `-yqu root.a.product` and `-yqu a.product` to submit applications to queue `root.a.product`.

We cannot use `-yqu product` to submit applications to queue `root.a.product`, as queue `product` does not exist, and got an exception like this:

{code:java}
2019-10-25 11:36:02,559 WARN org.apache.flink.yarn.YarnClusterDescriptor - The specified queue 'product' does not exist. Available queues: root.a, root.a.product, root.default, 
2019-10-25 11:36:03,065 WARN org.apache.flink.yarn.YarnClusterDescriptor - The configuration directory ('/home/yarn/flink/conf') contains both LOG4J and Logback configuration files. Please delete or rename one of them.
2019-10-25 11:36:04,938 INFO org.apache.flink.yarn.YarnClusterDescriptor - Submitting application master application_1571906019943_0102
Error while deploying YARN cluster: Couldn't deploy Yarn session cluster
java.lang.RuntimeException: Couldn't deploy Yarn session cluster
at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploySessionCluster(AbstractYarnClusterDescriptor.java:389)
at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:839)
at org.apache.flink.yarn.cli.FlinkYarnSessionCli$1.call(FlinkYarnSessionCli.java:627)
at org.apache.flink.yarn.cli.FlinkYarnSessionCli$1.call(FlinkYarnSessionCli.java:624)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1867)
at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:624)
Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1571906019943_0102 to YARN : Application rejected by queue placement policy, queue product does not exist.
at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:274)
at org.apache.flink.yarn.AbstractYarnClusterDescriptor.startAppMaster(AbstractYarnClusterDescriptor.java:1013)
at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deployJm(AbstractYarnClusterDescriptor.java:452)
at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deployInternal(AbstractYarnClusterDescriptor.java:418)
at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploySessionCluster(AbstractYarnClusterDescriptor.java:384)
... 8 more
{code}


If we have a `root.a.product` queue in YARN cluster with FairScheduler，we can use `-yqu root.a.product` and `-yqu a.product` to submit applications to the cluster successfully, and cannot use `product` to submit applications as it not exist.

But if we use `-yqu a.product` to submit applications will get an warning as follows, because the queues gettting from `yarnClient.getAllQueues()` method are have a `root` prefix for FairScheduer like `root.a, root.a.product`, and leaf queue name for CapacityScheduler like `a, product`.

So, I think we should check `root` prefix before checking queues.

{code:java}
2019-10-25 12:04:20,711 WARN org.apache.flink.yarn.YarnClusterDescriptor - The specified queue 'a.product' does not exist. Available queues: root.a, root.a.product, root.default, ...
{code}
;;;","17/Jun/20 09:09;trohrmann;Fixed via 159582a6b51ce3c95cc0f6bdf01eef97b558a99b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Python module build hangs,FLINK-14459,13263320,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hequn8128,hequn8128,hequn8128,19/Oct/19 10:26,19/Oct/19 12:23,13/Jul/23 08:10,19/Oct/19 12:23,1.9.0,1.9.1,,,,,,,,1.10.0,1.9.2,,,API / Python,,,,,0,pull-request-available,,,,"The build of python module hangs when installing conda. See travis log: https://api.travis-ci.org/v3/job/599704570/log.txt

Can't reproduce it neither on my local mac nor on my repo with travis. ",,hequn8128,,,,,,,,,,,,,"hequn8128 commented on pull request #9941: [FLINK-14459][python] Fix python module build hang problem
URL: https://github.com/apache/flink/pull/9941
 
 
   
   ## What is the purpose of the change
   
   This pull request is a hotfix for the build failure for the master and release-1.9.
   The problem is caused by the latest conda installer: https://github.com/conda/conda/issues/9345.
   In this PR, we specify a stable version instead of the latest one.
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Oct/19 10:42;githubbot;600","hequn8128 commented on pull request #9941: [FLINK-14459][python] Fix python module build hang problem
URL: https://github.com/apache/flink/pull/9941
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Oct/19 12:20;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 19 12:22:26 UTC 2019,,,,,,,,,,"0|z07r8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/19 10:27;hequn8128;It seems a problem with the latest conda installer. https://github.com/conda/conda/issues/9345;;;","19/Oct/19 12:22;hequn8128;Fix in
1.10.0: 2b1187d299bc6fd8dcae0d4e565238d7800dd4bb
1.9.2: 4474748c31a0d71e23147409ad338d7f5b37d5e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Usages of SavepointSerializers.setFailWhenLegacyStateDetected are never reverted,FLINK-14454,13263090,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,chesnay,chesnay,18/Oct/19 11:56,13/Apr/21 20:41,13/Jul/23 08:10,24/Mar/20 16:35,1.8.0,,,,,,,,,,,,,Runtime / State Backends,Tests,,,,0,pull-request-available,,,,"The {{AbstractOperatorRestoreTestBase}} and {{SavepointMigrationTestBase}} use {{SavepointSerializers#setFailWhenLegacyStateDetected}} to disable the logic that fails reading savepoints when they contain legacy state.

This logic is not re-enabled after the test.",,klion26,wind_ljy,,,,,,,,,,,,"buptljy commented on pull request #9944: [FLINK-14454] [test-stability] Usages of SavepointSerializers.setFailWhenLegacyStateDetected are never reverted
URL: https://github.com/apache/flink/pull/9944
 
 
   ## What is the purpose of the change
   
   Re-enable FAIL_WHEN_LEGACY_STATE_DETECTED after all state migration tests.
   
   ## Brief change log
   
   Adding the revert logic in a @AfterClass method.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Oct/19 10:38;githubbot;600","asfgit commented on pull request #9944: [FLINK-14454] [test-stability] Usages of SavepointSerializers.setFailWhenLegacyStateDetected are never reverted
URL: https://github.com/apache/flink/pull/9944
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Dec/19 01:29;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 16:35:21 UTC 2020,,,,,,,,,,"0|z07qag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/19 10:33;wind_ljy;[~chesnay] Can you assign this to me?  Re-enable the FAIL_WHEN_LEGACY_STATE_DETECTED in a @AfterClass method should fix this. ;;;","24/Mar/20 16:35;wind_ljy;Merged in e37f670c39387ad3d2538b80dbd9491b0be435b3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Network metrics doc table render confusion,FLINK-14447,13263062,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yanghua,yanghua,yanghua,18/Oct/19 10:21,16/Jan/20 16:17,13/Jul/23 08:10,22/Oct/19 12:36,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.2,,,Documentation,Runtime / Metrics,,,,0,pull-request-available,,,,More detail: https://ci.apache.org/projects/flink/flink-docs-master/monitoring/metrics.html#network-deprecated-use-default-shuffle-service-metrics,,gjy,yanghua,zjwang,,,,,,,,,,,"yanghua commented on pull request #9951: [FLINK-14447] Network metrics doc table render confusion
URL: https://github.com/apache/flink/pull/9951
 
 
   ## What is the purpose of the change
   
   *This pull request fixed Network metrics doc table render confusion*
   
   ## Brief change log
   
   *(for example:)*
     - *Network metrics doc table render confusion*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Oct/19 08:24;githubbot;600","azagrebin commented on pull request #9951: [FLINK-14447] Network metrics doc table render confusion
URL: https://github.com/apache/flink/pull/9951
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Oct/19 12:23;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 16 16:17:56 UTC 2020,,,,,,,,,,"0|z07q48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/19 10:22;yanghua;Will open a PR;;;","16/Jan/20 16:17;gjy;1.9: 644810014ecf7487137c06c4a4583bede855ac94
master: 56126bd43f6539dea39419e8f81779419ccea1c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python module build failed when making sdist,FLINK-14445,13262994,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,yunta,yunta,18/Oct/19 03:33,18/Oct/19 12:53,13/Jul/23 08:10,18/Oct/19 11:36,,,,,,,,,,1.10.0,1.9.2,,,,,,,,0,pull-request-available,,,,"From the description of error-log from building python module in travis, it seems invocation failed for {{sdist-make}} and then the phase of building python module exited.

The instance log: https://api.travis-ci.com/v3/job/246710918/log.txt",,dian.fu,hequn8128,jark,yunta,zhongwei,,,,,,,,,"WeiZhong94 commented on pull request #9932: [FLINK-14445][python] fix python module build failed when making sdist.
URL: https://github.com/apache/flink/pull/9932
 
 
   ## What is the purpose of the change
   
   *This pull request fixes the issue that python module build failed when making sdist.*
   
   
   ## Brief change log
   
     - *Ignore the licenses folder and NOTICE file if not exist when packaging source distribution of flink-python.*
   
   
   ## Verifying this change
   
   This change is already covered by existing tests. tox will create the source distribution of flink-python before tests begin.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Oct/19 07:19;githubbot;600","hequn8128 commented on pull request #9932: [FLINK-14445][python] fix python module build failed when making sdist.
URL: https://github.com/apache/flink/pull/9932
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Oct/19 09:57;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-14008,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 18 12:49:18 UTC 2019,,,,,,,,,,"0|z07pp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/19 03:44;yunta;CC [~dian.fu];;;","18/Oct/19 04:27;dian.fu;[~yunta]  Thanks for reporting this issue. It seems that it is caused by the latest license change in FLINK-14008.  

[~zhongwei] Could you help to take a look at this issue?;;;","18/Oct/19 07:04;zhongwei;[~yunta] [~dian.fu] The ""licenses"" directory in build-target is included when packaging flink-python source distribution. It is removed in FLINK-14008. For that reason the travis failed when packaging flink-python. I'll fix this issue as soon as possible.;;;","18/Oct/19 10:02;hequn8128;The pr has been merged into the master now. I will pick it into release-1.9 once Travis passed.;;;","18/Oct/19 11:36;hequn8128;fixed in 
1.10.0 via 6fbdab71f782be9e3d7b1bedc51402d46bab1e62
1.9.2 via ab48243e10a83c57d1c3c39c3ef08a25a26cc429;;;","18/Oct/19 12:49;jark;Hi [~hequn8128], the commit is fixed in 1.9.2, could you update the comment message above? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ValueLiteralExpression#getValueAs when valueClass is Period.class,FLINK-14441,13262888,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hailong wang,hailong wang,hailong wang,17/Oct/19 15:16,21/Oct/19 08:38,13/Jul/23 08:10,21/Oct/19 08:38,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"In ValueLiteralExpression#getValueAs, when valueClass is Period.class, the expect class is inconsistent with the result class",,hailong wang,jark,,,,,,,,,,,,"wangxlong commented on pull request #9931: [FLINK-14441][Table-Common] Fix ValueLiteralExpression#getValueAs when valueClass is Period.class
URL: https://github.com/apache/flink/pull/9931
 
 
   ## What is the purpose of the change
   
   In ValueLiteralExpression#getValueAs, when valueClass is Period.class, the expect class is inconsistent with the result class. This PR will fix it.
   
   ## Brief change log
   change clazz from Integer to Long when valueClass is Period.class
   
   ## Verifying this change
   This change added tests and can be verified as follows:
   (1)ExpressionTest#testPeriodlValueLiteralExtraction
   
   ## Does this pull request potentially affect one of the following parts:
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Oct/19 15:30;githubbot;600","wuchong commented on pull request #9931: [FLINK-14441][Table-Common] Fix ValueLiteralExpression#getValueAs when valueClass is Period.class
URL: https://github.com/apache/flink/pull/9931
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Oct/19 08:38;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 21 08:38:26 UTC 2019,,,,,,,,,,"0|z07p1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/19 02:02;jark;Hi [~hailong wang], could you give an example where the expected class and result class are inconsistent. 

After going through the pull request, I don't think this is a bug. The Period literal is a value of {{YearMonthIntervalType}} which represents the number of months. So the corresponding primitive type of Period is {{Integer}}, you should use {{getValueAs(Integer)}} or {{getValueAs(Period)}} if the valueClass is Period. ;;;","18/Oct/19 10:35;hailong wang;Hi [~jark], i am sorry for reply late. Yes, you are right. But i found 
{code:java}
period.toTotalMonths()
{code}
return a long type. So when using {{getValueAs(Integer), the return type is Long, but the given class is Integer.}};;;","18/Oct/19 13:50;jark;You are right. The return value should be casted to Integer. ;;;","18/Oct/19 14:19;hailong wang;Hi [~jark], The given class changes from Integer to Long or the return value should be casted to Integer, which maybe better. I think the first one. what do you think of it?;;;","21/Oct/19 02:15;jark;Hi [~hailong wang], the acceptable physical classes of YearMonthIntervalType are {{Period}} and {{Integer}}. Using {{Long}} is invalid here. So we should cast the return value to Integer. ;;;","21/Oct/19 03:31;hailong wang;Hi [~jark], Thanks you for your tip. I have fix it by casting the return value to Integer. But travis failed in python module which has be fixed. What should I do. ;;;","21/Oct/19 06:12;jark;Thanks [~hailong wang], I will reply you in the PR.;;;","21/Oct/19 08:38;jark;1.10.0: d4615e06eeec28742e4af4afa2234acb7894a885;;;",,,,,,,,,,,,,,,,,,,,,,,,
Dispatcher#createJobManagerRunner should not start JobManagerRunner,FLINK-14434,13262821,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,17/Oct/19 10:11,29/Nov/19 15:19,13/Jul/23 08:10,21/Oct/19 21:23,1.10.0,1.8.2,1.9.1,,,,,,,1.10.0,1.8.3,1.9.2,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In an edge case, let's said

1) job finished nearly immediately
2) Dispatcher has been suspended in {{#startJobManagerRunner}} after {{jobManagerRunner.start();}} but before {{return jobManagerRunner;}}

due to

1) we put {{jobManagerRunnerFutures}} with {{#startJobManagerRunner}} finished.
2) the creation of JobManagerRunner doesn't happen in MainThread.

it is a possible execution order

1) JobManagerRunner created in akka-dispatcher thread
2) then apply {{Dispatcher#startJobManagerRunner}}
3) until {{jobManagerRunner.start();}} and before {{return jobManagerRunner;}}
4) this thread suspended
5) job finished, execute callback on MainThread
6) {{jobManagerRunnerFutures.get(jobID).getNow(null)}} returns {{null}} because akka-dispatcher thread doesn't {{return jobManagerRunner;}}
7) it report {{There is a newer JobManagerRunner for the job}} but actually not.

**Solution**

Two perspective but we can even have them both.

1. return {{jobManagerRunnerFuture}} in {{#createJobManagerRunner}}, let {{#startJobManagerRunner}} an action
2. on JobManagerRunner created, execute {{#startJobManagerRunner}} in MainThread.

CC [~trohrmann]",,tison,trohrmann,wind_ljy,,,,,,,,,,,"TisonKun commented on pull request #9940: [FLINK-14434][coordination] Dispatcher#createJobManagerRunner returns on creation succeed
URL: https://github.com/apache/flink/pull/9940
 
 
   ## What is the purpose of the change
   
   Fixes a race condition described detailedly in [FLINK-14434](https://issues.apache.org/jira/browse/FLINK-14434#).
   
   ## Brief change log
   
   Let `Dispatcher#createJobManagerRunner` returns on creation succeed.
   
   
   ## Verifying this change
   
   I think this change is straightforward and current behavior doesn't break guarded by existing tests. The diff attached on corresponding JIRA can reproduce the issue but I'm afraid it is always invasive to emulate this case(add a debug latch between `jobManagerRunner.start` and `return` for testing hook in my mind). Due to the change is straightforward and easy to reason, I tend to avoid invasive testing hook.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Oct/19 03:23;githubbot;600","tillrohrmann commented on pull request #9940: [FLINK-14434][coordination] Dispatcher#createJobManagerRunner returns on creation succeed
URL: https://github.com/apache/flink/pull/9940
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Oct/19 21:20;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/19 10:35;tison;patch.diff;https://issues.apache.org/jira/secure/attachment/12983266/patch.diff",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 24 08:01:11 UTC 2019,,,,,,,,,,"0|z07omo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/19 10:35;tison;With the patch attached it can reproduce this case. You will see the new test hang while apply solutions mentioned above it passed.;;;","18/Oct/19 06:45;tison;Now I would prefer 2 only because with 1 we possibly miss exception in JobManagerRunner#start so that user receive submission success but request job result with job not found because start failed and the job manager runner future removed without a result.;;;","18/Oct/19 16:31;trohrmann;Thanks for reporting this issue [~tison]. I think you are right with your analysis. I would also prefer option two atm. 

In fact, I actually want to rework this part and the {{JobManagerRunner}} a bit with FLINK-11719. I hope to get rid of the concurrency introduced by the asynchronous creation of the {{JobManagerRunner}} at the cost of a slightly changed submission behaviour.;;;","19/Oct/19 03:13;tison;Thanks for your reply [~trohrmann]. I final a possibly better option3 for this issue. The diff is tiny and expressive.

https://github.com/TisonKun/flink/commit/baacecb92f11cd367dc89bc48744fea6de94670b

In short, the problem described above is mainly caused by when ""job manager runner result future"" called back, ""job manager runner created future"" conceptually finished but not completed. Revisit the semantic of {{#createJobManagerRunner}} we are able to just return a future represent the creation and let the caller take care of the start.

Compared with option 2, this approach has a clear semantic and a subtle difference that it execute {{JobManagerRunner}} in akka-dispatcher thread, not in MainThread. We internally have some issue if starting jm runner happens in dispatcher MainThread[1] but it doesn't exist in community codebase.

I'm glad to help with FLINK-11843 and FLINK-11719 on the review side. For this issue I'd like to send this tiny patch as a pull request  so that you can coordinate patches depending on your schedule.

[1] FYI: It is an interesting case but bias a bit from community codebase. We move the access to job registry totally to Dispatcher so that when job manager runner granted leadership it send a RPC to Dispatcher for querying what job scheduling status now. Our fork is currently based on 1.7 so that there is a dead-lock execution order with solution option 2 above.

1. job manager runner called {{#start}} in Dispatcher MainThread
2. job manager runner leader election service started, and if it is standalone(non-ha), it directly calls grantLeadership
3. job manager runner on granted leadership, send a RPC to Dispatcher for querying and wait for the result.
4. because {{#start}} occupied the MainThread, the later RPC cannot be processed.

We can workaround this case in many ways such as dispatch action a bit, but it might infer that if we can schedule an action out of Dispatcher MainThread without worry about synchronization provided by single-thread, we'd better to do it.;;;","21/Oct/19 13:03;trohrmann;Thanks for the additional information [~tison]. I always prefer better solutions :-) Will take a look at your PR.;;;","21/Oct/19 21:23;trohrmann;Fixed via

1.10.0: e1f5eade68b54f4771fe7ec50c5812c567e97174
1.9.2: 43ae34172fd6680861f7c5c54a498e3c5da4b8a2
1.8.3: 9add2b141e5142627ef397a3ab4be425ce389c8f;;;","24/Oct/19 08:01;tison;FYI I just notice that {{thenApply}} (and so on which doesn't have a {{Async}} suffix) possibly runs in the current thread(instead of the thread executing the previous computation) if the previous computation is completed on {{thenApply}} called. Thus the analysis above is a bit wrong because we don't ensure the starting scheduled to akka-dispatcher but possibly synchronously with {{thenApply}} which means in the MainThread.

However, this fix isn't affected by this discovery. Just for further information if this difference becomes significant.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ValueLiteralExpression#equals should take array value into account,FLINK-14415,13262640,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,16/Oct/19 14:19,28/Oct/19 06:35,13/Jul/23 08:10,28/Oct/19 06:35,,,,,,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"{{ValueLiteralExpression#equals}} uses {{Objects.equals}} to check the equality between two value object. Howeveer, the value object might be array object, using {{Object.equals}} will lead to wrong result. ",,jark,wind_ljy,,,,,,,,,,,,"wuchong commented on pull request #9915: [FLINK-14415][table-common] ValueLiteralExpression#equals should take array value into account
URL: https://github.com/apache/flink/pull/9915
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   `ValueLiteralExpression#equals` uses `Objects.equals` to check the equality between two value object. Howeveer, the value object might be an array object, using `Object.equals` will lead to wrong result.
   
   ## Brief change log
   
   - Use `Objects.deepEquals` instead of `Object.equals`
   
   ## Verifying this change
   
   
   This change added tests:
    - Adds array literal expression in `ExpressionTest`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/19 14:39;githubbot;600","wuchong commented on pull request #9915: [FLINK-14415][table-common] ValueLiteralExpression#equals should take array value into account
URL: https://github.com/apache/flink/pull/9915
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Oct/19 06:34;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 28 06:35:38 UTC 2019,,,,,,,,,,"0|z07nig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/19 06:35;jark;1.10.0: 27307b4f44d42a4d79a4ee52cb27f4f1581745c1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shade-plugin ApacheNoticeResourceTransformer uses platform-dependent encoding,FLINK-14413,13262626,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Oct/19 13:08,24/Jan/20 01:12,13/Jul/23 08:10,17/Oct/19 12:57,1.10.0,1.8.2,1.9.0,shaded-8.0,,,,,,1.10.0,1.8.3,1.9.2,shaded-9.0,Release System,,,,,0,pull-request-available,,,,"Some NOTICE files contain quotes that, at least on my system, result in some encoding errors when generating the binary licensing. One example can be found [here|https://github.com/apache/flink/blob/88b48619e2734505a6c2ba0d53168528bc0dc143/flink-filesystems/flink-fs-hadoop-shaded/src/main/resources/META-INF/NOTICE#L1867]; the closing quotes would be replaced with a question mark.

This is due to the ApacheNoticeResourceTransformer using the platform encoding.

",,,,,,,,,,,,,,,"zentol commented on pull request #9922: [FLINK-14413][release] Replace troublesome quotes
URL: https://github.com/apache/flink/pull/9922
 
 
   Replaces a few quotes that can cause encoding issues. The goal is to ensure that the generated NOTICE file for the binary release contains the unaltered notices.
   
   I wasn't able to pinpoint what exactly is causing these encoding issues on my system, so this is more of a last resort to unblock #9863
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Oct/19 10:00;githubbot;600","zentol commented on pull request #9922: [FLINK-14413][release] Replace troublesome quotes
URL: https://github.com/apache/flink/pull/9922
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Oct/19 11:00;githubbot;600","zentol commented on pull request #9925: [FLINK-14413][build] Specify encoding for ApacheNoticeResourceTransformer
URL: https://github.com/apache/flink/pull/9925
 
 
   Pins the encoding for the `ApacheNoticeResourceTransformer`.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Oct/19 11:07;githubbot;600","zentol commented on pull request #74: [FLINK-14413][build] Specify encoding for ApacheNoticeResourceTransformer
URL: https://github.com/apache/flink-shaded/pull/74
 
 
   Pins the encoding for the `ApacheNoticeResourceTransformer`. Without this the transformer reads/writes all files using a platform-dependent encoding, which makes some parts of the build non-deterministic.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Oct/19 11:09;githubbot;600","zentol commented on pull request #74: [FLINK-14413][build] Specify encoding for ApacheNoticeResourceTransformer
URL: https://github.com/apache/flink-shaded/pull/74
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Oct/19 12:41;githubbot;600","zentol commented on pull request #9925: [FLINK-14413][build] Specify encoding for ApacheNoticeResourceTransformer
URL: https://github.com/apache/flink/pull/9925
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Oct/19 12:56;githubbot;600",,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 17 12:57:45 UTC 2019,,,,,,,,,,"0|z07nfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/19 12:57;chesnay;master: 3f75754e2e506f7db54719ac2d605e365f1b5514
1.9: 3d6e0a04b1efc7a4ee30708eff9c9fbb1e9ac42d 
1.8: 1f18ab749971dde8745813dbc724f83d68228b30 

shaded-master: 83ac0c120d9a8b45f2810e926e52513fcaf21229;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapType doesn't accept any subclass of java.util.Map,FLINK-14409,13262596,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,16/Oct/19 10:01,16/Oct/19 15:45,13/Jul/23 08:10,16/Oct/19 15:45,,,,,,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Currently the conversion class of MapType is {{java.util.Map}}, but {{java.util.Map}} is an interface not a concrete class. So when verifying an instance of {{HashMap}} for MapType, it fails. 

For example:


{code:java}
		Map<String, Integer> map = new HashMap<>();
		map.put(""key1"", 1);
		map.put(""key2"", 2);
		map.put(""key3"", 3);
		assertEquals(
			""{key1=1, key2=2, key3=3}"",
			new ValueLiteralExpression(
				map,
				DataTypes.MAP(DataTypes.STRING(), DataTypes.INT()))
				.toString());
{code}

throws exception:

{code}
org.apache.flink.table.api.ValidationException: Data type 'MAP<STRING, INT>' does not support a conversion from class 'java.util.HashMap'.

	at org.apache.flink.table.expressions.ValueLiteralExpression.validateValueDataType(ValueLiteralExpression.java:236)
	at org.apache.flink.table.expressions.ValueLiteralExpression.<init>(ValueLiteralExpression.java:66)
{code}

It's easy to fix this by considering whether it's a subclass of Map. But I'm wondering what the default conversion class should be? ",,jark,ram_krish,twalthr,wind_ljy,,,,,,,,,,"wuchong commented on pull request #9913: [FLINK-14409][table-api] Fix MapType and MultisetType doesn't accept any subclass of java.util.Map for inputs
URL: https://github.com/apache/flink/pull/9913
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently the conversion class of MapType is java.util.Map, but java.util.Map is an interface not a concrete class. So when verifying an instance of HashMap for MapType or MultisetTYpe.
   
   
   ## Brief change log
   
   - Adds `isAssignableFrom(..)` check in `supportsInputConversion` for `MapType` and `MultisetType`.
   
   ## Verifying this change
   
   
   This change added tests:
    - Adds Map and Multiset value literal expression construction in `ExpressionTest` which will verify the instance.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/19 14:10;githubbot;600","asfgit commented on pull request #9913: [FLINK-14409][table-common] Fix MapType and MultisetType doesn't accept any subclass of java.util.Map for inputs
URL: https://github.com/apache/flink/pull/9913
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/19 15:45;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 16 15:45:39 UTC 2019,,,,,,,,,,"0|z07n8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/19 10:08;twalthr;Maybe we are too restrictive in {{org.apache.flink.table.types.logical.MapType#supportsInputConversion}} we could perform an {{isAssignable}} check instead. At least for the input conversion, I think the output conversion is still valid. What do you think?;;;","16/Oct/19 10:22;jark;Yes, you're right. The output conversion is valid for now. And I think the default conversion is also still valid. ;;;","16/Oct/19 15:45;twalthr;Fixed in 1.10.0: 8e81fc265d95f634401269b2acf9a4e80c0c1044;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnFileStageTestS3ITCase.testRecursiveUploadForYarnS3n failed on Travis,FLINK-14407,13262569,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,trohrmann,trohrmann,16/Oct/19 09:19,06/Dec/19 11:41,13/Jul/23 08:10,06/Dec/19 11:41,1.10.0,,,,,,,,,1.10.0,,,,Connectors / FileSystem,Deployment / YARN,,,,0,pull-request-available,test-stability,,,"The {{YarnFileStageTestS3ITCase.testRecursiveUploadForYarnS3n}} fails on Travis with 

{code}
01:41:19.215 [ERROR] testRecursiveUploadForYarnS3n(org.apache.flink.yarn.YarnFileStageTestS3ITCase)  Time elapsed: 6.891 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: <{1=Hello 1, 2=Hello 2, nested/4/5=Hello nested/4/5, test.jar=JAR Content, nested/3=Hello nested/3}>
     but: was <{1=Hello 1, 2=Hello 2, nested/4/5=Hello nested/4/5, test.jar=JAR Content}>
	at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarn(YarnFileStageTestS3ITCase.java:159)
	at org.apache.flink.yarn.YarnFileStageTestS3ITCase.testRecursiveUploadForYarnS3n(YarnFileStageTestS3ITCase.java:177)
{code}

https://api.travis-ci.org/v3/job/598412139/log.txt",,klion26,trohrmann,wind_ljy,,,,,,,,,,,"zentol commented on pull request #10445: [FLINK-14407][yarn] Retry S3 tests on failure
URL: https://github.com/apache/flink/pull/10445
 
 
   Adds a `RetryRule` to the `YarnFileStageTestS3ITCase` to alleviate issues when read-after-write operations aren't as reliable as we'd like them to be.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 14:45;githubbot;600","zentol commented on pull request #10445: [FLINK-14407][yarn] Retry S3 tests on failure
URL: https://github.com/apache/flink/pull/10445
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 11:41;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 11:41:19 UTC 2019,,,,,,,,,,"0|z07n2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/19 09:19;trohrmann;[~NicoK] could you help with this issue?;;;","05/Dec/19 14:40;chesnay;Don't see how we can fix this reliably by virtue of using S3; but I suppose we can add a retry to the tests that use s3.

However by interfacing with external systems it is bound to fail from time to time.;;;","06/Dec/19 11:41;chesnay;master: 7de2e4641dd437a04642b3c43dc9e62d8d21380a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Further split input unboxing code into separate methods,FLINK-14398,13262369,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,haodang,haodang,haodang,15/Oct/19 13:55,30/Oct/19 06:04,13/Jul/23 08:10,30/Oct/19 06:04,1.8.0,,,,,,,,,1.10.0,1.8.3,1.9.2,,Table SQL / Legacy Planner,Table SQL / Planner,,,,0,pull-request-available,,,,"In one of our production pipelines, we have a table with 1200+ columns.  At runtime, it failed due to a method inside the generated code exceeding 64kb when compiled to bytecode.

After we investigated the generated code, it appeared that the *map* method inside a generated *RichMapFunction* was too long. See attached file (codegen.example.txt).

In the problematic *map* method, *result setters* were correctly split into individual methods and did not have the largest footprint.

However, there were also 1000+ input unboxing expressions inside *reusableInputUnboxingExprs*, which, individually were not trivial and when flattened linearly in the *map* function [here|https://github.com/apache/flink/blob/release-1.8.0/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/codegen/FunctionCodeGenerator.scala#L187], pushed the method size beyond 64kb in bytecode.

We think it is worthwhile to split these input unboxing code snippets into individual methods.  We were able to verify, in our production environment, that splitting input unboxing code snippets into individual methods resolves the issue.  Would love to hear thoughts from the team and find the best path to fix it.",,haodang,jark,kisimple,twalthr,wind_ljy,,,,,,,,,"haodang commented on pull request #9980: [WIP][FLINK-14398][SQL/Legacy Planner]Further split input unboxing code into separate methods
URL: https://github.com/apache/flink/pull/9980
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   When a resultant table is wide, the large number of non-trivial input unboxing code can push the generated byte code beyond the 64kb limit for some methods.  This PR splits them into individual methods.
   
   ## Brief change log
   
   In CodeGen, when a split is needed, we create individual methods for each of the expressions inside `reusableInputUnboxingExprs`, which is in addition to the split of result setters.
   
   
   ## Verifying this change
   We verified in our own Flink environment.
   TODOs
   - [ ] To verify that this change is already covered by existing tests, or will add more.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (yes)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Oct/19 00:13;githubbot;600","haodang commented on pull request #10000: [FLINK-14398][SQL/Legacy Planner]Further split input unboxing code into separate methods
URL: https://github.com/apache/flink/pull/10000
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   When a resultant table is wide, the large number of non-trivial input unboxing code can push the generated byte code beyond the 64kb limit for some methods.  This PR splits them into individual methods.
   
   ## Brief change log
   
   In CodeGen, when a split is needed, we create individual methods for each of the expressions inside `reusableInputUnboxingExprs`, which is in addition to the split of result setters.
   
   
   ## Verifying this change
   The code path in this change is exercised at the same time when existing code split takes place and all existing unit tests that verify code splits should be able to verify this.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (yes)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/19 00:39;githubbot;600","haodang commented on pull request #9980: [FLINK-14398][SQL/Legacy Planner]Further split input unboxing code into separate methods
URL: https://github.com/apache/flink/pull/9980
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/19 00:41;githubbot;600","wuchong commented on pull request #10000: [FLINK-14398][SQL/Legacy Planner]Further split input unboxing code into separate methods
URL: https://github.com/apache/flink/pull/10000
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 03:14;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/19 13:42;haodang;codegen.example.txt;https://issues.apache.org/jira/secure/attachment/12983068/codegen.example.txt",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 30 06:04:49 UTC 2019,,,,,,,,,,"0|z07lu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/19 08:12;jark;Hi [~haodang], thanks for reporting this. I think you are using the old planner (or called flink planner) , right? 
Actually, old planner supports codegen splitting, did you try {{TableConfig#setMaxGeneratedCodeLength}} ?;;;","16/Oct/19 08:27;twalthr;[~jark] the code splitting is limited at the moment. It just splits by field but not within expressions or in this case for unboxing. I guess we need to introduce more logic here.;;;","16/Oct/19 08:32;haodang;Indeed, [~twalthr] and [~jark]. Thanks for the quick response.
 The code in the scenario above was already split based on the existing splitting logic. Yet the sheer number of code snippets produced by *reusableInputUnboxingExprs* took a large footprint in the end, and we probably should split them as well, i.e. putting them into individual methods.;;;","16/Oct/19 08:58;jark;Thanks. I agree with splitting can solve this problem. 
But from the experience of our internal implementation, the main problem is how to do this in a good way, cover all the paths and take fields/methods/class length into account. It should be a good framework/API for developers to make some codegen class support splitting without introducing too much duplicate codes.

So I think this deserves a design doc. And maybe we should aiming for blink planner first, because old planner will be deprecated in the future. 
;;;","16/Oct/19 09:02;jark;For fixing, I think it's fine to fix it first in old planner in a simple way. ;;;","16/Oct/19 09:51;haodang;Totally agree on a proper design doc for scoping and planning as this is a fundamental piece of the system.  We are happy to help in anyway possible to push the fix through, as we will be benefiting from it too.

For fixing 1.8.0, would it make sense for us to make a PR to start with?  We can go from there for further discussions.  I haven't dig into versions newer than 1.8.0, thus cannot say for sure if there will be more work needed for fixing versions post 1.8.0 on top of our existing fix.

Also, excuse me for my greenness here: _what's the difference between the blink planner vs. the old planner?_;;;","16/Oct/19 10:04;twalthr;[~jark] I agree, I think for the Blink planner we need a better code generation framework at some point. This is also the point when we can reimplement this code in Java.

As a short term solution, [~haodang] feel free to open a PR for further code splitting. Should I assign this issue to you? The Blink planner is similar to the Flink planner but has been further developed internally at Alibaba and was open sourced recently. Many code parts look similar to the old planner. The old planner is still the default planner for now.;;;","16/Oct/19 11:42;haodang;Thanks for the explanation!

And yea, feel free to assign it to me - happy open a PR for that.;;;","16/Oct/19 12:27;twalthr;I assigned it to you [~haodang]. It would be great to not introduce another config parameter and avoid splitting in most of the cases for performance reasons.;;;","16/Oct/19 12:46;haodang;Sounds good, [~twalthr].  I should be able to put up a PR in the next few days.;;;","30/Oct/19 06:04;jark;1.10.0: 4c87259c1b931318a917eba665a219b63788a735
1.9.2: 7fbada04fdafeb37bb6dc3e023ee502892e2e046
1.8.3: f6b855d9894bc783c760570053719b7a0c830ebb;;;",,,,,,,,,,,,,,,,,,,,,
Failed to run Hive UDTF with array arguments,FLINK-14397,13262351,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,15/Oct/19 12:35,28/Oct/19 23:38,13/Jul/23 08:10,28/Oct/19 23:38,1.10.0,1.9.0,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Tried to call {{org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2}} (in hive-contrib) with query:  ""{{select x,y from foo, lateral table(hiveudtf(arr)) as T(x,y)}}"". Failed with exception:
{noformat}
java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Ljava.lang.Integer;
{noformat}
",,jark,lirui,phoenixjiangnan,,,,,,,,,,,"lirui-apache commented on pull request #9927: [FLINK-14397][hive] Failed to run Hive UDTF with array arguments
URL: https://github.com/apache/flink/pull/9927
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix the issue that calling Hive UDTF with array arguments causes cast exception.
   
   
   ## Brief change log
   
     - When reading array type from Hive table, create arrays according to element type instead of Object[].
     - Add test case to cover it.
   
   
   ## Verifying this change
   
   New test case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): yes
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Oct/19 13:19;githubbot;600","asfgit commented on pull request #9927: [FLINK-14397][hive] Failed to run Hive UDTF with array arguments
URL: https://github.com/apache/flink/pull/9927
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Oct/19 23:38;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 28 23:38:48 UTC 2019,,,,,,,,,,"0|z07lq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/19 23:38;phoenixjiangnan;master: e18320b76047af4e15297e3e89b6c46ef3dae9bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect handling of FLINK_PLUGINS_DIR on Yarn,FLINK-14382,13261948,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,wangyang0918,wangyang0918,12/Oct/19 12:35,27/Nov/19 08:35,13/Jul/23 08:10,20/Nov/19 14:07,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.2,,,Deployment / YARN,,,,,0,pull-request-available,,,,"*(This ticket is a blocker for 1.10 release while critical for 1.9.x)*

When creating and starting up the yarn containers there are two issues with how the {{FLINK_PLUGINS_DIR}} is being handled.
 # Content of the {{plugins}} directory is currently added to the class path, braking the encapsulation of the plugins from one another
 # {{FLINK_PLUGINS_DIR}} is passed to the container as an absolute path as seen by the client. Because of that TaskManager or JobManager can not use it.

Both bugs are probably contained to {{YarnClusterDescriptor#startAppMaster}} method (which calls relevant {{addEnvironmentFoldersToShipFiles}} and {{uploadAndRegisterFiles}} methods)

(original description)

If we do not set FLINK_PLUGINS_DIR in flink-conf.yaml, it will be set to [flink configuration|https://github.com/apache/flink/blob/9e6ff81e22d6f5f04abb50ca1aea84fd2542bf9d/flink-core/src/main/java/org/apache/flink/configuration/GlobalConfiguration.java#L158] according to the environment.

In yarn mode, the local path will be set in flink-conf.yaml and used by jobmanager and taskmanager. We will find the warning log like below.
{code:java}
2019-10-12 19:24:58,165 WARN  org.apache.flink.core.plugin.PluginConfig                     - Environment variable [FLINK_PLUGINS_DIR] is set to [/Users/wangy/IdeaProjects/apache-flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/plugins] but the directory doesn't exist
{code}
It was in introduced by FLINK-12143.",,aljoscha,pnowojski,trohrmann,wangyang0918,wind_ljy,zhuzh,,,,,,,,"wangyang0918 commented on pull request #10084: [FLINK-14382][yarn] Incorrect handling of FLINK_PLUGINS_DIR on Yarn
URL: https://github.com/apache/flink/pull/10084
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   This PR is to fix the issue in [FLINK-14382](https://issues.apache.org/jira/browse/FLINK-14382). Flink plugin mechanism could not work for yarn cluster. The plugin directory only needs to be shipped and should not be added to classpath.
   
   After [FLINK-14561](https://issues.apache.org/jira/browse/FLINK-14561), the plugins will be loaded through relative path.
   
   
   ## Brief change log
   * Only ship plugin directory on yarn
   * Add `AnotherDummyFS` and `WordCount` example to check plugins isolation
   * Add e2e test to check plugins isolation for standalone cluster
   * Add e2e test to check plugins isolation for yarn cluster
   
   
   ## Verifying this change
   E2E tests has been added to check the plugin isolation.
   
   * `./run-single-test.sh test-scripts/test_plugins_isolation.sh`
   * `./run-single-test.sh test-scripts/test_plugins_isolation_yarn.sh`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (yes)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Nov/19 08:38;githubbot;600","wangyang0918 commented on pull request #10211: [FLINK-14382] Backport to 1.9
URL: https://github.com/apache/flink/pull/10211
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Backport #10084 to release-1.9
   
   
   ## Brief change log
   none
   
   
   ## Verifying this change
   none
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Nov/19 07:26;githubbot;600","pnowojski commented on pull request #10084: [FLINK-14382][yarn] Incorrect handling of FLINK_PLUGINS_DIR on Yarn
URL: https://github.com/apache/flink/pull/10084
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Nov/19 07:27;githubbot;600","pnowojski commented on pull request #10211: [FLINK-14382] Incorrect handling of FLINK_PLUGINS_DIR on Yarn (Backport to 1.9)
URL: https://github.com/apache/flink/pull/10211
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 14:05;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,FLINK-14968,,,,,FLINK-14561,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 20 14:07:50 UTC 2019,,,,,,,,,,"0|z07j8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/19 10:07;trohrmann;[~1u0] could you take a look at this problem?;;;","16/Oct/19 10:18;pnowojski;[~fly_in_gis] what is the issue? That we see this warning message? ;;;","18/Oct/19 13:13;wangyang0918;[~pnowojski]

What i mean is a local FLINK_PLUGINS_DIR has been set to flink-conf.yaml and used by flink jobmanager and taskmanager in yarn mode. So the user will find the warning log.;;;","18/Oct/19 13:55;pnowojski;Thanks for the answer [~fly_in_gis], but unfortunately I still do not understand the issue. As far as I understand it, the warning message you posted is being logged even before JobManager/TaskManager are started up in {{YarnClusterDescriptor#addPluginsFoldersToShipFiles}}. This code tries to assemble jar files from both {{lib}} AND {{plugins}} directories, to upload them to the TM/JM. At this stage presence of {{plugins}} directory is indeed optional, as users might not be using any plugins. And also it is perfectly fine, if this points to a local filesystem directory - that should be the whole point of preparing the containers. Or am I missing something? ;;;","18/Oct/19 15:01;trohrmann;I think Yang means that we write the locally configured plugins directory via the FLINK_PLUGINS_DIR env variable into the configuration we use to send to the Yarn JM and TM processes [~pnowojski]. This path is then not available in the Yarn container if one has specified an absolute path, for example. I think this sounds like a bug.;;;","21/Oct/19 09:58;wangyang0918;[~trohrmann]

Exactly. The local FLINK_PLUGINS_DIR should not be taken to jobmanager and taskmanager. It is not available for them.;;;","21/Oct/19 11:12;pnowojski;Thanks for the explanation [~trohrmann], I think I get the problem. We should handle this in a similar way how are we relativizing the paths in {{YarnClusterDescriptor#uploadAndRegisterFiles}}.

Also now I see, that there is one more bug. Currently in yarn mode {{plugins}} directory is not only shipped to the cluster, but also added to the class path, which definitely should not be the case. 

I will update the ticket to reflect both of the issues.;;;","25/Oct/19 10:01;wangyang0918;Thanks for updating the ticket [~pnowojski]. I think we just need to ship the plugin directory only and do

not add to classpath. Also the environment `ENV_FLINK_PLUGINS_DIR` should be set for job manager and task manager container.

Are you working on this issue, [~pnowojski] ? If not, would you mind to assign this ticket to me? I have some time to fix it.;;;","25/Oct/19 11:30;pnowojski;> I think we just need to ship the plugin directory only and do not add to classpath

Yes, exactly. 

> Also the environment `ENV_FLINK_PLUGINS_DIR` should be set for job manager and task manager container.

It was a conscious decision discussed by me, [~trohrmann]  and couple of other people, to not relay  environment variables from the entry points (for example flink-cli), but read them once and pass them programatically via a config file, so I would keep it as it is. Environment variables have all the drawbacks of using global variables plus some more (hard to search usages for example). Also it's not nice that we have a multiple ways how to configure Flink. In the long run we will probably want to unify everything and pass all of the configuration in the same way, and moving away from relaying on environment variables (unless we have to).

I'm not saying that we can not change the direction, but I'm just writing down our reasoning behind this. 

> Are you working on this issue, [~pnowojski] ? If not, would you mind to assign this ticket to me? I have some time to fix it.

Sure :) Done. Keep in mind that this probably needs some good test coverage (probably both unit test + maybe an end to end test).;;;","25/Oct/19 12:38;wangyang0918;[~pnowojski] [~trohrmann]

> About the environment

What i mean is to set `ENV_FLINK_PLUGINS_DIR` to `$PWD/plugins` for job manager in `YarnClusterDescriptor`. Then it will be parsed by `GlobalConfiguration#loadConfiguration` in `YarnJobClusterEntrypoint` and set to configuration. So we could use it from flink configuration just like `configuration.getString(ConfigConstants.ENV_FLINK_PLUGINS_DIR)` later. 

Are you suggesting to set `ConfigConstants.ENV_FLINK_PLUGINS_DIR` directly to a relative path(./plugins) to flink configuration in client?

 

> About the test

Do you mean to add an end to end test to cover the plugins function? Or just check the correct classpath and flink configuration have been set.;;;","25/Oct/19 13:07;pnowojski;> Are you suggesting to set `ConfigConstants.ENV_FLINK_PLUGINS_DIR` directly to a relative path(./plugins) to flink configuration in client?

Yes, that's the idea, unless this is for some reason much more difficult or has some other issues. Or at least create a copy of the client's configuration, that will be used only inside the yarn containers with the paths correctly converted to ones that are visible/valid from within the container.

> Do you mean to add an end to end test to cover the plugins function? Or just check the correct classpath and flink configuration have been set.

The first one. I think the biggest issue here is that the whole plugins system was not working as designed in yarn. It was loading the classes, but not via the plugins mechanism and without the separation. There is already an end-to-end test coverage of plugins, including yarn (introduced by [this commit|https://github.com/apache/flink/commit/b1d5bf963e9ce062b2e349f5296a56da6816340e]), but it's clearly not sufficient - current test is checking whether we can read a {{--input dummy://localhost/words}} file provided by a {{DummyFS}} configured as a plugin. And this works, however it doesn't test the essence of the Plugins.

I think probably we need to repeat in end-to-end test the similar logic that is already implemented by the unit test {{org.apache.flink.test.plugin.PluginLoaderTest}}.

For example in the end to end test load not only one file from one dummy FIleSystem, but from two ({{DummyFSFactory}} and {{AnotherDummyFSFactory}}). Each of this dummy FileSystem should validate that there they can not see any classes from the other FileSystem ({{DummyFSFactory}} shouldn't be visible when loading {{AnotherDummyFSFactory}} and vice versa). I guess for this we would also need to modify {{WordCount}} example to union multiple inputs.;;;","29/Oct/19 14:23;trohrmann;[~fly_in_gis] for your information. I intend to remove the part where we write the env variable into Flink's {{Configuration}}. Instead I want to directly read the env variable in the {{PluginConfig}}. If the variable has not been set, then it falls back to {{./plugins}}. See FLINK-14561 for more information.;;;","30/Oct/19 02:06;wangyang0918;Hi [~trohrmann],
I agree with you that we should not set the env `FLINK_PLUGIN_DIR` in flink configuration. Instead, make the `PluginConfig` to get the plugin directory from environment. So we will need to set the `FLINK_PLUGIN_DIR` environment in Yarn and mesos submission.

This jira will focus on only ship plugins on Yarn and check the plugin mechanism that classes of different plugins are isolated.;;;","30/Oct/19 08:43;pnowojski;> So we will need to set the `FLINK_PLUGIN_DIR` environment in Yarn and mesos submission.

There is another option. Flink could check if {{FLINK_PLUGIN_DIR}} is set, and if not, just fall back to checking {{$CWD/plugins}} (current working directory). This would give us an option in Yarn to just ship the plugins to the {{$CWD}}, to avoid setting and relaying on the environment variable.;;;","30/Oct/19 14:25;trohrmann;> There is another option. Flink could check if FLINK_PLUGIN_DIR is set, and if not, just fall back to checking $CWD/plugins (current working directory). This would give us an option in Yarn to just ship the plugins to the $CWD, to avoid setting and relaying on the environment variable.

That's how I intend to solve the problem in FLINK-14561.;;;","05/Nov/19 10:10;wangyang0918;[~pnowojski] [~trohrmann]

I have attached a PR to fix this issue. Please help to review.

[https://github.com/apache/flink/pull/10084];;;","15/Nov/19 07:31;pnowojski;Merged to master as 468415b85a59706c8e06b851ed792c5f5146b1ed..bc4e2458fc2a9be11fd2defbdba74ca0dd7b1ddf;;;","19/Nov/19 08:55;aljoscha;Can this issue be closed now?;;;","20/Nov/19 14:07;pnowojski;[~aljoscha] it was being back-ported to 1.9 release branch.

Merged to release-1.9 as {{2cecd2b17035d663281ab33fb45e8c7eb54511df..7d0455168ce5cb451ee6d0df6e96c9adf98d2b8b}}.;;;",,,,,,,,,,,,,
Type Extractor POJO setter check does not allow for Immutable Case Class,FLINK-14380,13261867,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mzuehlke,elliotvilhelm,elliotvilhelm,11/Oct/19 18:20,11/Nov/19 14:15,13/Jul/23 08:10,11/Nov/19 14:15,1.8.2,1.9.0,,,,,,,,1.10.0,,,,API / Scala,,,,,1,pull-request-available,,,,"When deciding if a class conforms to POJO using the type extractor Flink checks that the class implements a setter and getter method. For the setter method Flink makes the assertion that the return type is `Void`. This is an issue if using a case class as often the return type of a case class setter is a copy of the objects class. Consider the following case class:


{code:scala}
case class  SomeClass(x: Int) {
    x_=(newX: Int): SomeClass = { this.copy(x = newX) }
}
{code}


This class will be identified as not being valid POJO although getter (generated) and setter methods are provided because the return type of the setter is not void. 

This issue discourages immutabilaty and makes the usage of case classes not possible without falling back to Kryo Serializer.

The issue is located in https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/java/typeutils/TypeExtractor.java on line 1806. Here is a permalink to the line 

https://github.com/apache/flink/blob/80b27a150026b7b5cb707bd9fa3e17f565bb8112/flink-core/src/main/java/org/apache/flink/api/java/typeutils/TypeExtractor.java#L1806


A copy of the if check is here
{code:java}
if((methodNameLow.equals(""set""+fieldNameLow) || methodNameLow.equals(fieldNameLow+""_$eq"")) &&
					m.getParameterTypes().length == 1 && // one parameter of the field's type
					(m.getGenericParameterTypes()[0].equals( fieldType ) || (fieldTypeWrapper != null && m.getParameterTypes()[0].equals( fieldTypeWrapper )) || (fieldTypeGeneric != null && m.getGenericParameterTypes()[0].equals(fieldTypeGeneric) ) )&&
					// return type is void.
					m.getReturnType().equals(Void.TYPE)
				) {
					hasSetter = true;
				}
			}
{code}


I believe the 
{code:java}
m.getReturnType().equals(Void.TYPE)
{code}

should be modified to 

{code:java}
m.getReturnType().equals(Void.TYPE) || m.getReturnType().equals(clazz)
{code}

This will allow for case class setters which return copies of the object enabling to use case classes. This allows us to maintain immutability without being forced to fall back to the Kryo Serializer.  ","Not relavent

 ",aljoscha,elliotvilhelm,gaoyunhaii,mzuehlke,sewen,twalthr,,,,,,,,"aljoscha commented on pull request #9882: [FLINK-14380][Flink-Core] Type Extractor Setter
URL: https://github.com/apache/flink/pull/9882
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Oct/19 07:33;githubbot;600","mzuehlke commented on pull request #9999: [FLINK-14380][ScalaAPI] Passing inferred outputType to map and flatMap function
URL: https://github.com/apache/flink/pull/9999
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *As suggested by @aljoscha in the JIRA this pull request passes the inferred outputTypes from the ScalaAPI to the new `map` and `flatMap` function functions in the JavaAPI. An equivalent function already existed for `process`.*
   
   ## Brief change log
   
   The Scala API did not pass the inferred output type to the `map` or the `flatMap` function in the Java API. So the Java type extractor tried to deduce the type and failed for Scala Case Classes.
   With this pull request the inferred type is passed from the ScalaAPI to the JavaAPI.
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes )
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes)
     - If yes, how is the feature documented? (JavaDocs)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Oct/19 22:19;githubbot;600","asfgit commented on pull request #9999: [FLINK-14380][ScalaAPI] Passing inferred outputType to map and flatMap function
URL: https://github.com/apache/flink/pull/9999
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 08:24;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 11 14:15:50 UTC 2019,,,,,,,,,,"0|z07iqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/19 19:24;elliotvilhelm;Related pull request: https://github.com/apache/flink/pull/9882;;;","15/Oct/19 11:55;aljoscha;Copying my comment from github:

I don't think this will work, since the setter for case classes doesn't actually modify the case class and Flink only has a reference to the unmodified case class. When using the Scala API, the type analysis stack should correctly analyze case classes and use the CaseClassSerializer for them. (The TypeExtractor is only used for the Java API);;;","16/Oct/19 01:54;elliotvilhelm;[~aljoscha] Thanks for the response! That makes sense. I am seeing this output in our Scala application on launch for most case classes.


*""MyCaseClass.scala cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType.""*

We are using case classes so I was confused. I had isolated this output to here which I realize in in the Java api:
https://github.com/apache/flink/blob/80b27a150026b7b5cb707bd9fa3e17f565bb8112/flink-core/src/main/java/org/apache/flink/api/java/typeutils/TypeExtractor.java#L1857

Perhaps this is a bug or likely I am misunderstanding something? 😄 ;;;","16/Oct/19 08:15;twalthr;Hi [~elliotvilhelm], I agree that we should support immutable types in the future. The issues you have mentioned and also the interoperability with Scala are big arguments for improvements here. For the Table API, we will support immutable types as mentioned in [FLIP-65|https://docs.google.com/document/d/1Zf8-okGvCiTTRaTN0IqtTGrrjXYNGFJnhUQyinF4xcU/edit?ts=5d9f20a2#]. I hope some of the improvements will bubble up into Flink's the core modules. But for this (as Aljoscha mentioned) we need changes on the serializer side as well. The type extractor must be kept in sync with what the serializers support.;;;","16/Oct/19 13:39;sewen;[~elliotvilhelm] What [~aljoscha] mentioned is that the Scala Case Class code should not go through this logic in the first place.
It should be handled by the Scala type inference and extractor, which should support immutable Scala Case Classes.;;;","16/Oct/19 14:02;aljoscha;Again forwarding my comment from GH (we should stick to discussing this here, please):

Maybe you could give some more details about your job? Which API are you using? DataSet, DataStream, or Table? Are you using the Scala version of the API, that is org.apache.flink.api.scala if you're using the DataSet API?;;;","17/Oct/19 09:18;aljoscha;[~elliotvilhelm] could you please post a stack trace that shows how/why {{TypeExtractor}} is invoked? Or provide a minimal code example that triggers it?;;;","22/Oct/19 16:10;mzuehlke;I bumped into the same case. Using a pure scala streaming app.

Here is a stacktrace (on flink 1.9):
{code:java}
  java.lang.Thread.State: RUNNABLE
	  at org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo(TypeExtractor.java:1857)
	  at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:1746)
	  at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:1643)
	  at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy(TypeExtractor.java:921)
	  at org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo(TypeExtractor.java:803)
	  at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:587)
	  at org.apache.flink.api.java.typeutils.TypeExtractor.getFlatMapReturnTypes(TypeExtractor.java:196)
	  at org.apache.flink.streaming.api.datastream.DataStream.flatMap(DataStream.java:611)
	  at org.apache.flink.streaming.api.scala.DataStream.flatMap(DataStream.scala:647){code};;;","23/Oct/19 07:32;aljoscha;I see, the problem is here: https://github.com/apache/flink/blob/e0efabe8884f22b4a1c7ab9df3274b3fca03dcfb/flink-streaming-scala/src/main/scala/org/apache/flink/streaming/api/scala/DataStream.scala#L647.

The issue is that we call {{DataStream.flatMap()}} from the Java API, and this in turns calls the type extractor. But afterwards we set the type that was extracted from Scala using {{returns()}}. {{process()}} has a version that allows passing the {{TypeInformation}} in the method call, the solution for this issue here will be to also do the same for the other methods of {{DataStream}} and not use {{returns()}}.

Anyone want to take a try and cut a PR for that?;;;","23/Oct/19 12:03;mzuehlke;I will give it a try.

At a first look at least  {{Datastream#map}, {{Datastream#flatMap}},  {{ConnectedStreams#map}} and {{ConnectedStreams#flatMap}}  are using the {{returns()}} method.;;;","25/Oct/19 22:21;mzuehlke;A pull request in line with the suggestion by [~aljoscha]: https://github.com/apache/flink/pull/9999;;;","04/Nov/19 18:28;elliotvilhelm;Thanks everyone for your investigation, I thought I was crazy. I was unable to trace in our application, tried to add Flink source files for debugging but ran into issues. It has been a busy dev month for us. Very happy to see the issue was discovered, love the open source community.;;;","11/Nov/19 14:15;aljoscha;Fixed on master in 47c277d85aca6ce288505b13cd3a3595911e6bfa.

[~sewen] unless you wanted to keep this open to also backport the fix to previous versions.;;;",,,,,,,,,,,,,,,,,,,
KafkaProducerAtLeastOnceITCase>KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink fails on Travis,FLINK-14370,13261737,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,trohrmann,trohrmann,11/Oct/19 08:48,22/Jun/21 13:54,13/Jul/23 08:10,29/Oct/19 05:47,1.10.0,,,,,,,,,1.10.0,1.8.3,1.9.2,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,"The {{KafkaProducerAtLeastOnceITCase>KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink}} fails on Travis with

{code}
Test testOneToOneAtLeastOnceRegularSink(org.apache.flink.streaming.connectors.kafka.KafkaProducerAtLeastOnceITCase) failed with:
java.lang.AssertionError: Job should fail!
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnce(KafkaProducerTestBase.java:280)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink(KafkaProducerTestBase.java:206)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

https://api.travis-ci.com/v3/job/244297223/log.txt",,arvid.heise@gmail.com,becket_qin,jark,pnowojski,trohrmann,wind_ljy,yanghua,,,,,,,"becketqin commented on pull request #9912: [FLINK-14370][kafka][test-stability] Fix the cascading failure in kaProducerTestBase.
URL: https://github.com/apache/flink/pull/9912
 
 
   ## What is the purpose of the change
   This patch helps improve the stability of `KafkaProducerTestBase`, which is used by the following IT cases:
   
   - Kafka08ProducerITCase
   - Kafka09ProducerITCase
   - Kafka010ProducerITCase
   - KafkaProducerAtLeastOnceITCase
   - Kafka011ProducerAtLeastOnceITCase
   
   ## Brief change log
   1. Ensure `testOneToOneAtLeastOnce()` cleans up the environment to avoid cascading failures. 
   2. Configure the timeout properly so the test finishes quicker. This also address some of the test failures caused by timeout.
   3. Added a debugging message on failure to help determine the reason of failure.
   
   ## Verifying this change
   This change fixes the existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/19 13:35;githubbot;600","becketqin commented on pull request #9912: [FLINK-14370][kafka][test-stability] Fix the cascading failure in kaProducerTestBase.
URL: https://github.com/apache/flink/pull/9912
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Oct/19 08:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,FLINK-14369,,,,,,,,,FLINK-14308,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 28 09:06:48 UTC 2019,,,,,,,,,,"0|z07hxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/19 01:43;yanghua;another instance: https://api.travis-ci.com/v3/job/245838696/log.txt;;;","16/Oct/19 13:38;becket_qin;I haven't figured out the full story yet. But here are some clues:
 # The problem reported in FLINK-14308 is related to the failure in this ticket. The problem is that when this test fails, it did not unblock the traffic to the Kafka cluster, so the next test sharing the same Kafka cluster fails.
 # Some timeout failures reported in other tests may be related to the long timeout here. I reduced the timeout so the tests finishes faster.

The part that still not clear to me is how the job expected to fail succeeded. I added a debugging message to give us some clue when hit this again.;;;","17/Oct/19 01:59;jark;another instances: 
https://api.travis-ci.com/v3/job/246299399/log.txt
https://api.travis-ci.com/v3/job/246299491/log.txt;;;","19/Oct/19 08:29;arvid;We found the root cause and are currently discussing fixes as this is a non-trivial interplay of components.

If you are in need of a quick (and dirty) fix: [https://github.com/apache/flink/pull/9918/commits/921ef31baa96bfc7c0629854104515dd856a6d29|https://github.com/apache/flink/pull/9918/commits/6a79fb8e9272b5d56ecb286634170c72403c751e];;;","21/Oct/19 00:44;becket_qin;Thanks [~arvid.heise@gmail.com] for finding the problem. The cause was that when all the records are processed before a snapshot was taken, the records that could not be sent out trigger the snapshot to fail. That snapshot failure will not cause the job to exit. However, all the records in the KafkaProducer are already expired after the snapshot failure. So when the producer closes, there will be no more exception thrown. Thus the job finished successfully.

It looks that the expected behavior from connector is to report exception in {{close()}} method as long as there was a record that could not be sent. On the other hand, exception thrown from {{CheckpointedFunction.snapshotState()}} might be ignored. Not sure if this is reasonable, but this expectation is not super clear from the connector implementation's perspective.

In terms of immediate fix, [~arvid.heise@gmail.com] proposes to always throw exception as long as there has been a record sending failure. I agree that is the right fix per current expected behavior on the connectors.;;;","21/Oct/19 14:10;arvid;So [~pnowojski] will merge a better fix for stabilizing the test soonish. [~becket_qin] can you also merge your hotfix/cleanup commit where you improved on the test?

We create a follow-up ticket FLINK-14480 where we discuss changes to the production code and potentially fix it.;;;","22/Oct/19 02:59;becket_qin;[~arvid.heise@gmail.com] I am wondering if it would be better to wait for the patch from [~pnowojski] before checking my improvement as there is a high chance the build fails in my PR due to the test stability.;;;","22/Oct/19 07:12;arvid;Yes, that's a good idea. [https://github.com/apache/flink/pull/9959] for reference.;;;","22/Oct/19 14:38;pnowojski;My work is done <flies away>;;;","28/Oct/19 09:06;becket_qin;I've checked in the patch to master, will backport Piotr's patch and mine to {{release-1.8}} and {{release-1.9}} after travis on those two branches passes.;;;",,,,,,,,,,,,,,,,,,,,,,
KafkaProducerAtLeastOnceITCase>KafkaProducerTestBase.testOneToOneAtLeastOnceCustomOperator fails on Travis,FLINK-14369,13261736,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,csq,trohrmann,trohrmann,11/Oct/19 08:47,21/May/20 08:40,13/Jul/23 08:10,21/May/20 08:40,1.10.0,,,,,,,,,1.10.2,1.11.0,,,Connectors / Kafka,,,,,0,test-stability,,,,"The {{KafkaProducerAtLeastOnceITCase>KafkaProducerTestBase.testOneToOneAtLeastOnceCustomOperator}} fails on Travis with 

{code}
Test testOneToOneAtLeastOnceCustomOperator(org.apache.flink.streaming.connectors.kafka.KafkaProducerAtLeastOnceITCase) failed with:
java.lang.AssertionError: Create test topic : oneToOneTopicCustomOperator failed, org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment.
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:180)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:115)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:196)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnce(KafkaProducerTestBase.java:231)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnceCustomOperator(KafkaProducerTestBase.java:214)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

https://api.travis-ci.com/v3/job/244297223/log.txt",,becket_qin,csq,gjy,klion26,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14370,FLINK-14235,,,,,,,,,,FLINK-14402,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 08:40:46 UTC 2020,,,,,,,,,,"0|z07hxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/20 08:57;gjy;Are you also looking at this [~becket_qin]?;;;","19/Jan/20 03:43;csq;Hi [~gjy] I'm watching this issue, could you please assign it to me?;;;","21/Jan/20 09:26;gjy;[~csq] Sorry for the late reply. I have assigned you now.;;;","23/Jan/20 09:34;gjy;[~csq] Were you able to make progress?;;;","21/May/20 08:40;becket_qin;I believe this issue is related to FLINK-14370 & FLINK-14235. The problem is following:

Prior to FLINK-14235 may cause `testOneToOneAtLeastOnceRegularSink()` to fail because the job was expected to fail but it doesn't.

When that happens, due to the problem fixed in FLINK-14370, the traffic to the Kafka brokers will remain blocked by the proxy. This will cause the next test, in this case `testOneToOneAtLeastOnceCustomOperator` to fail in a cascading way.

Given those two issues have been resolved. I am closing this ticket.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Example code in state processor API docs doesn't compile,FLINK-14355,13261371,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wind_ljy,mlmitch,mlmitch,09/Oct/19 16:25,16/Oct/19 11:33,13/Jul/23 08:10,16/Oct/19 11:33,1.9.0,,,,,,,,,1.10.0,1.9.2,,,API / State Processor,,,,,0,pull-request-available,,,,"The example code in this doc page doesn't compile:

[https://ci.apache.org/projects/flink/flink-docs-stable/dev/libs/state_processor_api.html]

Here are two instances I found:
 * Reading State java and scala contain references to undefined {{stateDescriptor}} variable
 * Reading Keyed State scala has some invalid scala (""{{override def open(Configuration parameters)""}})

 ",,aljoscha,klion26,mlmitch,twalthr,wind_ljy,,,,,,,,,"buptljy commented on pull request #9889: [FLINK-14355] [docs] Example code in state processor API docs doesn't compile
URL: https://github.com/apache/flink/pull/9889
 
 
   Fix compiling errors in examples of state processor api.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/19 10:17;githubbot;600","zentol commented on pull request #9889: [FLINK-14355] [docs] Example code in state processor API docs doesn't compile
URL: https://github.com/apache/flink/pull/9889
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/19 09:36;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 16 11:33:28 UTC 2019,,,,,,,,,,"0|z07fog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/19 12:04;twalthr;CC [~sjwiesman];;;","13/Oct/19 10:20;wind_ljy;[~mlmitch] Thanks for pointing this out.

[~twalthr] [~sjwiesman]  I've already created a PR for this typo. ;;;","16/Oct/19 11:33;chesnay;master: b1f468a82b4445a757d78184ccf4519cd0419312
1.9: a7653478b480cfea087801e8975080e6b7cee28f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionFIFOITCase.checkForProhibitedLogContents found a log with prohibited string,FLINK-14347,13261227,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tison,TsReaper,TsReaper,09/Oct/19 03:10,20/Sep/20 13:40,13/Jul/23 08:10,11/Oct/19 16:36,1.10.0,1.8.3,1.9.1,,,,,,,1.10.0,1.8.3,1.9.2,,Deployment / YARN,Tests,,,,0,pull-request-available,,,,"YARNSessionFIFOITCase.checkForProhibitedLogContents fails with the following exception:
{code:java}
14:55:27.643 [ERROR]   YARNSessionFIFOITCase.checkForProhibitedLogContents:77->YarnTestBase.ensureNoProhibitedStringInLogFiles:461 Found a file /home/travis/build/apache/flink/flink-yarn-tests/target/flink-yarn-tests-fifo/flink-yarn-tests-fifo-logDir-nm-1_0/application_1570546069180_0001/container_1570546069180_0001_01_000001/jobmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:23760[{code}
Travis log link: [https://travis-ci.org/apache/flink/jobs/595082243]",,guoyangze,tison,trohrmann,TsReaper,wangyang0918,,,,,,,,,"TisonKun commented on pull request #9880: [FLINK-14347][test] Filter out expected exception string in YARN tests
URL: https://github.com/apache/flink/pull/9880
 
 
   ## What is the purpose of the change
   
   Filter out expected exception string in YARN tests, especially `YARNSessionFIFOITCase.checkForProhibitedLogContents`.
   
   The instability reported in FLINK-14347 depends on whether or not `jobmanager.log` has been dumped on the verification. Given that the ""forbidden"" string is actually expected[1] I propose we add the next line into whitelist. Locally verify when `jobmanager.log` dumped we find the ""forbidden"" string and filter out with the exclusion of expected Exception.
   
   Received shutdown request from YARN ResourceManager
   
   [1] Specifically, we call `YarnClient#killApplication` in `YARNSessionFIFOITCase#runDetachedModeTest` which always causes a shutdown request.
   
   ## Verifying this change
   
   This change is itself about tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Oct/19 02:12;githubbot;600","tillrohrmann commented on pull request #9880: [FLINK-14347][test] Filter out expected exception string in YARN tests
URL: https://github.com/apache/flink/pull/9880
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Oct/19 16:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-14333,,,,,,FLINK-14010,,,,,,,FLINK-19295,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 11 16:36:34 UTC 2019,,,,,,,,,,"0|z07esg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/19 12:53;chesnay;Please add the full excerpt to the JIRA description.;;;","10/Oct/19 16:05;trohrmann;I think this problem has been introduced with FLINK-14010. I suspect that the {{YarnResourceManager}} receives an {{onShutdownRequest}} during the clean up of the test. Since we are now calling the {{FatalExceptionHandler}}, the process terminates and logs an exception which then fails the test.;;;","10/Oct/19 16:55;tison;Thanks for reporting this issue [~TsReaper] and your analysis [~trohrmann].

I notice that the instability depends on whether or not {{jobmanager.log}} has been dumped on the verification. Given that the ""forbidden"" string is actually expected[1] I propose we add the next line into whitelist. Locally verify when {{jobmanager.log}} dumped we find the ""forbidden"" string and filter out with the exclusion of expected Exception.

{{Received shutdown request from YARN ResourceManager}}

[1] Specifically, we call {{YarnClient.killApplication}} in {{YARNSessionFIFOITCase#runDetachedModeTest}} which always causes a shutdown request.;;;","10/Oct/19 17:26;trohrmann;I'd be ok with this approach. Can you work on this issue [~tison]?;;;","11/Oct/19 02:29;wangyang0918;When there is `ApplicationAttemptNotFoundException` in AMRMClient, `onShutdownRequest` of the callback will be called. Usually it just because we kill the application through yarn client, so it is a expected exception. Also when there is some problem with Yarn in recover application meta info from zookeeper(lost some applications), `ApplicationAttemptNotFoundException` will also send to AMRMClient.

It is reasonable that we throw a fatal error in `onShutdownRequest`. Updating the check rules is ok for me.;;;","11/Oct/19 02:32;tison;Thanks for your information [~fly_in_gis]!;;;","11/Oct/19 02:39;tison;[~trohrmann] PR submitted.;;;","11/Oct/19 16:36;trohrmann;Fixed via

1.10.0: fbfa8beb6847ed14641399afbbfd9a378d91e6f5
1.9.2: 01c97d56deffdfdd27f2f3e7fcdf734fd166257d
1.8.3: 903ac2135a0932dacce40f158a9c1a7afd50f564;;;",,,,,,,,,,,,,,,,,,,,,,,,
Snapshot deployments may fail due to MapR HTTPS issue,FLINK-14345,13261063,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,08/Oct/19 08:21,08/Oct/19 08:25,13/Jul/23 08:10,08/Oct/19 08:25,1.7.2,,,,,,,,,1.7.3,,,,Release System,,,,,0,,,,,Snapshot deployments occasionally fail since the MapR HTTPS repository cannot be verified in some environments.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 08 08:25:31 UTC 2019,,,,,,,,,,"0|z07ds8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/19 08:25;chesnay;1.7: 808cc1a23abb25bd03d24d75537a1e7c6987eef7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServer does not handle NPE on corruped archives properly,FLINK-14337,13260916,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,trohrmann,trohrmann,07/Oct/19 15:42,29/Jan/20 09:01,13/Jul/23 08:10,16/Oct/19 13:12,1.10.0,1.8.2,1.9.0,,,,,,,1.10.0,1.8.3,1.9.2,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"The {{HistoryServerTest.testHistoryServerIntegration}} failed on Travis with

{code}
[ERROR] testHistoryServerIntegration[Flink version less than 1.4: false](org.apache.flink.runtime.webmonitor.history.HistoryServerTest)  Time elapsed: 10.667 s  <<< FAILURE!
java.lang.AssertionError: expected:<3> but was:<2>
{code}

https://api.travis-ci.org/v3/job/594533358/log.txt",,trohrmann,wind_ljy,,,,,,,,,,,,"zentol commented on pull request #9856: [FLINK-14337][hs] Better handling of failed fetches
URL: https://github.com/apache/flink/pull/9856
 
 
   Some small adjustments to the HistoryServer to prevent NPEs when processing empty/partial/corrupted archives, and ensure that archive fetches are retried unless the fetch was completed successfully.
   
   Tests weren't added intentionally. The NPE issue is difficult to test without exposing details about the JSON structure to the test, while the other issue is simply difficult to test without larger refactorings.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Oct/19 13:27;githubbot;600","zentol commented on pull request #9856: [FLINK-14337][hs] Better handling of failed fetches
URL: https://github.com/apache/flink/pull/9856
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/19 13:10;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 16 13:12:49 UTC 2019,,,,,,,,,,"0|z07cvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/19 15:46;trohrmann;The problem could have been that {{numExpectedArchivedJobs}} did not reach zero within the specified timeout when calling {{numExpectedArchivedJobs.await}}.;;;","08/Oct/19 07:53;chesnay;We know that 3 archives exist before the HistoryServer is started.

We also know it polls the directory every 100ms, so it should have plenty opportunities to fetch said archives; it seems unlikely that it just isn't able to fetch & extract them in time.

The logs that the HS hits an NPE:
{code}
java.lang.NullPointerException
	at org.apache.flink.runtime.history.FsJobArchivist.getArchivedJsons(FsJobArchivist.java:115)
	at org.apache.flink.runtime.webmonitor.history.HistoryServerArchiveFetcher$JobArchiveFetcherTask.run(HistoryServerArchiveFetcher.java:169)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

Looking into it...;;;","08/Oct/19 08:01;chesnay;The problem is that the archives may not be fully written while the HS is started. This can lead to exceptions when unpacking them, which shouldn't be a problem; this easily happens in production and the HS should simply retry on the next run.

However this retry doesn't happen. The HS adds the job to the list of processed archives before doing any processing, and only removes the entry if it runs into an IOException; but this is bypassed since it is an NPE.

The easy fix is to only add the job to the list of processed archives if everything went smoothly.;;;","16/Oct/19 13:12;chesnay;master:
26bc3c8c65c757285c58b2cfcb0ba81111395ea4
88ae9f8e3a5749e262fc2a9217ced4f2dc997b44
1.9:
7670e237d7d8d3727537c09b8695c860ea92d467 
6856809739510f1afb837c1ab76865a80981e289
1.8:
f44f598c472d470a99a6916870a30066d08657cc
4c4dc3d0e72d0b5277af9440e4a3168a88555403 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions in AsyncCheckpointRunnable#run are never logged,FLINK-14336,13260857,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,klion26,chesnay,chesnay,07/Oct/19 10:47,23/Oct/19 09:38,13/Jul/23 08:10,23/Oct/19 09:38,1.8.0,,,,,,,,,1.10.0,,,,API / DataStream,,,,,0,pull-request-available,,,,"Exceptions that are thrown in {{AsyncCheckpointRunnable#run}} and subsequently being handled in {{AsyncCheckpointRunnable#handleExecutionException}} are never logged on the TaskExecutor side, and are only forwarded to the JobMaster.",,klion26,wind_ljy,,,,,,,,,,,,"klion26 commented on pull request #9873: [FLINK-14336][DataStream] Log Exception for failed checkpoint on TaskExecutor side
URL: https://github.com/apache/flink/pull/9873
 
 
   ## What is the purpose of the change
   
   Exceptions that are thrown in `AsyncCheckpointRunnable#run` and subsequently being handled in `AsyncCheckpointRunnable#handleExecutionException` are never logged on the TaskExecutor side, and are only forwarded to the JobMaster. This PR will log the Exception on TaskExecutor side.
   
   ## Brief change log
   
   Log Exception for failed checkpoint on TaskExecutor side.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**no**)
     - The serializers: (**no**)
     - The runtime per-record code paths (performance sensitive): (**no**)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**no**)
     - The S3 file system connector: (**no**)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (**no**)
     - If yes, how is the feature documented? (**not applicable**)
   
   cc @zentol 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Oct/19 14:39;githubbot;600","zentol commented on pull request #9873: [FLINK-14336][DataStream] Log Exception for failed checkpoint on TaskExecutor side
URL: https://github.com/apache/flink/pull/9873
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Oct/19 09:38;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 23 09:38:24 UTC 2019,,,,,,,,,,"0|z07cig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/19 02:10;klion26;For now, we only have complete log in task side, and no exception log in task side, add a log for exception scenario is reasonable for me.

I can help to implement this issue, could you please assign this ticket to me [~chesnay].;;;","08/Oct/19 08:08;chesnay;I've assigned it to you. For clarification, we have the complete log on the JM side, and no loger at all on task side.;;;","23/Oct/19 09:38;chesnay;master: a99b84929eab0e8644498fea39c926a0d74ed28c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDK11 build stalls during shading,FLINK-14318,13260500,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,gjy,gjy,04/Oct/19 05:27,29/Oct/19 13:54,13/Jul/23 08:10,29/Oct/19 13:54,1.10.0,,,,,,,,,1.10.0,,,,Build System,,,,,0,test-stability,,,,"JDK11 build stalls during shading.

Travis stage: e2d - misc - jdk11

https://travis-ci.org/apache/flink/builds/593022581?utm_source=slack&utm_medium=notification

https://api.travis-ci.org/v3/job/593022629/log.txt

Relevant excerpt from logs:
{noformat}
01:53:43.889 [INFO] ------------------------------------------------------------------------
01:53:43.889 [INFO] Building flink-metrics-reporter-prometheus-test 1.10-SNAPSHOT
01:53:43.889 [INFO] ------------------------------------------------------------------------
...
01:53:44.508 [INFO] Including org.apache.flink:force-shading:jar:1.10-SNAPSHOT in the shaded jar.
01:53:44.508 [INFO] Excluding org.slf4j:slf4j-api:jar:1.7.15 from the shaded jar.
01:53:44.508 [INFO] Excluding com.google.code.findbugs:jsr305:jar:1.3.9 from the shaded jar.
01:53:44.508 [INFO] No artifact matching filter io.netty:netty
01:53:44.522 [INFO] Replacing original artifact with shaded artifact.
01:53:44.523 [INFO] Replacing /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT.jar with /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT-shaded.jar
01:53:44.524 [INFO] Replacing original test artifact with shaded test artifact.
01:53:44.524 [INFO] Replacing /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT-tests.jar with /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/flink-metrics-reporter-prometheus-test-1.10-SNAPSHOT-shaded-tests.jar
01:53:44.524 [INFO] Dependency-reduced POM written at: /home/travis/build/apache/flink/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/dependency-reduced-pom.xml

No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.
Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received

The build has been terminated
{noformat}",,gjy,guoyangze,liyu,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14226,MSHADE-329,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 29 13:54:12 UTC 2019,,,,,,,,,,"0|z07ab4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/19 05:28;gjy;cc: [~chesnay];;;","04/Oct/19 09:49;chesnay;yeah this is a known issue; we've also seen this recently when the plugin version was bumped to 3.2.1 for all builds. MSHADE-329 may be related, but IIRC I already tried disabling multi-threaded builds and still encountered it.

Unfortunately we simply need 3.2.1 for Java 11, so we're stuck with this issue for the time being.;;;","04/Oct/19 10:55;gjy;Should we keep this ticket open or is there another ticket tracking the problem?;;;","04/Oct/19 11:05;chesnay;We can keep this one open.;;;","11/Oct/19 07:20;liyu;Another instance: https://api.travis-ci.org/v3/job/596081750/log.txt;;;","12/Oct/19 09:15;trohrmann;Another instance: https://api.travis-ci.org/v3/job/596712083/log.txt;;;","15/Oct/19 08:38;gjy;Another instance: https://api.travis-ci.org/v3/job/597602318/log.txt;;;","23/Oct/19 08:28;liyu;Another instance: [https://api.travis-ci.org/v3/job/600316943/log.txt];;;","29/Oct/19 13:36;chesnay;I'm double checking right now whether we can simply disable parallel compiling as suggested in MSHADE-329; given that this only happens in the nightlies (which use parallel compiling) but not in the normal builds (which don't use parallel compiling) there's reason to be optimistic.

I may have checked the wrong builds in my last test.;;;","29/Oct/19 13:54;chesnay;master: 6134cbc7a2c63b6f0c602614000596818be13cdb ;;;",,,,,,,,,,,,,,,,,,,,,,
"Stuck in ""Job leader ... lost leadership"" error",FLINK-14316,13260454,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,stevenz3wu,stevenz3wu,03/Oct/19 21:58,17/Apr/20 02:22,13/Jul/23 08:10,02/Apr/20 09:01,1.7.2,,,,,,,,,1.10.1,1.11.0,1.9.3,,Runtime / Coordination,,,,,0,pull-request-available,,,,"This is the first exception caused restart loop. Later exceptions are the same. Job seems to stuck in this permanent failure state.

{code}
2019-10-03 21:42:46,159 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: clpevents -> device_filter -> processed_imps -> ios_processed_impression -> i
mps_ts_assigner (449/1360) (d237f5e99b6a4a580498821473763edb) switched from SCHEDULED to FAILED.
java.lang.Exception: Job leader for job id ecb9ad9be934edf7b1a4f7b9dd6df365 lost leadership.
        at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobLeaderListenerImpl.lambda$jobManagerLostLeadership$1(TaskExecutor.java:1526)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:332)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:158)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
        at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
        at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
        at akka.actor.ActorCell.invoke(ActorCell.scala:495)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
        at akka.dispatch.Mailbox.run(Mailbox.scala:224)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,liyu,pgoyal,stevenz3wu,trohrmann,wind_ljy,zhuzh,,,,,,,,"tillrohrmann commented on pull request #11603: [FLINK-14316] Properly manage rpcConnection in JobManagerLeaderListener under leader change
URL: https://github.com/apache/flink/pull/11603
 
 
   ## What is the purpose of the change
   
   This commit changes how the rpcConnection is managed in JobManagerLeaderListener under leader change.
   This component clears now the fields rpcConnection and currentJobMasterId if the leader loses leadership.
   Moreover, it only restarts a connection attempt if the leader session id is new.
   
   ## Verifying this change
   
   - Added `JobLeaderServiceTest.canReconnectToOldLeaderWithSameLeaderAddress`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 14:37;githubbot;600","tillrohrmann commented on pull request #11603: [FLINK-14316] Properly manage rpcConnection in JobManagerLeaderListener under leader change
URL: https://github.com/apache/flink/pull/11603
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 08:57;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-16836,,,,,,,,,,,"07/Oct/19 23:03;stevenz3wu;FLINK-14316.tgz;https://issues.apache.org/jira/secure/attachment/12982434/FLINK-14316.tgz","08/Oct/19 07:27;pgoyal;RpcConnection.patch;https://issues.apache.org/jira/secure/attachment/12982468/RpcConnection.patch",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 09:01:08 UTC 2020,,,,,,,,,,"0|z07a0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/19 10:03;trohrmann;Thanks for reporting this issue [~stevenz3wu]. Does the same problem occur with Flink {{1.8}} or {{1.9}}? I'm asking because the community no longer supports Flink {{1.7}} and below.;;;","04/Oct/19 16:16;stevenz3wu;[~trohrmann] don't know. we are just beginning rolling out of 1.9. It will take some time for those large-state jobs to pick up 1.9.;;;","07/Oct/19 10:42;trohrmann;Thanks for the information. Can you provide us with the full logs of the run where the problem occurred?;;;","07/Oct/19 23:07;stevenz3wu;[~trohrmann] in the uploaded tar ball, there is one TM log file. the rest are JM log files.

This is the line from TM log that TM thinks JM lost leadership.
{code}
2019-10-06 16:11:36,471 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - JobManager for job 3bb42eb7602c5ba25740d8360b1f0e27 with leader id 9aa48c6a49d009f7fb287754b61d4af8 lost leadership.
{code};;;","07/Oct/19 23:10;stevenz3wu;From TM log, I also noticed that ZooKeeperLeaderRetrievalService started, stopped, then started again.
{code:java}
2019-10-06 16:07:32,509 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Starting ZooKeeperLeaderRetrievalService /leader/resource_manager_lock.
...
2019-10-06 16:07:35,018 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Stopping ZooKeeperLeaderRetrievalService /leader/resource_manager_lock.
...
2019-10-06 16:07:37,546 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Starting ZooKeeperLeaderRetrievalService /leader/resource_manager_lock.

 {code};;;","07/Oct/19 23:13;stevenz3wu;My colleague [~pgoyal] will attach a patch, which does seems to fix this specific problem.

However, we still don't know how we get here. there is no change in the application/Flink code. and we suddenly start to experience this problem. Maybe some dynamics change in the infrastructure/env that caused this problem to show up.;;;","08/Oct/19 07:28;pgoyal;Added the patch that seemed to help. In addition to the original exception that [~stevenz3wu]  posted, We also observed the following IllegalStateExceptions that made us look into the RegisteredRpcConnection class and try this change. 

__________________________________________

Caused by: java.lang.IllegalStateException: {color:#de350b}The RPC connection is already started{color}
 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:195)
 at org.apache.flink.runtime.registration.RegisteredRpcConnection.start(RegisteredRpcConnection.java:92)
 at org.apache.flink.runtime.taskexecutor.JobLeaderService$JobManagerLeaderListener.notifyLeaderAddress(JobLeaderService.java:327)

_______________________________________________

Caused by: java.lang.IllegalStateException: {color:#de350b}The RPC connection is already closed{color}
 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:195)
 at org.apache.flink.runtime.registration.RegisteredRpcConnection.start(RegisteredRpcConnection.java:91)
 at org.apache.flink.runtime.taskexecutor.JobLeaderService$JobManagerLeaderListener.notifyLeaderAddress(JobLeaderService.java:327)

 

 ;;;","13/Oct/19 20:34;stevenz3wu;[~trohrmann] we would love to hear your thoughts on two specific questions
 # Piyush's patch clearly fixed whatever bug that caused this issue. Do you see any other implication/downside of such a change? if it is good, we can create an official PR to upstream.
 # We still haven't been able to identify the root cause for this bug to show up. This job has been running stable and there is no code change. Any idea?

Some background for this job
 * 235 containers. each with 8 CPUs/slots. parallelism is 1,880. When running into this problem, we also tried 50 containers (with 400 parallelism) and it was still failing.
 * it is a large-state job (a few TBs), although we don't think it matters. we tried to redeploy the job with empty state and it still suffered the same failure loop.;;;","14/Oct/19 13:56;trohrmann;[~stevenz3wu] sorry for being unresponsive the last couple of days. I'll still have this issue on my to do list and will look into it in the next couple of days. I hope to be able to give you an answer to your questions by then.;;;","31/Mar/20 17:29;trohrmann;Hi [~stevenz3wu] and [~pgoyal], sorry for my late reply. 

The logs did not show any specific problems and it looked as if there were some network issues causing the TM to think that the JM lost its leadership. However, what Piyush wrote and the patch itself pointed me into the right direction. The problem is that we don't reset the {{rpcConnection}} in the {{JobManagerLeaderListener}} if the JM loses its leadership. Due to this, Flink will try to reuse the {{rpcConnection}} if a TM learns about the new old leader (JM with the same leader session id). This, however, fails because of the {{checkState}} in {{RegisteredRpcConnection.start}} method because it already contains a {{pendingRegistration}}.

The patch works because it will always create a new {{RpcConnection}}. Alternatively, one could null the {{rpcConnection}} when the leader loses the leadership. I actually fixed this problem in 1.11 (FLINK-16836) because of another problem. I will backport this fix to 1.10 and 1.9.

Last but not least, you might ask why did the problem not occur in the logs you've attached. I think the reason is because it took ZooKeeper long enough to realize that the old JobManager is still the leader so that the slots on the {{TaskExecutor}} could time out. Once all slots belonging a job have timed out, the {{JobManagerLeaderListener}} will be closed.

Thanks again for reporting this issue and sorry for the long waiting time.

;;;","01/Apr/20 01:41;stevenz3wu;[~trohrmann] thanks a lot for the follow-up. we will upgrade to `1.10.1` as soon as it is released.;;;","01/Apr/20 02:31;liyu;1.10.1 is in good shape and ideally the first RC is coming in the next one or two weeks. The status will be updated in [this ML thread|https://s.apache.org/bhquo], JFYI. [~stevenz3wu];;;","02/Apr/20 09:01;trohrmann;Fixed via

1.11.0: 356e2b61f2ddd5cdefbe74b31db41b97210ceec6
1.10.1: 7ee7f23e6c5c972fb9055d82ffe63a2bc0160969
1.9.3: 323c406ef5c1cfccb17fc65053d2d13dc648366d;;;",,,,,,,,,,,,,,,,,,,
NPE with JobMaster.disconnectTaskManager,FLINK-14315,13260142,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,stevenz3wu,stevenz3wu,02/Oct/19 16:44,08/Oct/19 12:18,13/Jul/23 08:10,08/Oct/19 12:18,1.10.0,1.8.2,1.9.0,,,,,,,1.10.0,1.8.3,1.9.2,,Runtime / Task,,,,,0,pull-request-available,,,,"There was some connection issue with zookeeper that caused the job to restart.  But shutdown failed with this fatal NPE, which seems to cause JVM to exit

{code}
2019-10-02 16:16:19,134 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Unable to read additional data from server sessionid 0x16d83374c4206f8, likely server has clo
sed socket, closing socket connection and attempting reconnect
2019-10-02 16:16:19,234 INFO  org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager  - State change: SUSPENDED
2019-10-02 16:16:19,235 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.
2019-10-02 16:16:19,235 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.
2019-10-02 16:16:19,235 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender akka.tcp://flink@100.122.177.82:42043/u
ser/dispatcher no longer participates in the leader election.
2019-10-02 16:16:19,237 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint    - http://100.122.177.82:8081 lost leadership
2019-10-02 16:16:19,237 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager  - ResourceManager akka.tcp://flink@100.122.177.82:42043/user/resourcemanager was revoked leadershi
p. Clearing fencing token.
2019-10-02 16:16:19,237 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Stopping ZooKeeperLeaderRetrievalService /leader/e4e68f2b3fc40c7008cca624b2a2bab0/job_
manager_lock.
2019-10-02 16:16:19,237 WARN  org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore  - ZooKeeper connection SUSPENDING. Changes to the submitted job graphs are not monitored (tem
porarily).
2019-10-02 16:16:19,238 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunner           - JobManager for job ksrouter (e4e68f2b3fc40c7008cca624b2a2bab0) was revoked leadership at akka.tcp:
//flink@100.122.177.82:42043/user/jobmanager_0.
2019-10-02 16:16:19,239 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Stopping ZooKeeperLeaderRetrievalService /leader/resource_manager_lock.
2019-10-02 16:16:19,239 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender http://100.122.177.82:8081 no longer pa
rticipates in the leader election.
2019-10-02 16:16:19,239 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.
2019-10-02 16:16:19,239 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender akka.tcp://flink@100.122.177.82:42043/u
ser/jobmanager_0 no longer participates in the leader election.
2019-10-02 16:16:19,239 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.
2019-10-02 16:16:19,239 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job ksrouter (e4e68f2b3fc40c7008cca624b2a2bab0) switched from state RUNNING to SUSPENDED.
org.apache.flink.util.FlinkException: JobManager is no longer the leader.
        at org.apache.flink.runtime.jobmaster.JobManagerRunner.revokeJobMasterLeadership(JobManagerRunner.java:391)
        at org.apache.flink.runtime.jobmaster.JobManagerRunner.lambda$revokeLeadership$5(JobManagerRunner.java:377)
        at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:981)
        at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2124)
        at org.apache.flink.runtime.jobmaster.JobManagerRunner.revokeLeadership(JobManagerRunner.java:374)
        at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService.notLeader(ZooKeeperLeaderElectionService.java:247)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch$8.apply(LeaderLatch.java:640)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch$8.apply(LeaderLatch.java:636)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93)
        at org.apache.flink.shaded.curator.org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:635)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch.handleStateChange(LeaderLatch.java:623)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch.access$000(LeaderLatch.java:64)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.recipes.leader.LeaderLatch$1.stateChanged(LeaderLatch.java:82)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:259)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:255)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93)
        at org.apache.flink.shaded.curator.org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:253)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)
        at org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:111)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2019-10-02 16:16:19,240 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender akka.tcp://flink@100.122.177.82:42043/u
ser/resourcemanager no longer participates in the leader election.
2019-10-02 16:16:19,239 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Dispatcher akka.tcp://flink@100.122.177.82:42043/user/dispatcher was revoked leadership.
2019-10-02 16:16:19,239 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl  - Suspending the SlotManager.
2019-10-02 16:16:19,240 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Stopping all currently running jobs of dispatcher akka.tcp://flink@100.122.177.82:42043/user/dispa
tcher.
2019-10-02 16:16:19,258 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Stopping checkpoint coordinator for job e4e68f2b3fc40c7008cca624b2a2bab0.
2019-10-02 16:16:19,258 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCompletedCheckpointStore  - Suspending
2019-10-02 16:16:20,076 WARN  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration se
ction named 'Client' was found in specified JAAS configuration file: '/tmp/jaas-6650646464026425406.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeepe
r server allows it.
2019-10-02 16:16:20,076 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Opening socket connection to server 100.66.21.125/100.66.21.125:2181
2019-10-02 16:16:20,076 ERROR org.apache.flink.shaded.curator.org.apache.curator.ConnectionState  - Authentication failed
2019-10-02 16:16:20,077 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Socket connection established to 100.66.21.125/100.66.21.125:2181, initiating session
2019-10-02 16:16:20,080 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Session establishment complete on server 100.66.21.125/100.66.21.125:2181, sessionid = 0x16d8
3374c4206f8, negotiated timeout = 40000
2019-10-02 16:16:20,080 INFO  org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager  - State change: RECONNECTED
2019-10-02 16:16:20,080 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore  - ZooKeeper connection RECONNECTED. Changes to the submitted job graphs are monitored again.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.
2019-10-02 16:16:20,082 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.
2019-10-02 16:16:20,322 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager  - ResourceManager akka.tcp://flink@100.122.177.82:42043/user/resourcemanager was granted leadershi
p with fencing token 94628b472f22083fb4e611d108304613
2019-10-02 16:16:20,322 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl  - Starting the SlotManager.
2019-10-02 16:16:20,326 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint    - http://100.122.177.82:8081 was granted leadership with leaderSessionID=69531adf-a5eb-46d2-99dc-b85
0f66e1af1
2019-10-02 16:16:20,393 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunner           - JobManagerRunner already shutdown.
2019-10-02 16:16:20,393 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Dispatcher akka.tcp://flink@100.122.177.82:42043/user/dispatcher was granted leadership with fenci
ng token abe4ffde-0a18-412b-bffa-28067ccccbeb
2019-10-02 16:16:20,393 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher      - Recovering all persisted jobs.
2019-10-02 16:16:20,407 INFO  com.facebook.presto.s3fs.PrestoS3FileSystem                   - Opening path: s3://us-east-1.ksrouter.dev/recovery/4fa8-1569989613304/444/submittedJobGraphf4c0027
a08cf
2019-10-02 16:16:20,407 INFO  com.facebook.presto.s3fs.PrestoS3FileSystem                   - Seek with new stream for s3://us-east-1.ksrouter.dev/recovery/4fa8-1569989613304/444/submittedJobG
raphf4c0027a08cf to offset 0
2019-10-02 16:16:20,412 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager  - Registering TaskManager with ResourceID 015fd9b87fc4ceb7bb1c40318db0d854 (akka.tcp://flink@100.1
22.134.166:43709/user/taskmanager_0) at ResourceManager
2019-10-02 16:16:20,412 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceReconciler  - Task manager 100.122.134.166 re-registered.
2019-10-02 16:16:20,416 INFO  org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter  - Shutting down.
2019-10-02 16:16:20,416 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job e4e68f2b3fc40c7008cca624b2a2bab0 has been suspended.
2019-10-02 16:16:20,417 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl      - Suspending SlotPool.
2019-10-02 16:16:20,417 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Close ResourceManager connection dda8a9f54ec0239b7e2aa53e0a9b6174: JobManager is no longer the lea
der..
2019-10-02 16:16:20,418 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Stopping the JobMaster for job ksrouter(e4e68f2b3fc40c7008cca624b2a2bab0).
2019-10-02 16:16:20,420 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Stopping ZooKeeperLeaderElectionService ZooKeeperLeaderElectionService{leaderPath='/leader/e4e68f2b3fc40c7008cca624b2a2bab0/job_manager_lock'}.
2019-10-02 16:16:20,431 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceManager  - Registering TaskManager with ResourceID 8d41d9e915b5fe378b3d7cd18042fcff (akka.tcp://flink@100.122.50.217:44327/user/taskmanager_0) at ResourceManager
2019-10-02 16:16:20,431 INFO  com.netflix.spaas.runtime.resourcemanager.TitusResourceReconciler  - Task manager 100.122.50.217 re-registered.
2019-10-02 16:16:20,542 INFO  org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore  - Recovered SubmittedJobGraph(e4e68f2b3fc40c7008cca624b2a2bab0).
2019-10-02 16:16:20,542 INFO  org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore  - Recovered SubmittedJobGraph(e4e68f2b3fc40c7008cca624b2a2bab0).
2019-10-02 16:16:20,542 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Fatal error occurred in the cluster entrypoint.
org.apache.flink.runtime.dispatcher.DispatcherException: Failed to take leadership with session id abe4ffde-0a18-412b-bffa-28067ccccbeb.
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$null$30(Dispatcher.java:915)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:88)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
        at akka.actor.ActorCell.invoke(ActorCell.scala:561)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
        at akka.dispatch.Mailbox.run(Mailbox.scala:225)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.dispatcher.DispatcherException: Termination of previous JobManager for job e4e68f2b3fc40c7008cca624b2a2bab0 failed. Cannot submit job under the same job id.
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$waitForTerminatingJobManager$33(Dispatcher.java:949)
        at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)
        at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:884)
        at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2196)
        at org.apache.flink.runtime.dispatcher.Dispatcher.waitForTerminatingJobManager(Dispatcher.java:946)
        at org.apache.flink.runtime.dispatcher.Dispatcher.tryAcceptLeadershipAndRunJobs(Dispatcher.java:933)
        at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$null$28(Dispatcher.java:892)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
        ... 23 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: Could not properly shut down the JobManagerRunner
        at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)
        at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)
        at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:911)
        at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:899)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.jobmaster.JobManagerRunner.lambda$closeAsync$0(JobManagerRunner.java:207)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.postStop(AkkaRpcActor.java:132)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.postStop(FencedAkkaRpcActor.java:40)
        at akka.actor.Actor$class.aroundPostStop(Actor.scala:536)
        at akka.actor.AbstractActor.aroundPostStop(AbstractActor.scala:225)
        at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:210)
        at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)
        at akka.actor.ActorCell.terminate(ActorCell.scala:429)
        at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:533)
        at akka.actor.ActorCell.systemInvoke(ActorCell.scala:549)
        at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:261)
        ... 6 more
Caused by: org.apache.flink.util.FlinkException: Could not properly shut down the JobManagerRunner
        ... 22 more
Caused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Failure while stopping RpcEndpoint jobmanager_0.
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:513)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:175)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
        at akka.actor.ActorCell.invoke(ActorCell.scala:561)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
        ... 6 more
Caused by: java.lang.NullPointerException
        at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:425)
        at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:343)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:509)
        ... 18 more
{code}",,jark,stevenz3wu,tison,trohrmann,wind_ljy,,,,,,,,,"tillrohrmann commented on pull request #9837: [FLINK-14315] Make heartbeat manager fields non-nullable
URL: https://github.com/apache/flink/pull/9837
 
 
   ## What is the purpose of the change
   
   This commit introduces the `NoOpHeartbeatManager` which can be used to initialize
   an unset heartbeat manager field. This allows to make the heartbeat manager fields
   non-nullable which in turn avoids NPE.
   
   Moreover, this commit makes the heartbeat manager fields of the `TaskExecutor`
   final.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Oct/19 12:02;githubbot;600","tillrohrmann commented on pull request #9837: [FLINK-14315] Make heartbeat manager fields non-nullable
URL: https://github.com/apache/flink/pull/9837
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Oct/19 12:15;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 08 12:18:18 UTC 2019,,,,,,,,,,"0|z078hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Oct/19 17:06;tison;It looks like {{JobMaster#suspend}} races with {{JobMaster#onStop}} which

{{JobMaster#suspend}} set {{taskManagerHeartbeatManager = null}}

while 

{{JobMaster#onStop}} calls {{JobMaster#disconnectTaskManager}} calls {{taskManagerHeartbeatManager.unmonitorTarget(resourceID)}}.

Another perspective is we can properly tolerate zk unstable(connection loss exception) as discussed in FLINK-10052 (it doesn't solve the problem here but make it quite more rare)

CC [~trohrmann];;;","04/Oct/19 11:37;trohrmann;Thanks for reporting this issue [~stevenz3wu]. I think your analysis is correct [~tison]. The problem seems to be that we first suspend the {{JobMaster}} and then shortly afterwards shut it down. This causes the race condition in which we first {{null}} the {{taskManagerHeartbeatManager}} field and the call {{disconnectTaskManager}} which uses this field.

This is clearly a bug and should be fixed. I suggest to do a quick fix and introducing a {{null}} check.

The proper fix is in my opinion to not use a {{JobMaster}} instance across leader sessions. Removing the mutability should solve these kind of problems. The issue to track this effort is FLINK-11719 which I will continue once FLINK-11843 has been completed.;;;","08/Oct/19 12:18;trohrmann;Fixed via

1.10.0: 4064b5b67d6d220e1d5518bca96688f51cbbb891
1.9.2: 8e0d1b5fac2f10f1b34ef82dcc38df8fd83cd82b
1.8.3: 766e25027a88a75002d121c8204766ee76b466d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming File Sink end-to-end test failed on Travis,FLINK-14311,13260061,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,trohrmann,trohrmann,02/Oct/19 08:29,22/Jun/21 13:55,13/Jul/23 08:10,01/Apr/20 13:25,1.10.0,1.11.0,,,,,,,,1.10.1,1.11.0,,,Connectors / FileSystem,Tests,,,,0,pull-request-available,test-stability,,,"The {{Streaming File Sink end-to-end test}} fails on Travis because it does not produce output for 10 minutes.

https://api.travis-ci.org/v3/job/591992274/log.txt",,aljoscha,arvid heise,banmoy,gjy,klion26,liyu,rmetzger,trohrmann,,,,,,"aljoscha commented on pull request #11600: [FLINK-14311] Relax restart checker to harden StreaminFileSink e2e test
URL: https://github.com/apache/flink/pull/11600
 
 
   Before, we were checking for an exact number of restarts, this is too
   strict, because we could have more than the expected number of restarts.
   Now we check whether we have gte the number of expected restarts.
   
   @GJL could you maybe look at this? Or @zentol ?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/20 11:05;githubbot;600","aljoscha commented on pull request #11600: [FLINK-14311] Relax restart checker to harden StreaminFileSink e2e test
URL: https://github.com/apache/flink/pull/11600
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 06:44;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-14555,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 13:25:41 UTC 2020,,,,,,,,,,"0|z07800:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Oct/19 08:29;trohrmann;[~kkloudas] could you take a look at this issue?;;;","08/Oct/19 06:52;liyu;Another instance: https://api.travis-ci.org/v3/job/594192016/log.txt;;;","06/Nov/19 08:59;gjy;Another instance https://api.travis-ci.org/v3/job/607616431/log.txt;;;","06/Nov/19 08:59;gjy;Another instance https://api.travis-ci.org/v3/job/603882577/log.txt;;;","10/Nov/19 02:42;liyu;Another instance: https://api.travis-ci.org/v3/job/609213230/log.txt;;;","12/Nov/19 10:48;gjy;Another instance: https://api.travis-ci.org/v3/job/610323967/log.txt;;;","11/Dec/19 12:43;arvid;I adjusted this test with FLINK-14574 , so I'd probably wait for a new failure until actually debugging it.;;;","19/Dec/19 09:20;banmoy;I have been running the test on travis, and still can't reproduce the failure. Travis build is here [https://travis-ci.com/banmoy/flink/builds] and on branch debug_14311.;;;","20/Dec/19 05:04;liyu;Close the issue since cannot reproduce according to above comments. Thanks [~banmoy] and [~arvid heise] for looking into this.;;;","06/Jan/20 09:28;liyu;Reopen since this reproduces in latest master nightly build: https://api.travis-ci.org/v3/job/632927348/log.txt

Will update the affect and fix version to 1.11.0 since it only reproduces in master branch and not in release-1.10;;;","03/Feb/20 06:21;liyu;Another instance in release-1.10 nightly build: https://api.travis-ci.org/v3/job/645126352/log.txt;;;","04/Feb/20 08:57;gjy;I think the function that waits for the number of restarts will wait indefinitely if {{fullRestarts}} becomes greater than  {{expected_num_restarts}}, which can happen if the TMs startup time is slow.

{noformat}

function wait_for_restart_to_complete {
    local base_num_restarts=$1
    local jobid=$2

    local current_num_restarts=${base_num_restarts}
    local expected_num_restarts=$((current_num_restarts + 1))

    echo ""Waiting for restart to happen""
    while ! [[ ${current_num_restarts} -eq ${expected_num_restarts} ]]; do
        sleep 5
        current_num_restarts=$(get_job_metric ${jobid} ""fullRestarts"")
        if [[ -z ${current_num_restarts} ]]; then
            current_num_restarts=${base_num_restarts}
        fi
    done
}
{noformat};;;","06/Mar/20 19:14;rmetzger;Another case: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6022&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5;;;","08/Mar/20 10:23;rmetzger;Another case: [https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6022&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5];;;","08/Mar/20 16:39;liyu;Another instance in recent release-1.10 cron build: https://api.travis-ci.org/v3/job/658321195/log.txt;;;","09/Mar/20 09:32;rmetzger;Also in the master cron build: [https://travis-ci.org/apache/flink/jobs/659806531?utm_medium=notification&utm_source=slack];;;","13/Mar/20 13:44;liyu;Another instance in release-1.10 crone build: https://api.travis-ci.org/v3/job/661626127/log.txt;;;","24/Mar/20 03:30;liyu;Another instance in release-1.10 crone build: https://api.travis-ci.org/v3/job/665561299/log.txt;;;","26/Mar/20 16:20;liyu;Another instance in release-1.10 crone build: https://api.travis-ci.org/v3/job/666898985/log.txt;;;","01/Apr/20 13:25;aljoscha;I added a (potential) fix. Please re-open if it re-appears.

master: eb12141c25498610a2326dac6409a16246a9d5e3
release-1.10: 97976ed6f6a018603fc93e653ed4c0b479bbf105;;;",,,,,,,,,,,,
Kafka09ProducerITCase>KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink fails on Travis,FLINK-14309,13260048,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wind_ljy,trohrmann,trohrmann,02/Oct/19 05:59,17/Oct/19 14:08,13/Jul/23 08:10,10/Oct/19 20:14,1.10.0,,,,,,,,,1.10.0,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,"The {{Kafka09ProducerITCase>KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink}} fails on Travis with 

{code}
Test testOneToOneAtLeastOnceRegularSink(org.apache.flink.streaming.connectors.kafka.Kafka09ProducerITCase) failed with:
java.lang.AssertionError: Job should fail!
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnce(KafkaProducerTestBase.java:280)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink(KafkaProducerTestBase.java:206)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

https://api.travis-ci.com/v3/job/240747188/log.txt",,aljoscha,becket_qin,trohrmann,wind_ljy,,,,,,,,,,"buptljy commented on pull request #9835: [FLINK-14309] [test-stability] Add retries and acks config in producer test
URL: https://github.com/apache/flink/pull/9835
 
 
   ## What is the purpose of the change
   
   Fix unit test: KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink
   
   ## Brief change log
   
   Add retries and acks config to reduce the risk of failure of flush() method.
   
   
   ## Verifying this change
   
   Unit Testing.
   
   ## Does this pull request potentially affect one of the following parts:
   
   ## Documentation
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Oct/19 05:53;githubbot;600","becketqin commented on pull request #9835: [FLINK-14309] [test-stability] Add retries and acks config in producer test
URL: https://github.com/apache/flink/pull/9835
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Oct/19 20:07;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14308,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 17 14:08:15 UTC 2019,,,,,,,,,,"0|z077x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/19 05:37;wind_ljy;I didn't reproduce the error after tens of times trying. But the most possible reason is that the KafkaProducerSink fails to snapshot(but the checkpoint succeeds) and the job fails after the first checkpoint. Since the flush() method in KafkaProducer didn't have a message guarantee, maybe we can increase the retries config value(default=0) and set acks config value to ""all""(default=0) at first. I'm not completely sure this will help but it would be better if we add these. (the project already add the configs in consumer's tests)

Here is the comments from flush() method:
{code:java}
     * This example shows how to consume from one Kafka topic and produce to another Kafka topic:
     * <pre>
     * {@code
     * for(ConsumerRecord<String, String> record: consumer.poll(100))
     *     producer.send(new ProducerRecord(""my-topic"", record.key(), record.value());
     * producer.flush();
     * consumer.commit();
     * }
     * </pre>
     *
     * Note that the above example may drop records if the produce request fails. If we want to ensure that this does not occur
     * we need to set <code>retries=&lt;large_number&gt;</code> in our config.
{code}
cc [~becket_qin] [~trohrmann];;;","10/Oct/19 19:59;becket_qin;[~wind_ljy] Sorry for the late reply. I agree. In any case, adding retries would help.

BTW, the default value of acks is actually 1 (leader only) instead of 0 (fire and forget). But setting it to all (-1) would help avoid potential churn.;;;","10/Oct/19 20:16;becket_qin;Patch merged to master.

githash: d963eabdbc9ee59f4606c4f9bb6465ee1e88f0e5;;;","11/Oct/19 09:19;trohrmann;Does it make sense to backport this fix to earlier release branches [~becket_qin]?;;;","12/Oct/19 19:30;becket_qin;[~trohrmann] This patch just fixes the test cases. So it is not useful if a branch is not under active development. Do we usually backport test stability fixes to earlier release branches? That may help if we plan to have further bug fix releases on those bug fix branch. But from release perspective, backporting or not seems not making much difference.;;;","15/Oct/19 10:06;trohrmann;We usually do backport test instability fixes to all release branches which are still officially supported and are affected by the test instability. In this case it would be {{release-1.9}} and {{release-1.8}}. I think if this fixes the problems then we should backport it because it will make the release of bug fix releases easier.;;;","17/Oct/19 14:08;becket_qin;[~trohrmann] Thanks for the explanation. It looks that the mostly likely reason for seeing such failure is not the retries, but some cascading failure from the other test failures. I am still working on that to see what was the reason. I'll backport the patch to release-1.8 and release-1.9 when we fix the root cause. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
flink-python build fails with No module named pkg_resources,FLINK-14306,13259884,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dianfu,pnowojski,pnowojski,01/Oct/19 11:27,10/Oct/19 11:40,13/Jul/23 08:10,09/Oct/19 15:29,1.10.0,,,,,,,,,1.10.0,,,,API / Python,Build System,,,,0,pull-request-available,,,,"[Benchmark builds|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks/4576/console] started to fail with

{noformat}
[INFO] Adding generated sources (java): /home/jenkins/workspace/flink-master-benchmarks/flink/flink-python/target/generated-sources
[INFO] 
[INFO] --- exec-maven-plugin:1.5.0:exec (Protos Generation) @ flink-python_2.11 ---
Traceback (most recent call last):
  File ""/home/jenkins/workspace/flink-master-benchmarks/flink/flink-python/pyflink/gen_protos.py"", line 33, in <module>
    import pkg_resources
ImportError: No module named pkg_resources
[ERROR] Command execution failed.
(...)
[INFO] flink-state-processor-api .......................... SUCCESS [  0.299 s]
[INFO] flink-python ....................................... FAILURE [  0.434 s]
[INFO] flink-scala-shell .................................. SKIPPED
{noformat}

because of this ticket: https://issues.apache.org/jira/browse/FLINK-14018

I think I can solve the benchmark builds failing quite easily by installing {{setuptools}} python package, so this ticket is not about this, but about deciding how should we treat such kind of external dependencies. I don't see this dependency being mentioned anywhere in the documentation ([for example here|https://ci.apache.org/projects/flink/flink-docs-stable/flinkDev/building.html]).

Probably at the very least those external dependencies should be documented, but also I fear about such kind of manual steps to do before building the Flink can become a problem if grow out of control. Some questions:

# Do we really need this dependency?
# Could this dependency be resolve automatically? By installing into a local python virtual environment?
# Should we document those dependencies somewhere?
# Maybe we should not build flink-python by default?
# Maybe we should add a pre-build script for flink-python to verify the dependencies and to throw an easy to understand error with hint how to fix it?

CC [~hequn] [~dian.fu] [~trohrmann] [~jincheng]",,aljoscha,dian.fu,hequn8128,pnowojski,trohrmann,,,,,,,,,"dianfu commented on pull request #9855: [FLINK-14306][python] Add the code-generated flink_fn_execution_pb2.py to the source code
URL: https://github.com/apache/flink/pull/9855
 
 
   
   ## What is the purpose of the change
   
   *This pull request fixes the flink-python build fails of ""No module named pkg_resources"" by adding the code-generated flink_fn_execution_pb2.py to the Flink codebase.*
   
   ## Brief change log
   
     - *Adds flink_fn_execution_pb2.py to Flink codebase*
     - *Removes the build process of flink_fn_execution_pb2.py from the pom file of flink-python*
     - *Adds tests test_flink_fn_execution_pb2_synced to make sure that flink_fn_execution_pb2.py is synced with flink_fn_execution.proto*
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added tests test_flink_fn_execution_pb2_synced*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Oct/19 12:27;githubbot;600","hequn8128 commented on pull request #9855: [FLINK-14306][python] Add the code-generated flink_fn_execution_pb2.py to the codebase
URL: https://github.com/apache/flink/pull/9855
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Oct/19 15:28;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-14018,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 10 11:40:50 UTC 2019,,,,,,,,,,"0|z076wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/19 12:57;pnowojski;After manually installing {{python-pip}} (this installed {{setuptools}} as well) on the machine I'm getting a new error:

{noformat}
[INFO] --- exec-maven-plugin:1.5.0:exec (Protos Generation) @ flink-python_2.11 ---
/home/jenkins/workspace/flink-master-benchmarks/flink/flink-python/pyflink/gen_protos.py:49: UserWarning: Installing grpcio-tools is recommended for development.
  warnings.warn('Installing grpcio-tools is recommended for development.')
WARNING:root:Installing grpcio-tools into /home/jenkins/workspace/flink-master-benchmarks/flink/flink-python/pyflink/../.eggs/grpcio-wheels
ERROR: Can not combine '--user' and '--prefix' as they imply different installation locations
You are using pip version 8.1.1, however version 19.2.3 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Process Process-1:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/usr/lib/python2.7/multiprocessing/process.py"", line 114, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/jenkins/workspace/flink-master-benchmarks/flink/flink-python/pyflink/gen_protos.py"", line 126, in _install_grpcio_tools_and_generate_proto_files
    '--upgrade', GRPC_TOOLS, ""-I""])
  File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call
    raise CalledProcessError(retcode, cmd)
CalledProcessError: Command '['/usr/bin/python', '-m', 'pip', 'install', '--prefix', '/home/jenkins/workspace/flink-master-benchmarks/flink/flink-python/pyflink/../.eggs/grpcio-wheels', '--build', '/home/jenkins/workspace/flink-master-benchmarks/flink/flink-python/pyflink/../.eggs/grpcio-wheels-build', '--upgrade', 'grpcio-tools>=1.3.5,<=1.14.2', '-I']' returned non-zero exit status 1
Traceback (most recent call last):
  File ""/home/jenkins/workspace/flink-master-benchmarks/flink/flink-python/pyflink/gen_protos.py"", line 146, in <module>
    generate_proto_files(force=True)
  File ""/home/jenkins/workspace/flink-master-benchmarks/flink/flink-python/pyflink/gen_protos.py"", line 91, in generate_proto_files
    raise ValueError(""Proto generation failed (see log for details)."")
ValueError: Proto generation failed (see log for details).
[ERROR] Command execution failed.
org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)
	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
	at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:166)
	at org.codehaus.mojo.exec.ExecMojo.executeCommandLine(ExecMojo.java:764)
	at org.codehaus.mojo.exec.ExecMojo.executeCommandLine(ExecMojo.java:711)
	at org.codehaus.mojo.exec.ExecMojo.execute(ExecMojo.java:289)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:154)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:146)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:309)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:194)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:107)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:993)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:345)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:191)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
{noformat}
;;;","01/Oct/19 13:37;pnowojski;Upgrading the {{pip}} to the latest version solved the above problem.;;;","02/Oct/19 09:16;chesnay;does this mean that everyone needs pip/setuptools on their machines to build flink now?;;;","02/Oct/19 09:32;trohrmann;Thanks for reporting this issue [~pnowojski]. I agree with you that the documentation should always be up to date with the current requirements. Moreover, I would be more in favour of not failing the build if flink-python fails. If this should be the case, then one could report it but continue with the remaining modules. Maybe one could have a switch saying that flink-python is required and the build would fail if this switch is provided. But by default, people not interested in Python should not be bothered by it.;;;","02/Oct/19 09:49;pnowojski;It looks like so [~chesnay]. I guess most of the developers wouldn't notice this dependency. Building flink on some servers/data centers/virtual machines is a different story.

[~trohrmann] that's another work around. If we could avoid this dependency altogether that would be great, but also having {{pip}} and/or {{setuptools}} as dependency when building something python related sounds reasonable. 

Things might complicate a bit, if some modules start depending on {{flink-python}} (end to end tests?).;;;","04/Oct/19 11:03;chesnay;Note that this also broke the website generation since python isn't installed on buildbot. I'm inclined to revert the responsible commits because they are creating too many problems imo.;;;","04/Oct/19 11:04;chesnay;cc [~hequn8128] [~dian.fu];;;","05/Oct/19 04:12;hequn8128;Hi [~pnowojski] [~chesnay] [~trohrmann], sorry for the trouble that brings to you and many thanks for the advice.

The problem is caused by the plugin in the pom under flink-python. The plugin calls gen_protos.py to generate python files in pyflink.zip. This introduces python dependencies and causes builds failing. These dependencies are necessary because we don't want to provide a semi-finished package(i.e., the package without the generated python files). 

[~dianfu] and I discussed offline and we think it's better to use local virtualenv to solve the problem so that the dependencies can be resolved automatically and we don't need to document it either. This is also mentioned by [~pnowojski] above. 

The virtual env solution may take a couple of days(we should also take the builds under windows into consideration). Before this, we can create a hotfix to remove the plugin which calls gen_protos.py to unblock these build failures asap.

What do you guys think? [~pnowojski][~chesnay][~trohrmann];;;","06/Oct/19 06:37;pnowojski;From my perspective there is no need to revert this. I've fixed manually the benchmarking CI ([~chesnay] can you do the same thing for the buildbot?).

Can this issue be solved by using {{virtualenv}}? I've always thought that you install {{python-pip}} outside of the {{virtualenv}}. Secondly, doesn't using the {{virtualenv}} require some dependencies?
  
If so, as I mentioned before, as long as we keep python code tree separated (Python's end to end test shouldn't mix with the non python tests) and as long as we won't keep adding more and more dependencies I think it would be good enough to:
* don't build {{flink-python}} by default
* document dependency on the {{pip}} either in the main README or {{flink-python}}'s README.

After all, {{pip}} and {{python}} can be think of for {{flink-python}} the same type of ""core dependency"" as {{mvn}} and {{java}} are for the rest of Flink.;;;","06/Oct/19 12:05;hequn8128;[~pnowojski] Yes, virtualenv requires some dependencies. We can address all these dependencies automatically. For example, we can download all dependencies and install them, similar to the behavior in {{lint-python.sh}} under flink-python/dev. Currently, the {{lint-python.sh}} is used to run python tests and generate python docs.

For this problem, there are probably two options to solve it:

 1. Don't build {{flink-python}} by default. For example, only build python if we use the command {{mvn clean install -Ppython}}. If {{-Ppython}} is provided, we can use a script to download and install dependencies automatically. We can use virtualenv to manage these dependencies. One thing needs to be noticed is the compilation needs to be done in a networked environment.

2. Avoid dependencies. Currently, the dependencies are introduced by the {{gen_protos.py}}. It is used to generate python files during the building. To remove the dependencies, we can generate python files manually and add the generated python files into the git. Currently, there is only one generated file(flink_fn_execution_pb2.py, 493 lines of code in total) and the file is seldom altered. Furthermore, there are probably no more generated python files.

Personally, I prefer the second option not only we can avoid these dependencies but also makes the build more simple, i.e., without additional flags and users will not make a mistake on it. The second option also has no strong requirement for the network during the build. 
Does it make sense to you? [~pnowojski] [~trohrmann][~chesnay];;;","07/Oct/19 07:37;chesnay;Option 2 sounds fine to me.

I was suggesting to revert the commit because the website hasn't been built for several days, and it is significantly faster to revert commits than to add another dependency to buildbot.;;;","07/Oct/19 08:24;pnowojski;Option 2 sounds also fine for me for now. If this will become an issue at some point, we can always revisit it.

Just for the record [~chesnay], I'm not against reverting. It makes even more sense, if we will decide that we are getting rid of this dependency in the long run. In my case it boiled down to:

{code:bash}
root> apt install python-pip
root> su jenkins
jenkins> pip install --upgrade pip
{code};;;","08/Oct/19 06:54;dian.fu;Sorry for this problem caused by my commit and also thanks for your discussion.

I also prefer option 2. Besides, I think there should be no license problems adding code-generated protobuf files to the source code per the [LICENSE|https://github.com/protocolbuffers/protobuf/blob/master/LICENSE] statement of protobuf. [~chesnay] Could you help to confirm this as I think you are more experienced at this?;;;","08/Oct/19 08:13;chesnay;Shouldn't be a problem.;;;","08/Oct/19 08:24;dian.fu;[~chesnay] Thanks for your confirmation. Seems that everyone agrees on adding the code-generated file to the source code to solve this problem. I will provide a fix ASAP. Just as [~pnowojski] said, we can revisit it if we find there are problems for this solution or there are other better solutions in the future.;;;","08/Oct/19 15:03;hequn8128;Thank you all for your valuable feedbacks. Also thank you [~dian.fu] for the PR. I will take a look ASAP.;;;","09/Oct/19 15:29;hequn8128;Fixed in 1.10.0 via df44cce7f1e1bc42715e6cd0e7268c1a46b07af8;;;","10/Oct/19 11:40;pnowojski;Thanks [~hequn8128] and [~dianfu] for quickly fixing this issue.;;;",,,,,,,,,,,,,,
FlinkKafkaInternalProducer should not send `ADD_PARTITIONS_TO_TXN` request if `newPartitionsInTransaction` is empty when enable EoS,FLINK-14302,13259834,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tonywei,tonywei,tonywei,01/Oct/19 08:04,31/Oct/19 10:27,13/Jul/23 08:10,31/Oct/19 10:27,,,,,,,,,,1.10.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"As the survey in this mailing list thread [1], kafka server will bind the error with topic-partition list when it handles `AddPartitionToTxnRequest`. So when the request body contains no topic-partition, the error won't be sent back to kafka producer client. Moreover, it producer internal api, it always check if `newPartitionsInTransaction` is empty before sending ADD_PARTITIONS_TO_TXN request to kafka cluster. We should apply it as well if you need to explicitly call it in the first commit phase of two-phase commit sink.

[1] [http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Kafka-producer-failed-with-InvalidTxnStateException-when-performing-commit-transaction-td29384.html]",,becket_qin,tonywei,,,,,,,,,,,,"tony810430 commented on pull request #9824: [FLINK-14302] FlinkKafkaInternalProducer should not send `ADD_PARTITIONS_TO_TXN` request if `newPartitionsInTransaction` is empty when enable EoS
URL: https://github.com/apache/flink/pull/9824
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   - Check `newPartitionsInTransaction` in kafka producer is empty before sending `ADD_PARTITIONS_TO_TXN` request to kafka cluster.
   
   ## Verifying this change
   
    - This bug can be reproduced in real environment that mentioned in the original mailing list thread, but I couldn't find out how to reproduce it in embedded kafka cluster flink integration test environment.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): don't know
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: don't know
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not documented
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Oct/19 08:12;githubbot;600","becketqin commented on pull request #9824: [FLINK-14302] [kafka] FlinkKafkaInternalProducer should not send `ADD_PARTITIONS_TO_TXN` request if `newPartitionsInTransaction` is empty when enable EoS
URL: https://github.com/apache/flink/pull/9824
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Oct/19 09:58;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 09 06:27:58 UTC 2019,,,,,,,,,,"0|z076lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/19 08:44;tonywei;Sorry for that I didn't notice that the contribution flow has changed. And I already sent a PR for this issue, but couldn't assign it to myself.

Hi [~becket_qin] 
 If you have free time, please help me to verify if the issue in mailing list thread I mentioned is a bug, and should it be fix like I suggested?
 If this is a bug, and the solution is good to you, please assign this Jira issue to me. Thanks.;;;","06/Oct/19 23:47;becket_qin;[~tonywei] Really sorry for the late response. I was on a business trip and just saw this ping. Thanks for keeping debugging this. The current way Flink constructs the KafkaProducer on recovery is indeed a little flaky. I am not sure if the NOT_COORDINATOR error code was caused by empty partition list in AddPartitionsToTxnRequest.

More specifically, I am not sure why the following log was printed. 
 
*kafka-broker-1:*
{quote} [2019-09-20 02:31:46,182] INFO [TransactionCoordinator id=1] Initialized transactionalId map -> Sink: sink-2e588ce1c86a9d46e2e85186773ce4fd-3 with producerId 1008 and producer epoch 1 on partition __transaction_state-37 (kafka.coordinator.transaction.TransactionCoordinator){quote}
{quote}*[2019-09-20 02:32:45,962] DEBUG [TransactionCoordinator id=1] Returning NOT_COORDINATOR error code to client for map -> Sink: sink-2e588ce1c86a9d46e2e85186773ce4fd-3's AddPartitions request (kafka.coordinator.transaction.TransactionCoordinator)*
[2019-09-20 02:32:46,453] DEBUG [TransactionCoordinator id=1] Aborting append of COMMIT to transaction log with coordinator and returning NOT_COORDINATOR error to client for map -> Sink: sink-2e588ce1c86a9d46e2e85186773ce4fd-3's EndTransaction request (kafka.coordinator.transaction.TransactionCoordinator){quote}
Can you double check the version of the Kafka broker? I'd like to reproduce the problem if possible.

Thanks.;;;","07/Oct/19 01:00;tonywei;Hi [~becket_qin] 

The root cause of NOT_COORDINATOR is not from empty  partition list.

It is due to that broker who host the original transaction coordinator was restarted. The error should be propagate to client to refetch new coordinator, but the error will be bind with each element of partition list. Since this list was empty, the error got lost in response message. And that lead to the following commit failed. 

For more information, you can refer to my replys in the mailing thread. Thanks. ;;;","08/Oct/19 18:46;becket_qin;[~tonywei] Thanks for the explanation. I am still a little confused. From the source code it looks the log I mentioned should be printed in [1], but it looks that this logging should never see NOT_COORDINATOR error code.

In any case, I think it is a good idea to ensure on the Flink side we are not committing empty transactions. However, technically speaking, the Kafka brokers should not assume the behavior of the clients because there could be 3rd party Kafka clients implementations. So I think there should be a fix on the Kafka side as well. But that might be a protocol change to add an error field in {{AddPartitionsToTxnResponse}}. We can leave it as is for now.

[https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala#L268];;;","09/Oct/19 01:51;tonywei;Hi [~becket_qin] 

I think the error was from `txnManager.getTransactionState(transactionalId)` [1].

You are right. Maybe we can report this issue back to kafka community as well. For now, could we conclude the temporary solution that we should prevent from committing empty transaction? I have opened a pull request already. Could you help me review it and give me advice? I had a problem about how to add an integration test on it. Thanks.

[1] [https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala#L239];;;","09/Oct/19 06:27;becket_qin;[~tonywei] Ah, you are right. I apparently misunderstood the behavior of Scala Either here. 

I just left some ideas on how to test this case in the PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTask#invoke leaks threads if OperatorChain fails to be constructed,FLINK-14300,13259727,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,mklein0,mklein0,30/Sep/19 17:40,11/Oct/19 12:38,13/Jul/23 08:10,09/Oct/19 13:57,1.8.1,1.8.2,1.9.0,,,,,,,1.10.0,,,,Runtime / Task,,,,,0,pull-request-available,,,,"In the *StreamTask#invoke* method if an exception occurs during the allocation of the [operatorChain|[https://github.com/apache/flink/blob/release-1.9.0/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L370]] class, the [exception handling|[https://github.com/apache/flink/blob/release-1.9.0/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L485-L491]] fails to cleanup the threads allocated as *StreamTask#recordWriters*. This causes threads to leak as flink attempts to continually restart and fail for the same cause.

 

An example cause is a deserialization issue on a custom operator from a checkpoint.

 

Attached is a suggested fix for the master branch.",,aljoscha,hwanju,mklein0,pnowojski,,,,,,,,,,"mklein0 commented on pull request #9857: [FLINK-14300][Runtime / Task] Cleanup operator threads in case StreamTask fails to al…
URL: https://github.com/apache/flink/pull/9857
 
 
   …locate operatorChain
   
   ## What is the purpose of the change
   
   This pull request fixes a thread leak on the task manager when the StreamTask class fails to deserialize an operator when instantiating the operatorChain property. The error handling code cleans up all operator threads if the OperatorChain class is instantiated, but fails to clean up threads created before the OperatorChain class is instantiated if the operatorChain property is null.
   
   An example of a deserialization exception thrown when instantiating the OperatorChain object is as follows:
   ```
   org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function.
           at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperator(StreamConfig.java:239)
           at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:104)
           at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:267)
           at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
           at java.lang.Thread.run(Thread.java:748)
   Caused by: java.io.InvalidClassException: com.example.DBWriter; local class incompatible: stream classdesc serialVersionUID = -3648304528349867488, local class serialVersionUID = -7371568206393076603
   	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)
   	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)
   	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)
   	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)
   	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
   	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)
   	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)
   	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)
   	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)
   	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
   	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:566)
   	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:552)
   	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:540)
   	at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:501)
   	at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperator(StreamConfig.java:224)
   ```
   
   ## Brief change log
   
     - Stop leaking OutputFlusher threads if a flink job fails to deserialize an intermediate operator in a StreamTask operator chain.
   
   ## Verifying this change
   
   This change can be verified as follows:
     - Create a User Defined Function which can be serialized, but fails to deserialize. I think reusing the base class definition of `serialVersionUID` will do it.
     - Create a 2-3 operator stream with badly serialized operator in every position of the chain.
     - When the serialization exception occurs, the `recordWriters` property should be holding an *OutputFlusher* thread which will be left to run because the `operatorChain` instance is not there to clean it up.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies: no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes, should improve job restarts by stopping thread leak.
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Oct/19 23:53;githubbot;600","pnowojski commented on pull request #9857: [FLINK-14300][Runtime / Task] Cleanup operator threads in case StreamTask fails to al…
URL: https://github.com/apache/flink/pull/9857
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Oct/19 13:56;githubbot;600","pnowojski commented on pull request #9879: [FLINK-14300][runtime, test] Add test making sure that RecordWriter is properly closed in case of early StreamTask failure
URL: https://github.com/apache/flink/pull/9879
 
 
   This adds a missing test coverage for the previously merged https://github.com/apache/flink/pull/9857 pull request.
   ## Verifying this change
   
   This PR only adds a new test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
   CC @mklein0 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Oct/19 12:40;githubbot;600","pnowojski commented on pull request #9879: [FLINK-14300][runtime, test] Add test making sure that RecordWriter is properly closed in case of early StreamTask failure
URL: https://github.com/apache/flink/pull/9879
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Oct/19 12:38;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/19 17:39;mklein0;thread-leak-patch.diff;https://issues.apache.org/jira/secure/attachment/12981819/thread-leak-patch.diff",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 09 13:57:11 UTC 2019,,,,,,,,,,"0|z075xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Oct/19 12:10;pnowojski;Hi [~mklein0]. Thank you for reporting and proposing a fix for this bug.

Could you open a pull request for this change on the [github|https://github.com/apache/flink/]? If you do so, please notify me so I can quickly review it.;;;","08/Oct/19 23:56;mklein0;Hi [~pnowojski], I have create a Pull Request on github as requested. I am unable to put together a concise unit test for the issue which is why I only made  ticket.;;;","09/Oct/19 13:57;pnowojski;Merged to master as commit 0d112f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Py4j  NOTICE for source release,FLINK-14288,13259559,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dian.fu,sunjincheng121,sunjincheng121,30/Sep/19 01:15,30/Sep/19 03:31,13/Jul/23 08:10,30/Sep/19 03:31,1.9.0,,,,,,,,,1.10.0,1.9.1,,,API / Python,,,,,0,pull-request-available,,,,I just found that we should add Py4j  NOTICE for source release. ,,dian.fu,hequn8128,sunjincheng121,,,,,,,,,,,"dianfu commented on pull request #9816: [FLINK-14288][legal] Add Py4J NOTICE for source release
URL: https://github.com/apache/flink/pull/9816
 
 
   
   ## What is the purpose of the change
   
   *This pull request adds Py4J to the NOTICE as it's bundled in the source release*
   
   
   ## Brief change log
   
     - *Adds Py4J to the NOTICE file*
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Sep/19 01:30;githubbot;600","hequn8128 commented on pull request #9816: [FLINK-14288][legal] Add Py4J NOTICE for source release
URL: https://github.com/apache/flink/pull/9816
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Sep/19 03:28;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 30 03:31:04 UTC 2019,,,,,,,,,,"0|z074wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/19 01:22;dian.fu;Hi [~sunjincheng121], good catch! I'd like to take this issue. Would you please assign it to me?;;;","30/Sep/19 01:24;sunjincheng121;Done, great thanks to take this ticket. [~dian.fu];;;","30/Sep/19 03:30;hequn8128;[~sunjincheng121] [~dian.fu] thanks a lot for reporting and fixing the problem. Merged.;;;","30/Sep/19 03:31;hequn8128;Fixed in
1.9.1: c9c10d84f063a2a2bcf98563891e43cae2e95697
1.10.0: 2ac48f0730c42121f5f88daec79209c2fe9cbc2d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala quickstart project does not compile on Java9+,FLINK-14276,13259502,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,trohrmann,trohrmann,29/Sep/19 09:46,04/Oct/19 13:37,13/Jul/23 08:10,04/Oct/19 13:37,1.10.0,,,,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,test-stability,,,"The {{Quickstarts Scala nightly end-to-end test}} fails on Travis when running the {{e2e - misc - jdk11}} profile. The failure cause is

{code}
19:32:57.344 [ERROR] error: java.lang.NoClassDefFoundError: javax/tools/ToolProvider
19:32:57.344 [INFO] 	at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)
19:32:57.344 [INFO] 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
19:32:57.344 [INFO] 	at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)
19:32:57.344 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)
19:32:57.344 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)
19:32:57.344 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)
19:32:57.344 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)
19:32:57.344 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)
19:32:57.344 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)
19:32:57.345 [INFO] 	at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)
19:32:57.345 [INFO] 	at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)
19:32:57.345 [INFO] 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
19:32:57.345 [INFO] 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
19:32:57.345 [INFO] 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
19:32:57.345 [INFO] 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
19:32:57.345 [INFO] 	at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)
19:32:57.345 [INFO] 	at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)
19:32:57.345 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)
19:32:57.345 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)
19:32:57.345 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)
19:32:57.345 [INFO] 	at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)
19:32:57.346 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)
19:32:57.346 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)
19:32:57.346 [INFO] 	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)
19:32:57.346 [INFO] 	at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)
19:32:57.346 [INFO] 	at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)
19:32:57.346 [INFO] 	at scala.tools.nsc.Global.rootMirror(Global.scala:71)
19:32:57.346 [INFO] 	at scala.tools.nsc.Global.rootMirror(Global.scala:39)
19:32:57.346 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)
19:32:57.346 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)
19:32:57.346 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)
19:32:57.346 [INFO] 	at scala.tools.nsc.Global$Run.<init>(Global.scala:1242)
19:32:57.346 [INFO] 	at scala.tools.nsc.Driver.doCompile(Driver.scala:31)
19:32:57.346 [INFO] 	at scala.tools.nsc.MainClass.doCompile(Main.scala:23)
19:32:57.346 [INFO] 	at scala.tools.nsc.Driver.process(Driver.scala:51)
19:32:57.346 [INFO] 	at scala.tools.nsc.Driver.main(Driver.scala:64)
19:32:57.347 [INFO] 	at scala.tools.nsc.Main.main(Main.scala)
19:32:57.347 [INFO] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
19:32:57.347 [INFO] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
19:32:57.347 [INFO] 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
19:32:57.347 [INFO] 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
19:32:57.347 [INFO] 	at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)
19:32:57.347 [INFO] 	at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)
19:32:57.347 [INFO] java.lang.reflect.InvocationTargetException
19:32:57.347 [INFO] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
19:32:57.347 [INFO] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
19:32:57.347 [INFO] 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
19:32:57.347 [INFO] 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
19:32:57.348 [INFO] 	at scala_maven_executions.MainHelper.runMain(MainHelper.java:164)
19:32:57.348 [INFO] 	at scala_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)
19:32:57.348 [ERROR] Caused by: java.lang.NoClassDefFoundError: javax/tools/ToolProvider
19:32:57.348 [INFO] 	at scala.reflect.io.JavaToolsPlatformArchive.iterator(ZipArchive.scala:301)
19:32:57.348 [INFO] 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
19:32:57.349 [INFO] 	at scala.reflect.io.AbstractFile.foreach(AbstractFile.scala:92)
19:32:57.349 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.traverse(ClassPath.scala:277)
19:32:57.349 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.x$15$lzycompute(ClassPath.scala:299)
19:32:57.349 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.x$15(ClassPath.scala:299)
19:32:57.349 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages$lzycompute(ClassPath.scala:299)
19:32:57.350 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:299)
19:32:57.350 [INFO] 	at scala.tools.nsc.util.DirectoryClassPath.packages(ClassPath.scala:264)
19:32:57.350 [INFO] 	at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)
19:32:57.351 [INFO] 	at scala.tools.nsc.util.MergedClassPath$$anonfun$packages$1.apply(ClassPath.scala:358)
19:32:57.351 [INFO] 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
19:32:57.351 [INFO] 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
19:32:57.352 [INFO] 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
19:32:57.352 [INFO] 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
19:32:57.352 [INFO] 	at scala.tools.nsc.util.MergedClassPath.packages$lzycompute(ClassPath.scala:358)
19:32:57.352 [INFO] 	at scala.tools.nsc.util.MergedClassPath.packages(ClassPath.scala:353)
19:32:57.352 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply$mcV$sp(SymbolLoaders.scala:269)
19:32:57.352 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)
19:32:57.352 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader$$anonfun$doComplete$1.apply(SymbolLoaders.scala:260)
19:32:57.352 [INFO] 	at scala.reflect.internal.SymbolTable.enteringPhase(SymbolTable.scala:235)
19:32:57.352 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$PackageLoader.doComplete(SymbolLoaders.scala:260)
19:32:57.352 [INFO] 	at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:211)
19:32:57.352 [INFO] 	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)
19:32:57.352 [INFO] 	at scala.reflect.internal.Mirrors$RootsBase.init(Mirrors.scala:256)
19:32:57.352 [INFO] 	at scala.tools.nsc.Global.rootMirror$lzycompute(Global.scala:73)
19:32:57.352 [INFO] 	at scala.tools.nsc.Global.rootMirror(Global.scala:71)
19:32:57.352 [INFO] 	at scala.tools.nsc.Global.rootMirror(Global.scala:39)
19:32:57.352 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)
19:32:57.352 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)
19:32:57.353 [INFO] 	at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)
19:32:57.353 [INFO] 	at scala.tools.nsc.Global$Run.<init>(Global.scala:1242)
19:32:57.353 [INFO] 	at scala.tools.nsc.Driver.doCompile(Driver.scala:31)
19:32:57.353 [INFO] 	at scala.tools.nsc.MainClass.doCompile(Main.scala:23)
19:32:57.353 [INFO] 	at scala.tools.nsc.Driver.process(Driver.scala:51)
19:32:57.353 [INFO] 	at scala.tools.nsc.Driver.main(Driver.scala:64)
19:32:57.353 [INFO] 	at scala.tools.nsc.Main.main(Main.scala)
19:32:57.353 [INFO] 	... 6 more
{code}

https://api.travis-ci.org/v3/job/590390311/log.txt

The issue might be a Java 11 problem and might be fixable with passing {{-nobootcp}} to the scala compiler plugin.",,trohrmann,,,,,,,,,,,,,"zentol commented on pull request #9833: [FLINK-14276][quickstarts] Scala quickstart compiles on JDK 11
URL: https://github.com/apache/flink/pull/9833
 
 
   Fixes an issue where a scala quickstart project would not compile on Java 9+. Same approach as in #6623.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Oct/19 09:58;githubbot;600","zentol commented on pull request #9833: [FLINK-14276][quickstarts] Scala quickstart compiles on JDK 11
URL: https://github.com/apache/flink/pull/9833
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Oct/19 13:37;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-14294,,,,,,,,,,FLINK-9781,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 04 13:37:17 UTC 2019,,,,,,,,,,"0|z074js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Oct/19 08:26;trohrmann;Another instance: https://api.travis-ci.org/v3/job/591992292/log.txt

[~chesnay] could you take a look?;;;","04/Oct/19 13:37;chesnay;master: 3f495d8215e1276ad91de5e4a5b6f160ba15eb13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka010ProducerITCase>KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink fails on Travis,FLINK-14245,13259213,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,trohrmann,trohrmann,27/Sep/19 08:41,22/Oct/19 14:17,13/Jul/23 08:10,22/Oct/19 14:17,1.10.0,,,,,,,,,1.10.0,,,,Connectors / Kafka,,,,,0,test-stability,,,,"The {{Kafka010ProducerITCase>KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink}} fails on Travis with

{code}
Test testOneToOneAtLeastOnceRegularSink(org.apache.flink.streaming.connectors.kafka.Kafka010ProducerITCase) failed with:
java.lang.AssertionError: Job should fail!
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnce(KafkaProducerTestBase.java:280)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink(KafkaProducerTestBase.java:206)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

https://api.travis-ci.com/v3/job/239463674/log.txt",,gaofeilong,pnowojski,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14235,,,,,,,FLINK-14224,FLINK-14235,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 22 14:17:57 UTC 2019,,,,,,,,,,"0|z072rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/19 08:41;trohrmann;Might be related to FLINK-14224.;;;","17/Oct/19 12:56;trohrmann;Another instance: https://api.travis-ci.com/v3/job/246637212/log.txt;;;","22/Oct/19 14:17;pnowojski;This was probably a duplicated issue of https://issues.apache.org/jira/browse/FLINK-14235

I hope this issue should be fixed by *c31e44e* (merged to the master branch). Closing the ticket for now, please re-open if you get the same failure message in the same test on a branch that includes this fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Emitting the max watermark in StreamSource#run may cause it to arrive the downstream early,FLINK-14239,13259172,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sunhaibotb,sunhaibotb,sunhaibotb,27/Sep/19 02:34,14/Nov/19 12:55,13/Jul/23 08:10,14/Nov/19 12:55,,,,,,,,,,1.10.0,,,,API / DataStream,,,,,0,pull-request-available,,,,"For {{Source}}, the max watermark is emitted in {{StreamSource#run}} currently. If some records are also output in {{close}} of {{RichSourceFunction}}, then the max watermark will reach the downstream operator before these records.",,aljoscha,jark,sunhaibotb,,,,,,,,,,,"sunhaibotb commented on pull request #9804: [FLINK-14239] Fix the max watermark in StreamSource may arrive the downstream operator early
URL: https://github.com/apache/flink/pull/9804
 
 
   ## What is the purpose of the change
   
   This pull request fixes the problem that the max watermark in `StreamSource` may arrive the downstream operator early.
   
   For `Source`, the max watermark is emitted in `StreamSource#run currently`. If some records are also output in `close` of `RichSourceFunction`, then the max watermark will reach the downstream operator before these records.
   
   ## Brief change log
   
     - Modify `StreamSource` to move the logic of emitting the max watermark to `close` from the `run` method.
   
   ## Verifying this change
   
   This change modifies some existing tests and can be verified as follows:
   
     - When some records are output in the `close` of `RichSourceFunction`, the max watermark also will arrive the downstream operator after the last record.
     - Do not emit the max watermark when the task is cancelled asynchronously.
     - Do not emit the max watermark when the task is cancelled immediately after it is created.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/19 11:15;githubbot;600","aljoscha commented on pull request #9804: [FLINK-14239] Fix the max watermark in StreamSource may arrive the downstream operator early
URL: https://github.com/apache/flink/pull/9804
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Nov/19 12:55;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 14 12:55:30 UTC 2019,,,,,,,,,,"0|z072i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/19 12:55;aljoscha;Fixed on master in 3171edfe78a7f9feea88488273bf4341eee7b7f0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka010ProducerITCase>KafkaProducerTestBase.testOneToOneAtLeastOnceCustomOperator fails on travis,FLINK-14235,13259073,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,26/Sep/19 14:44,21/May/20 08:40,13/Jul/23 08:10,22/Oct/19 14:15,1.10.0,,,,,,,,,1.10.0,1.8.3,1.9.2,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,"Fails with following message:

{noformat}
 Expected to contain all of: <[0]>, but was: <[]>
{noformat}

with the following stack trace:

{noformat}
Test testOneToOneAtLeastOnceCustomOperator(org.apache.flink.streaming.connectors.kafka.Kafka010ProducerITCase) failed with:
java.lang.AssertionError: Expected to contain all of: <[0]>, but was: <[]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.assertAtLeastOnceForTopic(KafkaTestBase.java:235)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnce(KafkaProducerTestBase.java:289)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnceCustomOperator(KafkaProducerTestBase.java:214)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}


https://api.travis-ci.com/v3/job/239300010/log.txt

This might be related to https://issues.apache.org/jira/browse/FLINK-14224 but the failure message is different (here it's data loss, there the job has failed)",,dian.fu,gaofeilong,gjy,hequn8128,klion26,pnowojski,trohrmann,,,,,,,"pnowojski commented on pull request #9959: [FLINK-14235][kafka,tests] Change source in at-least-once test from finite to infinite  
URL: https://github.com/apache/flink/pull/9959
 
 
   Previously it was possible that the source would end before a first chcekpoint could complete. If that was the case, any exceptions thrown during checkpointing are swallowed, which could explain the apparent data loss from FLINK-14235.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicabl**e / docs / JavaDocs / not documented) 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Oct/19 14:18;githubbot;600","pnowojski commented on pull request #9959: [FLINK-14235][kafka,tests] Change source in at-least-once test from finite to infinite  
URL: https://github.com/apache/flink/pull/9959
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Oct/19 14:11;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,FLINK-14369,,,,,,,,,FLINK-14224,FLINK-14245,,,,,,,,,FLINK-14224,,,FLINK-14245,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 29 07:54:22 UTC 2019,,,,,,,,,,"0|z071w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/19 08:39;trohrmann;Another instance https://api.travis-ci.com/v3/job/239463674/log.txt;;;","27/Sep/19 12:07;gjy;Another instance https://api.travis-ci.com/v3/job/239747350/log.txt;;;","29/Sep/19 03:15;gaofeilong;Another instance [https://api.travis-ci.com/v3/job/239836223/log.txt];;;","30/Sep/19 09:46;gjy;Another instance https://api.travis-ci.com/v3/job/240237316/log.txt;;;","30/Sep/19 13:59;gjy;Another instance https://api.travis-ci.com/v3/job/239876444/log.txt;;;","02/Oct/19 08:38;gjy;Another instance https://api.travis-ci.com/v3/job/240397880/log.txt;;;","17/Oct/19 12:55;trohrmann;Another instance: https://api.travis-ci.com/v3/job/246637212/log.txt;;;","22/Oct/19 14:15;pnowojski;I hope this issue should be fixed by *c31e44e* (merged to the master branch). Closing the ticket for now, please re-open if you get the same failure message in the same test on a branch that includes this fix.

Special thanks to [~AHeise] for analysing and finding the root cause.;;;","29/Nov/19 07:54;hequn8128;Resolved in 1.8.3 via e0387a8007707ab29795e3aa3794ad279eaaeaf9
in 1.9.2 via 7bed1e229b87670ca9f80778f7b75829b48c91b1
in master via c31e44e5402c5fd7deb10b83534740ac7f66d0f8;;;",,,,,,,,,,,,,,,,,,,,,,,
"The runtime support for Bounded[One|Multi]Input#endInput does not properly implement their semantics",FLINK-14228,13259021,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sunhaibotb,sunhaibotb,sunhaibotb,26/Sep/19 11:31,20/Jul/20 15:33,13/Jul/23 08:10,21/Feb/20 11:28,1.9.0,,,,,,,,,1.11.0,,,,Runtime / Task,,,,,0,,,,,"Currently, the runtime support implementation of {{Bounded[One|Multi]Input#endInput}} has the following problems:
 * The runtime are propagating {{endInput}} immediately on the operator chain when input of the head operator is finished. Because some operators flush the buffered data in {{close}}, the downstream operators still receive records after executing {{endInput}}. This need the operators to flush the buffered data in {{endInput}} instead of {{close}}, like the PRs for fixing [issue#13491|https://issues.apache.org/jira/browse/FLINK-13491] and [issue#13376|https://issues.apache.org/jira/browse/FLINK-13376].

 * Timers are not taken into account.

Actually, {{StreamOperator#close}} tells the operator to finish all its processing and flush output (all remaining buffered data), while {{endInput}} indicates that no more data will arrive on some input of the operator. That is to say, for the non-tail operators on the operator chain, when the upstream operator is closed, the input of its downstream operator arrives at the end. So for an operator chain {{OP1 -> OP2 -> ... }},  the logic should be:
 # {{(Source/Network)Input}} of {{OP1}} is finished.
 # call {{OP1#endInput}}
 # quiesce {{ProcessingTimeService}} to disallow {{OP1}} from registering new timers.
 # wait for the pending timers (in processing) of {{OP1}} to finish.
 # call {{OP1#close}}
 # call {{OP2#endInput}}
 # quiesce {{ProcessingTimeService}} to disallow {{OP2}} from registering new timers.
 # ...",,mproch,pnowojski,ram_krish,sunhaibotb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18647,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-09-26 11:31:31.0,,,,,,,,,,"0|z071ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Travis is unable to parse one of the secure environment variables,FLINK-14225,13258983,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gjy,gjy,26/Sep/19 07:31,15/Oct/19 08:21,13/Jul/23 08:10,15/Oct/19 08:21,1.10.0,,,,,,,,,,,,,Build System,,,,,0,,,,,"Example: https://travis-ci.org/apache/flink/jobs/589531009

{noformat}
We were unable to parse one of your secure environment variables.

Please make sure to escape special characters such as ' ' (white space) and $ (dollar symbol) with \ (backslash) .

For example, thi$isanexample would be typed as thi\$isanexample. See https://docs.travis-ci.com/user/encryption-keys.
{noformat}

",,gjy,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 15 08:21:36 UTC 2019,,,,,,,,,,"0|z071c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/19 08:54;trohrmann;[~chesnay] could you help with this?;;;","27/Sep/19 08:59;gjy;Elevated to blocker because it is likely that some tests do not run.;;;","02/Oct/19 13:12;gjy;[~sewen] Can you help with this?;;;","03/Oct/19 12:33;sewen;Do you know which variable that is?

I am not aware of any changes in the secret environment variables in a while. Was that error always there that recent? ;;;","04/Oct/19 05:41;gjy;I think it is either {{IT_CASE_GCS_BUCKET}} or {{IT_CASE_GCS_TOKEN}} (tests against S3 are running, and artifacts are being uploaded). I cannot find usages of these anywhere in the code.;;;","10/Oct/19 17:03;gjy;If the GCS environment variables are not used, can we delete them? Does anyone know why these were added? {{git log --grep}} and {{git log -S}} did not yield anything.;;;","12/Oct/19 09:34;gjy;If there are no objections, I will remove these variables on Monday, 2019-10-14, 11 am CEST.;;;","15/Oct/19 08:21;gjy;Variables were deleted.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Temporal Table Function Joins do not work on Tables (only TableSources) on the query side,FLINK-14200,13258591,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,BenoitParis,BenoitParis,24/Sep/19 16:13,07/Jan/20 10:00,13/Jul/23 08:10,07/Jan/20 09:49,1.9.0,,,,,,,,,1.10.0,1.9.2,,,Table SQL / Planner,,,,,2,pull-request-available,,,,"This only affects the Blink planner. The legacy planner works fine.

With Orders as a TableSource, and Orders2 as a Table with the same content:
{code:java}
tEnv.registerTableSource(""Orders"", new FooSource(new String[] {""o_currency"", ""o_amount"", ""o_proctime""}));
Table orders2 = tEnv.sqlQuery(""SELECT * FROM Orders"");
tEnv.registerTable(""Orders2"", orders2);{code}
This works (TableSource on the query side):
{code:java}
SELECT 
 o_amount * r_amount AS amount 
FROM Orders  
 , LATERAL TABLE (Rates(o_proctime)) 
WHERE r_currency = o_currency{code}
While this does not (Table on the query side):
{code:java}
SELECT 
 o_amount * r_amount AS amount 
FROM Orders2 
 , LATERAL TABLE (Rates(o_proctime)) 
WHERE r_currency = o_currency{code}
Throwing an NPE in FlinkRelBuilder, called from LogicalCorrelateToJoinFromTemporalTableFunctionRule. Attached is Java code for reproduction, along with the full log and stacktrace, and a pom.xml.

EDIT: This may not be Table vs TableSource, maybe more a projection or a decorrelate issue? Don't know how the planner works well enough to characterize it better.

 ","Java 8, Scala 2.11, Flink 1.9",BenoitParis,f.pompermaier,jark,leonard,lzljs3620320,twalthr,,,,,,,,"wuchong commented on pull request #10763: [FLINK-14200][table] Fix NPE for Temporal Table Function Join when left side is a query instead of a source
URL: https://github.com/apache/flink/pull/10763
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix NPE when left input is a query (e.g. a filter/project/temporal join) in temporal table function join. 
   
   ## Brief change log
   
   - Get the RelOptSchema from the leaf RelNode which holds the non-null RelOptSchema
   
   ## Verifying this change
   
   - Add a plan test that left input is a filter query
   - Add a IT case that left input is a temporal join
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 16:18;githubbot;600","wuchong commented on pull request #10763: [FLINK-14200][table] Fix NPE for Temporal Table Function Join when left side is a query instead of a source
URL: https://github.com/apache/flink/pull/10763
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 05:44;githubbot;600","wuchong commented on pull request #10782: [FLINK-14200][table] Fix NPE for Temporal Table Function Join when left side is a query instead of a source
URL: https://github.com/apache/flink/pull/10782
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   **This is a cherry-pick of #10763 to release 1.9.**
   
   Fix NPE when left input is a query (e.g. a filter/project/temporal join) in temporal table function join. 
   
   ## Brief change log
   
   - Get the RelOptSchema from the leaf RelNode which holds the non-null RelOptSchema
   
   ## Verifying this change
   
   - Add a plan test that left input is a filter query
   - Add a IT case that left input is a temporal join
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 05:51;githubbot;600","wuchong commented on pull request #10782: [FLINK-14200][table] Fix NPE for Temporal Table Function Join when left side is a query instead of a source
URL: https://github.com/apache/flink/pull/10782
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 09:48;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,FLINK-15112,,,,,,,,,,,,,,,,,,"24/Sep/19 16:13;BenoitParis;temporal-table-function-query-side-as-not-table-source.zip;https://issues.apache.org/jira/secure/attachment/12981211/temporal-table-function-query-side-as-not-table-source.zip",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 10:00:15 UTC 2020,,,,,,,,,,"0|z06yx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/19 03:28;jark;Hi [~BenoitParis], thanks for reporting this. I think this is a bug. 
;;;","14/Oct/19 09:11;BenoitParis;[As mentioned in the mailing-list|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/flink-sql-table-in-subquery-join-temporal-table-raise-java-lang-NullPointerException-td30494.html] by another user, the following syntax fails as well:
{code:java}
SELECT o_amount * r_amount AS amount
FROM (SELECT * FROM Orders) as Orders, LATERAL TABLE (Rates(o_proctime))
WHERE r_currency = o_currency
{code};;;","10/Dec/19 10:57;BenoitParis;This issue:

https://issues.apache.org/jira/browse/FLINK-15112

seems to be the equivalent bug in the Legacy planner.;;;","06/Jan/20 04:00;lzljs3620320;Consider it is bug of 1.10 too, I'll set its version to 1.10 too. FYI, [~jark] [~BenoitParis];;;","06/Jan/20 05:26;jark;Sure.;;;","07/Jan/20 05:45;jark;1.11.0: 1121949bd6018d5c44238dc5550364285751faf9
1.10.0: 13a5cf92a1c66089a80fc03048fdb4b46c3009b0
1.9.2: 632a6dbebb81c950f0c72da64bdc7e5112820a9d;;;","07/Jan/20 10:00;BenoitParis;Thanks a lot for the fix [~jark] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Nightly connectors-jdk11 fails because of missing jaxb classes,FLINK-14195,13258526,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,liyu,liyu,24/Sep/19 11:25,09/Oct/19 07:39,13/Jul/23 08:10,27/Sep/19 12:08,1.10.0,,,,,,,,,1.10.0,,,,Build System,Connectors / FileSystem,Tests,,,0,test-stability,,,,"As titled, https://api.travis-ci.org/v3/job/588652149/log.txt is one example. We could see below errors from the log message:
{noformat}
23:48:12.419 [ERROR] testResumeAfterCommit(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 5.117 s  <<< ERROR!
java.lang.Exception: Unexpected exception, expected<java.io.IOException> but was<java.lang.NoClassDefFoundError>
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:165)
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeAfterCommit(HadoopS3RecoverableWriterExceptionITCase.java:165)

23:48:12.419 [ERROR] testResumeWithWrongOffset(org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase)  Time elapsed: 0.205 s  <<< ERROR!
java.lang.Exception: Unexpected exception, expected<java.io.IOException> but was<java.lang.NoClassDefFoundError>
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:185)
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException
	at org.apache.flink.fs.s3hadoop.HadoopS3RecoverableWriterExceptionITCase.testResumeWithWrongOffset(HadoopS3RecoverableWriterExceptionITCase.java:185)
{noformat}",,kevin.cyj,liyu,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13748,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 12:08:54 UTC 2019,,,,,,,,,,"0|z06yio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Sep/19 12:35;trohrmann;Maybe we shouldn't have removed the jaxb dependency with FLINK-14157 but instead only do the relocation in the Java 11 profile.;;;","26/Sep/19 07:54;kevin.cyj;Hi [~trohrmann], I am trying to fix the problem but encountered some new problems. As you said, I added the jaxb dependency back and do the relocation only in the java 11 profile. When I unzipped the target jar, I found the jaxb dependency was successfully included and relocated. However, when I was trying to verify the fix on Travis, I found the test case can not be triggered. Looking into the code, I found some environment variable must be set for aws s3 and it seems that I must have aws s3 access which I don't have. Do you have any ideas about how could trigger the test case?;;;","26/Sep/19 10:17;trohrmann;[~kevin.cyj] thanks for looking into the problem. Can you give me a pointer to the branch, then I'll create a cron job running on Flink's Travis account to verify the change.;;;","26/Sep/19 10:22;kevin.cyj;Thanks [~trohrmann].

My branch is [https://github.com/wsry/flink/tree/fix-jaxb-dependency-problem-for-java11-in-s3].;;;","26/Sep/19 12:37;trohrmann;Here is the link to the cron job run: https://travis-ci.org/apache/flink/builds/589937876;;;","27/Sep/19 12:04;trohrmann;The branch seems to solve the problem [~kevin.cyj]. I will merge your changes now.;;;","27/Sep/19 12:08;trohrmann;Fixed via

72ea59e7e4eac07a8c5d28661d0a329c758193c8
249cae1ce746a4a639cfc3b83f38f3c5b34e3dba;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Nightly e2e-misc-jdk11 fails complaining unrecognized VM option,FLINK-14186,13258516,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,liyu,liyu,24/Sep/19 11:19,27/Sep/19 08:43,13/Jul/23 08:10,27/Sep/19 08:43,,,,,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,test-stability,,,"As titled, this failure reproduces stably in recent nightly runs, such as https://api.travis-ci.org/v3/job/588201921/log.txt . From the log we could see message like below:
{noformat}
Unrecognized VM option 'UseParNewGC'
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{noformat}",,aljoscha,gaoyunhaii,liyu,trohrmann,,,,,,,,,,"gaoyunhaii commented on pull request #9775: [FLINK-14186][e2e] Let JVM ignores unrecoginzed options to avoid end to end test failure in es 2.3.5 with JDK11
URL: https://github.com/apache/flink/pull/9775
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This PR fixes the end-to-end test failure when testing elastic search link with ES 2.3.5 on JDK 11. Since Java has removed some GC options with the change on the GC algorithms, we need to let the JVM ignore the unrecognized options, otherwise ES 2.3.5 will fail to startup with JDK 11 due to it configures some default options that have been removed in JDK 11.
   
   ## Brief change log
   
   - 7ca40f4410862da362c16e5a9410ca692854451f configures JVM to ignore the unrecognized options so that when starting ES cluster.
   
   ## Verifying this change
   
   This change is tested by manually run run-nightly-tests.sh under flink-end-to-end-tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature?**no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Sep/19 03:57;githubbot;600","tillrohrmann commented on pull request #9775: [FLINK-14186][e2e] Skip Elastic Search 2.3.5 case when running with JDK11
URL: https://github.com/apache/flink/pull/9775
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Sep/19 08:43;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 08:43:31 UTC 2019,,,,,,,,,,"0|z06ygg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/19 03:42;gaoyunhaii;The reason of this issue is that -XX:+UseParNewGC [has been removed from the JDK11|[https://bugs.openjdk.java.net/browse/JDK-8151084]] as the garbage collector algorithms have evolved. However, Elastic Search 2.3.5  has configured this option in its startup script. Therefore, the Elastic Search 2.3.5 will fail to start on JDK11, which cause the test fail.

To fix this problem, we can set environment variable JAVA_OPTS=""-XX:+IgnoreUnrecognizedVMOptions"" when starting the Elastic Search cluster. This environment variable parameter is provided by Elastic Search 2.3.5 script and IgnoreUnrecognizedVMOptions let JVM ignores unrecognized options instead throw an error.

The Elastic Search 5.x and 6.x have removed -XX:+UseParNewGC from the default JVM config and thus they should not met this issue. Besides, in these two version JAVA_OPTS is deprecated and ES_JAVA_OPTS is used instead. Since the two version currently do not have problem, I think we do not need to also configure ES_JAVA_OPTS now. 

 

For reference, the code involved in Flink is:
 # e2e - misc - jdk11 configured in .travis.yml have finally called _tools/travis/splits/split_misc.sh_
 # _split_misc.sh_ have three ElasticSearch related tests. They all finally called flink-end-to-end-tests/test-scripts/test_streaming_elasticsearch.sh. 
 # test_streaming_elasticsearch.sh further called the function setup_elasticsearch included from flink-end-to-end-tests/test-scripts/elasticsearch-common.sh
 # setup_elasticsearch downloads the specified ElasticSearch distribution package and unzip it. Then it calls ""_$elasticsearchDir/bin/elasticsearch &""_ to start  the _ _ElasticSearch_ _ cluster. We should then modify the startup environment here.

 

 

 ;;;","26/Sep/19 06:48;gaoyunhaii;I have tested the elastic search e2e test cases locally, and now is still testing the whole tests by manually trigger it on Travis and the test is still running.;;;","26/Sep/19 17:21;gaoyunhaii;With the discussion under the [PR|[https://github.com/apache/flink/pull/9775]], we should indeed skip the ES 2.3.5 cases when running with JDK 11. The ES 2.3.5 cases is used in three scripts and only _tools/travis/splits/split_misc.sh_ is tested with JDK 11, thus we skip the tests if we found it is running with JDK 11.;;;","27/Sep/19 07:39;aljoscha;I think this is one more reason for removing ES 2 support.;;;","27/Sep/19 08:43;trohrmann;Fixed via b70639d372f75c9bf602d096ef691b6d7bd4b607;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractServerTest failed on Travis,FLINK-14185,13258494,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,24/Sep/19 09:45,15/Oct/19 12:24,13/Jul/23 08:10,15/Oct/19 12:24,,,,,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,test-stability,,,"AbstractServerTest failed on Travis. The Travis link is [https://api.travis-ci.org/v3/job/588761985/log.txt]. The error message is as follows.
{code:java}
05:53:23.924 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.113 s <<< FAILURE! - in org.apache.flink.queryablestate.network.AbstractServerTest
05:53:23.924 [ERROR] testPortRangeSuccess(org.apache.flink.queryablestate.network.AbstractServerTest)  Time elapsed: 0.064 s  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<1>
	at org.apache.flink.queryablestate.network.AbstractServerTest.testPortRangeSuccess(AbstractServerTest.java:126)
{code}
The failure can be reproduced by running the test case multi times (until failure) locally.

The failure reason is that the requestStats is shared by two instance of TestServer which relies on AutoCloseable interface for connection release. If one of the server has been shut down and the anther is not, the assertion in the close method can fail. To fix the problem, we can move the assertion out of the close method.",,aljoscha,kevin.cyj,,,,,,,,,,,,"wsry commented on pull request #9758: [FLINK-14185][tests]Move unstable assertion statement out of close method of QS test server.
URL: https://github.com/apache/flink/pull/9758
 
 
   ## What is the purpose of the change
   
   The purpose of this pr is to fix the unstable test case AbstractServerTest. The test case failed on Travis. The failure reason is that the requestStats is shared by two instance of TestServer which relies on AutoCloseable interface for connection release. If one of the server has been shut down and the anther is not, the assertion in the close method can fail. This pr fix the problem by moving the assertion out of the close method.
   
   
   ## Brief change log
   
     - The unstable assertion statement was moved out of the close method of TestServer.
   
   
   ## Verifying this change
   
   The change can be verified by the test case itself.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Sep/19 10:39;githubbot;600","aljoscha commented on pull request #9758: [FLINK-14185][tests]Move unstable assertion statement out of close method of QS test server.
URL: https://github.com/apache/flink/pull/9758
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Oct/19 12:22;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 15 12:24:41 UTC 2019,,,,,,,,,,"0|z06ybk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Oct/19 12:24;aljoscha;Fixed on master in
2894caf4a2a054a4838af5b631d40d15c43369ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the description of 'SHOW FUNCTIONS' in SQL Client,FLINK-14179,13258440,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,felixzheng,felixzheng,felixzheng,24/Sep/19 03:10,29/Sep/19 07:59,13/Jul/23 08:10,29/Sep/19 07:58,1.9.0,,,,,,,,,1.10.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"Currently '*SHOW FUNCTIONS*' lists not only user-defined functions, but also system-defined ones, the description {color:#172b4d}*'Shows all registered user-defined functions.'* not correctly depicts this functionality. I think we can change the description to '*Shows all system-defined and user-defined functions.*'{color}

 

{color:#172b4d}!image-2019-09-24-10-59-26-286.png|width=640,height=191!{color}

{color:#172b4d}!image-2019-09-29-14-34-45-151.png|width=561,height=465!{color}

 ",,felixzheng,jark,tison,,,,,,,,,,,"zhengcanbin commented on pull request #9752: [FLINK-14179]Correct description of 'SHOW FUNCTIONS'
URL: https://github.com/apache/flink/pull/9752
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Sep/19 06:29;githubbot;600","zhengcanbin commented on pull request #9752: [FLINK-14179]Correct description of 'SHOW FUNCTIONS'
URL: https://github.com/apache/flink/pull/9752
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/19 07:04;githubbot;600","zhengcanbin commented on pull request #9752: [FLINK-14179]Correct description of 'SHOW FUNCTIONS'
URL: https://github.com/apache/flink/pull/9752
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/19 07:04;githubbot;600","wuchong commented on pull request #9752: [FLINK-14179]Correct description of 'SHOW FUNCTIONS'
URL: https://github.com/apache/flink/pull/9752
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/19 07:57;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13741,,,,,,,,"24/Sep/19 02:59;felixzheng;image-2019-09-24-10-59-26-286.png;https://issues.apache.org/jira/secure/attachment/12981119/image-2019-09-24-10-59-26-286.png","29/Sep/19 06:29;felixzheng;image-2019-09-29-14-29-02-274.png;https://issues.apache.org/jira/secure/attachment/12981712/image-2019-09-29-14-29-02-274.png","29/Sep/19 06:29;felixzheng;image-2019-09-29-14-29-54-861.png;https://issues.apache.org/jira/secure/attachment/12981713/image-2019-09-29-14-29-54-861.png","29/Sep/19 06:34;felixzheng;image-2019-09-29-14-34-45-151.png;https://issues.apache.org/jira/secure/attachment/12981714/image-2019-09-29-14-34-45-151.png",,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 29 07:58:49 UTC 2019,,,,,,,,,,"0|z06xzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/19 15:27;felixzheng;Not merged yet.;;;","29/Sep/19 05:48;felixzheng;[~Tison] [~jark] It's a quite minor change, could any take a look to merge then close this ticket.;;;","29/Sep/19 06:12;tison;Hi [~felixzheng] I've assigned the issue to you. Will give it a review now.;;;","29/Sep/19 06:35;felixzheng;Thanks [~tison];;;","29/Sep/19 07:58;jark;1.10.0: 47b0d1569ee8543aaee80b3a5443a112ba5235a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade KPL version in flink-connector-kinesis to fix application OOM,FLINK-14175,13258405,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,abseth,abseth,abseth,23/Sep/19 22:01,29/Nov/19 15:45,13/Jul/23 08:10,24/Oct/19 18:16,1.6.3,1.6.4,1.6.5,1.7.2,1.7.3,1.8.0,1.8.1,1.8.2,1.9.0,1.8.3,1.9.2,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,"The [KPL version|https://github.com/apache/flink/blob/release-1.9/flink-connectors/flink-connector-kinesis/pom.xml#L38] (0.12.9) used by flink-connector-kinesis in the affected Flink versions has a thread leak bug that causes applications to run out of memory after frequent restarts:

KPL Issue - [https://github.com/awslabs/amazon-kinesis-producer/issues/224]

Fix - [https://github.com/awslabs/amazon-kinesis-producer/pull/225/files]

Upgrading KPL to 0.12.10 or higher is necessary to avoid this issue. The recommended version to upgrade would be the latest (0.13.1)

Note that KPL version in Flink 1.10.0 has been updated to the latest version (0.13.1): https://issues.apache.org/jira/browse/FLINK-12847

 ",,abseth,phoenixjiangnan,rmetzger,thw,,,,,,,,,,"abhilashaseth commented on pull request #9938: [FLINK-14175] [flink-connector-kinesis] Update KPL version to 0.13.1
URL: https://github.com/apache/flink/pull/9938
 
 
   This fixes https://github.com/awslabs/amazon-kinesis-producer/issues/224. 
   
   * JIRA - https://issues.apache.org/jira/browse/FLINK-14175
   * mvn clean package succeeds after this change
   * The version upgrade has already been performed (hence tested) in Flink 1.10.0 as part of FLINK-12847.
   
   ## What is the purpose of the change
   
   This pull request upgrades the KPL version in flink-kinesis-connector to fix thread leak mentioned in ttps://github.com/awslabs/amazon-kinesis-producer/issues/224.
   
   ## Brief change log
   
   Update the KPL version from 0.12.9 to 0.13.1
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Oct/19 19:14;githubbot;600","abhilashaseth commented on pull request #9939: [FLINK-14175] [flink-connector-kinesis] Update KPL version to 0.13.1
URL: https://github.com/apache/flink/pull/9939
 
 
   This fixes https://github.com/awslabs/amazon-kinesis-producer/issues/224. 
   
   * JIRA - https://issues.apache.org/jira/browse/FLINK-14175
   * mvn clean package succeeds after this change
   * The version upgrade has already been performed (hence tested) in Flink 1.10.0 as part of FLINK-12847.
   
   ## What is the purpose of the change
   
   This pull request upgrades the KPL version in flink-kinesis-connector to fix thread leak mentioned in ttps://github.com/awslabs/amazon-kinesis-producer/issues/224.
   
   ## Brief change log
   
   Update the KPL version from 0.12.9 to 0.13.1
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Oct/19 19:21;githubbot;600","tweise commented on pull request #9938: [FLINK-14175] [flink-connector-kinesis] Update KPL version to 0.13.1
URL: https://github.com/apache/flink/pull/9938
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Oct/19 18:15;githubbot;600","tweise commented on pull request #9939: [FLINK-14175] [flink-connector-kinesis] Update KPL version to 0.13.1
URL: https://github.com/apache/flink/pull/9939
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Oct/19 18:15;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,FLINK-14995,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 22 16:06:31 UTC 2019,,,,,,,,,,"0|z06xrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/19 08:03;rmetzger;[~phoenixjiangnan]: you have recently worked on Kinesis as part of https://issues.apache.org/jira/browse/FLINK-12847, could you take a look here?;;;","02/Oct/19 18:10;phoenixjiangnan;I don't feel really necessary to update KPL in Flink 1.9 and past, since users are already building kinesis connector themselves and they can just choose whatever KPL version they like.

Feel free to submit a PR though, and I'll try to help review.;;;","09/Oct/19 22:57;abseth;Which branch should the PR target?

Also, is there a possibility of backporting https://issues.apache.org/jira/browse/FLINK-12847 to older versions? Since that already has the fix.;;;","15/Oct/19 08:32;rmetzger;I would target {{master}} for the upcoming 1.10 release (and maybe to the {{release-1.9}} branch for the next 1.9.x release)

For backporting FLINK-12847, to older versions: For bugfix releases, we usually only upgrade dependency versions for severe bugs or security issues.

Since connectors are only distributed via maven repositories, you could consider backporting the version change yourself, and then releasing the connector under a different name to maven central? ;;;","18/Oct/19 19:22;abseth;The master branch already has the updated KPL version as it was updated in https://issues.apache.org/jira/browse/FLINK-12847.

PR for release-1.9 branch: [https://github.com/apache/flink/pull/9938]

PR for release-1.8 branch: [https://github.com/apache/flink/pull/9939];;;","22/Oct/19 15:12;rmetzger;[~thw]: Afaik you're using the Kinesis connector. What's your take on backporting a version upgrade in a Flink bugfix release?;;;","22/Oct/19 16:06;thw;Given the nature of the issue I think it would be justifiable to backport just the version change to 1.9.x and possibly 1.8.x (if possible). That would ensure that users building the connector don't run into the bug. Backporting entire FLINK-12847 is not appropriate for a patch release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Execution#producedPartitions is possibly not assigned when used,FLINK-14163,13258137,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ym,zhuzh,zhuzh,22/Sep/19 08:49,18/May/21 07:01,13/Jul/23 08:10,17/Jan/20 14:35,1.10.0,1.9.0,,,,,,,,1.10.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Currently {{Execution#producedPartitions}} is assigned after the partitions have completed the registration to shuffle master in {{Execution#registerProducedPartitions(...)}}.
The partition registration is an async interface ({{ShuffleMaster#registerPartitionWithProducer(...)}}), so {{Execution#producedPartitions}} is possible[1] not set when used. 

Usages includes:
1. deploying this task, so that the task may be deployed without its result partitions assigned, and the job would hang. (DefaultScheduler issue only, since legacy scheduler handled this case)
2. generating input descriptors for downstream tasks: 
3. retrieve {{ResultPartitionID}} for partition releasing: 

[1] If a user uses Flink default shuffle master {{NettyShuffleMaster}}, it is not problematic at the moment since it returns a completed future on registration, so that it would be a synchronized process. However, if users implement their own shuffle service in which the {{ShuffleMaster#registerPartitionWithProducer}} returns an pending future, it can be a problem. This is possible since customizable shuffle service is open to users since 1.9 (via config ""shuffle-service-factory.class"").

To avoid issues to happen, we may either 
1. fix all the usages of {{Execution#producedPartitions}} regarding the async assigning, or 
2. change {{ShuffleMaster#registerPartitionWithProducer(...)}} to a sync interface",,azagrebin,gaoyunhaii,gjy,guoyangze,trohrmann,ym,zhuzh,zjwang,,,,,,"curcur commented on pull request #10832: [FLINK-14163][runtime]Enforce synchronous registration of Execution#producedPartitions
URL: https://github.com/apache/flink/pull/10832
 
 
   ## What is the purpose of the change
   
   `Execution#producedPartitions` are registered through an asynchronous interface `ShuffleMaster#registerPartitionWithProducer` to `ShuffleMaster`, however they are not always accessed through callbacks. So, it is possible that `Execution#producedPartitions` have not been available yet when accessed (in `Execution#deploy` for example). Since the only implementation of `ShuffleMaster` is `NettyShuffleMaster`, which indeed registers producedPartition in a synchronous way, hence this pull request enforces synchronous registrations under an asynchronous interface as a temporary fix to prevent future misuses.
   
   ## Brief change log
   Make sure the future of registration is done immediately after registration. Throws IllegalStateException otherwise.
   
   ## Verifying this change
   unit test: 
   ExecutionTest#testSynchronousRegisterProducedPartitions
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? JavaDocs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jan/20 03:20;githubbot;600","curcur commented on pull request #10832: [FLINK-14163][runtime]Enforce synchronous registration of Execution#producedPartitions
URL: https://github.com/apache/flink/pull/10832
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 11:14;githubbot;600","curcur commented on pull request #10832: [FLINK-14163][runtime]Enforce synchronous registration of Execution#producedPartitions
URL: https://github.com/apache/flink/pull/10832
 
 
   ## What is the purpose of the change
   
   `Execution#producedPartitions` are registered through an asynchronous interface `ShuffleMaster#registerPartitionWithProducer` to `ShuffleMaster`, however they are not always accessed through callbacks. So, it is possible that `Execution#producedPartitions` have not been available yet when accessed (in `Execution#deploy` for example). Since the only implementation of `ShuffleMaster` is `NettyShuffleMaster`, which indeed registers producedPartition in a synchronous way, this pull request enforces synchronous registrations under an asynchronous interface as a temporary fix to prevent future misuses.
   
   ## Brief change log
   - Make sure the future of registration is done immediately after registration. 
   - Throws IllegalStateException otherwise.
   
   ## Verifying this change
   unit test: 
   ExecutionTest#testSynchronousRegisterProducedPartitions
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? JavaDocs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jan/20 11:15;githubbot;600","zentol commented on pull request #10832: [FLINK-14163][runtime]Enforce synchronous registration of Execution#producedPartitions
URL: https://github.com/apache/flink/pull/10832
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/20 14:34;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22677,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 17 14:35:47 UTC 2020,,,,,,,,,,"0|z06w48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/20 06:53;zjwang;Thanks for reporting this potential issue [~zhuzh]!

After double checking the related codes, this issue indeed exists only for DefaultScheduler. When the ShuffleMaster#registerPartitionWithProducer was firstly introduced before, we already considered the async behavior in the scheduler process (legacy now) at that time.

The above mentioned three usages are mainly caused by the deployment process not considering the completed future of registering partition in new DefaultScheduler. If the new scheduler can also take the async way into account like the legacy scheduler did during deployment, I think we can solve all the existing concerns. 

I also feel that the current public method of Execution#getPartitionIds might bring potential risks to use in practice, because the returned partition might be an empty collection if the registration future was not completed yet, but the caller is not aware of this thing. 

From the shuffle aspect, it is indeed meaningful for providing the async way for registerPartitionWithProducer in a long term, which is flexible to satisfy different scenarios. But from the existing implementation and possible future extending implementations like yarn shuffle service etc, the sync way can also satisfy the requirements I guess. So if this way would bring more troubles for the scheduler and it is not easy to adjust for other components, it also makes sense to adjust the registerPartitionWithProducer as sync way instead on my side. We can make things easy. 

Are there any thoughts or inputs [~azagrebin]?;;;","06/Jan/20 06:54;zjwang;In addition, no matter which way we take to solve this issue, I think we can make it ready in release-1.11, not a blocker for release-1.10.;;;","06/Jan/20 08:04;zhuzh;Thanks [~zjwang] for the feedbacks.
I think partition releasing could also be a problem for both legacy and DefaultScheduler. If a task is released before its partitions are successfully registered to shuffle master, JM will see no partition to release when cancelling the task, so that the partitions could be leaked in shuffle master.
And agree that making {{ShuffleMaster#registerPartitionWithProducer}} a sync interface would be much easier to avoid such possible issues at the moment.
[~gjy] do you think we need to fix this in 1.10? The major problem (streaming job tasks get deployed without knowing its inputs) is only possible to happen with DefaultScheduler (with custom shuffle service). I'd prefer to fix it in 1.10 if we can have an easy and low risk fix.;;;","06/Jan/20 14:58;gjy;Since there is no issue when using the NettyShuffleMaster (the only implementation), I do not see immediate call for action in 1.10. I would welcome it, if we can simplify the {{ShuffleMaster#registerPartitionWithProducer()}} interface but this is for [~zjwang] and [~azagrebin] to decide.;;;","06/Jan/20 15:59;azagrebin;Thanks for analysis [~zhuzh] and [~zjwang]. Indeed, this is a separate state whether we triggered registering of partitions or not. If task is just in scheduled state while being canceled we ignore partitions as we handle partitions only for deployed tasks (next state).

I agree that this is not an immediate problem but we have to be honest with the API and at least use it consistently as synchronous API. The quickest fix could be to just call get on the partitions future immediately and document that it is not used asynchronously atm. Later, if we see that it is a problem for some implementations, we can fix it to correctly handle this asynchronous state of task/partitions lifecycle.

Wdyt?

cc [~chesnay];;;","07/Jan/20 08:08;ym;Accidentally navigating here when digging into the ShuffleMaster interface, and learned a lot reading the comments! Thanks everyone.

 

I agree that keeping the interface and usage consistent is important. One thing worth to mention is that the ShuffleMaster/ShuffleService interface `might` be extended/reused to persist output for the purpose of single task failure recovery in the future. If that's the case, partition registration/release might be more complicated during recovery (and may take longer time). Keep the interface as it is provides more flexibility for shuffle services.

 ;;;","07/Jan/20 08:20;zhuzh;+1 to the solution proposed by [~azagrebin]. The fix should be simple enough and it's good that we do not need to change the shuffle interface back&forth. We can also easily improve the usage implementation of the returned completable future when needed in the future.

[~ym], yes keeping the async interface is generally good in long term. So using the future in a sync way for now as proposed by Andrey is better than changing the shuffle interface to sync pattern.;;;","07/Jan/20 13:11;azagrebin;One more thing to add after talking to [~trohrmann], we should fail if the partitions future is not done immediately (basically for true async implementations) because if scheduler does not support it properly (as now) then we risk to block main thread and fail heartbeat in a more vague way.;;;","07/Jan/20 14:56;zhuzh;Makes sense. Unexpected blocked main thread can be hard to troubleshoot for users, and throwing explicit errors may be better.

;;;","08/Jan/20 02:32;ym;That's a good point: fail loudly if acting in an unexpected way for easier trouble shooting. This makes the fix safe even if a developer accidentally neglects the documentation, since he/she can get notified explicitly.;;;","09/Jan/20 08:57;ym;Thanks for assigning the task to me!

 

I have written a first version based on the discussion, key changes:
 # change Execution#producedPartitions to Execution#producedPartitionsFuture, and initiate it as an incomplete future 
 # assign the producedPartitionsFuture in Execution#registerProducedPartitions
 # wrap any access to producedPartitions in a synchronous function. If later async registration is needed, callbacks can be added to substitute this method. The function is quite simple: fail if the producedPartitionsFuture is not done, otherwise return producedPartitionsFuture.get()

Code Link:

[https://github.com/apache/flink/compare/master...curcur:shuffle_master_async_interface?expand=1]

 

This works fine if producedPartitions are not supposed to be accessed without registration, which is natural since we have to assign before using. Notice that `registration` and `registration finished` is different. The former refers to whether registration is always required (Execution#registerProducedPartitions is always called before accessed), and the latter refers to whether partitions are successfully registered.

However, I find a lot of the unit tests fail because producedPartitions are accessed without Execution#registerProducedPartitions are called, for example 

ExecutionVertexDeploymentTest#testDeployCall()

ExecutionGraphCheckpointCoordinatorTest#testShutdownCheckpointCoordinatorOnFailure()

e.t.c.

 

In the old version, producedPartitions is initiated as an empty map, and works well in cases the real value of producedPartitions are not necessary.

 

I am wondering whether this is just a shortcut for tests or it is also used/allowed in some places in prod path?

 

If access without registration is possible in prod, we can make producedPartitionsFuture Optional to differentiate whether Execution#registerProducedPartitions is called or not, like this:

[https://github.com/apache/flink/compare/master...curcur:optional?expand=1]

 

Or a safer and simpler change is to keep all interfaces and usages as it is, and directly check isDone() and call get() after registration of producedPartitions, like this:

[https://github.com/apache/flink/compare/master...curcur:simpler_way?expand=1] 

 

 What do you think? ;;;","09/Jan/20 10:09;zhuzh;Thanks [~ym] for trying out the solutions.
I'd prefer option 3 which simply adds a check to ensure the registration future is done on return. 
And I think we also need to update the java docs for {{ShuffleMaster#registerPartitionWithProducer}} to ask users to only return completed future at the moment.
Feel free to open the PR.;;;","09/Jan/20 10:15;ym;[~zhuzh] Thank you sooo much. I prefer option 3 as well, will file a PR. ;;;","09/Jan/20 10:51;chesnay;Can someone explain to me where exactly a task is being deployed (\{{Execution#deploy}}) without the futures for slots/partitions being completed? Both {{SchedulingUtils#schedulerEager}} and {{Execution#scheduleForExecution}} only call {{deploy}} if the future has finished.

I don't understand how there could be a pending partition registration during deployment.

 

The only I problem I see is that a partition could leak if one of the registration fail, since the combined future will never finish and hence the partitions will never reach the book-keeping.;;;","09/Jan/20 10:57;zhuzh;[~chesnay] The deployment issue can happen with ng scheduler since it does not wait for the future to complete in {{DefaultScheduler#assignResourceOrHandleError}}.
The calling chain of deployment is DefaultScheduler#deployTaskSafe(...) -> DefaultExecutionVertexOperations#deploy(...) -> ExecutionVertex#deploy() -> Execution#deploy().;;;","09/Jan/20 14:03;chesnay;Right.

So [~gjy], [~azagrebin] and me have had an offline discussion about this.

Let me summarize our conclusion:

The Problem:
The DefaultScheduler does not wait for the partition registration to complete before issuing the deploy call.
 As a result:
 - the TDD of the deployed task may be missing output descriptors since these are derived from the {{ResultPartitionDeploymentDescriptor}} returned by the ShuffleMaster
 - if the job is cancelled or fails before the partition registration is complete we are
 a) not issuing release calls since nothing has been tracked yet
 b) leaking partition tracking entries since the registration process isn't canceled and may start tracking partitions after the execution was canceled

The raised issue about input descriptors for downstreams tasks should not be a problem since the runtime handles this case via notifications about consumable partitions.

The solution:
Ensuring that we wait for the registration to complete (in {{DefaultScheduler#assignResourceOrHandleError)}}) should resolve the issue. The partition registration and deployment are then executed as a single scheduling unit, preventing issues with concurrent cancel calls, and it naturally ensures that partition descriptors are available during deployment.
We prefer this option as it should be relatively simple and does not require changes to the ShuffleMaster API.


Another orthogonal issue we stumbled on is that if the registration of a single partition fails the remaining partitions are not cleaned up on the ShuffleMaster, since the cleanup only occurs after partitions have been tracked, and partitions are only tracked if all registrations succeed. This seems to be a separate issue entirely and I'll open a separate ticket.;;;","09/Jan/20 14:26;zhuzh;Thanks [~chesnay] for the update! Then we are now taking the option to fix all the problematic usages of {{Execution#producedPartitions}}.
I think the partition leak could also happen with the legacy scheduler if a task is cancelled/failed before its partition registration is completed. Shall we also fix it since legacy scheduler is the only scheduler in 1.9 and still available to users in 1.10?;;;","09/Jan/20 14:35;chesnay;Hold on; what I meant is that, for now, we'll just amend {{DefaultScheduler#assignResourceOrHandleError}} to call {{get()}} on the result from {{Execution#registerProducedPartitions}}.

There are some follow-ups we will need to support ""true"" asynchronous scheduling, but this isn't a priority right now since neither netty nor yarn require it, and we are not aware of any other plans for possible implementations.

Let me double-check quickly on the cancel/fail before registration complete issue.
;;;","09/Jan/20 14:40;chesnay;Fair enough, if the task is cancelled/failed before the registration completes then yes, we may be leaking partitions in general.

For this I would amend the lambda function in {{Execution#registerProducedPartitions}} that starts tracking partitions to check the state of the execution after starting the tracking of partitions, and if the execution is not in a scheduled state to immediately untrack them again.;;;","09/Jan/20 15:22;zhuzh;Oh I understand what your mean. 
Previously [~azagrebin] & Till had raised the concern that calling {{get()}} on the partition registration future would block the main thread in a vague way. (see this [comment|https://issues.apache.org/jira/browse/FLINK-14163?focusedCommentId=17009710&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17009710]). That's why we had decided to require all {{ShuffleMaster#registerPartitionWithProducer}} implementations to return completed future only at the moment.
If we would allow to call {{get()}} on the future, looks to me it's even simpler to call it in {{Execution#registerProducedPartitions(...)}}, so that neither the deployment racing nor the partition leak would happen in this way.
;;;","10/Jan/20 06:57;ym;Hey, [~chesnay], [~zhuzh], [~azagrebin] and [~gjy], thank you so much for the summary! It is thorough and clear. Learned a lot :)

 

The fundamental problem of this issue is the assignment and access of `Execution#producedPartitions` mismatches. That is, assignment is through an asynchronous interface, but not all of the accesses are using callbacks. That is the reason why misuses are introduced in the first place.

 

Given the only implementation of `ShuffleMaster` is synchronous (`NettyShuffleMaster`), there is no immediate needs to clean up all the usage. Instead, the more important thing is to eliminate the risk to introduce more misuses or lead the system behave in an unexpected way.

 

Along with the discussion, I think everyone agrees to keep the async interface of `ShuffleMaster#registerPartitionWithProducer` for the purpose of flexibility, but enforce a synchronous registration underneath. Then the question is what is the best place to put the enforcement.

 

[In my previous comment|https://issues.apache.org/jira/browse/FLINK-14163?focusedCommentId=17011568&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17011568],  

Option1 changes all access to `Execution#producedPartitions` as a callback, hence the interfaces for both assignment and access are consistent. The synchronous registration is enforced and wrapped before accessing.

But since there are different ways to access producedPartitions, Option1 has some limitations (discussed in the comment). This is also the reason I am a bit hesitate to fix the usage (access) of producedPartitions directly.

 

Option2 is a refinement of Option1 but a bit ugly.

 

Option3 enforces the synchronous registration from the registration side, and immediately fails if the registration is not synchronous. This is a much simpler and safe change for the purpose of  `eliminating the risk to introduce more misuses and to lead the system behave in an unexpected way.`

 

If asynchronous registration is needed in the future, we should fix the access part in no doubt.

 ;;;","10/Jan/20 09:13;azagrebin;[~chesnay] [~zhuzh] [~ym]

I think we should still not call get directly on Future returned from ShuffleMaster to protect from accidental usage of currently unsupported async shuffle API.

If we immediately assert isDone on shuffle registration Future or any of its immediate derivatives to be true, it will be the same as calling get but we can judge about actual implementation of used shuffle implementation whether it is sync or async and fail fast for currently unsupported async shuffle API. After checking isDone, we can safely call get.;;;","11/Jan/20 03:22;ym;PR: pull request #10832;;;","13/Jan/20 07:40;zjwang;Thanks for the above good suggestions from you guys! Sorry for coming back this issue a bit late, especially for the PR already ready.

My previous guessing was that the formal support of async way would bring big trouble for scheduler, or it may be conflict with new scheduler direction in long term. Also considering the shuffle async way a bit over design then and no real users atm, so I mentioned before that I can accept the way of adjusting into the sync way to stop loss early. Although I also thought in general it is not a good way to break compatibility for exposed public interface. If it is not a problem for scheduler for handling the async way in future, I am happy to retain the async shuffle way.

If we decide to retain the async way and work around it in scheduler temporarily, it might be better to not fail directly after checking the future not completed. I mean we can step forward to bear a timeout before failing. This timeout is not only used for waiting future completion, also used for waiting for the future return by shuffle master while calling to avoid main thread stuck long time.;;;","13/Jan/20 09:20;ym; 

Synced up with [~zjwang] offline. 

 

In general, I would prefer not to change interfaces unless having to, since interface changes have more profound impact on users than implementation changes. Especially in this case, given the future design is not determined (both Shuffle and Scheduler), changing async -> sycn and then sync -> async again bring much more pain to users than keeping the asycn interfaces and enforces a sync implementation underneath. 

In the latter case, a user barely needs to do anything if we do want to support async implementation in the future, while in the first case, every single user that uses the interface has to make a new release to adjust to the updated interface.

 

I do not have strong preferences other than the point of keeping the interface. I think retain a timeout is a good compromise, but may introduce some inconsistent system behavior. Hence even if we decide to retain a timeout, I would prefer to document as `enforce a synchronous implementation under asynchronous interface`.

 ;;;","13/Jan/20 14:37;azagrebin;Thanks [~zjwang] and [~ym] ! The question is how long should the timeout be? It probably depends on heartbeat timeouts. Not sure it is easy to come up with the reasonable value. I would rather go with the simple check and fix the scheduler problems before we decide to have a shuffle service which requires async behaviour.;;;","14/Jan/20 03:43;zhuzh;+1 to fix it with a simple check first. As long as we are call `get()` on the future directly, a timeout may still lead to blocked main thread which we are trying to avoid. And of course finally we will need a timeout to support the async handling of the partition registration.;;;","14/Jan/20 06:42;ym; 

Thanks [~azagrebin] and [~zhuzh]! I do not have a strong opinion on whether to add a time-out or not. If I have to choose, I would probably prefer to going with a simple check because this makes system behavior consistent and easy to reason about.

 

I double checked with [~zjwang] before he left (he is on vacation right now), and he is fine with either case as well.;;;","17/Jan/20 14:35;chesnay;master: 6fa1ea1caa0de4892e5942f760305422859585da
1.10: a16f7c7cf9c5e0d210591b93690784d593ff0910 ;;;",,,
Unnecessary __pycache__ directories appears in pyflink.zip,FLINK-14150,13257908,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zhongwei,zhongwei,zhongwei,20/Sep/19 10:08,13/Apr/21 20:41,13/Jul/23 08:10,22/Sep/19 06:11,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,API / Python,,,,,0,pull-request-available,,,,It seems we are packaging __pycache__ directories into pyflink.zip. These directories contain bytecode cache files that are automatically generated by python3. We should remove them from the python source code folder before packaging.,,hequn8128,zhongwei,,,,,,,,,,,,"WeiZhong94 commented on pull request #9729: [FLINK-14150][python] Clean up the __pycache__ directories and other empty directories in flink-python source code folder before packaging.
URL: https://github.com/apache/flink/pull/9729
 
 
   ## What is the purpose of the change
   
   *This pull request adds configurations of maven-antrun-plugin to pom.xml of flink-python to clean up the __pycache__ directories and other empty directories in flink-python source code folder before packaging*
   
   
   ## Brief change log
   
     - *add configurations in pom.xml of flink-python to delete __pycache__ directories and empty directories.*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Sep/19 11:18;githubbot;600","hequn8128 commented on pull request #9729: [FLINK-14150][python] Clean up the __pycache__ directories and other empty directories in flink-python source code folder before packaging.
URL: https://github.com/apache/flink/pull/9729
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Sep/19 06:08;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 22 06:11:16 UTC 2019,,,,,,,,,,"0|z06ups:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/19 06:11;hequn8128;Fix in
1.10.0 via 8155d465520c4b616866d35395c3b10f7e809b78
1.9.1 via bdee6e20d501f3f6a72c90a4ad031b3cccce425b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getLatestCheckpoint(true) returns wrong checkpoint,FLINK-14145,13257880,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,uce,uce,20/Sep/19 08:06,29/Sep/19 03:17,13/Jul/23 08:10,24/Sep/19 09:07,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"The flag to prefer checkpoints for recovery introduced in FLINK-11159 returns the wrong checkpoint if:
* checkpoints are preferred  ({{getLatestCheckpoint(true)}}),
* the latest checkpoint is *not* a savepoint,
* more than a single checkpoint is retained.

The current implementation assumes that the latest checkpoint is a savepoint and skips over it. I attached a patch for {{StandaloneCompletedCheckpointStoreTest}} that demonstrates the issue.

You can apply the patch via {{git am -3 < *.patch}}.

 ",,gyfora,jark,pnowojski,SleePy,uce,,,,,,,,,"gyfora commented on pull request #9727: [FLINK-14145] Fix getLatestCheckpoint(true) returns wrong checkpoint
URL: https://github.com/apache/flink/pull/9727
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fixes a bug where the CompletedCheckpointStore would not return the last checkpoint correctly if checkpoints are preferred.
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   Added new test case by @uce to StandaloneCompletedCheckpointStoreTest
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Sep/19 09:08;githubbot;600","asfgit commented on pull request #9727: [FLINK-14145] Fix getLatestCheckpoint(true) returns wrong checkpoint
URL: https://github.com/apache/flink/pull/9727
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Sep/19 09:03;githubbot;600","gyfora commented on pull request #9756: [backport to 1.9][FLINK-14145] Fix getLatestCheckpoint(true) returns wrong checkpoint
URL: https://github.com/apache/flink/pull/9756
 
 
   Backporting of the fix that is already merged to master
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Sep/19 09:09;githubbot;600","gyfora commented on pull request #9756: [backport to 1.9][FLINK-14145] Fix getLatestCheckpoint(true) returns wrong checkpoint
URL: https://github.com/apache/flink/pull/9756
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Sep/19 13:21;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,FLINK-11159,,,,,,,,,,,,,,,"20/Sep/19 08:47;uce;0001-FLINK-14145-runtime-Add-getLatestCheckpoint-test.patch;https://issues.apache.org/jira/secure/attachment/12980876/0001-FLINK-14145-runtime-Add-getLatestCheckpoint-test.patch",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 29 03:17:14 UTC 2019,,,,,,,,,,"0|z06ujk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/19 09:10;gyfora;Thanks [~uce] for catching this, I should have tested this feature more thoroughly ;;;","29/Sep/19 03:17;jark;1.10.0: de92c698bbba464d15de9b70546f1b1ceee5b48b
1.9.1: 3aeeb5fede885f9efeb66fe271828b25ffd6571e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Flink Logo Displayed in Flink Python Shell is Broken,FLINK-14140,13257866,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zhongwei,zhongwei,zhongwei,20/Sep/19 07:10,13/Apr/21 20:41,13/Jul/23 08:10,21/Sep/19 11:25,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,API / Python,,,,,0,pull-request-available,,,,"when executing ""./pyflink-shell.sh local""，we can get such a Logo:

!image-2019-09-20-15-03-09-111.png|width=396,height=553!

It seems that the upper right corner of the squirrel is broken.",,hequn8128,zhongwei,,,,,,,,,,,,"WeiZhong94 commented on pull request #9725: [FLINK-14140][python] fix the broken flink logo in flink python shell.
URL: https://github.com/apache/flink/pull/9725
 
 
   ## What is the purpose of the change
   
   *This pull request repairs the flink logo in flink python shell.*
   
   
   ## Brief change log
   
     - *Remove the line continuations in the logo.*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Sep/19 07:35;githubbot;600","hequn8128 commented on pull request #9725: [FLINK-14140][python] Fix the broken flink logo in flink python shell.
URL: https://github.com/apache/flink/pull/9725
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Sep/19 11:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/19 07:03;zhongwei;image-2019-09-20-15-03-09-111.png;https://issues.apache.org/jira/secure/attachment/12980852/image-2019-09-20-15-03-09-111.png","20/Sep/19 07:23;zhongwei;image-2019-09-20-15-23-17-810.png;https://issues.apache.org/jira/secure/attachment/12980856/image-2019-09-20-15-23-17-810.png","20/Sep/19 07:25;zhongwei;image-2019-09-20-15-25-23-674.png;https://issues.apache.org/jira/secure/attachment/12980859/image-2019-09-20-15-25-23-674.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 21 11:24:35 UTC 2019,,,,,,,,,,"0|z06ugg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/19 07:14;hequn8128;Good catch!
Could you also check whether SQL client or other clients have a similar problem? Thank you.;;;","20/Sep/19 07:26;zhongwei;Hi [~hequn8128], I have checked the Logo in start-scala-shell.sh and sql-client.sh. There is no similar problem on those clients. Their logo are as follows:

!image-2019-09-20-15-23-17-810.png|width=341,height=550!

!image-2019-09-20-15-25-23-674.png|width=398,height=518!;;;","20/Sep/19 13:43;hequn8128;[~zhongwei] Thanks a lot for the checking. The PR looks good. Will merge later.;;;","21/Sep/19 11:24;hequn8128;Fixed via
1.9.1: 8109d7ed7a4f05694017d3e97ede4a99186c22aa
1.10.0: af37ae7607f93c3dd14f8eca55dd69af0dc3675d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix potential memory leak of rest server when using session/standalone cluster,FLINK-14139,13257865,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,20/Sep/19 07:03,25/Sep/19 14:21,13/Jul/23 08:10,25/Sep/19 14:21,,,,,,,,,,1.10.0,1.9.1,,,Runtime / REST,,,,,0,pull-request-available,,,,"Flink's rest server uses netty decoder for http request processing and file uploading. However io.netty.handler.codec.http.multipart.DiskAttribute and io.netty.handler.codec.http.multipart.DiskFileUpload class of netty would register some temp files, including post chunks and upload file chunks, to java.io.DeleteOnExitHook which has a potential of memory leak, because the registered file names will never be deleted before the cluster stops. 

Most of the time, this is not a big problem, however we use Flink session cluster a long running service for ad-hoc SQL query, this problem gets worse.

In fact, Flink handles the clean up of temp files through org.apache.flink.util.ShutdownHookUtil (though not including the post chunks), so it is no need to register these files to java.io.DeleteOnExitHook.",,kevin.cyj,maguowei,trohrmann,,,,,,,,,,,"wsry commented on pull request #9750: [FLINK-14139][rest]Fix potential memory leak problem of rest server.
URL: https://github.com/apache/flink/pull/9750
 
 
   ## What is the purpose of the change
   
   The purpose of this pr is to fix the potential memory leak problem of rest server when using session/standalone cluster. More specifically, Flink's rest server uses netty decoder for http request processing and file uploading. However io.netty.handler.codec.http.multipart.DiskAttribute and io.netty.handler.codec.http.multipart.DiskFileUpload class of netty would register some temp files, including post chunks and upload file chunks, to java.io.DeleteOnExitHook which has a potential of memory leak, because the registered file names will never be deleted before the cluster stops. This pr attempts to solve the problem by disabling the registration of temp files to java.io.DeleteOnExitHook and makes Flink's shutdown hook to take the responsibility of temp file clean up.
   
   ## Brief change log
   
     - *The deleteOnExitTemporaryFile filed of io.netty.handler.codec.http.multipart.DiskAttribute and io.netty.handler.codec.http.multipart.DiskFileUpload is set to false to disable netty to register the shutdown hook.*
     - *Set the baseDirectory of io.netty.handler.codec.http.multipart.DiskAttribute to the file upload directory (shares the same directory with file upload), which will be cleaned up by Flink's shutdown hook when jvm exits.*
     - *Add tests to verify that no temp files will be registered to java.io.DeleteOnExitHook after the fix.*
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Based on the existing FileUploadHandlerTest which will register temp file (both post chunks and uploaded file chunks) to java.io.DeleteOnExitHook, after file uploading, each test case verifies that no file is register to java.io.DeleteOnExitHook. Before the fix, the test will fail and after the fix, the test will pass.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Sep/19 02:55;githubbot;600","tillrohrmann commented on pull request #9750: [FLINK-14139][rest]Fix potential memory leak problem of rest server.
URL: https://github.com/apache/flink/pull/9750
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Sep/19 14:20;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 25 14:21:59 UTC 2019,,,,,,,,,,"0|z06ug8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/19 07:06;kevin.cyj;Some other user of netty also encountered this problem: [https://github.com/netty/netty/issues/8310].;;;","24/Sep/19 09:08;trohrmann;Thanks for reporting this issue [~kevin.cyj]. I'll take a look at the PR.;;;","25/Sep/19 14:21;trohrmann;Fixed via

1.10.0: 63cdae9a8fbf139fce959e0d57029c39bd5b7333
1.9.1: c2956f512f6c4c0f93e87a04a090ceaaf9fd64da;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SystemProcessingTimeServiceTest.testImmediateShutdown failed on Travis,FLINK-14120,13257412,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,anatolii_siuniaev,trohrmann,trohrmann,18/Sep/19 15:08,27/Sep/19 13:59,13/Jul/23 08:10,27/Sep/19 13:59,1.10.0,,,,,,,,,1.10.0,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,,"The test {{SystemProcessingTimeServiceTest.testImmediateShutdown}} failed on Travis with

{code}
java.lang.AssertionError: 

Expected: is null
     but: was <java.lang.InterruptedException: sleep interrupted>
	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.testImmediateShutdown(SystemProcessingTimeServiceTest.java:196)
{code}

https://api.travis-ci.org/v3/job/586514264/log.txt",,anatolii_siuniaev,trohrmann,,,,,,,,,,,,"a-siuniaev commented on pull request #9767: [FLINK-14120][API/Datastream] Fix testImmediateShutdown
URL: https://github.com/apache/flink/pull/9767
 
 
   ##  What is the purpose of the change
   
   The subtle asynchronous bug appearing in test is fixed.
   
   ## Brief change log
   
   *(for example:)*
     - add exception handling in testImmediateShutdown() in SystemProcessingTimeServiceTest to avoid asynchonous error appearing in unexpected parts of test
   
   ## Verifying this change
   
   This change is already covered by existing test (verified by 10k runs with 0 fails). 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Sep/19 07:30;githubbot;600","tillrohrmann commented on pull request #9767: [FLINK-14120][API/Datastream] Fix testImmediateShutdown
URL: https://github.com/apache/flink/pull/9767
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Sep/19 13:58;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 13:59:09 UTC 2019,,,,,,,,,,"0|z06rnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/19 19:53;anatolii_siuniaev;[~till.rohrmann] I'd like to look into this issue. Could you assign it to me, please?;;;","19/Sep/19 07:39;trohrmann;Yes, I've assigned the issue to you [~anatolii_siuniaev].;;;","26/Sep/19 07:34;trohrmann;Another instance: https://api.travis-ci.org/v3/job/589649277/log.txt;;;","27/Sep/19 13:59;trohrmann;Fixed via b8d491c97d305fed7ac680929bb04dd714657c7c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean idle state for RetractableTopNFunction,FLINK-14119,13257385,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Alec.Ch,jark,jark,18/Sep/19 13:23,24/Sep/19 08:32,13/Jul/23 08:10,24/Sep/19 08:31,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"We cleaned the idle state for AppendOnlyTopNFunction and UpdatableTopNFunction, but missed this thing for RetractableTopNFunction. We should add it to avoid unlimited state size. ",,Alec.Ch,jark,,,,,,,,,,,,"AlecCh0402 commented on pull request #9741: [FLINK-14119][table] Clean idle state for RetractableTopNFunction
URL: https://github.com/apache/flink/pull/9741
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The goal for this PR is to fix an issue in `RetractableTopNFunction` where sort keys' states are never cleaned and state size grows without limit. By registering a state-cleanup timer in `processElement` and clean up idle states when `onTimer` is invoked, idle states can be cleaned. More information can be found in  [FLINK-14119](https://issues.apache.org/jira/browse/FLINK-14119).
   
   ## Brief change log
   
   Main changes are here:
   [34b5399](https://github.com/AlecCh0402/flink/commit/511ae6ef82ec45f34e0270c4c534d64de2856107): add `registerProcessingCleanupTimer()`, `onTimer()` in `RetractableTopNFunction` and test for the changes. 
   
   ## Verifying this change
   
   This PR adds a new test for `RetractableTopNFunction`, which can be found in
   `RetractableTopNFunctionTest.testCleanIdleState()`. 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Sep/19 11:34;githubbot;600","wuchong commented on pull request #9741: [FLINK-14119][table] Clean idle state for RetractableTopNFunction
URL: https://github.com/apache/flink/pull/9741
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Sep/19 08:26;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 24 08:31:12 UTC 2019,,,,,,,,,,"0|z06rhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/19 14:49;Alec.Ch;Correct me if I'm wrong. It seems like we need to register state cleanup timer  to RetractableTopNFunction as UpdatableTopNFunction and RetractableTopNFunction did. And I would like to be assigned to solve this issue if possible, thanks.;;;","19/Sep/19 02:02;jark;Yes. We need to cleanup the idle state in {{onTimer()}} callback, just like UpdatableTopNFunction  did. I assigned this to you.;;;","24/Sep/19 08:31;jark;master: 1728e03a179235b70e6b6ad6532eb307717fd171
1.9.1: 0aafa4766387686d868159c027d38140de661736;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis consumer record emitter deadlock under event time alignment,FLINK-14107,13257174,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,17/Sep/19 16:59,19/Sep/19 17:36,13/Jul/23 08:10,18/Sep/19 22:17,1.8.2,1.9.0,,,,,,,,1.10.0,1.8.3,1.9.1,,Connectors / Kinesis,,,,,0,pull-request-available,,,,"When the emitter reaches the max timestamp for the current queue, it stops emitting and waits for the max timestamp to advance. Since it simultaneously selects the next queue as the new ""minimum"" queue, it may deadlock if the previous min queue represents the new global lower bound after the max timestamp advanced. This occurs very infrequently and we were finally able to reproduce.",,thw,,,,,,,,,,,,,"tweise commented on pull request #9702: [FLINK-14107][kinesis] Erroneous queue selection in record emitter may lead to deadlock
URL: https://github.com/apache/flink/pull/9702
 
 
   ## What is the purpose of the change
   
   When the emitter reaches the max timestamp for the current queue, it stops emitting and waits for the max timestamp to advance. Since it simultaneously selects the next queue as new ""minimum"" queue, it may subsequently deadlock if the previous min queue represents the new global lower bound after the max timestamp advanced. This occurs very infrequently and we were finally able to reproduce.
   
   ## Verifying this change
   
   This change was verified on our internal system and reproduced with the unit test.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 18:12;githubbot;600","tweise commented on pull request #9702: [FLINK-14107][kinesis] Erroneous queue selection in record emitter may lead to deadlock
URL: https://github.com/apache/flink/pull/9702
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Sep/19 22:07;githubbot;600","tweise commented on pull request #9708: [FLINK-14107][kinesis] Erroneous queue selection in record emitter may lead to deadlock
URL: https://github.com/apache/flink/pull/9708
 
 
   backport from master
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Sep/19 22:11;githubbot;600","tweise commented on pull request #9709: [FLINK-14107][kinesis] Erroneous queue selection in record emitter may lead to deadlock
URL: https://github.com/apache/flink/pull/9709
 
 
   backport from master
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Sep/19 22:13;githubbot;600","tweise commented on pull request #9709: [FLINK-14107][kinesis] Erroneous queue selection in record emitter may lead to deadlock
URL: https://github.com/apache/flink/pull/9709
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Sep/19 23:56;githubbot;600","tweise commented on pull request #9708: [FLINK-14107][kinesis] Erroneous queue selection in record emitter may lead to deadlock
URL: https://github.com/apache/flink/pull/9708
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Sep/19 23:59;githubbot;600",,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-09-17 16:59:16.0,,,,,,,,,,"0|z06q7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix OperatorIOMetricGroup repeat register problem,FLINK-14094,13257065,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xymaqingxiang,xymaqingxiang,xymaqingxiang,17/Sep/19 09:12,23/Sep/19 14:21,13/Jul/23 08:10,23/Sep/19 14:21,1.10.0,1.9.0,,,,,,,,1.10.0,,,,Runtime / Metrics,,,,,0,pull-request-available,,,,There will be OperatorIOMetricGroup duplicate registration in the TaskMetricGroup's getOrAddOperator() method.,,trohrmann,xymaqingxiang,,,,,,,,,,,,"maqingxiang commented on pull request #9697: [FLINK-14094] [runtime] [metric] Fix OperatorIOMetricGroup repeat register problem
URL: https://github.com/apache/flink/pull/9697
 
 
   ## What is the purpose of the change
   
   There will be OperatorIOMetricGroup duplicate registration in the TaskMetricGroup's getOrAddOperator() method.
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 09:23;githubbot;600","tillrohrmann commented on pull request #9697: [FLINK-14094] [runtime] [metric] Fix OperatorIOMetricGroup repeat register problem
URL: https://github.com/apache/flink/pull/9697
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Sep/19 14:21;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 23 14:21:56 UTC 2019,,,,,,,,,,"0|z06pj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/19 14:21;trohrmann;Fixed via 627e7cb1f265e8cf8e953c17ee492ad957539f2f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job can not trigger checkpoint forever after zookeeper change leader ,FLINK-14091,13257010,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tison,moon_storm,moon_storm,17/Sep/19 02:00,10/Feb/20 11:41,13/Jul/23 08:10,15/Jan/20 08:36,1.9.0,,,,,,,,,1.10.0,,,,Runtime / Checkpointing,,,,,2,pull-request-available,,,,"when zk change leader, the state of curator is suspended,job manager can not tigger checkpoint.but it doesn't tigger checkpoint after zk resume.

we found that the lastState in the class ZooKeeperCheckpointIDCounter  never change back to normal when it fall into SUSPENDED or LOST.
h6. _/**_
 _* Connection state listener. In case of \{@link ConnectionState#SUSPENDED} or {@link_
 _* ConnectionState#LOST} we are not guaranteed to read a current count from ZooKeeper._
 _*/_
_private static class SharedCountConnectionStateListener implements ConnectionStateListener {_

 _private volatile ConnectionState lastState;_

 _@Override_
 _public void stateChanged(CuratorFramework client, ConnectionState newState) {_
 _if (newState == ConnectionState.SUSPENDED || newState == ConnectionState.LOST) {_
 _lastState = newState;_
 _}_
 _}_

 _private ConnectionState getLastState() {_
 _return lastState;_
 _}_
_}_

 

we change the state back. after test, solve the problem.

 
h6. _/**_
 _* Connection state listener. In case of \{@link ConnectionState#SUSPENDED} or {@link_
 _* ConnectionState#LOST} we are not guaranteed to read a current count from ZooKeeper._
 _*/_
_private static class SharedCountConnectionStateListener implements ConnectionStateListener {_

 _private volatile ConnectionState lastState;_

 _@Override_
 _public void stateChanged(CuratorFramework client, ConnectionState newState) {_
 _if (newState == ConnectionState.SUSPENDED || newState == ConnectionState.LOST) {_
 _lastState = newState;_
 _}_
 _else{_
 _/* if connectionState is not SUSPENDED and LOST, reset lastState. */_
 _lastState = null;_
 _}_
 _}_

 _private ConnectionState getLastState() {_
 _return lastState;_
 _}_
_}_

 

log：
h6. {{{{2019-09-16 13:38:38,020 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Unable to }}{{read}} {{additional data from server sessionid 0x26cff6487c2000e, likely server has closed socket, closing socket connection and attempting reconnect}}}}{{{{2019-09-16 13:38:38,122 INFO  org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager  - State change: SUSPENDED}}}}{{{{2019-09-16 13:38:38,123 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.}}}}{{{{2019-09-16 13:38:38,126 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.}}}}{{{{2019-09-16 13:38:38,126 WARN  org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore  - ZooKeeper connection SUSPENDING. Changes to the submitted job graphs are not monitored (temporarily).}}}}{{{{2019-09-16 13:38:38,128 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender akka.tcp:}}{{//flink}}{{@node007224:19115}}{{/user/dispatcher}} {{no longer participates }}{{in}} {{the leader election.}}}}{{{{2019-09-16 13:38:38,128 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender akka.tcp:}}{{//flink}}{{@node007224:19115}}{{/user/resourcemanager}} {{no longer participates }}{{in}} {{the leader election.}}}}{{{{2019-09-16 13:38:38,128 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.}}}}{{{{2019-09-16 13:38:38,128 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender http:}}{{//node007224}}{{:8081 no longer participates }}{{in}} {{the leader election.}}}}{{{{2019-09-16 13:38:38,128 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.}}}}{{{{2019-09-16 13:38:38,128 WARN  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper suspended. The contender akka.tcp:}}{{//flink}}{{@node007224:19115}}{{/user/jobmanager_2}} {{no longer participates }}{{in}} {{the leader election.}}}}{{{{2019-09-16 13:38:38,128 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.}}}}{{{{2019-09-16 13:38:38,128 WARN  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper suspended. Can no longer retrieve the leader from ZooKeeper.}}}}{{{{2019-09-16 13:38:39,109 WARN  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named }}{{'Client'}} {{was found }}{{in}} {{specified JAAS configuration }}{{file}}{{: }}{{'/tmp/jaas-4823064314619540149.conf'}}{{. Will }}{{continue}} {{connection to Zookeeper server without SASL authentication, }}{{if}} {{Zookeeper server allows it.}}}}{{{{2019-09-16 13:38:39,109 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Opening socket connection to server 192.168.7.231}}{{/192}}{{.168.7.231:2181}}}}{{{{2019-09-16 13:38:39,109 ERROR org.apache.flink.shaded.curator.org.apache.curator.ConnectionState  - Authentication failed}}}}{{{{2019-09-16 13:38:39,110 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Socket connection established to 192.168.7.231}}{{/192}}{{.168.7.231:2181, initiating session}}}}{{{{2019-09-16 13:38:39,112 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Unable to }}{{read}} {{additional data from server sessionid 0x26cff6487c2000e, likely server has closed socket, closing socket connection and attempting reconnect}}}}{{{{2019-09-16 13:38:39,778 WARN  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named }}{{'Client'}} {{was found }}{{in}} {{specified JAAS configuration }}{{file}}{{: }}{{'/tmp/jaas-4823064314619540149.conf'}}{{. Will }}{{continue}} {{connection to Zookeeper server without SASL authentication, }}{{if}} {{Zookeeper server allows it.}}}}{{{{2019-09-16 13:38:39,778 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Opening socket connection to server 192.168.7.230}}{{/192}}{{.168.7.230:2181}}}}{{{{2019-09-16 13:38:39,778 ERROR org.apache.flink.shaded.curator.org.apache.curator.ConnectionState  - Authentication failed}}}}{{{{2019-09-16 13:38:39,778 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Socket connection established to 192.168.7.230}}{{/192}}{{.168.7.230:2181, initiating session}}}}{{{{2019-09-16 13:38:39,780 INFO  org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn  - Session establishment complete on server 192.168.7.230}}{{/192}}{{.168.7.230:2181, sessionid = 0x26cff6487c2000e, negotiated timeout = 60000}}}}{{{{2019-09-16 13:38:39,780 INFO  org.apache.flink.shaded.curator.org.apache.curator.framework.state.ConnectionStateManager  - State change: RECONNECTED}}}}{{{{2019-09-16 13:38:39,780 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.}}}}{{{{2019-09-16 13:38:39,780 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.}}}}{{{{2019-09-16 13:38:39,780 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.}}}}{{{{2019-09-16 13:38:39,780 INFO  org.apache.flink.runtime.jobmanager.ZooKeeperSubmittedJobGraphStore  - ZooKeeper connection RECONNECTED. Changes to the submitted job graphs are monitored again.}}}}{{{{2019-09-16 13:38:39,780 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.}}}}{{{{2019-09-16 13:38:39,781 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.}}}}{{{{2019-09-16 13:38:39,781 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.}}}}{{{{2019-09-16 13:38:39,781 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.}}}}{{{{2019-09-16 13:38:39,781 INFO  org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionService  - Connection to ZooKeeper was reconnected. Leader election can be restarted.}}}}{{{{2019-09-16 13:38:39,781 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.}}}}{{{{2019-09-16 13:38:39,781 INFO  org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalService  - Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.}}}}{{{{2019-09-16 13:38:43,142 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 6995 }}{{for}} {{job 21b6ef566750f5766443641254e8e1a9 (16841 bytes }}{{in}} {{49 ms).}}}}{{{{2019-09-16 13:38:43,144 ERROR org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Exception }}{{while}} {{triggering checkpoint }}{{for}} {{job 21b6ef566750f5766443641254e8e1a9.}}}}{{{{java.lang.IllegalStateException: Connection state: SUSPENDED}}}}{{{{    }}{{at org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.checkConnectionState(ZooKeeperCheckpointIDCounter.java:159)}}}}{{{{    }}{{at org.apache.flink.runtime.checkpoint.ZooKeeperCheckpointIDCounter.get(ZooKeeperCheckpointIDCounter.java:133)}}}}{{{{    }}{{at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.triggerCheckpoint(CheckpointCoordinator.java:448)}}}}{{{{    }}{{at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$ScheduledTrigger.run(CheckpointCoordinator.java:1323)}}}}{{{{    }}{{at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)}}}}{{{{    }}{{at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)}}}}{{{{    }}{{at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)}}}}{{{{    }}{{at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)}}}}{{{{    }}{{at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)}}}}{{{{    }}{{at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)}}}}{{{{    }}{{at java.lang.Thread.run(Thread.java:745)}}}}",,andrew_lin,elevy,greeensy,liyu,moon_storm,SleePy,tison,trohrmann,yuchuanchen,yunta,,,,"TisonKun commented on pull request #10754: [FLINK-14091][coordination] Allow updates to connection state when ZKCheckpointIDCounter reconnects to ZK
URL: https://github.com/apache/flink/pull/10754
 
 
   ## What is the purpose of the change
   
   `ZKCheckpointIDCounter` doesn't tolerate ZK suspended & reconnected while it could do. This causes that job can not trigger checkpoint forever after zookeeper change leader.
   
   ## Brief change log
   
   Allow updates to connection state when ZKCheckpointIDCounter reconnects to ZK.
   
   ## Verifying this change
   
   This change is a trivial fix that can be reasoned by code.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jan/20 03:39;githubbot;600","tillrohrmann commented on pull request #10754: [FLINK-14091][coordination] Allow updates to connection state when ZKCheckpointIDCounter reconnects to ZK
URL: https://github.com/apache/flink/pull/10754
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jan/20 08:35;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-14685,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 15 08:36:48 UTC 2020,,,,,,,,,,"0|z06p6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Sep/19 02:35;greeensy;Same problem！;;;","17/Sep/19 02:57;yunta;From the error description, I think {{SharedCountConnectionStateListener}} should also handle the connection state after {{RECONNECTED}}, CC [~uce];;;","17/Sep/19 03:16;moon_storm;{{SharedCountConnectionStateListener already handle it(as shown below), but ZooKeeperCheckpointIDCounter not. }}
h6. @Override
public void stateChanged(CuratorFramework client, ConnectionState newState) {
 if (newState == ConnectionState.SUSPENDED || newState == ConnectionState.LOST) {
 lastState = newState;
 }
 else{
 /* if connectionState is not SUSPENDED and LOST, reset lastState. */
 lastState = null;
 }
};;;","03/Jan/20 03:22;tison;It's a known issue we also faced internally. I have a fix and will push a pull request later today.

cc [~trohrmann];;;","03/Jan/20 03:40;tison;Here is the [pull request|https://github.com/apache/flink/pull/10754]. You can review and comment. Also I'm wondering which version(s) will have this fix.;;;","08/Jan/20 12:41;liyu;Seems to be something we should try to fix in 1.10.0;;;","15/Jan/20 08:36;trohrmann;Fixed via

master:
4b98956f968cb7abf83673e570262f439ca99fe9
25b169744d348afa9d7deac98fa7ab3592343b32
7a0fa1e09979f91f6787c63db2af8143faa8e973
7455a0946ef80ea45f0e79116f99c2812cb6aa5f

1.10.0:
7181254cb45be275039b47db14ac8ff1c030577e
7f27bb6ae139e8628230e5caaaf7b2550c2d4490
7fcda36fe58a28891c4104ec9926b1bf281e7c49
4b78a4e41a138820e0a07ccdc056729180aa7dd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,
SQL CLI doesn't support explain DMLs,FLINK-14089,13256874,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lirui,lirui,16/Sep/19 13:11,29/Apr/21 08:51,13/Jul/23 08:10,29/Apr/21 08:51,,,,,,,,,,,,,,Table SQL / Client,,,,,0,stale-major,,,,"It seems SQL CLI only supports explaining queries. Tried to explain some INSERT statement and get
{noformat}
2019-09-16 21:04:51,755 WARN  org.apache.flink.table.client.cli.CliClient                   - Could not execute SQL statement.
org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL statement.
        at org.apache.flink.table.client.gateway.local.LocalExecutor.explainStatement(LocalExecutor.java:310)
        at org.apache.flink.table.client.cli.CliClient.callExplain(CliClient.java:459)
        at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:289)
        at java.util.Optional.ifPresent(Optional.java:159)
        at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:201)
        at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:123)
        at org.apache.flink.table.client.SqlClient.start(SqlClient.java:105)
        at org.apache.flink.table.client.SqlClient.main(SqlClient.java:194)
Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL statement.
        at org.apache.flink.table.client.gateway.local.LocalExecutor.createTable(LocalExecutor.java:537)
        at org.apache.flink.table.client.gateway.local.LocalExecutor.explainStatement(LocalExecutor.java:306)
        ... 7 more
Caused by: org.apache.flink.table.api.ValidationException: Unsupported SQL query! sqlQuery() only accepts a single SQL query of type SELECT, UNION, INTERSECT, EXCEPT, VALUES, and ORDER_BY.
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:301)
        at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$createTable$12(LocalExecutor.java:534)
        at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:216)
        at org.apache.flink.table.client.gateway.local.LocalExecutor.createTable(LocalExecutor.java:534)
        ... 8 more
{noformat}",,godfreyhe,jark,lirui,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 08:51:34 UTC 2021,,,,,,,,,,"0|z06oco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 11:42;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 08:51;jark;This has been supported in 1.13 by FLINK-21985.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
throws java.lang.ArrayIndexOutOfBoundsException  when emiting the data using RebalancePartitioner. ,FLINK-14087,13256830,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,jiangyu,jiangyu,16/Sep/19 09:38,04/Sep/20 06:58,13/Jul/23 08:10,04/Sep/20 03:04,1.8.0,1.8.1,1.9.0,,,,,,,1.10.3,1.11.2,1.12.0,,API / DataStream,Runtime / Network,,,,0,pull-request-available,,,,"There is the condition the RecordWriter sharing the ChannelSelector instance.

When two RecordWriter instance shared the same ChannelSelector Instance , It may throws 

java.lang.ArrayIndexOutOfBoundsException .  For example,  two recordWriter instance shared the RebalancePartitioner instance. the RebalancePartitioner instance setup 2 number of Channels when the first RecordWriter initializing, next the some RebalancePartitioner instance setup 3 number of  channels When the second RecordWriter initializing. this  throws ArrayIndexOutOfBoundsException when the first RecordWriter instance emits the data.

The Exception likes
|java.lang.RuntimeException: 2 at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:112) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:91) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:47) at org.apache.flink.streaming.runtime.tasks.OperatorChain$BroadcastingOutputCollector.collect(OperatorChain.java:673) at org.apache.flink.streaming.runtime.tasks.OperatorChain$BroadcastingOutputCollector.collect(OperatorChain.java:617) at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:726) at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:699) at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104) at com.xx.flink.demo.wordcount.case3.StateTest$Source.run(StateTest.java:107) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:94) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:57) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:97) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:302) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:734) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.ArrayIndexOutOfBoundsException: 2 at org.apache.flink.runtime.io.network.api.writer.RecordWriter.getBufferBuilder(RecordWriter.java:255) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.copyFromSerializerToTargetChannel(RecordWriter.java:177) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:162) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:128) at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:109) ... 14 more|",,aitozi,aljoscha,dwysakowicz,gaoyunhaii,HideOnBush,jiangyu,kisimple,klion26,maguowei,pnowojski,tartarus,zjwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/19 11:14;jiangyu;image-2019-09-16-19-14-39-403.png;https://issues.apache.org/jira/secure/attachment/12980386/image-2019-09-16-19-14-39-403.png","16/Sep/19 11:15;jiangyu;image-2019-09-16-19-15-34-639.png;https://issues.apache.org/jira/secure/attachment/12980387/image-2019-09-16-19-15-34-639.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 04 06:58:46 UTC 2020,,,,,,,,,,"0|z06o2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/19 09:43;jiangyu;[~zjwang];;;","16/Sep/19 11:04;zjwang;Thanks for reporting this [~jiangyu].

We actually refactored the internal logic for RebalancePartitioner before. In the past it would check whether the selected channel index is beyond the number of channels before return. If exceeds, it would return the first channel index.

But it is breaking my previous assumption that multiple RecordWriter instances would share the same ChannelSelector instance. I need to double check the process of StreamGraph generation whether it would happen this case. It seems not reasonable to share the same internal state for different output edges. If so, for different parallelism in one output edge, it is not strictly rebalanced.

Could you share me your topology structure or the codes you submit the job, then I could easily debug the process of generating StreamGraph.;;;","16/Sep/19 11:16;jiangyu;hi
[~zjwang] ,The topology structure is as follow.

!image-2019-09-16-19-15-34-639.png!;;;","17/Sep/19 08:17;zjwang;Thanks for the further offline confirmation with me [~jiangyu].

After checking the relevant process in StreamGraph, the StreamPartitioner is actually shared for different edges via the structure of {{virtualPartitionNodes}}. IMO this sharing mechanism is not very reasonable because of two issues:
 * From semantic aspect, it is the global rebalance among different stream edges, which would cause actually imbalanced case among different parallelism of one edge. It seems more make sense to rebalance partially in one edge scope.
 * It would limit the runtime implementation or it assumes that there is no state maintaince in runtime implementation. Otherwise it would bring unexpected behavior. In previous version of RebalancePartitioner, it would check whether the selected channel index is beyond the number of channels for every call, so it hides this potential issue before, no matter with whether this behavior is actually balanced or not. And the latest refactoring work removes this check and makes the number of channels as the property of partitioner, then it exposes this bug.

Considering the solution, I prefer to adjust the process of stream graph to build separate partitioner instance for every stream edge. Are there other suggestions or inputs [~pnowojski] ?;;;","17/Sep/19 09:04;pnowojski;Sharing the same {{StreamPartitioner}} instance indeed sounds like a bad idea, especially that we call {{#setup(...)}} twice on them.

However I didn't follow the code and I do not understand what was the rationale behind the current code structure. What could brake if we change the current semantic? I could think about the following thing:
# what if a partitioner has ""huge"" state? Could that be the case? I think all of the current partitioners are very light weight?
 
I also am not familiar with the virtual nodes concept. It looks like [~aljoscha] wrote most of this code, could you [~zjwang] reach out to him? ;;;","17/Sep/19 09:25;zjwang;Thanks for the replies [~pnowojski]!

Unless there are some requirements of sharing information among different edges for special partitioner such as users' CustomPartitioner, the existing implementations of StreamPartitioner seem be independent for different edges. 

Do you think it is feasible to generate separate partitioner instance for different edges? [~aljoscha];;;","17/Sep/19 09:43;aljoscha;I think the partitioners should be separate for the different edges, yes. Can you pinpoint where in the code they use the same one?;;;","17/Sep/19 10:48;zjwang;Thanks for the quick response [~aljoscha]!

The topology from [~jiangyu] is like this :

DataStream dataStream = env.addSource().rebalance();
 dataStream.map().setParallelism(2).filter();
 dataStream.map().setParallelism(3).sink();

 

In StreamGraph#addVirtualPartitionNode, the {{RebalancePartitioner}} is cached inside the structure of {{virtualPartitionNodes}}.

And during StreamGraph#addEdgeInternal(), the same {{RebalancePartitioner}} instance would be fetched from virtualPartitionNodes while adding different edges.;;;","06/Feb/20 08:00;gaoyunhaii;Hi all, we have met with a similar problem online recently: uses write the code 
{code:java}
 a = env.addSource().rebalance();
 b = a.map()
 c = a.map() {code}
 

and the rebalance partition is then shared between the edges _a -> b_ and _a -> c_, which causes half of tasks of _b_ and _c_ do not receive records. As a temporary fix, we asked users to use the following code instead
{code:java}
a = env.addSource();
b = a.rebalance().map();
c = a.rebalance().map()
{code}
 

 

In summary, currently when downstream operators are connected to the same rebalance() PartitionTransformation, the partitioner is shared between all the edges at runtime. Otherwise when the downstream operators are connected to different rebalance() PartitionTransformation, each edge will have its own partitioner at runtime. I think this should also be reasonable, but it seems to be a little divergence from the intuitive thought of users.

 

Therefore, do we still think always using distinct partitioner for each edge is better ? If so, we should copy the partitioner for each edge when creating graph. The down side of the change will still be that if users really want to share the same partitioner between the edges, he would not be able to achieve this after the change.;;;","06/Feb/20 08:52;pnowojski;I think the partitioners should be separate for the different edges, as sharing is not very intuitive, especially the behaviour that you [~gaoyunhaii] have mentioned is weird (half task not getting any records). [~aljoscha] already confirmed that it shouldn't be like that, so if someone would like to fox this, I can assign this ticket to him.;;;","06/Feb/20 08:56;gaoyunhaii;[~pnowojski]  Got it. Very thanks for the explanation! :) I'd like to have this issue fixed, and will open a PR as soon as possible. ;;;","04/Sep/20 03:04;gaoyunhaii;fix in master: d7fe9af0b6caa1c76e811240e53434969be771b1;;;","04/Sep/20 06:58;dwysakowicz;Fixed in:
* master
** d7fe9af0b6caa1c76e811240e53434969be771b1
* 1.11.2
** 7ecffb4e31505b1f33289c278dc7f397c1f229d6
* 1.10.3
** 56bb246edc778a1360cfd16a4a9d6e3adc132d7b;;;",,,,,,,,,,,,,,,,,,,
'ClassNotFoundException: KafkaException' on Flink v1.9 w/ checkpointing,FLINK-14076,13256702,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jdm2212,jdm2212,jdm2212,14/Sep/19 21:26,25/Oct/19 05:03,13/Jul/23 08:10,24/Sep/19 17:21,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Connectors / Kafka,Runtime / Checkpointing,,,,0,pull-request-available,,,,"A Flink job that worked with checkpointing on a Flink v1.8.0 cluster fails on a Flink v1.9.0 cluster with checkpointing. It works on a Flink v1.9.0 cluster _without_ checkpointing. It is specifically _enabling checkpointing on v1.9.0_ that causes the JM to start throwing ClassNotFoundExceptions. Full stacktrace: [^error.txt]

The job reads from Kafka via FlinkKafkaConsumer and writes to Kafka via FlinkKafkaProducer.

The jobmanagers and taskmanagers are standalone.

The exception is being raised deep in some Flink serialization code, so I'm not sure how to go about stepping through this in a debugger. The issue is happening in an internal repository at my job, but I can try to get a minimal repro on GitHub if it's not obvious from the error message alone what's broken.",,jdm2212,klion26,pnowojski,Terry1897,tison,trohrmann,,,,,,,,"tillrohrmann commented on pull request #9742: [FLINK-14076] Ensure CheckpointException can always be deserialized on JobManager
URL: https://github.com/apache/flink/pull/9742
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Sep/19 17:20;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,FLINK-14519,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/19 21:01;jdm2212;error.txt;https://issues.apache.org/jira/secure/attachment/12980335/error.txt",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 24 17:21:07 UTC 2019,,,,,,,,,,"0|z06nag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Sep/19 17:54;jdm2212;It looks like this is due to a KafkaException getting thrown while Flink is trying to checkpoint. The KafkaException gets wrapped in a 'DeclineCheckpoint' and sent back to the JobManager. The JobManager can't deserialize it because the JobManager's classpath does not include org.apache.kafka:kafka-clients by default.

 

Gonna dig in some more and see if I can figure out which exception is at fault.;;;","17/Sep/19 19:56;jdm2212;Confirmed: the issue is that FlinkKafkaProducer#checkErroneous throws a wrapped KafkaException, which causes a ClassNotFoundException when it hits the JobManager. Not sure how/why this is different on Flink v1.9 vs v1.8.;;;","18/Sep/19 18:37;jdm2212;Two steps to get the JobManager to stay happy:
 # add 'org.apache.kafka:kafka-clients:2.2.0' (and all its transitive deps) to the JM and TM classpaths
 # add this config entry in the JM and TM: _classloader.parent-first-patterns.additional: ""org.apache.kafka.;org.apache.commons.""_

The first one is necessary to allow JM to deserialize exceptions from failed checkpoints. The second one is necessary to ensure that all code uses those jars and there aren't conflicts of the form ""can't assign _LinkedMap from TM classpath_ to field of type _LinkedMap from task classpath_"";;;","22/Sep/19 12:08;Terry1897;This problem  is caused by TM and JM jar package environment inconsistent or jar loaded behavior inconsistent in nature.
Maybe the behavior  of standalone cluster’s dynamic class loader changed in flink 1.9 since you mentioned that your program run normally in flink 1.8.;;;","24/Sep/19 17:21;trohrmann;Fixed via

1.10.0: 2bcf5094919a0b3e6db99efcea0b259826f59ad0
1.9.1: 9ee42ec6b009f1c4da7265851e23cfaa45f4c83a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
MesosResourceManager can't create new taskmanagers in Session Cluster Mode.,FLINK-14074,13256478,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,Atlaster,Atlaster,13/Sep/19 08:46,31/Oct/19 13:01,13/Jul/23 08:10,31/Oct/19 13:01,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.2,,,Deployment / Mesos,,,,,1,pull-request-available,,,,"Hi, I'm trying to launch multiple jobs in Flink Session Cluster, deployed on mesos.
 Flink's version is 1.9.0.

The very first resource allocation completes successfully, and first submitted job launches, but submitting any amount of jobs afterwords doesn't affect the cluster in any way and no additional TaskManagers are allocated.

From the logs I see that MesosResourceManager is requesting Slots for the newly submitted jobs:  ""{{o.a.f.m.r.c.MesosResourceManager - Request slot with profile ResourceProfile...""}} but line {{""Starting a new worker.}}"" appears in log only the same amount of times as taskmanagers count, allocated for the first job.

I'm a complete noob in flink internals, but took a wild guess about a reason. I think that the problem is in this check: [https://github.com/apache/flink/blob/release-1.9.0/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java#L436]

It might be that RM is lazily allocated at the first call by a factory, and then a private final field {{slotsPerWorker}} is set. So this check will prevent creation of any new worker after iterator traverses the entire collection. My main assumption is that {{slotsPerWorker}} is never modified again.

 

I'm sorry that I didn't do much of investigation before reporting, but I'll try to do some after a weekend. I plan to build flink without this check and see if it helps. Also I'll play around with tests for this RM. Since it's my time running time flink internals, I'll be back after a few days.

Any help will much appreciated.

Thanks in advance.",Flink HA Session cluster 1.9.0 on mesos.,Atlaster,guoyangze,trohrmann,,,,,,,,,,,"tillrohrmann commented on pull request #10002: [FLINK-14074][mesos] Forward configuration to Mesos TaskExecutor
URL: https://github.com/apache/flink/pull/10002
 
 
   ## What is the purpose of the change
   
   This commit forwards the Flink configuration to a Mesos `TaskExecutor` by
   creating the `ContainerSpecification` with it. This will forward all options
   which are dynamically configured until the `ResourceManager` has been started
   to the Mesos `TaskExecutor`.
   
   ## Verifying this change
   
   - Added `LaunchableMesosWorkerTest#launch_withNonDefaultConfiguration_forwardsConfigurationValues`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/19 12:15;githubbot;600","tillrohrmann commented on pull request #10002: [FLINK-14074][mesos] Forward configuration to Mesos TaskExecutor
URL: https://github.com/apache/flink/pull/10002
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Oct/19 13:00;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-12812,FLINK-13241,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 31 13:01:43 UTC 2019,,,,,,,,,,"0|z06lwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/19 10:28;trohrmann;Hi [~Atlaster], this sounds indeed like a bug. The strange thing is that the check should only fail if the request {{resourceProfile}} does not match the {{ResourceProfile}} in {{slotsPerWorker}}. At the moment all workers are being started with the same {{ResourceProfile}} based on the configured number of slots and the total {{TaskManager}} memory. Moreover, the slot request should always set the required {{ResourceProfile}} to {{ResourceProfile.UNKNOWN}} which should match any other {{ResourceProfile}} instance.

Hence my question would be what kind of {{ResourceProfiles}} are the slot requests asking for? It should be visible in the {{jobmanager.log}} file on INFO level. The statement looks like {{""Requesting new slot [{}] and profile {} from resource manager""}}. Maybe you could also share the logs with us so that I could take a look as well.

You are right that {{slotsPerWorker}} should never be modified after its creation. It acts as a template for every {{TaskManager's}} set of slots with which it is started.;;;","13/Sep/19 13:31;Atlaster;Hi [~till.rohrmann], thanks for replying.
All {{ResourceProfiles}} for requested slots are of {{UNKNOWN}} type:

{{ResourceProfile\{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=-1, nativeMemoryInMB=-1, networkMemoryInMB=-1, managedMemoryInMB=-1}}}

Sorry, can't share log at the moment, but if you want, I can prepare an example log next week.;;;","13/Sep/19 19:51;trohrmann;This is strange, because {{UNKNOWN}} should match with every slot. Hence one should never return https://github.com/apache/flink/blob/release-1.9.0/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java#L437. Really curious what the problem here is.

If you manage to produce an example log, then this would help a lot with understanding the problem better.;;;","18/Sep/19 13:07;Atlaster;Hi [~till.rohrmann], I'm sorry for the delay on my end. I was able to look into this issue only today.

Firstly, I've prepared example logs, but I need this log to be validated (still in progress), so I can share it outside of company.

But anyway, I've built flink with extended logging and found out some interesting things.
 1. This check: [https://github.com/apache/flink/blob/release-1.9.0/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java#L436] has nothing to do with the problem, it isn't even called after the first job is cancelled/completed.

2. Last thing which happens during new slot allocation is this slot request submitted here: [https://github.com/apache/flink/blob/release-1.9.0/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java#L437], which I traced to method {{internalRequestSlot}} [https://github.com/apache/flink/blob/release-1.9.0/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManagerImpl.java#L750] but method {{allocateResource}} was never called: [https://github.com/apache/flink/blob/release-1.9.0/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManagerImpl.java#L795]

 

Right now I'm adding even more logging to see what is blocking slot allocation. I'll keep you posted, but as an update, I'm leaning towards some problems with pending slots.

 

UPD. I've rerun my test-case one more time and been able to locate possible problem even further. For some reason when first job is completed or cancelled, it still leaves pending slots behind, even if UI shows 0 total task slots. So when the next job is submitted, it is successfully getting a slot here: [https://github.com/apache/flink/blob/release-1.9.0/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManagerImpl.java#L779] (because {{pendingSlots}} map is not empty) and acquiring it here: [https://github.com/apache/flink/blob/release-1.9.0/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManagerImpl.java#L814] . But since mesos worker are already stopped this slot acquiring freezes job execution and every task ends up in SCHEDULED state. 
Since I haven't been able to find a place where {{pendingSlots}} map is cleaned after a worker shutdown,  I think that might be a reason for such behaviour.

[~till.rohrmann] , any ideas here?;;;","18/Sep/19 16:38;trohrmann;Thanks a lot for looking into the problem in more depth [~Atlaster]. In the {{YarnResourceManager}} we always check whether a new container needs to be started if a container/task stops or cannot get started via: https://github.com/apache/flink/blob/master/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java#L510.

Since we restart failed Mesos tasks in https://github.com/apache/flink/blob/master/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java#L668 the only way I can think of how we could ""lose"" Mesos tasks is at start up which is not reported or maybe there is a problem with the matching of pending slots https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManagerImpl.java#L594. For the latter problem it would be interesting to see with which slot profile the {{TaskManagers}} are being started and which profile the slots report when they register.;;;","19/Sep/19 07:14;trohrmann;So maybe it is the same problem as with FLINK-13241.;;;","19/Sep/19 11:13;Atlaster;Yeah, looks suspiciously similar to FLINK-13241. I'll try to look into this similarity.

I'll add a little bit of info about my test-case: 
I deploy a simple word-count job, then wait for its completion, which results in something like this: [https://gist.github.com/Atlaster/c4ebc39e02006c2fe4daf6fedcb22481]

I'm logging method names and a little bit extra like this: [https://gist.github.com/Atlaster/1a50229e62cc710ebcac2734b98e0f32]

[~trohrmann], can you please correct me if I'm wrong?
When taskmanager is removed (I assumed that it was removed based by logs I have) internalUnregisterTaskManager method [https://github.com/apache/flink/blob/release-1.9.0/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManagerImpl.java#L1146] is called. When it supposed to remove all slots ac cancel all pending slots here [https://github.com/apache/flink/blob/release-1.9.0/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManagerImpl.java#L937]. But when new job is started there're still 4 pending slots (1 taskmanager has 4 slots in my configuration).
Isn't flink supposed to remove all pending slots when taskmanager is removed?;;;","24/Oct/19 14:24;trohrmann;I think the problem is the following: Flink thinks that it starts a {{TaskExecutor}} with 1 GB of memory. However, for some reason, the {{TaskExecutor}} is started with more memory. Due to this, the reported slots will have more resources than Flink expects. Internally, Flink registers pending slots in the {{SlotManager#pendingSlots}} [1]. The contract is that a pending slot gets removed if Flink receives an slot report which exactly matches the pending slot's profile [2]. If this is not the case (e.g. if the {{TaskExecutor}} is started with more memory than asked for), then Flink still accepts the reported slot but it does not remove the pending slot. As a consequence, Flink still thinks that there will be a slot coming which will exactly match the pending slot. Hence, when you submit the second job and if the first {{TaskExecutor}} has been released, then Flink thinks that it does not need to start a new {{TaskExecutor}} because there is still a pending slot in {{SlotManagerImpl#pendingSlots}}.

A similar problem has been reported here [3].

A quick solution would be to figure out why the {{MesosResourceManager}} starts {{TaskExecutors}} with more memory. A cleaner and more robust solution would probably be to not rely on exact {{ResourceProfile}} matchings in order to remove pending slots. It would be better if one could set the {{TaskExecutor's}} {{ResourceID}} and match against this instead of the {{ResourceProfile}}.

[1] https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManagerImpl.java#L801
[2] https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManagerImpl.java#L594
[3] https://stackoverflow.com/questions/58537199/apache-flink-resource-manager-app-master-fails-allocating-new-task-managers-af;;;","24/Oct/19 20:33;trohrmann;I believe the problem is that we don't set {{TaskManagerOptions#MANAGED_MEMORY_SIZE}} in the case of Mesos. Due to this, the {{TaskExecutor}} uses {{EnvironmentInformation.getSizeOfFreeHeapMemoryWithDefrag()}} to calculate the free memory and based on that the managed memory size. Since this is not predictable, it is very likely that the reported slot won't match the expected slot profile.

As a quick fix, I propose to set this configuration option when starting a Mesos {{TaskExecutor}}.;;;","25/Oct/19 11:10;Atlaster;[~trohrmann] thanks a lot for explanation and for finding the root case.
Sorry for absence of any actions/research from my side. I was sidetracked with other things.

I've tested setting [taskmanager.memory.size|[https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/config.html#taskmanager-memory-size]] and it worked like a charm for me (Version: 1.9.0 Commit: 9c32ed9).;;;","26/Oct/19 12:48;trohrmann;The same happened to me [~Atlaster]. Great to hear that setting {{taskmanager.memory.size}} resolves the problem :-);;;","31/Oct/19 13:01;trohrmann;Fixed via

1.10.0: c0f3c8044a319c3630c390f8b1603a7edf076181
1.9.2: 97ff5a06931c4c5af44f36b8812cfbbc62966747;;;",,,,,,,,,,,,,,,,,,,,
Pyflink building failure in master and 1.9.0 version,FLINK-14066,13256289,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dian.fu,coldmoon7777777,coldmoon7777777,12/Sep/19 08:13,20/Nov/19 03:11,13/Jul/23 08:10,20/Nov/19 03:11,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.2,,,API / Python,Build System,,,,0,beginner,build,pull-request-available,,"ATTENTION: This is a issue about building pyflink, not development.

During we build pyflink...

After we have built flink from flink source code, a folder named ""target"" is generated.

Then, following the document description, ""cd flink-python; python3 setup.py sdist bdist_wheel"", error happens.

Root cause: in the setup.py file, line 75, ""FLINK_HOME = os.path.abspath(""../build-target"")"", the program can't found folder ""build-target"", however, the building of flink generated a folder named ""target"". So error happens in this way...

 

The right way:

in ../flink-python/setup.py line 75, modify code as following:

FLINK_HOME = os.path.abspath(""../target"")","windows 10 enterprise x64（mentioned as build environment, not development environment.）

powershell x64

flink source master and 1.9.0 version

jdk-8u202

maven-3.2.5",aljoscha,coldmoon7777777,dian.fu,gjy,hequn8128,sunjincheng121,,,,,,,,"dianfu commented on pull request #10188: [FLINK-14066][python] Support to install pyflink in windows
URL: https://github.com/apache/flink/pull/10188
 
 
   ## What is the purpose of the change
   
   *This pull request adds support to install pyflink in windows.*
   
   ## Brief change log
   
     - *Fixes setup.py to allow pyflink could be installed in windows*
   
   ## Verifying this change
   
   Test manually that pyflink could be installed in windows with both of the following commands:
   - ""python setup.py install""
   - ""python setup.py sdist && pip install .\dist\apache-flink-1.10.dev0.tar.gz""
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Nov/19 06:48;githubbot;600","hequn8128 commented on pull request #10188: [FLINK-14066][python] Support to install pyflink in windows
URL: https://github.com/apache/flink/pull/10188
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 01:23;githubbot;600",,,,,,,,,,,,,3600,2400,1200,33%,3600,2400,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/19 08:11;coldmoon7777777;setup.py;https://issues.apache.org/jira/secure/attachment/12980172/setup.py",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Wed Nov 20 03:10:48 UTC 2019,,,,,,,,,,"0|z06kqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/19 08:34;dian.fu;Hi [~coldmoon7777777], currently PyFlink is still not supported on Windows. So I'm afraid that there may be also other issues beside this one, i.e. the corresponding window scripts for pyflink-gateway-server.sh is needed to run on windows (There is an ticket FLINK-12717 for this). ;;;","12/Sep/19 12:26;coldmoon7777777;Dear Dian Fu, tks for watching my issue. I know pyflink is not supported for development on windows. All my work is about the building stage of pyflink.
The issue I report is about the building stage of pyflink.

I am a little confused about your comment, do you mean the ""pyflink-gateway-server.sh"" that can't be run on win is the root cause of building pyflink failure？;;;","12/Sep/19 13:06;dian.fu;I mean that for windows support, we need to consider both building and running pyflink on windows.
If you want to just build pyflink on windows(do not need to run it on windows), then we can firstly fix the build issue. If you also have requirements to run pyflink on windows, then we need also add the corresponding window scripts for scripts such as pyflink-gateway-server.sh.;;;","12/Sep/19 13:27;coldmoon7777777;Right now, I only focus on the building stage of pyflink.

The application is running on liunx or docker.

I think I will take time to commit new setup.py next week if no one argues other opinions.;;;","18/Sep/19 11:10;hequn8128;[~coldmoon7777777] Hi, thanks for reporting the problem!

The flink/target folder can not be used as the FLINK_HOME. However, you can set FLINK_HOME to flink/flink-dist/target/flink-${project.version}-bin/flink-${project.version} if flink/build-target is not generated when build under windows. 

The flink/build-target is generated only under Unix systems as a symbolic link of the flink/flink-dist/target/flink-${project.version}-bin/flink-${project.version} folder.;;;","12/Nov/19 08:42;aljoscha;So is this a not a real issue? If yes then I would like to close it.;;;","13/Nov/19 03:17;dian.fu;[~aljoscha] Thanks a lot for the remind. I think it's better to fix it as the changes are small and it's valuable for windows users. I propose to just update the setup.py script for the value of FLINK_HOME, i.e. set FLINK_HOME as ""../flink-dist/target/flink-${project.version}-bin/flink-${project.version}"". 

If my suggestion makes sense to you, I'll fix it ASAP. ;;;","13/Nov/19 05:48;sunjincheng121;+1 for fix this issue as your proposal.;;;","20/Nov/19 03:10;hequn8128;Fixed in 
1.9.2 via 8fb54e5a3a69ea3250c1cba01bc1d2504acd968d
in 1.10.0 via 32a322713cb917f59825bc61a1d9b3c36c81affd;;;",,,,,,,,,,,,,,,,,,,,,,,
blink planner dense_rank corner case bug,FLINK-14053,13256100,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,11/Sep/19 12:10,24/Oct/19 12:24,13/Jul/23 08:10,24/Oct/19 12:24,1.9.0,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"sql :

val rank =
 """"""
 |SELECT
 | gradeId,
 | classId,
 | stuId,
 | score,
 | dense_rank() OVER (PARTITION BY gradeId, classId ORDER BY score asc) as dense_rank_num
 |FROM student
 |
 """""".stripMargin

sample date:

row(""grade2"", ""class2"", ""0006"", 90),
row(""grade1"", ""class2"", ""0007"", 90),
row(""grade1"", ""class1"", ""0001"", 95),
row(""grade1"", ""class1"", ""0002"", 94),
row(""grade1"", ""class1"", ""0003"", 97),
row(""grade1"", ""class1"", ""0004"", 95),
row(""grade1"", ""class1"", ""0005"", 0)

the dense_rank ranks from 0, but it should be from 1

 ",,jackylau,jark,lzljs3620320,,,,,,,,,,,"liuyongvs commented on pull request #9966: [FLINK-14053] [blink-planner] DenseRankAggFunction.accumulateExpressions. it should be thinki…
URL: https://github.com/apache/flink/pull/9966
 
 
   # What is the purpose of the change
   - dense_rank is not thinking about the the order by expression equals to  inital lastValue. This PR is to fix this corner case
   # Brief change log
   - modify the logic 
   - add one unit test
   # Does this pull request potentially affect one of the following parts:
   - doen't affect others
   # Documentation
   Does this pull request introduce a new feature? (yes / ***no***)
   If yes, how is the feature documented? (***not applicable*** / docs / JavaDocs / not documented)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Oct/19 07:48;githubbot;600","KurtYoung commented on pull request #9966: [FLINK-14053] [blink-planner] DenseRankAggFunction.accumulateExpressions. it should be thinki…
URL: https://github.com/apache/flink/pull/9966
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Oct/19 12:23;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 24 12:24:39 UTC 2019,,,,,,,,,,"0|z06jko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Sep/19 12:16;jackylau;[~jark] it is a bug in DenseRankAggFunction.accumulateExpressions. it should be thinking about a corner case when the order by expression equals to  inital lastValue.

it should be fix at form below.  could I commit a PR ？

accExpressions[0] = ifThenElse(and(orderKeyEqualsExpression(), not(equalTo(sequence, literal(0L)))), sequence, plus(sequence, literal(1L)));

 ;;;","12/Sep/19 05:45;jark;Yes. I think this is a bug. But I'm not sure whether this fix is a right way. [~lzljs3620320];;;","12/Sep/19 06:59;lzljs3620320;[~jackylau] Thanks for reporting this bug. Yes, DenseRankAggFunction should be same as RankAggFunction, you can do some abstract to RankLikeAggFunctionBase, these two functions can share some logical. Feel free to submit a PR, I can review it.;;;","24/Oct/19 12:24;ykt836;Fixed in master: 37f67b7d394ccb3355ebc995af1c3fee04ce060f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update error message for failed partition updates to include task name,FLINK-14049,13256044,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sewen,sewen,sewen,11/Sep/19 07:45,29/Sep/19 02:39,13/Jul/23 08:10,29/Sep/19 02:39,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The error message for failed partition updates does not include the task name.
That makes it useless during debugging.

Adding the task name is a simple addition that make this error message much more helpful.",,jark,sewen,,,,,,,,,,,,"StephanEwen commented on pull request #9670: [FLINK-14049] [coordination] Add task name to error message for failed partition updates
URL: https://github.com/apache/flink/pull/9670
 
 
   ## What is the purpose of the change
   
   The error message for failed partition updates does not include the task name.
   That makes it useless during debugging.
   
   Adding the task name is a simple addition that make this error message much more helpful.
   
   ## Brief change log
   
   Updates the error message in the future completion handler.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature: no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Sep/19 08:09;githubbot;600","asfgit commented on pull request #9670: [FLINK-14049] [coordination] Add task name to error message for failed partition updates
URL: https://github.com/apache/flink/pull/9670
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Sep/19 13:16;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 29 02:39:45 UTC 2019,,,,,,,,,,"0|z06j88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/19 02:39;jark;1.10.0: fecd108d2b5164e65b46d61ffe5e69142f2c84d5
1.9.1: abaa901adc6a115756b2099339d80c992ceb14f0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointMigrationTestBase is super slow,FLINK-14043,13255889,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,trohrmann,trohrmann,10/Sep/19 13:22,11/Sep/19 12:27,13/Jul/23 08:10,11/Sep/19 12:27,1.10.0,1.8.1,1.9.0,,,,,,,1.10.0,1.8.3,1.9.1,,Runtime / State Backends,Tests,,,,0,pull-request-available,,,,"The subclasses of {{SavepointMigrationTestBase}} take super long to execute. On my local machine

* {{TypeSerializerSnapshotMigrationITCase}} takes 2min 30s
* {{StatefulJobWBroadcastStateMigrationITCase}} takes 1min 45s
* {{StatefulJobSavepointMigrationITCase}} takes 2min 5s

to execute. The reasons for the long runtimes seem to be that we are using the {{AccumulatorCountingSink}} which uses the accumulators to signal when a job is done. Since the accumulators are being sent with the TM heartbeats, the heartbeat interval how fast the client realizes that the job can be shut down. The default heartbeat interval is {{10 s}} and hence it takes always at least 10 seconds until the client stops the job.

I suggest to decrease the heartbeat interval in the {{SavepointMigrationTestBase}} to 300ms in order to speed up the tests. On my machine the test runtimes with this settings are:

* {{TypeSerializerSnapshotMigrationITCase}} takes 13s
* {{StatefulJobWBroadcastStateMigrationITCase}} takes 10s
* {{StatefulJobSavepointMigrationITCase}} takes 11s
",,trohrmann,,,,,,,,,,,,,"tillrohrmann commented on pull request #9664: [FLINK-14043] Speed up SavepointMigrationTestBase sub classes
URL: https://github.com/apache/flink/pull/9664
 
 
   ## What is the purpose of the change
   
   Since all SavepointMigrationTestBase sub classes rely on the
   MigrationTestUtils.AccumulatorCountingSink which uses user code accumulators
   in order to communicate with the test driver, we set the heartbeat interval
   to 300ms in order to speed the test execution up. The reason this works is
   that Flink transports user code accumulators from the TM to the JM via
   the heartbeats. Hence, the heartbeat interval represents the lower boundary
   for the test completion.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Sep/19 13:37;githubbot;600","tillrohrmann commented on pull request #9664: [FLINK-14043] Speed up SavepointMigrationTestBase sub classes
URL: https://github.com/apache/flink/pull/9664
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Sep/19 12:24;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 11 12:27:16 UTC 2019,,,,,,,,,,"0|z06i9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Sep/19 12:27;trohrmann;Fixed via

1.10.0: 610d0aa26c63ee09182b20ea46235455f00f687e
1.9.1: 7a9640f2638da3f6b400a207f89dabae3c28cdaf
1.8.3: b03c94360440b3926afff10b605497a46883f5eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionGraph deploy failed due to akka timeout,FLINK-14038,13255784,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liupengcheng,liupengcheng,liupengcheng,10/Sep/19 02:53,21/Feb/20 16:38,13/Jul/23 08:10,21/Feb/20 16:38,1.9.0,,,,,,,,,1.10.1,1.11.0,,,Runtime / Task,,,,,0,pull-request-available,,,,"When launching the flink application, the following error was reported, I downloaded the operator logs, but still have no clue. The operator logs provided no useful information and was cancelled directly.

JobManager logs:
{code:java}
java.lang.IllegalStateException: Update task on TaskManager container_e860_1567429198842_571077_01_000006 @ zjy-hadoop-prc-st320.bj (dataPort=50990) failed due to:
	at org.apache.flink.runtime.executiongraph.Execution.lambda$sendUpdatePartitionInfoRpcCall$14(Execution.java:1395)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@zjy-hadoop-prc-st320.bj:62051/user/taskmanager_0#-171547157]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:871)
	at akka.dispatch.OnComplete.internal(Future.scala:263)
	at akka.dispatch.OnComplete.internal(Future.scala:261)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:644)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
	at java.lang.Thread.run(Thread.java:748)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@zjy-hadoop-prc-st320.bj:62051/user/taskmanager_0#-171547157]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
	... 9 more
{code}
operator logs:
{code:java}
2019-09-09 18:34:06,867 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Received task Partition (4/5).
2019-09-09 18:34:06,868 INFO  org.apache.flink.runtime.taskmanager.Task                     - Partition (4/5) (97d7df744b93f4ee46750bbd6a0113e8) switched from CREATED to DEPLOYING.
2019-09-09 18:34:06,870 INFO  org.apache.flink.runtime.taskmanager.Task                     - Creating FileSystem stream leak safety net for task Partition (4/5) (97d7df744b93f4ee46750bbd6a0113e8) [DEPLOYING]
2019-09-09 18:34:06,870 INFO  org.apache.flink.runtime.taskmanager.Task                     - Loading JAR files for task Partition (4/5) (97d7df744b93f4ee46750bbd6a0113e8) [DEPLOYING].
2019-09-09 18:34:06,871 INFO  org.apache.flink.runtime.taskmanager.Task                     - Registering task at network: Partition (4/5) (97d7df744b93f4ee46750bbd6a0113e8) [DEPLOYING].
2019-09-09 18:34:07,075 INFO  org.apache.flink.runtime.taskmanager.Task                     - Partition (4/5) (97d7df744b93f4ee46750bbd6a0113e8) switched from DEPLOYING to RUNNING.
2019-09-09 18:34:07,255 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Received task Sort-Partition (4/5).
2019-09-09 18:34:07,258 INFO  org.apache.flink.runtime.taskmanager.Task                     - Sort-Partition (4/5) (a721ca202bc8bf2e2aa4b41b1e4a1091) switched from CREATED to DEPLOYING.
2019-09-09 18:34:07,261 INFO  org.apache.flink.runtime.taskmanager.Task                     - Creating FileSystem stream leak safety net for task Sort-Partition (4/5) (a721ca202bc8bf2e2aa4b41b1e4a1091) [DEPLOYING]
2019-09-09 18:34:07,261 INFO  org.apache.flink.runtime.taskmanager.Task                     - Loading JAR files for task Sort-Partition (4/5) (a721ca202bc8bf2e2aa4b41b1e4a1091) [DEPLOYING].
2019-09-09 18:34:07,263 INFO  org.apache.flink.runtime.taskmanager.Task                     - Registering task at network: Sort-Partition (4/5) (a721ca202bc8bf2e2aa4b41b1e4a1091) [DEPLOYING].
2019-09-09 18:34:07,303 INFO  org.apache.flink.runtime.taskmanager.Task                     - Sort-Partition (4/5) (a721ca202bc8bf2e2aa4b41b1e4a1091) switched from DEPLOYING to RUNNING.
2019-09-09 18:34:54,625 INFO  org.apache.flink.runtime.taskmanager.Task                     - Attempting to cancel task DataSource (at org.apache.flink.api.scala.ExecutionEnvironment.createInput(ExecutionEnvironment.scala:390) (org.apache.flink.api.scala.hadoop.mapreduce.HadoopInpu) (5/5) (8c6262b3f802f82d60a1999f2e040a68).
2019-09-09 18:34:54,806 INFO  org.apache.flink.runtime.taskmanager.Task                     - DataSource (at org.apache.flink.api.scala.ExecutionEnvironment.createInput(ExecutionEnvironment.scala:390) (org.apache.flink.api.scala.hadoop.mapreduce.HadoopInpu) (5/5) (8c6262b3f802f82d60a1999f2e040a68) switched from RUNNING to CANCELING.
2019-09-09 18:34:54,806 INFO  org.apache.flink.runtime.taskmanager.Task                     - Triggering cancellation of task code DataSource (at org.apache.flink.api.scala.ExecutionEnvironment.createInput(ExecutionEnvironment.scala:390) (org.apache.flink.api.scala.hadoop.mapreduce.HadoopInpu) (5/5) (8c6262b3f802f82d60a1999f2e040a68).

{code}
I checked the network and it's good. so maybe there are some problems with the taskManager? ","Flink on yarn

Flink 1.9.0",klion26,liupengcheng,sewen,tison,trohrmann,wind_ljy,zhuzh,,,,,,,"liupc commented on pull request #9703: [FLINK-14038]Add default GC options for flink on yarn to facilitate debugging
URL: https://github.com/apache/flink/pull/9703
 
 
   
   ## What is the purpose of the change
   
   When long full gc or oom happen in flink applications running on yarn, there are no good ways to debugging or check the reason. mostly we just got an timeout exceptions like:
   ```
   java.lang.IllegalStateException: Update task on TaskManager container_e860_1567429198842_571077_01_000006 @ zjy-hadoop-prc-st320.bj (dataPort=50990) failed due to:
   	at org.apache.flink.runtime.executiongraph.Execution.lambda$sendUpdatePartitionInfoRpcCall$14(Execution.java:1395)
   	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
   	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
   	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
   	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
   	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
   	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
   	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
   	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
   	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
   	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
   	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
   	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
   	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
   	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
   	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
   	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
   	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
   Caused by: java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@zjy-hadoop-prc-st320.bj:62051/user/taskmanager_0#-171547157]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
   	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
   	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
   	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
   	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
   	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
   	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:871)
   	at akka.dispatch.OnComplete.internal(Future.scala:263)
   	at akka.dispatch.OnComplete.internal(Future.scala:261)
   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
   	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
   	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
   	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
   	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:644)
   	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
   	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
   	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
   	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
   	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
   	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
   	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
   	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
   	at java.lang.Thread.run(Thread.java:748)
   Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@zjy-hadoop-prc-st320.bj:62051/user/taskmanager_0#-171547157]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
   	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
   	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
   	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
   	... 9 more
   ```
   This PR is trying to add default GC options, and thus to facillitate debugging such issues.
   
   ## Brief change log
   
     - Add default GC options when depolying application on yarn.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   
   This change added tests and can be verified as follows:
   
     - slightly modified the YarnClusterDescriptorTest and BootstrapToolsTest, and passed
     - Manually tested on real yarn cluster
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: ( no )
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no )
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Sep/19 08:10;githubbot;600","tillrohrmann commented on pull request #11165: [FLINK-14038][docs] Extend Application Profiling documentation
URL: https://github.com/apache/flink/pull/11165
 
 
   ## What is the purpose of the change
   
   This commit adds explanations on how to analyze/debug out of memory problems and
   how to analyze GC behaviour.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Feb/20 18:03;githubbot;600","tillrohrmann commented on pull request #11165: [FLINK-14038][docs] Extend Application Profiling documentation
URL: https://github.com/apache/flink/pull/11165
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Feb/20 16:36;githubbot;600","tillrohrmann commented on pull request #9703: [FLINK-14038]Add default GC options for flink on yarn to facilitate debugging
URL: https://github.com/apache/flink/pull/9703
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Feb/20 16:37;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 21 16:38:15 UTC 2020,,,,,,,,,,"0|z06hmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/19 03:08;zhuzh;Hi [~liupengcheng], would you check the TM GC log to see whether the TM was stuck in GC when this error happens?

GC problem is the most common cause for late response from TM.

 

You can also increase the config ""akka.ask.timeout"" (by default it is 10 s) to make the job more robust for late response cases.;;;","11/Sep/19 11:08;sewen;What happened here is that the update to a partition failed, meaning that a batch shuffle has some results ready, but not all. (I assume you are running a DataSet job with {{InputDependencyConstraint.ANY}} or the default config).

Trying to see whether this is a GC problem makes sense. It looks like the ""ACK"" is sent immediately after receiving the ""update input channel"" message, so it should not be affected by the actual asynchronous update.
;;;","18/Sep/19 08:11;liupengcheng;Thank you [~zhuzh] [~sewen], I finally verified that it's caused by GC. and I also put a PR to add some gc options to facillitate debugging, I think it's helpful.;;;","18/Sep/19 08:16;liupengcheng;Currently, this PR does not show this gc log in UI, maybe another PR can do this.;;;","18/Sep/19 08:57;zhuzh;Currently users can enable GC logs by configuring GC opts in ""env.java.opts"".
And I think that's an easy way already.;;;","18/Sep/19 09:30;trohrmann;I agree with [~zhuzh]. Let's close the PR for now [~liupengcheng]. Maybe the best thing is to better document what to do in these kind of cases?;;;","18/Sep/19 09:46;zhuzh;[~till.rohrmann] Agreed. I think we can update it in the [Getting Help|https://flink.apache.org/gettinghelp.html#got-an-error-message] page which collects common issues.;;;","19/Sep/19 01:45;liupengcheng;[~zhuzh] [~trohrmann] This PR not only provide gc logs, but also heapdump, I don't think the simple docs in [Getting Help|https://flink.apache.org/gettinghelp.html#got-an-error-message]  take place of it. In online environment, the time cost is expensive, expecially for routine tasks.  If we just provide documents, then user must rerun the application,  and also the exceptions may varies, that depends on when OOM or long full gc happen. What's more, I think the options is not simple for user to get these infos (gc logs, heapdump), but they are basically needed for debugging an application.

If there are no drawbacks or regressions, I think we should made it done already as an default options for users.

 ;;;","19/Sep/19 08:17;trohrmann;I see your point [~liupengcheng] and I think you are right that it makes it easier to debug Flink problems. It looks as if gc logging is not prohibitively expensive and, hence, could be activated. On the other hand we also have the {{MemoryLogger}} which can be enabled via {{taskmanager.debug.memory.log}}. It uses the {{MemoryMXBean}} to report the current memory statistics. Do you know what the JVM GC logger logs additionally?

For {{-XX:HeapDumpOnOutOfMemoryError}}, it should not any add additional overhead as far as I can tell. So apart from the disk space occupied by heap dumps there should not be a big problem with it.

What about the following proposal: We make both things configurable and enable the heap dump per default and disable the GC logging as we do it wit the {{MemoryLogger}}. If we do this, then we should also make sure that the head dumping and GC logging works for all deployment options (Yarn, Mesos, Standalone).;;;","19/Sep/19 11:20;liupengcheng;[~trohrmann] This is an example of the JVM GC logger logs: 
{code:java}
2019-09-18T13:41:37.503+0800: 119.744: [Full GC (Ergonomics) [PSYoungGen: 40960K->39766K(81920K)] [ParOldGen: 245683K->245683K(245760K)] 286643K->285450K(327680K), [Metaspace: 56311K->56311K(1099776K)], 0.0963178 secs] [Times: user=0.22 sys=0.00, real=0.10 secs] 
2019-09-18T13:41:37.599+0800: 119.841: Total time for which application threads were stopped: 0.0967768 seconds, Stopping threads took: 0.0001084 seconds
{code}
Compared to MemroyLogger, it provide some extra informations:
 # space size changes of each area on every GC
 # GCCause 
 # more fine-grained level: for each GC

I agreed with you proposal, I will try to work on this, and make it works for all deployment options.;;;","21/Feb/20 16:38;trohrmann;Fixed via

master: 326b482a7f1a39fa93e15bcb826bb5e76704416c
1.10.1: d6778b3b95d1e72664b9e6f24b65420054816023;;;",,,,,,,,,,,,,,,,,,,,,
Distributed caches are not registered in Yarn Per Job Cluster Mode,FLINK-14033,13255692,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,09/Sep/19 16:39,09/Apr/20 16:20,13/Jul/23 08:10,03/Dec/19 13:19,1.9.0,,,,,,,,,1.10.0,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"CacheFiles in StreamExecutionEnvironment is not used in Job Submission in the Yarn per job cluster mode. Compare to the job submission in session cluster mode that will upload distributed cache files onto http server in application master, we should get the cache files in job graph and register them into blob store in YarnJobClusterEntrypoint.",,aljoscha,dian.fu,tison,trohrmann,wangyang0918,ZhenqiuHuang,ziqian hu,,,,,,,"HuangZhenQiu commented on pull request #9711: [FLINK-14033] upload user artifacts for yarn job cluster
URL: https://github.com/apache/flink/pull/9711
 
 
   ## What is the purpose of the change
   
   Upload user artifacts that registered in distributed caches for yarn job clusters. Currently, distributed caches are not handled correctly. The solution proposed is to upload local files onto the remote file system and update the registered file in cache entry. 
   
   ## Brief change-log
     - Modified the AbstractClusterDescriptor to handle with artifacts for per job mode before starting 
        application master.
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Sep/19 05:11;githubbot;600","aljoscha commented on pull request #9711: [FLINK-14033][yarn] upload user artifacts for yarn job cluster
URL: https://github.com/apache/flink/pull/9711
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Dec/19 13:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,FLINK-14019,,,,,,,FLINK-10370,,,,,,,,FLINK-15055,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 05:49:42 UTC 2019,,,,,,,,,,"0|z06h20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/19 16:42;trohrmann;Sounds like a good thing to fix. I've assigned you to the issue [~ZhenqiuHuang].;;;","16/Sep/19 08:01;wangyang0918;Seems that UserArtifacts is not added to yarn distributed cache and then upload to blob server so that it could be accessed by
 all tasks. The jars have been added to yarn distributed cache and do not need to put into blob server, because they have been added into system classpath.;;;","17/Sep/19 18:21;ZhenqiuHuang;Agree. It is not for a jar. The issue happens on another path. Basically, if a user wants to put a hdfs file as resource in the distributed cache, the file is not accessible in runtime due to the blob item is not prepared for per job mode. For session mode, the cache files will be created through rest-client before job graph submission.;;;","19/Sep/19 02:19;wangyang0918;[~ZhenqiuHuang]
Yeah, even many users are using -yt to distribute files so that they could be accessed by all tasks. We should still support to register by `StreamExecutionEnvironment#registerCachedFile`. Just go ahead to fix this. I could help to review.;;;","19/Sep/19 05:12;ZhenqiuHuang;[~trohrmann][~fly_in_gis]
Would you please take a look at the fix? Thank you in advance.;;;","03/Dec/19 13:19;aljoscha;Implemented on master in f985ff36bc94acb3072356ae3250ef1a0a5a2f1f;;;","04/Dec/19 05:49;tison;I'm afraid the dedicate test introduced in this patch is unstable and possibly dead lock.

See also https://api.travis-ci.com/v3/job/262854881/log.txt

[~ZhenqiuHuang] please take a look.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Update Flink's Mesos scheduling behavior to reject all expired offers,FLINK-14029,13255662,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,pnarang,pnarang,pnarang,09/Sep/19 14:39,16/Sep/19 11:58,13/Jul/23 08:10,16/Sep/19 11:58,,,,,,,,,,1.10.0,,,,,,,,,0,pull-request-available,,,,"While digging into why our Flink jobs weren't being scheduled on our internal Mesos setup we noticed that we were hitting Mesos quota limits tied to the way we've set up the Fenzo (https://github.com/Netflix/Fenzo/) library defaults in the Flink project. 

Behavior we noticed was that we got a bunch of offers from our Mesos master (50+) out of which only 1 or 2 of them were super skewed and took up a huge chunk of our disk resource quota. Thanks to this we were not sent any new / different offers (as our usage at the time + resource offers reached our Mesos disk quota). As the Flink / Fenzo Mesos scheduling code was not using the 1-2 skewed disk offers they end up expiring. The way we've set up the Fenzo scheduler is to use the default values on when to expire unused offers (120s) and maximum number of unused offer leases at a time (4). Unfortunately as we have a considerable number of outstanding expired offers (50+) we end up in a situation where we reject only 4 or so every 2 mins and we never get around to rejecting the super skewed disk ones which are stopping us from scheduling our Flink job. Thanks to this we end up in a situation where our job is waiting to be scheduled for more than an hour. 

An option to work around this is to reject all expired offers at 2 minute expiry time rather than hold on to them. This will allow Mesos to send alternate offers that might be scheduled by Fenzo. 
",,pnarang,trohrmann,,,,,,,,,,,,"piyushnarang commented on pull request #9652: [FLINK-14029][mesos] Update Mesos scheduling behavior to reject all expired offers
URL: https://github.com/apache/flink/pull/9652
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   While digging into why our Flink jobs weren't being scheduled on our internal Mesos setup we noticed that we were hitting Mesos quota limits tied to the way we've set up the Fenzo (https://github.com/Netflix/Fenzo/) library defaults in the Flink project.
   
   Behavior we noticed was that we got a bunch of offers from our Mesos master (50+) out of which only 1 or 2 of them were super skewed and took up a huge chunk of our disk resource quota. Thanks to this we were not sent any new / different offers (as our usage at the time + resource offers reached our Mesos disk quota). As the Flink / Fenzo Mesos scheduling code was not using the 1-2 skewed disk offers they end up expiring. The way we've set up the Fenzo scheduler is to use the default values on when to expire unused offers (120s) and maximum number of unused offer leases at a time (4). Unfortunately as we have a considerable number of outstanding expired offers (50+) we end up in a situation where we reject only 4 or so every 2 mins and we never get around to rejecting the super skewed disk ones which are stopping us from scheduling our Flink job. Thanks to this we end up in a situation where our job is waiting to be scheduled for more than an hour.
   
   An option to work around this is to reject all expired offers at 2 minute expiry time rather than hold on to them. This will allow Mesos to send alternate offers that might be scheduled by Fenzo.
   
   We could also make this aspect of the behavior configurable if needed. Though I'm not sure if it's worth it to add a config knob for this setting to allow users to have Fenzo hold on to expired offers for longer. 
   
   ## Brief change log
   
     - Include missing details on networkMbps, disk resources in the offer and Mesos task startup logs to help debugging. 
     - Update `TaskSchedulerBuilder` to setup the Fenzo `TaskScheduler` to reject all expired offers (rather than just 4). 
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): No
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: No
     - The serializers: No
     - The runtime per-record code paths (performance sensitive): No
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: Yes (Mesos deployment)
     - The S3 file system connector: No
   
   ## Documentation
   
     - Does this pull request introduce a new feature? No
     - If yes, how is the feature documented? Not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Sep/19 14:53;githubbot;600","tillrohrmann commented on pull request #9652: [FLINK-14029][mesos] Update Mesos scheduling behavior to reject all expired offers
URL: https://github.com/apache/flink/pull/9652
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Sep/19 11:55;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 16 11:58:23 UTC 2019,,,,,,,,,,"0|z06gvc:",9223372036854775807,Flink's Mesos integration now rejects all expired offers instead of only 4. This improves the situation where Fenzo holds on a lot of expired offers without giving them back to the Mesos resource manager.,,,,,,,,,,,,,,,,,,,"16/Sep/19 11:58;trohrmann;Fixed via e6f87d33ae891dce89463868500f91c3fe01265c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to start job for consuming Secure Kafka after the job cancel,FLINK-14012,13255599,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,Daebeom,Daebeom,09/Sep/19 10:41,22/Sep/20 07:20,13/Jul/23 08:10,09/Sep/20 01:57,1.9.0,,,,,,,,,,,,,Connectors / Kafka,,,,,0,,,,,"Hello, this is Daebeom Lee.
h2. Background

I installed Flink 1.9.0 at this our Kubernetes cluster.

We use Flink session cluster. - build fatJar file and upload it at the UI, run serval jobs.

At first, our jobs are good to start.

But, when we cancel some jobs, the job failed

This is the error code.


{code:java}
// code placeholder
java.lang.NoClassDefFoundError: org/apache/kafka/common/security/scram/internals/ScramSaslClient
    at org.apache.kafka.common.security.scram.internals.ScramSaslClient$ScramSaslClientFactory.createSaslClient(ScramSaslClient.java:235)
    at javax.security.sasl.Sasl.createSaslClient(Sasl.java:384)
    at org.apache.kafka.common.security.authenticator.SaslClientAuthenticator.lambda$createSaslClient$0(SaslClientAuthenticator.java:180)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.kafka.common.security.authenticator.SaslClientAuthenticator.createSaslClient(SaslClientAuthenticator.java:176)
    at org.apache.kafka.common.security.authenticator.SaslClientAuthenticator.<init>(SaslClientAuthenticator.java:168)
    at org.apache.kafka.common.network.SaslChannelBuilder.buildClientAuthenticator(SaslChannelBuilder.java:254)
    at org.apache.kafka.common.network.SaslChannelBuilder.lambda$buildChannel$1(SaslChannelBuilder.java:202)
    at org.apache.kafka.common.network.KafkaChannel.<init>(KafkaChannel.java:140)
    at org.apache.kafka.common.network.SaslChannelBuilder.buildChannel(SaslChannelBuilder.java:210)
    at org.apache.kafka.common.network.Selector.buildAndAttachKafkaChannel(Selector.java:334)
    at org.apache.kafka.common.network.Selector.registerChannel(Selector.java:325)
    at org.apache.kafka.common.network.Selector.connect(Selector.java:257)
    at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:920)
    at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:287)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:474)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:255)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:215)
    at org.apache.kafka.clients.consumer.internals.Fetcher.getTopicMetadata(Fetcher.java:292)
    at org.apache.kafka.clients.consumer.KafkaConsumer.partitionsFor(KafkaConsumer.java:1803)
    at org.apache.kafka.clients.consumer.KafkaConsumer.partitionsFor(KafkaConsumer.java:1771)
    at org.apache.flink.streaming.connectors.kafka.internal.KafkaPartitionDiscoverer.getAllPartitionsForTopics(KafkaPartitionDiscoverer.java:77)
    at org.apache.flink.streaming.connectors.kafka.internals.AbstractPartitionDiscoverer.discoverPartitions(AbstractPartitionDiscoverer.java:131)
    at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.open(FlinkKafkaConsumerBase.java:508)
    at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:529)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:393)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)
    at java.lang.Thread.run(Thread.java:748)
{code}
h2. Our workaround
 * I think that this is Flink JVM classloader issue.
 * Classloader unloads when job cancels by the way kafka client library is included fatJar.
 * So, I located Kafka client library to /opt/flink/lib 
 ** /opt/flink/lib/kafka-clients-2.2.0.jar
 * And then all issue solved.
 * But there are weird points
 ** When Flink 1.8.1 has no problem before 2 weeks
 ** Before 1 week I rollback from 1.9.0 to 1.8.1, same errors occurred.
 ** Maybe docker image is changed at docker repository ( [https://github.com/docker-flink/docker-flink ) |https://github.com/docker-flink/docker-flink]

 
h2. Suggestion
 * I'd like to know why this error occurred exactly reason after upgrade 1.9.0.
 * Does anybody know a better solution in this case?

 

Thank you in advance.

 ","* Kubernetes 1.13.2
 * Flink 1.9.0
 * Kafka client libary 2.2.0",aljoscha,andrew_lin,atealxt,Daebeom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 07:20:25 UTC 2020,,,,,,,,,,"0|z06ghc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/20 09:34;aljoscha;Hi, I'm very sorry that nobody has looked at this issue so far. Did you also try this with a more recent Flink version, maybe 1.10.x or 1.11.x. Sometimes the class loading behaviour changes so it's good to see if this is still an issue.;;;","09/Sep/20 01:55;Daebeom;I have used 1.11.0 and 1.10.x. Maybe this issue is resolved at the versions.

I'll close this issue.

Thank you for your comment.;;;","09/Sep/20 01:57;Daebeom;It is fixed in the recent version (1.10.x, 1.11.x);;;","09/Sep/20 09:42;aljoscha;Thanks for letting us know!;;;","21/Sep/20 09:57;andrew_lin;[~aljoscha] [~Daebeom] ,  Hi may I ask which pr or commit fixed this？ Can I cherry-pick to lower version？;;;","22/Sep/20 07:20;aljoscha;I recall there were different classloader changes in Flink 1.10.x. I don't know right now which exactly fixed it. Could you maybe try Flink 1.10.x to see if your problem is fixed. Then we could try and find which commit actually fixed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Dispatcher & JobManagers don't give up leadership when AM is shut down,FLINK-14010,13255566,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tison,tison,tison,09/Sep/19 08:14,10/Oct/19 16:06,13/Jul/23 08:10,24/Sep/19 17:24,1.10.0,1.7.2,1.8.2,1.9.0,,,,,,1.10.0,1.8.3,1.9.1,,Deployment / YARN,Runtime / Coordination,,,,0,pull-request-available,,,,"In YARN deployment scenario, YARN RM possibly launches a new AM for the job even if the previous AM does not terminated, for example, when AMRM heartbeat timeout. This is a common case that RM will send a shutdown request to the previous AM and expect the AM shutdown properly.

However, currently in {{YARNResourceManager}}, we handle this request in {{onShutdownRequest}} which simply close the {{YARNResourceManager}} *but not Dispatcher and JobManagers*. Thus, Dispatcher and JobManager launched in new AM cannot be granted leadership properly. Visually,

on previous AM: Dispatcher leader, JM leaders
on new AM: ResourceManager leader

since on client side or in per-job mode, JobManager address and port are configured as the new AM, the whole cluster goes into an unrecoverable inconsistent status: client all queries the dispatcher on new AM who is now the leader. Briefly, Dispatcher and JobManagers on previous AM do not give up their leadership properly.",,gjy,jark,Paul Lin,tison,trohrmann,wind_ljy,,,,,,,,"TisonKun commented on pull request #9719: [FLINK-14010][coordination] Shutdown cluster if ResourceManager terminated unexpectedly
URL: https://github.com/apache/flink/pull/9719
 
 
   ## What is the purpose of the change
   
   Context can be found at [FLINK-14010](https://issues.apache.org/jira/browse/FLINK-14010)
   
   ## Brief change log
   
   Add callback logic when ResourceManager terminates unexpectedly, complete `DispatcherResourceManagerComponent#shutdownFuture` exceptionally.
   
   ## Verifying this change
   
   The change is technically trivial and can be easily reasoned. However, I'd like to introduce a test of it, but cannot find a good way.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes, now in YARN deployment Flink App can correctly response to shutdownRequest)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Sep/19 11:36;githubbot;600","tillrohrmann commented on pull request #9719: [FLINK-14010][coordination] Shutdown cluster if ResourceManager terminated unexpectedly
URL: https://github.com/apache/flink/pull/9719
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Sep/19 17:22;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,FLINK-14347,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 10 16:06:48 UTC 2019,,,,,,,,,,"0|z06ga0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/19 08:31;tison;CC [~sewen] [~till.rohrmann] [~xiaogang.shi]

Here comes a high-level problem, do we explicitly constrain Dispatcher, ResourceManager and JobManagers run on one process?

1. the usage of reference to {{JobManagerGateway}} in Dispatcher already infers that we require this.
2. back to the design of FLIP-6, we have a global singleton of Dispatcher, and for each job, launch a JobManager and ResourceManager. The implementation diverges quite a lot. Could you please provide any background?
3. if we explicitly constrain as above, we actually need not to start leader election services per components, actually, we can use the abstraction and layout as below:

- start a leader election service per dispatcher-resource-manager component, in cluster entrypoint level. It will participant the election and all metadata commits are delegate to this service.
- all cluster level components that need to publish their address, such as Dispatcher, ResourceManager and WebMonitor publish their address via this leader election service.
- Actors can be started as {{PermanentlyFencedRpcEndpoint}} and thus we survive from handling a lot of mutable shared state among leadership epoch. Specifically, cluster entrypoint acts as DispatcherRunner and so on, like JobManagerRunner to JobMaster. See also [this branch|https://github.com/tillrohrmann/flink/commits/removeSuspendFromJobMaster].

- back to this issue, cluster entrypoint({{YARNClusterEntrypoint}} maybe) reacts to AMRM request and thus all components can be required to shutdown properly.;;;","09/Sep/19 15:26;trohrmann;Thanks for reporting this issue [~Tison]. I think your analysis is correct that we currently do not react properly to {{onShutdownRequest}} signals. What happens is that we only terminate the {{ResourceManager}} but not the other Flink components ({{Dispatcher}} and {{JobMasters}}).

Before looking into a potential solutions let's answer your questions because they are important for understanding the bigger picture.

The two components {{Dispatcher}} and {{ResourceManager}} have been designed to run independently of the other component, potentially also in a separate process. The current implementation couples the {{Dispatcher}} with the {{JobMasters}} but there should be nothing fundamental preventing from decoupling them. The main thing one needs to do is to add logic to start {{JobMaster}} processes somewhere. So long story short, the idea is that all components can run independently (modulo the existing {{Dispatcher}} implementation).

At the moment we run all these components in one process because it was easier and less work to implement it this way. This is reflected in the {{DispatcherResourceManagerComponent}}. Hence, for the current implementation we can assume that if the {{ResourceManager}} terminates, then the other components should shut down as well. This could be implemented by registering in the {{DispatcherResourceManagerComponent}} a callback on the {{ResourceManager's}} termination future which triggers the closing of the {{DispatcherResourceManagerComponent}}.

Concerning using a single leader election service, I would say that this is not feasible if we still want to keep the option to deploy the components in separate processes eventually.

Would you have time to work on the fix for this problem [~Tison]?;;;","17/Sep/19 10:47;tison;[~till.rohrmann] Thanks to your explanation, I learn where the components layout comes from.

Back to this issue, what if we call {{#onFatalError}} in {{YarnResourceManager#onShutdownRequest}}? {{#onShutdownRequest}} is only called when AM exceptionally switched and we can regard it as a fatal error. For implementation details, it calls {{System.exit}} that correctly shutdown the AM and release leadership.;;;","17/Sep/19 12:36;trohrmann;{{#onFatalError}} could also be an option but I would prefer to distinguish here. I would consider {{#onShutdownRequest}} as request and not an error case. Hence, I would suggest to try to gracefully shut down. If this does not work, then we could fail fatally.;;;","17/Sep/19 14:04;tison;Well, it's reasonable we try to gracefully shut down. I start to work on it but I'm not sure about what the future should look like.

There are two options in my mind, both of which introduce a {{shutdownFuture}} in {{ResourceManager}}.

1. {{ResourceManager#shutdownFuture}} is completed on {{YarnResourceManager#onShutdownRequest}} gets called. And we register callback in {{DispatcherResourceManagerComponent#registerShutDownFuture}}, when {{ResourceManager#shutdownFuture}} complete, we complete {{DispatcherResourceManagerComponent#shutDownFuture}} exceptionally. Concern here is that {{ResourceManager#shutdownFuture}} is never completed if {{YarnResourceManager#onShutdownRequest}} never gets called. I'm not sure if it is well.

2. {{ResourceManager#shutdownFuture}} is completed normally on {{ResourceManager#stopResourceManagerServices}} gets called, while completed exceptionally on {{YarnResourceManager#onShutdownRequest}} gets called. Also we register callback in {{DispatcherResourceManagerComponent#registerShutDownFuture}}, when {{ResourceManager#shutdownFuture}} complete exceptionally, we complete {{DispatcherResourceManagerComponent#shutDownFuture}} exceptionally; when when {{ResourceManager#shutdownFuture}} complete normally we do nothing. It might be a bit more complex than 1 and we should ensure that codepaths {{ResourceManager}} exit are all covered.

WDYT [~till.rohrmann]?;;;","18/Sep/19 12:46;trohrmann;Couldn't we simply use {{ResourceManager#getTerminationFuture}} to trigger the shut down of the {{DispatcherResourceManagerComponent}} [~Tison]?;;;","18/Sep/19 13:06;tison;I have thought of this. The problem is that when the situation described here happens, we actually complete {{ResourceManager#getTerminationFuture}} normally, which cannot be sourced that it comes from {{YarnResourceManager#onShutdownRequest}}.

If we achieve the function by using {{ResourceManager#getTerminationFuture}} to trigger the shut down of the {{DispatcherResourceManagerComponent}}, the assumption is:

If ResourceManager is closed first(since termination future completes normally in both cases, we cannot distinguish by {{whenComplete}}), it infers an exceptionally status so that we should complete {{DispatcherResourceManagerComponent#getShutDownFuture}} exceptionally. Otherwise ResourceManager closes normally by other triggers, and then either {{DispatcherResourceManagerComponent#getShutDownFuture}} is already completed or {{ClusterEntrypoint#shutdownAsync}} is guarded to be executed once.

I think this assumption is counter-intuitive that ResourceManager terminates ""normally"" but we complete shutdownFuture exceptionally.;;;","18/Sep/19 17:06;trohrmann;Can't we say that we always complete {{DispatcherResourceManagerComponent#shutDownFuture}} exceptionally if {{ResourceManager.getTerminationFuture()}} terminates while {{DispatcherResourceManagerComponent#isRunning}} is {{true}}? The contract could be that the {{ResourceManager}} always needs to run and if it stops, then this is an indicator that something went wrong and we should stop the {{ClusterEntrypoint}}. We could do this by completing {{DispatcherResourceManagerComponent#shutDownFuture}} exceptionally if {{DispatcherResourceManagerComponent#isRunning}} is {{true}}. However, one could similarly also simply call {{onFatalError}} from within the {{ResourceManager}} as you've initially proposed.;;;","19/Sep/19 03:30;tison;[~trohrmann] Technically I agree that it is a valid solution. Give it another look I think we can complete shutdown future exceptionally ""ResourceManager got closed when DispatcherResourceManagerComponent is running"". It infers that the application goes into an UNKNOWN state so that the semantic is also correct.;;;","19/Sep/19 07:45;trohrmann;Ok, then let's do it like this. Do you have to time to work on this issue [~Tison]?;;;","19/Sep/19 07:48;tison;Will send a pull request in hours :-);;;","24/Sep/19 17:24;trohrmann;Fixed via

1.10.0: 67e73fad6cd80911377b2009e5e9c661f0324867
1.9.1: 4d70447f6daac7914dc3a8749a1d9963c61f8d4c
1.8.3: 42cb6ddb1018e734444943d5f23215e26a156ff4;;;","10/Oct/19 16:06;trohrmann;I think we introduced a Yarn test instability with this fix. FLINK-14347 looks as if we are reacting to an {{onShutdownRequest}} during the test clean up phase. Since we are calling the {{FatalExceptionHandler}} we terminate the process and log a fatal error in the logs which fails the test.;;;",,,,,,,,,,,,,,,,,,,
Cron jobs broken due to verifying incorrect NOTICE-binary file,FLINK-14009,13255562,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,trohrmann,trohrmann,09/Sep/19 07:46,09/Sep/19 16:11,13/Jul/23 08:10,09/Sep/19 16:11,1.10.0,,,,,,,,,1.10.0,1.8.3,1.9.1,,Build System,,,,,0,pull-request-available,,,,"With FLINK-13968 we introduced an automatic {{NOTICE-binary}} file check. However, since we don't use the correct {{NOTICE-binary}} file (FLINK-14008) for Scala 2.12 it fails currently our cron jobs.

I suggest to only enable the automatic {{NOTICE-binary}} files for Scala 2.11 until FLINK-14008 has been fixed.",,jark,trohrmann,,,,,,,,,,,,"tillrohrmann commented on pull request #9651: [FLINK-14009][build] Ignore license file check for Scala version different than 2.11
URL: https://github.com/apache/flink/pull/9651
 
 
   ## What is the purpose of the change
   
   Ignore license file check for Scala version different than 2.11
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Sep/19 11:47;githubbot;600","tillrohrmann commented on pull request #9651: [FLINK-14009][build] Ignore license file check for Scala version different than 2.11
URL: https://github.com/apache/flink/pull/9651
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Sep/19 16:09;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-13968,FLINK-14008,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 09 16:11:48 UTC 2019,,,,,,,,,,"0|z06g94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/19 08:51;jark;FLINK-13968  is fixed in 1.8.3, so I changed this issue to 1.8.3.;;;","09/Sep/19 16:11;trohrmann;Fixed via

1.10.0: 31aa5678b7377fbbd7c325d28a08401eb16fc520
1.9.1: a00805eea4e20e4aff83f7de2650530af96da4bb
1.8.3: 1d32e397a804b8534112a7e22f96d886869b4d7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the documentation of MATCH_RECOGNIZE,FLINK-13999,13255406,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,07/Sep/19 05:40,14/Oct/19 14:39,13/Jul/23 08:10,14/Oct/19 14:39,1.8.2,1.9.1,,,,,,,,1.10.0,1.8.3,1.9.2,,Documentation,,,,,0,pull-request-available,,,,"Regarding to the following [example|https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/streaming/match_recognize.html#aggregations] in the doc:
{code:java}
SELECT *
FROM Ticker
    MATCH_RECOGNIZE (
        PARTITION BY symbol
        ORDER BY rowtime
        MEASURES
            FIRST(A.rowtime) AS start_tstamp,
            LAST(A.rowtime) AS end_tstamp,
            AVG(A.price) AS avgPrice
        ONE ROW PER MATCH
        AFTER MATCH SKIP TO FIRST B
        PATTERN (A+ B)
        DEFINE
            A AS AVG(A.price) < 15
    ) MR;
{code}
Given the inputs shown in the doc, it should be:
{code:java}
 symbol       start_tstamp       end_tstamp          avgPrice
=========  ==================  ==================  ============
ACME       01-APR-11 10:00:00  01-APR-11 10:00:03     14.5{code}
instead of:
{code:java}
 symbol       start_tstamp       end_tstamp          avgPrice
=========  ==================  ==================  ============
ACME       01-APR-11 10:00:00  01-APR-11 10:00:03     14.5
ACME       01-APR-11 10:00:04  01-APR-11 10:00:09     13.5
{code}",,aljoscha,dian.fu,dwysakowicz,eastcirclek,sunjincheng121,,,,,,,,,"dianfu commented on pull request #9893: [FLINK-13999][cep][docs] Correct the example in the section of Aggregations of MATCH_RECOGNIZE
URL: https://github.com/apache/flink/pull/9893
 
 
   
   ## What is the purpose of the change
   
   *This pull request corrects the [example](http://0.0.0.0:4000/dev/table/streaming/match_recognize.html#aggregations) defined in the section of ""Aggregations"" of MATCH_RECOGNIZE.*
   
   ## Brief change log
   
     - *Correct the example in the section of ""Aggregations"" of MATCH_RECOGNIZE*
   
   ## Verifying this change
   
   This change is a documentation rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Oct/19 08:01;githubbot;600","dawidwys commented on pull request #9893: [FLINK-13999][cep][docs] Correct the example in the section of Aggregations of MATCH_RECOGNIZE
URL: https://github.com/apache/flink/pull/9893
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Oct/19 14:33;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 14 14:39:25 UTC 2019,,,,,,,,,,"0|z06fag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/19 05:44;dian.fu;This issue is reported in the user list: [http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Wrong-result-of-MATCH-RECOGNIZE-clause-td29820.html] ;;;","16/Sep/19 09:41;aljoscha;I quickly talked to [~dwysakowicz] about this, he mentioned that it might be a bug and not a documentation glitch. We'll get back once we know more.;;;","07/Oct/19 14:13;dwysakowicz;Hi, I had a bit more time to have a proper look at this problem. [~dian.fu] is right. It is an error in the example. The row with timestamp {{'01-Apr-11 10:00:04'}} obviously cannot start a new match as it exceeds the average of 15 (25/1 > 15).

I would suggest two changes to the example:
1. Change the skip strategy to the default {{AFTER MATCH SKIP PAST LAST ROW}}, as the strategy here is not important for the example.
2. Add a new at the end to get two output rows. I suggest to have:

input:
{code}
symbol         rowtime         price    tax
======  ====================  ======= =======
'ACME'  '01-Apr-11 10:00:00'   12      1
'ACME'  '01-Apr-11 10:00:01'   17      2
'ACME'  '01-Apr-11 10:00:02'   13      1
'ACME'  '01-Apr-11 10:00:03'   16      3
'ACME'  '01-Apr-11 10:00:04'   25      2
'ACME'  '01-Apr-11 10:00:05'   2       1
'ACME'  '01-Apr-11 10:00:06'   4       1
'ACME'  '01-Apr-11 10:00:07'   10      2
'ACME'  '01-Apr-11 10:00:08'   15      2
'ACME'  '01-Apr-11 10:00:09'   25      2
'ACME'  '01-Apr-11 10:00:10'   28 (change this value to have a nicer average in the result)      1
'ACME'  '01-Apr-11 10:00:11'   30      1
{code}

output:
{code}
 symbol       start_tstamp       end_tstamp          avgPrice
=========  ==================  ==================  ============
ACME       01-APR-11 10:00:00  01-APR-11 10:00:03     14.5
ACME       01-APR-11 10:00:05  01-APR-11 10:00:10     14.0
{code};;;","08/Oct/19 01:53;dian.fu;[~dwysakowicz] Thanks for the confirmation and suggestion! The suggested changes make sense to me. Regarding to the comment ""change this value to have a nicer average in the result"", the price of ""28"" of the new row is good enough for me given the average ""14.0"" is different from ""14.5"". Other good candidates could be ""22"", ""25"", etc and the average will be ""13.0"", ""13.5"", etc. What do you think?;;;","09/Oct/19 09:41;dwysakowicz;I'm fine with any of the suggested values as long as the resulting avgPrice for the second row is different from the first one and it has a short fractional part.

Would you like to prepare a PR for it? [~dian.fu];;;","14/Oct/19 07:34;dian.fu;[~dwysakowicz] Very sorry for late response. I'll submit a PR for this later today.;;;","14/Oct/19 08:06;sunjincheng121;I have assign it to you [~dian.fu], Thank you!:);;;","14/Oct/19 08:12;dian.fu;[~sunjincheng121] Thanks a lot! 
[~dwysakowicz] I have submitted the PR and looking forward to your review.;;;","14/Oct/19 14:39;dwysakowicz;Fixed in:
master: 51bd2f68b3d1dbf5e553e26730695915e13a34f4
1.9: 12de704c2ecf42ddc5449a1c93d8c96e0e280fae
1.8: dc90f43e050a71aeaf0ced91c14d895f6512dfae;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix shading of the licence information of netty,FLINK-13995,13255258,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,arvid,arvid.heise@gmail.com,06/Sep/19 13:01,22/Jun/21 14:06,13/Jul/23 08:10,26/Nov/19 10:49,1.8.0,,,,,,,,,1.10.0,1.8.3,1.9.2,,BuildSystem / Shaded,,,,,0,pull-request-available,,,,"The license filter isn't actually filtering anything. It should be META-INF/license/**.

The first filter seems to be outdated btw.

Multiple modules affected.

{code:xml}
<filter>
	<artifact>io.netty:netty</artifact>
	<excludes>
		<exclude>META-INF/maven/io.netty/**</exclude>
		<!-- Only some of these licenses actually apply to the JAR and have been manually
placed in this module's resources directory. -->
		<exclude>META-INF/license</exclude>
		<!-- Only parts of NOTICE file actually apply to the netty JAR and have been manually
copied into this modules's NOTICE file. -->
		<exclude>META-INF/NOTICE.txt</exclude>
	</excludes>
</filter>
{code}",,arvid.heise@gmail.com,hequn8128,,,,,,,,,,,,"zentol commented on pull request #10195: [FLINK-13995][legal] Properly exclude netty license directory
URL: https://github.com/apache/flink/pull/10195
 
 
   Corrects the exclusion for netty's `license` directory. Currently the directory not being excluded as it should, since it includes a number of licenses that don't actually apply.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Nov/19 14:00;githubbot;600","zentol commented on pull request #10195: [FLINK-13995][legal] Properly exclude netty license directory
URL: https://github.com/apache/flink/pull/10195
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/19 10:47;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 26 10:49:57 UTC 2019,,,,,,,,,,"0|z06edk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/19 04:11;hequn8128;[~chesnay] Hi, as legal problems are always important, so I think this is also a blocker for 1.8.3? ;;;","25/Nov/19 09:34;chesnay;I wouldn't say that it is a blocker; a missing license would be. Shipping a license which doesn't apply, provided that they are no negative consequences, is fine.

That said, a PR is already open and pretty trivial to review, so there isn't really a reason to not include it in the release.;;;","26/Nov/19 10:49;chesnay;master: 08f1f4205c23ea5473c5c80725aa7794cfdf3719
1.9: 53c1c183fb6b4059627f4a66cd8feaeed45c1b4c
1.8: b7e532a4f17ee602cfce420510f515f52c10c379;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint recovery failed after user set uidHash,FLINK-13973,13254973,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,peizhouyu,peizhouyu,05/Sep/19 10:45,01/Sep/21 10:08,13/Jul/23 08:10,27/Aug/21 03:01,1.13.0,1.8.0,1.8.1,1.9.0,,,,,,1.14.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"Checkpoint recovery failed after user set uidHash, the possible reasons are as follows:

If altOperatorID is not null, operatorState will be obtained by altOperatorID and will not be given",,andrew_lin,apus66,gaoyunhaii,peizhouyu,roman,yanghua,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21090,,,,,,,,,,,,,,,,,FLINK-23770,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 26 07:07:02 UTC 2021,,,,,,,,,,"0|z06cm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/19 03:25;apus66;I've solved the problem. I have already tested in the production environment. I want to submit my code to the official version. I want to be assigned this task.
Thanks.;;;","22/Apr/21 11:43;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 23:22;flink-jira-bot;This issue was labeled ""stale-major"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","26/Aug/21 07:07;gaoyunhaii;Fix on master via 8bf1fbd4d4bb0c0ce664aeb36395238612341a44;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resuming Externalized Checkpoint (rocks, incremental, scale down) end-to-end test fails on Travis",FLINK-13969,13254936,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,klion26,trohrmann,trohrmann,05/Sep/19 07:53,07/Nov/19 22:38,13/Jul/23 08:10,07/Nov/19 22:38,1.10.0,,,,,,,,,1.10.0,1.9.2,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"The {{Resuming Externalized Checkpoint (rocks, incremental, scale down)}} end-to-end test fails on Travis because its log contains an exception

{code}
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 16 for operator ArtificalKeyedStateMapper_Avro -> ArtificalOperatorStateMapper (2/4). Failure reason: Checkpoint was declined.
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:431)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1302)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1236)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:892)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:797)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:728)
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:88)
	at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)
	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:118)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:48)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:144)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performDefaultAction(StreamTask.java:277)
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:147)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:404)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Cannot register Closeable, registry is already closed. Closing argument.
	at org.apache.flink.util.AbstractCloseableRegistry.registerCloseable(AbstractCloseableRegistry.java:85)
	at org.apache.flink.runtime.state.AsyncSnapshotCallable$AsyncSnapshotTask.<init>(AsyncSnapshotCallable.java:122)
	at org.apache.flink.runtime.state.AsyncSnapshotCallable$AsyncSnapshotTask.<init>(AsyncSnapshotCallable.java:110)
	at org.apache.flink.runtime.state.AsyncSnapshotCallable.toAsyncSnapshotFutureTask(AsyncSnapshotCallable.java:104)
	at org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.doSnapshot(RocksIncrementalSnapshotStrategy.java:170)
	at org.apache.flink.contrib.streaming.state.snapshot.RocksDBSnapshotStrategyBase.snapshot(RocksDBSnapshotStrategyBase.java:126)
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.snapshot(RocksDBKeyedStateBackend.java:439)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:411)
	... 17 more
{code}

https://api.travis-ci.org/v3/job/580915660/log.txt",,gjy,klion26,SleePy,trohrmann,txhsj,,,,,,,,,"tillrohrmann commented on pull request #10111: [FLINK-13969] Resuming Externalized Checkpoint (rocks, incremental, scale down) end-to-end test fails on Travis
URL: https://github.com/apache/flink/pull/10111
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:37;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 07 22:38:08 UTC 2019,,,,,,,,,,"0|z06ce0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/19 02:06;klion26;From the log,  I found that the restore is successful (from checkpoint 15, checkpoint 1 to checkpoint 15 are all successful), and the second job can do checkpoint successfully, but we have a failed checkpoint(checkpoint 16) in the first job, because the first job has been canceled when doing the snapshot. when we check whethe there is any exception in the log, then found the snapshot exception there, and the test failed.

I think for this test, maybe modify the check logic is ok.

 
{code:java}
2019-09-05 02:26:19,154 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 15 for job 5614f2d16ac6f58966f738f0c93a2f63 (628221 bytes in 1824 ms).
2019-09-05 02:26:19,189 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 16 @ 1567650379153 for job 5614f2d16ac6f58966f738f0c93a2f63.
2019-09-05 02:26:19,788 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Discarding checkpoint 16 of job 5614f2d16ac6f58966f738f0c93a2f63.
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 16 for operator ArtificalKeyedStateMapper_Avro -> ArtificalOperatorStateMapper (2/4). Failure reason: Checkpoint was declined.
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:431)
        at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1302)
        at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1236)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:892)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:797)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:728)
        at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:88)
        at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)
        at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:118)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:48)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:144)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.performDefaultAction(StreamTask.java:277)

2019-09-05 02:26:25,862 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Starting job 74cca894c6d50cad64d5377be54cf14b from savepoint file:///home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-44167013242/externalized-chckpt-e2e-backend-dir/5614f2d16ac6f58966f738f0c93a2f63/chk-15 ()
2019-09-05 02:26:25,916 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Reset the checkpoint ID of job 74cca894c6d50cad64d5377be54cf14b to 16.
2019-09-05 02:26:25,917 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Restoring job 74cca894c6d50cad64d5377be54cf14b from latest valid checkpoint: Checkpoint 15 @ 0 for 74cca894c6d50cad64d5377be54cf14b.
2019-09-05 02:26:25,928 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - No master state to restore
2019-09-05 02:26:25,928 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunner           - JobManager runner for job General purpose test job (74cca894c6d50cad64d5377be54cf14b) was granted leadership with session id 00000000-0000-0000-0000-000000000000 at akka.tcp://flink@localhost:6123/user/jobmanager_1.
2019-09-05 02:26:25,929 INFO  org.apache.flink.runtime.jobmaster.JobMaster                  - Starting execution of job General purpose test job (74cca894c6d50cad64d5377be54cf14b) under job master id 000000000000

2019-09-05 02:26:27,402 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 16 @ 1567650387401 for job 74cca894c6d50cad64d5377be54cf14b.
2019-09-05 02:26:27,979 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 16 for job 74cca894c6d50cad64d5377be54cf14b (482657 bytes in 577 ms).
2019-09-05 02:26:28,402 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 17 @ 1567650388402 for job 74cca894c6d50cad64d5377be54cf14b.
{code};;;","29/Oct/19 14:37;klion26;After thinking it a little deeper, I think the root cause here is that we triggered a new checkpoint after the job status is not running, so we should change the logical in {{CheckpointCoordinator to not trigger checkpoint anymore when job status is not running, what do you think about this [~trohrmann]. If this is ok, could you please assign this ticket to me?}}

 
{code:java}
2019-09-05 02:26:19,126 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job General purpose test job (5614f2d16ac6f58966f738f0c93a2f63)
switched from state RUNNING to CANCELLING.
2019-09-05 02:26:19,154 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 15 for job 5614f2d16ac6f58966f738f0c93a2f63
 (628221 bytes in 1824 ms).
2019-09-05 02:26:19,189 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 16 @ 1567650379153 for job 5614f2d16ac6f58
966f738f0c93a2f63.

......

2019-09-05 02:26:19,788 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Discarding checkpoint 16 of job 5614f2d16ac6f58966f738f0c93a2f63.
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 16 for operator ArtificalKeyedStateMapper_Avro -> ArtificalOperatorStateMapper (2/4). Failure reason: Checkpoint was declined.
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:431)
        at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1302)
        at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1236)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:892)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:797)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:728)
        at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:88)
        at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)
        at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:118)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:48)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:144)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.performDefaultAction(StreamTask.java:277)
        at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:147)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:404)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Cannot register Closeable, registry is already closed. Closing argument.
        at org.apache.flink.util.AbstractCloseableRegistry.registerCloseable(AbstractCloseableRegistry.java:85)
        at org.apache.flink.runtime.state.AsyncSnapshotCallable$AsyncSnapshotTask.<init>(AsyncSnapshotCallable.java:122)
        at org.apache.flink.runtime.state.AsyncSnapshotCallable$AsyncSnapshotTask.<init>(AsyncSnapshotCallable.java:110)
        at org.apache.flink.runtime.state.AsyncSnapshotCallable.toAsyncSnapshotFutureTask(AsyncSnapshotCallable.java:104)
        at org.apache.flink.contrib.streaming.state.snapshot.RocksIncrementalSnapshotStrategy.doSnapshot(RocksIncrementalSnapshotStrategy.java:170)
        at org.apache.flink.contrib.streaming.state.snapshot.RocksDBSnapshotStrategyBase.snapshot(RocksDBSnapshotStrategyBase.java:126)
        at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.snapshot(RocksDBKeyedStateBackend.java:439)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:411)
        ... 17 more
{code};;;","29/Oct/19 15:58;trohrmann;Don't we already stop the {{CheckpointCoordinator}} if the job status changes from {{RUNNING}} to something else in the {{CheckpointCoordinatorDeActivator}}? If this is the case, then this must be a race condition between the {{stopCheckpointScheduler}} and the trigger a new checkpoint operation.;;;","31/Oct/19 01:43;klion26;Hi, [~trohrmann], from the log I think the event order is such as below:
 * triggerCheckpoint  eager pre-checks
 * job cancel
 * triggerCheckpoint actual trigger checkpoint

all the three steps are guarded by lock, will release the lock after step 1 and require the lock in step 3.

we checked whether the coordinator has been stopped in eager pre-checks, but not in actual trigger checkpoint.;;;","01/Nov/19 15:16;trohrmann;Then this sounds like a bug. We should also do the check in step 3.;;;","04/Nov/19 11:47;klion26;Yes, I agree that this is a bug and should add the check in step 3. Could you please assign this ticket to me, then I'll fix it. [~trohrmann];;;","04/Nov/19 12:25;trohrmann;I've assigned this ticket to you [~klion26]. Thanks for fixing this problem!;;;","07/Nov/19 22:38;trohrmann;Fixed via

1.10.0: 7e8218515baf630e668348a68ff051dfa49c90c3
1.9.2: 830f2d7a378e440fbaae2d95ac793ec5c87ed088;;;",,,,,,,,,,,,,,,,,,,,,,,,
Jar sorting in collect_license_files.sh is locale dependent,FLINK-13966,13254928,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Sep/19 07:13,05/Sep/19 14:49,13/Jul/23 08:10,05/Sep/19 14:49,1.8.1,,,,,,,,,1.10.0,1.8.3,1.9.1,,Build System,,,,,0,pull-request-available,,,,"The {{collect_license_files.sh}} searches jars for NOTICE files and concatenates them to assemble the NOTICE-binary file. To  make the order deterministic we order the file paths using {{sort}}, however this util is locale dependent.",,,,,,,,,,,,,,,"zentol commented on pull request #9623: [FLINK-13966][licensing] Pin locale for deterministic sort order
URL: https://github.com/apache/flink/pull/9623
 
 
   Pins the locale during the sort operation in `collect_license_files.sh` to `C`, which appears to be the one we've been using so far.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 07:20;githubbot;600","zentol commented on pull request #9623: [FLINK-13966][licensing] Pin locale for deterministic sort order
URL: https://github.com/apache/flink/pull/9623
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 14:49;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 05 14:49:59 UTC 2019,,,,,,,,,,"0|z06cc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/19 14:49;chesnay;master: 6f8b79ebd98a6affb508038001d6cb870c218a7d
1.9: 1dd2611c0b5446254b91c5cbc6c14ab8ab3f1a2d 
1.8: ba1cd43b6ed42070fd9013159a5b65adb18b3975 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartitionableTableSink can not work with OverwritableTableSink,FLINK-13952,13254572,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lzljs3620320,lzljs3620320,04/Sep/19 06:13,06/Sep/19 02:58,13/Jul/23 08:10,06/Sep/19 02:58,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"{code:java}
tableSink match {
  case partitionableSink: PartitionableTableSink
    if partitionableSink.getPartitionFieldNames != null
      && partitionableSink.getPartitionFieldNames.nonEmpty =>
    partitionableSink.setStaticPartition(insertOptions.staticPartitions)
  case overwritableTableSink: OverwritableTableSink =>
    overwritableTableSink.setOverwrite(insertOptions.overwrite)
{code}
Code in TableEnvImpl and PlannerBase

overwrite will not be invoked when there are static partition columns.",,lirui,lzljs3620320,phoenixjiangnan,,,,,,,,,,,"lirui-apache commented on pull request #9615: [FLINK-13952][table-planner][hive] PartitionableTableSink can not wor…
URL: https://github.com/apache/flink/pull/9615
 
 
   …k with OverwritableTableSink
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To support insert overwrite partition.
   
   
   ## Brief change log
   
     - Make sure planner supports sinks that are both PartitionableTableSink and OverwritableTableSink
     - Added tests for insert overwrite partitions.
   
   
   ## Verifying this change
   
   Added new test cases.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 13:31;githubbot;600","asfgit commented on pull request #9615: [FLINK-13952][table-planner][hive] PartitionableTableSink can not wor…
URL: https://github.com/apache/flink/pull/9615
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 02:57;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 06 02:58:04 UTC 2019,,,,,,,,,,"0|z06b08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/19 09:24;lirui;[~lzljs3620320] Thanks for reporting the issue. Let me know if you want to work on this, otherwise I'll submit a PR for it.;;;","04/Sep/19 10:00;lzljs3620320;Feel free to fix it. I can review your PR.;;;","06/Sep/19 02:58;phoenixjiangnan;master: d32af521cbe83f88cd0b822c4d752a1b5102c47c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Instructions for building flink-shaded against vendor repository don't work,FLINK-13945,13254416,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,Elise Ramé,Elise Ramé,03/Sep/19 14:56,06/Dec/19 15:17,13/Jul/23 08:10,06/Dec/19 15:17,1.9.0,,,,,,,,,1.10.0,1.9.2,,,BuildSystem / Shaded,,,,,0,pull-request-available,,,,"According to [documentation|https://ci.apache.org/projects/flink/flink-docs-release-1.9/flinkDev/building.html#custom--vendor-specific-versions], to build Flink against a vendor specific Hadoop version it is necessary to build flink-shaded against this version first : 
{code:bash}
mvn clean install -DskipTests -Pvendor-repos -Dhadoop.version=<hadoop_version>
{code}
vendor-repos profile has to be activated to include Hadoop vendors repositories.
 But Maven cannot find expected Hadoop dependencies and returns an error because vendor-repos profile isn't defined in flink-shaded.

Example using flink-shaded 8.0 and HDP 2.6.5 Hadoop version :
{code:bash}
mvn clean install -DskipTests -Pvendor-repos -Dhadoop.version=2.7.3.2.6.5.0-292
{code}
{code:bash}
[INFO] ---------------< org.apache.flink:flink-shaded-hadoop-2 >---------------
[INFO] Building flink-shaded-hadoop-2 2.7.3.2.6.5.0-292-8.0             [10/11]
[INFO] --------------------------------[ jar ]---------------------------------
[WARNING] The POM for org.apache.hadoop:hadoop-common:jar:2.7.3.2.6.5.0-292 is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-hdfs:jar:2.7.3.2.6.5.0-292 is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.7.3.2.6.5.0-292 is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-yarn-client:jar:2.7.3.2.6.5.0-292 is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-yarn-common:jar:2.7.3.2.6.5.0-292 is missing, no dependency information available
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] flink-shaded 8.0 ................................... SUCCESS [  2.122 s]
[INFO] flink-shaded-force-shading 8.0 ..................... SUCCESS [  0.607 s]
[INFO] flink-shaded-asm-7 7.1-8.0 ......................... SUCCESS [  0.667 s]
[INFO] flink-shaded-guava-18 18.0-8.0 ..................... SUCCESS [  1.452 s]
[INFO] flink-shaded-netty-4 4.1.39.Final-8.0 .............. SUCCESS [  4.597 s]
[INFO] flink-shaded-netty-tcnative-dynamic 2.0.25.Final-8.0 SUCCESS [  0.620 s]
[INFO] flink-shaded-jackson-parent 2.9.8-8.0 .............. SUCCESS [  0.018 s]
[INFO] flink-shaded-jackson-2 2.9.8-8.0 ................... SUCCESS [  0.914 s]
[INFO] flink-shaded-jackson-module-jsonSchema-2 2.9.8-8.0 . SUCCESS [  0.627 s]
[INFO] flink-shaded-hadoop-2 2.7.3.2.6.5.0-292-8.0 ........ FAILURE [  0.047 s]
[INFO] flink-shaded-hadoop-2-uber 2.7.3.2.6.5.0-292-8.0 ... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  11.947 s
[INFO] Finished at: 2019-09-03T16:52:59+02:00
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile ""vendor-repos"" could not be activated because it does not exist.
[ERROR] Failed to execute goal on project flink-shaded-hadoop-2: Could not resolve dependencies for project org.apache.flink:flink-shaded-hadoop-2:jar:2.7.3.2.6.5.0-292-8.0: The following artifacts could not be resolved: org.apache.hadoop:hadoop-common:jar:2.7.3.2.6.5.0-292, org.apache.hadoop:hadoop-hdfs:jar:2.7.3.2.6.5.0-292, org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.7.3.2.6.5.0-292, org.apache.hadoop:hadoop-yarn-client:jar:2.7.3.2.6.5.0-292, org.apache.hadoop:hadoop-yarn-common:jar:2.7.3.2.6.5.0-292: Failure to find org.apache.hadoop:hadoop-common:jar:2.7.3.2.6.5.0-292 in https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :flink-shaded-hadoop-2
{code}
vendor-repos profile exists in Flink pom.xml file : [https://github.com/apache/flink/blob/3079d11913f153ec40c75afb5356fd3be1a1e550/pom.xml#L1037]",,aljoscha,Elise Ramé,trohrmann,,,,,,,,,,,"zentol commented on pull request #10176: [FLINK-13945][build][docs] Fix instructions for building against custom hadoop versions
URL: https://github.com/apache/flink/pull/10176
 
 
   Updates the documentation for building flink-shaded against vendor repositories. Users are now instructed to setup the repository in their local maven setup.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Nov/19 15:32;githubbot;600","zentol commented on pull request #10176: [FLINK-13945][build][docs] Fix instructions for building against custom hadoop versions
URL: https://github.com/apache/flink/pull/10176
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 15:15;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-14072,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 15:17:04 UTC 2019,,,,,,,,,,"0|z06a1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/19 08:34;trohrmann;Thanks for reporting this issue [~Elise Ramé]. 

[~chesnay] I guess it could be helpful to update the documentation how to build flink shaded against a vendor specific Hadoop version and update the Flink documentation accordingly.;;;","06/Sep/19 13:40;chesnay;It depends. We can either add the vendor profiles to flink-shaded, or instruct users to setup these repositories in their local environments.

I'd prefer the latter; it's vastly more powerful and less work for us.;;;","06/Sep/19 14:05;trohrmann;I prefer the latter approach as well.;;;","06/Dec/19 15:17;chesnay;master: a61f26124285447de89d4a63baab4c39e2423cb0
1.9: e4ad3b035da5958a4beb0c0d6d8d0124a5c54b7c ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
S3RecoverableWriter causes job to get stuck in recovery,FLINK-13940,13254215,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kkl0u,jweibel22,jweibel22,02/Sep/19 10:51,31/Jan/20 10:53,13/Jul/23 08:10,19/Nov/19 13:50,1.8.0,1.8.1,1.9.0,,,,,,,1.10.0,1.8.2,1.9.1,,Connectors / FileSystem,,,,,0,,,,," 
 The cleaning up of tmp files in S3 introduced by this ticket/PR:
 https://issues.apache.org/jira/browse/FLINK-10963
  is preventing the flink job from being able to recover under some circumstances.
  
 This is what seems to be happening:
 When the jobs tries to recover, it will call initializeState() on all operators, which results in the Bucket.restoreInProgressFile method being called.
 This will download the part_tmp file mentioned in the checkpoint that we're restoring from, and finally it will call fsWriter.cleanupRecoverableState which deletes the part_tmp file in S3.
  Now the open() method is called on all operators. If the open() call fails for one of the operators (this might happen if the issue that caused the job to fail and restart is still unresolved), the job will fail again and try to restart from the same checkpoint as before. This time however, downloading the part_tmp file mentioned in the checkpoint fails because it was deleted during the last recover attempt.

The bug is critical because it results in data loss.
  
  
  
 I discovered the bug because I have a flink job with a RabbitMQ source and a StreamingFileSink that writes to S3 (and therefore uses the S3RecoverableWriter).
 Occasionally I have some RabbitMQ connection issues which causes the job to fail and restart, sometimes the first few restart attempts fail because rabbitmq is unreachable when flink tries to reconnect.
  
 This is what I was seeing:
 RabbitMQ goes down
 Job fails because of a RabbitMQ ConsumerCancelledException
 Job attempts to restart but fails with a Rabbitmq connection exception (x number of times)
 RabbitMQ is back up
 Job attempts to restart but fails with a FileNotFoundException due to some _part_tmp file missing in S3.
  
 The job will be unable to restart and only option is to cancel and restart the job (and loose all state)
  
  
  ",,aljoscha,jark,jweibel22,kkl0u,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 04 07:50:28 UTC 2019,,,,,,,,,,"0|z068sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/19 11:48;aljoscha;I think the solution is to never clean up anything in the recover code-path. However, this would mean that we will have those lingering small files when we have a failure->recovery cycle. This should be preferable to data loss, I think.

Are there any other opinions?;;;","02/Sep/19 12:16;kkl0u;I am also leaning towards this solution. At least this can be a first solution. 

A nicer approach would be to delete these small files at the ""next"" successful checkpoint. This implies that Flink has to remember them. I think this is doable though.

 

For now I will create a subtask with the solution of not deleting anything. And I will leave the main Jira open to remind us that there may be a better solution to be implemented.

 

What do you think [~jweibel22]  and [~aljoscha]?;;;","02/Sep/19 14:11;kkl0u;Actually I think not deleting anything is the safest solution to be sure that we play well also with other features like externalized checkpoints, savepoints, etc. So I think that the solution will be that we simply do not clear up anything. I already have a branch and I will open a PR as soon as travis given green.;;;","02/Sep/19 20:45;jweibel22;I do agree that the solution would have to work for externalized checkpoints and savepoints as well, but in those cases wouldn't it be more natural to store the tmp data together with the checkpoint/savepoint state, in the checkpoint dir / rocksdb? That way the lifecycle would be managed automatically. (I'm new to flink, so don't know if this is difficult to achieve...)

I tried setting the requiresCleanupOfRecoverableState to false myself and I ended up with a LOT of tmp files in S3 because I have a pretty short checkpoint interval. It wastes a lot of space and all of a sudden my sink is full of duplicated data. So I have to either deal with the fact that I have duplicated data, explicitly ignore tmp files when reading from the bucket or create my own process for cleaning up the tmp files. I guess it could work, but it's not ideal, and it's definitely not the behaviour you would expect when enabling EXACTLY_ONCE checkpointing as a user.;;;","03/Sep/19 08:30;kkl0u;I pushed a commit in the PR that keeps the cleaning up of small files for intermediate checkpoints but not when we are restoring. Please have a look [~jweibel22] and let me know what you think. Personally I would go with the solution of not deleting anything. The reason is that this plays well with the other features like savepoints. But I would also like to hear other opinions on this.

 

Buffering in the state backend was a design alternative that we investigated when implementing the streaming file sink but we rejected it because at the end, these files will have to be copied from the state backend and be sent to the final location over the network. This can lead to slow {{commit}} phases,  especially for big part files, which is also the most common case I would assume for block-based storage systems like filesystems. In addition, it stresses unnecessarily the state-backend and it could also lead to long queues of ""pending commits"".;;;","03/Sep/19 13:38;kkl0u;So I merged the solution of simply not deleting upon recovery ([FLINK-13941)|https://issues.apache.org/jira/browse/FLINK-13941]. This solves the issue without leaving a lot of small files behind.;;;","04/Sep/19 07:50;jweibel22;Thank you Kostas!

Thanks for explaining the problems with the state backend solution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
NOTICE-binary is outdated,FLINK-13936,13254108,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,01/Sep/19 09:43,02/Oct/19 17:48,13/Jul/23 08:10,05/Sep/19 13:49,1.10.0,1.8.2,1.9.0,,,,,,,1.10.0,1.8.3,1.9.1,,Build System,,,,,0,pull-request-available,,,,"The NOTICE-binary wasn't updated for the click-event example, the state processing API and changes to the table API packaging.",,trohrmann,,,,,,,,,,,,,"zentol commented on pull request #9610: [FLINK-13936][licensing] Update NOTICE-binary
URL: https://github.com/apache/flink/pull/9610
 
 
   Regenerate NOTICE-binary to reflect the latest state.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 10:03;githubbot;600","zentol commented on pull request #9611: [FLINK-13936][licensing] Update NOTICE-binary
URL: https://github.com/apache/flink/pull/9611
 
 
   Regenerate NOTICE-binary to reflect the latest state.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 10:06;githubbot;600","zentol commented on pull request #9610: [FLINK-13936][licensing] Update NOTICE-binary
URL: https://github.com/apache/flink/pull/9610
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 13:49;githubbot;600","zentol commented on pull request #9611: [FLINK-13936][licensing] Update NOTICE-binary
URL: https://github.com/apache/flink/pull/9611
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 13:49;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 06 10:13:04 UTC 2019,,,,,,,,,,"0|z06854:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/19 08:35;trohrmann;Do you have time to tackle this issue [~chesnay] since it is a blocker issue.;;;","04/Sep/19 08:43;chesnay;I would have time to just regenerate the file, but not enough time to do this in a way where the individual changes could be traced back to specific commits. I already tried the latter and blew more time on that than I'd like to admit.

While doing that I noticed that it shouldn't be too difficult to add an automatic check for this; if we modify {{collect_license_files.sh}} to add a preamble (containing the entries for log4j/slf4j) we could just generate it on Travis and fail if they differ.;;;","04/Sep/19 08:52;trohrmann;Then let's only regenerate the file. +1 for the automatic check.;;;","05/Sep/19 13:49;chesnay;master: 3e2f5fad3ed2dc1db42a9bf028fe97a5e7ddf521
1.9: 70fa917b8867a7b39dd5aa34f5e4fb16516e4b07;;;","06/Sep/19 10:13;chesnay;Minor whitespace issue on 1.8: 1ebc30c5e5d9f49f4af715cedd9cf99478418f1c ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnPrioritySchedulingITCase fails on hadoop 2.4.1,FLINK-13935,13254104,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,chesnay,chesnay,01/Sep/19 07:38,01/Sep/19 12:01,13/Jul/23 08:10,01/Sep/19 12:01,1.10.0,,,,,,,,,1.10.0,,,,Deployment / YARN,Tests,,,,0,test-stability,,,,"The {{YarnPrioritySchedulingITCase}} does an early exit in BeforeClass if run against a hadoop version lower than 2.8 . The AfterClass method in the YarnTestBase however cannot handle this case and fails with an NPE.
{code}
22:33:21.941 [ERROR] org.apache.flink.yarn.YarnPrioritySchedulingITCase.org.apache.flink.yarn.YarnPrioritySchedulingITCase
22:33:21.942 [ERROR]   Run 1: YarnPrioritySchedulingITCase.setup:40 Â» AssumptionViolated Priority scheduling...
22:33:21.943 [ERROR]   Run 2: YarnPrioritySchedulingITCase>YarnTestBase.teardown:956 Â» NullPointer
{code}

https://travis-ci.org/apache/flink/jobs/579264625
",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 01 12:01:32 UTC 2019,,,,,,,,,,"0|z06848:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/19 12:01;trohrmann;Fixed via 28811a0cd4a39bab99daafdd719077e2678cd704;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServerStaticFileServerHandlerTest failed on Travis,FLINK-13934,13254103,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,chesnay,chesnay,01/Sep/19 07:36,28/Feb/20 16:51,13/Jul/23 08:10,28/Feb/20 16:51,1.10.0,,,,,,,,,1.11.0,,,,Runtime / REST,,,,,0,pull-request-available,test-stability,,,"{code}
23:34:44.903 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.28 s <<< FAILURE! - in org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest
23:34:44.917 [ERROR] testRespondWithFile(org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest)  Time elapsed: 0.279 s  <<< FAILURE!
java.lang.AssertionError
	at org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.testRespondWithFile(HistoryServerStaticFileServerHandlerTest.java:66)
{code}
https://travis-ci.org/apache/flink/jobs/579264633",,klion26,rmetzger,roman,xtsong,,,,,,,,,,"zentol commented on pull request #11237: [FLINK-13934][rest] Throw RestHandlerException instead of sending inline response
URL: https://github.com/apache/flink/pull/11237
 
 
   The `StaticFileServerHandlers` so far used a different approach to error-handling as other REST handlers, as they were sending response directly (and returning afterwards), instead of throwing a `RestHandlerException` that is processed higher up the stack.
   This apprahc is both more noisy and increases the risk of introducing bugs during refactorings, since you cannot just take a block of error handling code and move it into a separate method, since you're then missing the return.
   
   This exact problem has occurred when I introduced the `checkFileValidity()` method. Currently the handlers continue processing even if one check failed, leading to inconsistent responses.
   
   This PR adjusts the error handling in these handlers accordingly, and adjust the `HistoryServerStaticFileServerHandlerTest` to check for the correct error messages, and strenghtens the assertions by also verifying the response codes.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 10:43;githubbot;600","zentol commented on pull request #11237: [FLINK-13934][rest] Throw RestHandlerException instead of sending inline response
URL: https://github.com/apache/flink/pull/11237
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Feb/20 16:20;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 28 16:51:16 UTC 2020,,,,,,,,,,"0|z06840:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/19 10:23;chesnay;Hasn't appeared for a while.;;;","20/Feb/20 16:27;rmetzger;Happened again here: https://travis-ci.org/apache/flink/jobs/652874433

{code}10:10:30.033 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.136 s <<< FAILURE! - in org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest
10:10:30.044 [ERROR] testRespondWithFile(org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest)  Time elapsed: 0.136 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: a string containing ""not found""
     but: was ""{""errors"":[""Forbidden.""]}""
	at org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.testRespondWithFile(HistoryServerStaticFileServerHandlerTest.java:81)

10:10:30.051 [INFO] Running org.apache.flink.runtime.webmonitor.history.FsJobArchivistTest
10:10:30.119 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.066 s - in org.apache.flink.runtime.webmonitor.history.FsJobArchivistTest
{code};;;","27/Feb/20 07:42;xtsong;Another instance: [https://api.travis-ci.org/v3/job/655663018/log.txt];;;","27/Feb/20 09:53;chesnay;This is a separate issue.;;;","27/Feb/20 10:05;chesnay;Although...it could be the same. We recently changed the error code for when you access files outside the web directory, but I didn't adjust this test for it. Strangely, it most of the time still passes, so something odd is going on.;;;","28/Feb/20 16:51;chesnay;master:

ba295fab6990a771cd3368f83283b53d89d2165f

ddbe9f7e5a95019cecce2852acb16610eee0c6f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Broken markdown of ""Breaking the lines of too long statements"" section",FLINK-13908,13253863,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,azagrebin,dwysakowicz,dwysakowicz,30/Aug/19 08:24,03/Sep/19 14:02,13/Jul/23 08:10,03/Sep/19 14:02,,,,,,,,,,,,,,Documentation,Project Website,,,,0,pull-request-available,,,,"The section ""Breaking the lines of too long statements"" in code style guidelines is wrongly rendered.",,dwysakowicz,,,,,,,,,,,,,"azagrebin commented on pull request #257: [FLINK-13908][code style] Fix formatting issue in Breaking the lines of too long statements
URL: https://github.com/apache/flink-web/pull/257
 
 
   fix formatting issue introduced in #254
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 13:23;githubbot;600","azagrebin commented on pull request #257: [FLINK-13908][code style] Fix formatting issue in Breaking the lines of too long statements
URL: https://github.com/apache/flink-web/pull/257
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 07:43;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-13802,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 30 08:25:30 UTC 2019,,,,,,,,,,"0|z066mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/19 08:25;dwysakowicz;cc [~azagrebin];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master documentation cannot be built,FLINK-13907,13253862,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,30/Aug/19 08:24,30/Aug/19 08:27,13/Jul/23 08:10,30/Aug/19 08:27,1.10.0,,,,,,,,,1.10.0,,,,Documentation,,,,,0,,,,,"The documentation currently cannot be built on buildbot because due to recent changes we require a newer ruby version.
While a newer ruby version is installed, I can't find a way to actually activate it.

For the time being I'll revert these changes:
* ef74a61f54f190926a8388f46db7919e0e94420b
* 065de4b573a05b0c3436ff2d3af3e0c16589a1a7
* f802e16b06b0c3a3682af7f9017f9c0a69e5d4de
* ac1b8dbf15c405d0646671a138a53c9953153165
* c64e167b8003b7379545c1b83e54d9491164b7a8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-08-30 08:24:20.0,,,,,,,,,,"0|z066mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionConfigTests.test_equals_and_hash failed on Travis,FLINK-13906,13253853,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,weizhong,trohrmann,trohrmann,30/Aug/19 07:54,04/Sep/19 01:21,13/Jul/23 08:10,02/Sep/19 09:21,1.10.0,,,,,,,,,1.10.0,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"The {{ExecutionConfigTests.test_equals_and_hash}} Python test failed on Travis with

{code}
=================================== FAILURES ===================================
__________________ ExecutionConfigTests.test_equals_and_hash ___________________

self = <pyflink.common.tests.test_execution_config.ExecutionConfigTests testMethod=test_equals_and_hash>

    def test_equals_and_hash(self):
    
        config1 = ExecutionEnvironment.get_execution_environment().get_config()
    
        config2 = ExecutionEnvironment.get_execution_environment().get_config()
    
        self.assertEqual(config1, config2)
    
>       self.assertEqual(hash(config1), hash(config2))
E       AssertionError: 1609772339 != -295934785

pyflink/common/tests/test_execution_config.py:277: AssertionError
==================== 1 failed, 373 passed in 50.62 seconds =====================
ERROR: InvocationError for command /home/travis/build/flink-ci/flink/flink-python/.tox/py27/bin/pytest (exited with code 1)
{code}

https://api.travis-ci.com/v3/job/229361674/log.txt",,hequn8128,jark,trohrmann,zhongwei,,,,,,,,,,"WeiZhong94 commented on pull request #9593: [FLINK-13906] Implement hashCode() method in class `GlobalJobParameters`.
URL: https://github.com/apache/flink/pull/9593
 
 
   ## What is the purpose of the change
   
   *This pull request implements the hashCode() method in class `GlobalJobParameters` to fix the test failure of python test `ExecutionConfigTests.test_equals_and_hash`.*
   
   
   ## Brief change log
   
     - *implements hashCode() method in class `GlobalJobParameters`.*
   
   
   ## Verifying this change
   
   This change is already covered by existing python test `ExecutionConfigTests.test_equals_and_hash`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 06:57;githubbot;600","asfgit commented on pull request #9593: [FLINK-13906] Implement hashCode() method in class `GlobalJobParameters`.
URL: https://github.com/apache/flink/pull/9593
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 09:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-13932,,,FLINK-13939,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 04 01:21:24 UTC 2019,,,,,,,,,,"0|z066kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/19 14:39;trohrmann;Another instance: https://api.travis-ci.com/v3/job/229608490/log.txt;;;","30/Aug/19 14:40;trohrmann;Another instance: https://api.travis-ci.org/v3/job/578758446/log.txt;;;","02/Sep/19 02:21;jark;cc [~dian.fu];;;","02/Sep/19 07:03;zhongwei;This problem seems to be introduced in this commit: [https://github.com/apache/flink/pull/8175].

The GlobalJobParameters class implemented the ""equals"" method but not implemented the ""hashCode"" method so the hashCode of different GlobalJobParameters instances are not equal though these instances are all empty.;;;","02/Sep/19 09:21;hequn8128;Fixed in 1.10.0: 7470e44354b97a0320c35eccb80342affa654714;;;","03/Sep/19 03:21;jark;Hi [~hequn8128] [~WeiZhong], I would suggest you can also subscribe builds@flink.apache.org, so that you can get the failure notification ASAP.
I know this issue was introduced by other modules/people.  But before the community have a good understanding that the python failure may relative to their commits, I think maybe you can subscribe the builds notification and have the fix ASAP and then remind this note in that broken PR.;;;","04/Sep/19 01:21;hequn8128;[~jark] Thanks for the advice, good idea.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
 Documentation links check errors in release-1.9,FLINK-13901,13253806,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,30/Aug/19 02:10,30/Aug/19 02:16,13/Jul/23 08:10,30/Aug/19 02:16,1.9.0,,,,,,,,,1.9.1,,,,Documentation,,,,,0,,,,,"[2019-08-29 16:04:44] ERROR `/zh/dev/table/config.html' not found.
[2019-08-29 16:04:47] ERROR `/zh/dev/table/catalog.html' not found.
http://localhost:4000/zh/dev/table/config.html:
Remote file does not exist -- broken link!!!

Here is an instance: https://travis-ci.org/apache/flink/jobs/578322338",,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 30 02:16:47 UTC 2019,,,,,,,,,,"0|z066a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/19 02:16;jark;Fixed in 1.9.1: c175cc424458fd2425cd8e8463e4f6cb7bd228f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OSS FS NOTICE file is placed in wrong directory,FLINK-13897,13253705,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,29/Aug/19 12:11,03/Sep/19 09:21,13/Jul/23 08:10,03/Sep/19 09:21,1.8.0,,,,,,,,,1.10.0,1.8.2,1.9.1,,Build System,FileSystems,,,,0,pull-request-available,,,,"The NOTICE file for the OSS filesystem is directly in the resources directory, and not in META-INF where it belongs. As a result the contained dependencies are not properly listed in NOTICE-binary.",,,,,,,,,,,,,,,"zentol commented on pull request #9560: [FLINK-13897][oss] Move NOTICE file into META-INF directory
URL: https://github.com/apache/flink/pull/9560
 
 
   Moves the NOTICE file for flink-oss-fs-hadoop to the correct directory (META-INF), and updates the NOTICE-binary.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Aug/19 12:41;githubbot;600","zentol commented on pull request #9560: [FLINK-13897][oss] Move NOTICE file into META-INF directory
URL: https://github.com/apache/flink/pull/9560
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Sep/19 09:14;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 03 09:21:06 UTC 2019,,,,,,,,,,"0|z065ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Sep/19 09:21;chesnay;master: 162a10a4adaea9832d7fe5dea23609dfd658ad11
1.9: de9fcdbbe43043391c27222e54290f0fcb40fc25
1.8: 4fcbefba2a55f327daa8f187e40547a926939d7d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala 2.11 maven compile should target Java 1.8,FLINK-13896,13253690,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Terry1897,Terry1897,Terry1897,29/Aug/19 10:49,19/Sep/19 12:08,13/Jul/23 08:10,19/Sep/19 12:08,1.9.0,,,,,,,,,1.10.0,,,,Build System,,,,,0,pull-request-available,,,,"When setting TableEnvironment in scala as follwing:

 
{code:java}
// we can repoduce this problem by put following code in 
// org.apache.flink.table.api.scala.internal.StreamTableEnvironmentImplTest

@Test
def testCreateEnvironment(): Unit = {
 val settings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();
 val tEnv = TableEnvironment.create(settings);
}
{code}

Then mvn test would fail with an error message like:

 

error: Static methods in interface require -target:JVM-1.8

 

We can fix this bug by adding:

<configuration>
 <args>
 <arg>-target:jvm-1.8</arg>
 </args>
</configuration>

 

to scala-maven-plugin config

 

 

 ",,jark,sewen,Terry1897,twalthr,,,,,,,,,,"zjuwangg commented on pull request #9692: [FLINK-13896]make scala-maven-plugin compile target jvm version consistent with java version
URL: https://github.com/apache/flink/pull/9692
 
 
   ## What is the purpose of the change
   
   *Make scala-maven-plugin compile target jvm version consistent with java version*
   
   
   ## Brief change log
     - *specify target JVM language*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Sep/19 13:19;githubbot;600","asfgit commented on pull request #9692: [FLINK-13896]make scala-maven-plugin compile target jvm version consistent with java version
URL: https://github.com/apache/flink/pull/9692
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Sep/19 12:05;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 19 12:08:03 UTC 2019,,,,,,,,,,"0|z065kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/19 12:52;twalthr;Hi [~Terry1897], the environment field is rather used for hardware/OS information. Please move the content to the issue description. Would you like to work on a fix for this?;;;","29/Aug/19 13:07;Terry1897;Hi [~twalthr] Sorry to put the description on the environment section, just correct it.
I'd like to fix this one, plz feel free to assign this ticket to me.;;;","19/Sep/19 12:08;sewen;Fixed via 289e147c489c3d0c28d5ea55c95d2d08b2d781b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServerTest failed on Travis,FLINK-13892,13253672,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/Aug/19 10:04,29/Sep/19 09:53,13/Jul/23 08:10,06/Sep/19 10:31,1.8.0,,,,,,,,,1.10.0,1.8.3,1.9.1,,Runtime / Coordination,Runtime / REST,,,,0,pull-request-available,,,,"{code}
16:56:29.548 [ERROR] testHistoryServerIntegration[Flink version less than 1.4: false](org.apache.flink.runtime.webmonitor.history.HistoryServerTest)  Time elapsed: 0.69 s  <<< ERROR!
org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: 
Cannot construct instance of `org.apache.flink.runtime.messages.webmonitor.MultipleJobsDetails`, problem: `java.lang.NullPointerException`
 at [Source: (String)""{""errors"":[""File not found.""]}""; line: 1, column: 30]
	at org.apache.flink.runtime.webmonitor.history.HistoryServerTest.testHistoryServerIntegration(HistoryServerTest.java:142)
Caused by: java.lang.NullPointerException
	at org.apache.flink.runtime.webmonitor.history.HistoryServerTest.testHistoryServerIntegration(HistoryServerTest.java:142)
{code}

https://api.travis-ci.org/v3/job/577860508/log.txt",,gjy,,,,,,,,,,,,,"zentol commented on pull request #9558: [FLINK-13892][hs] Harden HistoryServerTest
URL: https://github.com/apache/flink/pull/9558
 
 
   The HistoryServerTest can fail if the HS attempts to unpack a partially written archive. The test only checked that the HS made 1 _attempt_ at fetching archives, and just assumed that the polling cannot fail.
   
   The test now instead tracks how many archives have been successfully fetched instead. Since we are now taking scenarios into account where the HS polls archives multiple times (in case of errors) the polling interval was reduced to 100ms to prevent excessive test durations.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Aug/19 11:29;githubbot;600","zentol commented on pull request #9558: [FLINK-13892][hs] Harden HistoryServerTest
URL: https://github.com/apache/flink/pull/9558
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 07:41;githubbot;600","zentol commented on pull request #9628: [FLINK-13892][hs] Harden HistoryServerTest
URL: https://github.com/apache/flink/pull/9628
 
 
   Follow-up to #9558.
   
   The current code only ensures that the archives have been fetched and written to disc, however they do ensure that the job overview was adjusted accordingly.
   
   The solution is to only call countdown() once the overview was updated, once for each archive that was fetched.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 11:42;githubbot;600","zentol commented on pull request #9628: [FLINK-13892][hs] Harden HistoryServerTest
URL: https://github.com/apache/flink/pull/9628
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 10:30;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 06 10:31:18 UTC 2019,,,,,,,,,,"0|z065gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/19 07:42;chesnay;master: 5531b23d8fd3da7058087d95e43a4a9c1c3e0111
1.9: 2f26f894be9905efa5cc90e28479ef8d96a4fc8d
1.8: d5407947f23f7a6a9e8a8dbc7d2e78ea6257b7a8 ;;;","05/Sep/19 10:07;chesnay;Reopening since a new issue has popped up after the fix was merged:
https://api.travis-ci.org/v3/job/581082651/log.txt
{code}
org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.exc.MismatchedInputException: 
No content to map due to end-of-input
 at [Source: (String)""""; line: 1, column: 0]
	at org.apache.flink.runtime.webmonitor.history.HistoryServerTest.testHistoryServerIntegration(HistoryServerTest.java:143)
{code};;;","06/Sep/19 07:47;gjy;Another instance

https://api.travis-ci.org/v3/job/581179686/log.txt;;;","06/Sep/19 10:31;chesnay;master: 878b6b19e8e23d69299aa00fb48fa19c3a9116c9
1.9: 94aa4db79dca89391cd6c70cc97c38705c979a31
1.8: 750648d4fb0dd83edf7e0a8462cc52118a98e484 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure defaultInputDependencyConstraint to be non-null when setting it in ExecutionConfig,FLINK-13887,13253607,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,zhuzh,zhuzh,29/Aug/19 02:51,02/Oct/19 17:48,13/Jul/23 08:10,03/Sep/19 09:24,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"If a user invokes ExecutionConfig#setDefaultInputDependencyConstraint(null) to set the defaultInputDependencyConstraint to be null, the scheduling topology building will throw NPE in ExecutionGraph creating stage, causing a master node fatal error.

Thus we need to do checkNotNull on the ExecutionConfig#setDefaultInputDependencyConstraint param to remind users in an early stage.

 

Exception is as blow:

2019-08-28T15:19:21.287+0000 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Fatal error occurred in the cluster entrypoint.
 org.apache.flink.runtime.dispatcher.DispatcherException: Failed to take leadership with session id 2f8f7919-a81b-4529-ad57-9789dbf07707.
         at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$null$30(Dispatcher.java:915)
         at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
         at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
         at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
         at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
         at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:691)
         at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
         at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
         at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
         at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561)
         at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:739)
         at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
         at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
         at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
         at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
         at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
         at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
         at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
         at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
         at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
         at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
         at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
         at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
         at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
         at akka.actor.Actor.aroundReceive(Actor.scala:517)
         at akka.actor.Actor.aroundReceive$(Actor.scala:515)
         at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
         at akka.actor.ActorCell.invoke(ActorCell.scala:561)
         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
         at akka.dispatch.Mailbox.run(Mailbox.scala:225)
         at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
         at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
         at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
         at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
         at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
 Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
         at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)
         at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
         at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
         at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
         ... 4 more
 Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager
         at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:152)
         at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:83)
         at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$5(Dispatcher.java:375)
         at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)
         ... 7 more
 Caused by: java.lang.NullPointerException
         at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:58)
         at org.apache.flink.runtime.scheduler.adapter.DefaultSchedulingExecutionVertex.<init>(DefaultSchedulingExecutionVertex.java:59)
         at org.apache.flink.runtime.scheduler.adapter.ExecutionGraphToSchedulingTopologyAdapter.generateSchedulingExecutionVertex(ExecutionGraphToSchedulingTopologyAdapter.java:113)
         at org.apache.flink.runtime.scheduler.adapter.ExecutionGraphToSchedulingTopologyAdapter.<init>(ExecutionGraphToSchedulingTopologyAdapter.java:65)
         at org.apache.flink.runtime.executiongraph.ExecutionGraph.attachJobGraph(ExecutionGraph.java:939)
         at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:230)
         at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:106)
         at org.apache.flink.runtime.scheduler.LegacyScheduler.createExecutionGraph(LegacyScheduler.java:207)
         at org.apache.flink.runtime.scheduler.LegacyScheduler.createAndRestoreExecutionGraph(LegacyScheduler.java:184)
         at org.apache.flink.runtime.scheduler.LegacyScheduler.<init>(LegacyScheduler.java:176)
         at org.apache.flink.runtime.scheduler.LegacySchedulerFactory.createInstance(LegacySchedulerFactory.java:70)
         at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:275)
         at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:265)
         at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)
         at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)
         at org.apache.flink.runtime.jobmaster.JobManagerRunner.<init>(JobManagerRunner.java:146)
         ... 10 more",,zhuzh,,,,,,,,,,,,,"zhuzhurk commented on pull request #9567: [FLINK-13887] [core] Ensure defaultInputDependencyConstraint to be non-null when setting it in ExecutionConfig
URL: https://github.com/apache/flink/pull/9567
 
 
   
   ## What is the purpose of the change
   
   *If a user invokes ExecutionConfig#setDefaultInputDependencyConstraint(null) to set the defaultInputDependencyConstraint to be null, the scheduling topology building will throw NPE in ExecutionGraph creating stage, causing a master node fatal error.*
   
   
   ## Brief change log
   
   *Setting defaultInputDependencyConstraint to ANY in ExecutionConfig if ExecutionConfig#setDefaultInputDependencyConstraint is invoked with a `null` param.*
   
   
   ## Verifying this change
   
   This change is trivial so no test case is added to reduce code redundancy.
   *Manually verified the change by invoking ExecutionConfig#setDefaultInputDependencyConstraint in BatchFineGrainedRecoveryITCase. Without this change there will be job submission error and the case will fail. With this change, the case can pass.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**yes** / no / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 11:25;githubbot;600","zentol commented on pull request #9567: [FLINK-13887] [core] Ensure defaultInputDependencyConstraint to be non-null when setting it in ExecutionConfig
URL: https://github.com/apache/flink/pull/9567
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Sep/19 09:23;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 03 09:24:31 UTC 2019,,,,,,,,,,"0|z06520:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/19 09:47;chesnay;Given that this method isn't new, and apparently accepted null in the past, we may instead want to opt for setting the constraint to ANY if null is passed.;;;","29/Aug/19 10:51;zhuzh;[~chesnay] yes you are right. We shouldn't break users' existing jobs.;;;","30/Aug/19 09:38;chesnay;[~zhuzh] Do you want do work on this?;;;","30/Aug/19 09:49;zhuzh;[~chesnay] Yes I can fix it.;;;","03/Sep/19 09:24;chesnay;master: 21a21a652324411babd7772665703fb75729687d
1.9: 7687b5bdffa01ec71ea176a199c6b559a37ed642;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
The behavior of JobExecutionResult.getAccumulatorResult does not match its java doc,FLINK-13880,13253401,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,28/Aug/19 05:34,15/Apr/20 07:33,13/Jul/23 08:10,15/Apr/20 07:33,1.11.0,,,,,,,,,1.11.0,,,,API / Core,,,,,0,pull-request-available,,,,"The java doc of `JobExecutionResult.getAccumulatorResult` states that ""Returns \{@code null}, if no accumulator with that name was produced"", but actually an NPE will be triggered if no accumulator with that name is produced.

I'm going to rewrite the `getAccumulatorResult` method to the following:
{code:java}
public <T> T getAccumulatorResult(String accumulatorName) {
   OptionalFailure<Object> result = this.accumulatorResults.get(accumulatorName);
   if (result != null) {
      return (T) result.getUnchecked();
   } else {
      return null;
   }
}
{code}
Please assign this issue to me if this solution is acceptable.

 ",,gjy,jark,TsReaper,,,,,,,,,,,"TsReaper commented on pull request #11698: [FLINK-13880] Fix possible NPE in JobExecutionResult#getAccumulatorResult
URL: https://github.com/apache/flink/pull/11698
 
 
   ## What is the purpose of the change
   
   This PR fixes the possible NPE in `JobExecutionResult#getAccumulatorResult`. Currently the behavior of this method does not match its Java doc and will throw NPE.
   
   ## Brief change log
   
    - Fix possible NPE in JobExecutionResult#getAccumulatorResult
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Apr/20 08:24;githubbot;600","wuchong commented on pull request #11698: [FLINK-13880][core] Correct the behavior of JobExecutionResult.getAccumulatorResult to match Javadoc
URL: https://github.com/apache/flink/pull/11698
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Apr/20 07:33;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 07:33:41 UTC 2020,,,,,,,,,,"0|z063s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/19 09:39;gjy;That's not the only affected method in {{JobExecutionResult}}. 

Is it legal for a user accumulator to return {{null}}? If yes, I think the implementation of {{OptionalFailure}} is problematic – {{getUnchecked()}} will throw {{NullPointerException}}.;;;","10/Apr/20 07:55;jark;+1 to fix this. 

Hi [~gjy], I don't think user accumulator should be null, that is checked by {{OptionalFailure#getUnchecked()}} and {{RuntimeContext#addAccumulator}}. But it is orthogonal to this issue. According to the Javadoc of {{JobExecutionResult.getAccumulatorResult}}, the returned null value doesn't mean the accumulator is null, but the accumulator is not produced. ;;;","10/Apr/20 08:39;TsReaper;[~gjy] thanks for raising this problem. As this problem is not related to this issue, I've created a [new issue|https://issues.apache.org/jira/browse/FLINK-17083] to discuss this.;;;","15/Apr/20 07:33;jark;Fixed in master (1.11.0): 936cb6553580b72a37652417063accf5773971df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing redirects to the flink documentation,FLINK-13875,13253275,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,27/Aug/19 14:45,28/Aug/19 16:15,13/Jul/23 08:10,28/Aug/19 16:15,,,,,,,,,,1.10.0,1.9.1,,,Documentation,,,,,0,pull-request-available,,,,"Add the following redirects.

 

/examples_index.md => /getting_started/examples_index.md
/examples_index.zh.md => /getting_started/examples_index.zh.md
/tutorials_api_tutorials.md => /getting_started/tutorials_api_tutorials.md
/tutorials_api_tutorials.zh.md => /getting_started/tutorials_api_tutorials.zh.md
/tutorials_datastream_api.md => /getting_started/tutorials_datastream_api.md
/tutorials_datastream_api.zh.md => /getting_started/tutorials_datastream_api.zh.md
/tutorials_flink_on_windows.md => /getting_started/tutorials_flink_on_windows.md
/tutorials_flink_on_windows.zh.md => /getting_started/tutorials_flink_on_windows.zh.md
/tutorials_index.md => /getting_started/tutorials_index.md
/tutorials_index.zh.md => /getting_started/tutorials_index.zh.md
/tutorials_local_setup.md => /getting_started/tutorials_local_setup.md
/tutorials_local_setup.zh.md => /getting_started/tutorials_local_setup.zh.md
/tutorials_setup_instructions.md => /getting_started/tutorials_setup_instructions.md
/tutorials_setup_instructions.zh.md => /getting_started/tutorials_setup_instructions.zh.md",,fhueske,sjwiesman,,,,,,,,,,,,"sjwiesman commented on pull request #9544: [FLINK-13875][docs] Add missing redirects to the documentation
URL: https://github.com/apache/flink/pull/9544
 
 
   
   ## What is the purpose of the change
   
   Add missing redirects to the flink documentation 
   
   /examples_index.md                  =>  /getting_started/examples_index.md
   /examples_index.zh.md               =>  /getting_started/examples_index.zh.md
   /tutorials_api_tutorials.md         =>  /getting_started/tutorials_api_tutorials.md
   /tutorials_api_tutorials.zh.md      =>  /getting_started/tutorials_api_tutorials.zh.md
   /tutorials_datastream_api.md        =>  /getting_started/tutorials_datastream_api.md
   /tutorials_datastream_api.zh.md     =>  /getting_started/tutorials_datastream_api.zh.md
   /tutorials_flink_on_windows.md      =>  /getting_started/tutorials_flink_on_windows.md
   /tutorials_flink_on_windows.zh.md   =>  /getting_started/tutorials_flink_on_windows.zh.md
   /tutorials_index.md                 =>  /getting_started/tutorials_index.md
   /tutorials_index.zh.md              =>  /getting_started/tutorials_index.zh.md
   /tutorials_local_setup.md           =>  /getting_started/tutorials_local_setup.md
   /tutorials_local_setup.zh.md        =>  /getting_started/tutorials_local_setup.zh.md
   /tutorials_setup_instructions.md    =>  /getting_started/tutorials_setup_instructions.md
   /tutorials_setup_instructions.zh.md =>  /getting_started/tutorials_setup_instructions.zh.md
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 14:47;githubbot;600","asfgit commented on pull request #9544: [FLINK-13875][docs] Add missing redirects to the documentation
URL: https://github.com/apache/flink/pull/9544
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 09:36;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 28 16:15:03 UTC 2019,,,,,,,,,,"0|z06308:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/19 16:15;fhueske;Fixed for 1.9 with 7bba0b32e779612343e069cdbab15adc77d51c0e
Fixed for 1.10 with d93e6b01c9c495318d8e10348b2110f588092042;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive functions can not work in blink planner stream mode,FLINK-13869,13253199,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,Terry1897,Terry1897,27/Aug/19 07:48,30/Oct/19 12:19,13/Jul/23 08:10,30/Oct/19 12:12,,,,,,,,,,1.10.0,,,,Connectors / Hive,Table SQL / Planner,,,,0,pull-request-available,,,,"In flink, specifying the StreamTableEnvironment through the EnvironmentSetting using the blink planner, when using the UDAF in hive in the table API, the error is reported.

The hive function should been make by correct constants and argTypes. Otherwise it will throw an exception. (See HiveAggSqlFunction)
In this isTableAggregate, it just want to check the aggregate function class type, so the better way is get the function instead of make a function.


{code:java}
Caused by: java.lang.NullPointerException
	at java.util.Arrays.stream(Arrays.java:5004)
	at java.util.stream.Stream.of(Stream.java:1000)
	at org.apache.flink.table.types.utils.TypeConversions.fromLogicalToDataType(TypeConversions.java:67)
	at org.apache.flink.table.planner.functions.utils.HiveFunctionUtils.invokeSetArgs(HiveFunctionUtils.java:59)
	at org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.makeFunction(HiveAggSqlFunction.java:68)
	at org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.makeFunction(HiveAggSqlFunction.java:47)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$$anonfun$isTableAggregate$2.apply(AggregateUtil.scala:750)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$$anonfun$isTableAggregate$2.apply(AggregateUtil.scala:750)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$.isTableAggregate(AggregateUtil.scala:750)
	at org.apache.flink.table.planner.plan.utils.RelExplainUtil$.streamGroupAggregationToString(RelExplainUtil.scala:346)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.explainTerms(StreamExecGroupAggregate.scala:109)
	at org.apache.calcite.rel.AbstractRelNode.explain(AbstractRelNode.java:307)
	at org.apache.calcite.rel.AbstractRelNode.computeDigest(AbstractRelNode.java:388)
	at org.apache.calcite.rel.AbstractRelNode.recomputeDigest(AbstractRelNode.java:351)
	at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:345)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1668)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:846)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:868)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1939)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:129)
	... 60 more
{code}",,faaronzheng,jark,lzljs3620320,phoenixjiangnan,QiLuo,Terry1897,,,,,,,,"JingsongLi commented on pull request #10013: [FLINK-13869][table-planner-blink][hive] Hive functions can not work in blink planner stream mode
URL: https://github.com/apache/flink/pull/10013
 
 
   ##  What is the purpose of the change
   
   Fix ""Hive functions can not work in blink planner stream mode""
   isTableAggregate should get function instead of makeFunction.
   
   ## Verifying this change
   
   HiveCatalogUseBlinkITCase
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Oct/19 09:13;githubbot;600","wuchong commented on pull request #10013: [FLINK-13869][table-planner-blink][hive] Hive functions can not work in blink planner stream mode
URL: https://github.com/apache/flink/pull/10013
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 12:11;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/19 07:37;Terry1897;image-2019-08-27-15-36-57-662.png;https://issues.apache.org/jira/secure/attachment/12978651/image-2019-08-27-15-36-57-662.png","27/Aug/19 07:37;Terry1897;image-2019-08-27-15-37-11-230.png;https://issues.apache.org/jira/secure/attachment/12978650/image-2019-08-27-15-37-11-230.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 30 12:19:06 UTC 2019,,,,,,,,,,"0|z062jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/19 08:00;faaronzheng;the demo is as follows, it causes above issue, but works fine in batch mode.
{code:java}
//代码占位符
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
      EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
      StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env,bsSettings);
//
//        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
//        BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);
        tableEnv.registerCatalog(""myHive1"",new HiveCatalog(""myHive1"",""p1_stream"","""",""1.2.1""));
        tableEnv.useCatalog(""myHive1"");
        tableEnv.useDatabase(""p1_stream"");
        Optional<Catalog> myHive1 = tableEnv.getCatalog(""myHive1"");
        CatalogFunction catalogFunction= myHive1.get().getFunction(new ObjectPath(""p1_stream"",""abs""));
        System.out.println(catalogFunction.getClassName());

        Table result = tableEnv.sqlQuery(""SELECT abs(a) FROM  abs_test"");

      tableEnv.toAppendStream(result,Row.class).writeAsText(""hdfs:/user/result5.txt"");
//        tableEnv.toDataSet(result,Row.class).writeAsText(""hdfs:/user/result5.txt"");

        env.execute();

{code};;;","27/Aug/19 08:04;lzljs3620320;[~Terry1897] Thanks for reporting this issue, Can you share the testing code?;;;","27/Aug/19 18:22;phoenixjiangnan;[~Terry1897] thanks for reporting the bug. I renamed it to ""Hive functions"" from ""Hive built-in functions"" as we havent' supported Hive built-in functions yet.

Also assigned to [~lzljs3620320]. Thanks Jingsong, let me know if you need anything from me;;;","23/Sep/19 04:08;jark;Is there any progress on this issue ?;;;","23/Sep/19 04:33;lzljs3620320;Still need fix, 1.9 should work, but master code has some problem. You can see detail in AggregateUtil.isTableAggregate.;;;","30/Oct/19 12:12;jark;1.10.0: 1da3055338e5f23a3ec4a54152dbf6b4e35e3862;;;","30/Oct/19 12:19;jark;I tried it in current 1.9  branch, and it doesn't have this problem. Removing the affects version 1.9. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Update Execution Plan docs,FLINK-13862,13253040,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,sewen,sewen,26/Aug/19 13:42,29/Jan/20 08:59,13/Jul/23 08:10,06/Dec/19 16:29,1.9.0,,,,,,,,,1.10.0,1.8.4,1.9.2,,Documentation,,,,,0,pull-request-available,,,,"The *Execution Plans* section is totally outdated and refers to the old {{tools/planVisalizer.html}} file that has been removed for two years.

https://ci.apache.org/projects/flink/flink-docs-master/dev/execution_plans.html",,fhueske,gjy,jark,klion26,sewen,tison,,,,,,,,"zentol commented on pull request #10440: [FLINK-13862][docs] Update execution plans docs
URL: https://github.com/apache/flink/pull/10440
 
 
   Updates the execution plans page to point to https://flink.apache.org/visualizer/ and removes an incorrect statement about web-submissions being disabled by default.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Dec/19 12:31;githubbot;600","zentol commented on pull request #10440: [FLINK-13862][docs] Update execution plans docs
URL: https://github.com/apache/flink/pull/10440
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Dec/19 16:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 06 16:29:59 UTC 2019,,,,,,,,,,"0|z004ba:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/19 16:19;tison;I'd like to take a look at this issue tomorrow(UTC+8). It could be regarded as part of ongoing client api discussion since we may or may not remove {{PreviewPlanEnvironment}} this page mainly talked about.;;;","27/Aug/19 02:22;tison;After a close look I would prefer removing it at least for now. Because it mainly talk about how to use the removed plan visualizer. For {{Web Interface}} part, it only talk about {{web.submit.enable}} which is also documented at [this page|https://ci.apache.org/projects/flink/flink-docs-master/ops/config.html].

We can anyway add back a document on how to preview a plan depend on our discussion about client api. It should cover this topic.;;;","12/Sep/19 14:11;gjy;{{planVisualizer.html}} was just a redirect to https://flink.apache.org/visualizer/ so technically it still exists.

See https://github.com/apache/flink/blob/e1b0e0bb7e4715a4315e6d8367a70276dbb8eb7a/flink-dist/src/main/flink-bin/tools/planVisualizer.html#L27;;;","12/Sep/19 15:54;sewen;Is that plan visualizer something we want to keep supporting?

The new Web UI is pretty decent at the visualization and I would guess subsumes it for the majority of users.;;;","13/Sep/19 09:00;fhueske;I found the plan visualizer quite handy when debugging / explaining more complex user programs.
The WebUI requires a running job (AFAIK), the visualizer just the JSON representation of the JobGraph that can be more easily shared (no JAR, no environment dependency, ...).;;;","23/Sep/19 03:29;jark;I agree with [~fhueske] that the plan visualizer is useful without launching the job. 
Shall we update the documentation to refer https://flink.apache.org/visualizer/  instead of {{tools/planVisalizer.html}}?;;;","29/Sep/19 02:28;jark;As this is a documentation issue, I would like to not block 1.9.1. So I moved it to 1.9.2. ;;;","23/Oct/19 08:33;gjy;I think it's reasonable to update the documentation to point to https://flink.apache.org/visualizer/ ;;;","06/Dec/19 16:29;chesnay;master: c8d69f41a0e894b84940a30029a86c66aae38c79
1.9: b9ce2a19629d2f5a006afd30d2c43457a36dbd33
1.8: 20df2d03858c2dc71ae305ebe22af67921c4bb31 ;;;",,,,,,,,,,,,,,,,,,,,,,,
"Running HA (file, async) end-to-end test failed on Travis",FLINK-13853,13252967,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,26/Aug/19 08:23,27/Aug/19 07:38,13/Jul/23 08:10,27/Aug/19 07:38,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"{{Running HA (file, async) end-to-end test}} failed on Travis:

https://api.travis-ci.org/v3/job/576002743/log.txt
https://api.travis-ci.org/v3/job/576002736/log.txt
https://api.travis-ci.org/v3/job/576002730/log.txt
https://api.travis-ci.org/v3/job/576002724/log.txt",,tison,trohrmann,,,,,,,,,,,,"tillrohrmann commented on pull request #9534: [FLINK-13853][e2e] Update common_ha.sh test expression to count recoveries
URL: https://github.com/apache/flink/pull/9534
 
 
   ## What is the purpose of the change
   
   With FLINK-13573 we removed the SubmittedJobGraph and replaced it with the JobGraph.
   Consequently, we no longer see the log statement ""Recovered SubmittedJobGraph"" which
   was used by the common_ha.sh script to count the number of job recoveries. Now we need
   to look for ""Recovered JobGraph"" instead.
   
   ## Verifying this change
   
   - Manually tested that failing e2e test passes
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/19 11:52;githubbot;600","tillrohrmann commented on pull request #9534: [FLINK-13853][e2e] Update common_ha.sh test expression to count recoveries
URL: https://github.com/apache/flink/pull/9534
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 07:37;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-13573,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 07:38:01 UTC 2019,,,,,,,,,,"0|z0613s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/19 07:38;trohrmann;Fixed via a4607c847fbbd7892f154b8388a2b0bb651282b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The back-pressure monitoring tab in Web UI may cause errors,FLINK-13849,13252913,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,xccui,xccui,26/Aug/19 05:57,14/Apr/21 23:04,13/Jul/23 08:10,14/Apr/21 23:04,1.9.0,,,,,,,,,1.13.0,,,,Runtime / Web Frontend,,,,,0,stale-minor,,,,"Clicking the back-pressure monitoring tab for a finished job in Web UI will cause an internal server error. The exceptions are as follows.
{code:java}
2019-08-26 01:23:54,845 ERROR org.apache.flink.runtime.rest.handler.job.JobVertexBackPressureHandler - Unhandled exception.
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (09e107685e0b81b443b556062debb443)
    at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGatewayFuture(Dispatcher.java:825)
    at org.apache.flink.runtime.dispatcher.Dispatcher.requestOperatorBackPressureStats(Dispatcher.java:524)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
    at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
    at akka.actor.ActorCell.invoke(ActorCell.scala:561)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
    at akka.dispatch.Mailbox.run(Mailbox.scala:225)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
    at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,xccui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 23:04:13 UTC 2021,,,,,,,,,,"0|z060rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/21 22:47;flink-jira-bot;This issue and all of its Sub-Tasks have not been updated for 180 days. So, it has been labeled ""stale-minor"". If you are still affected by this bug or are still interested in this issue, please give an update and remove the label. In 7 days the issue will be closed automatically.;;;","14/Apr/21 23:04;chesnay;Backpressure monitoring now only works against the metric system (FLINK-20852), so this no longer occurs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Maven dependencies for Flink-Hive connector,FLINK-13833,13252637,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lmagics,lmagics,23/Aug/19 11:06,19/Nov/19 13:38,13/Jul/23 08:10,19/Nov/19 13:38,1.9.0,,,,,,,,,,,,,Documentation,,,,,0,,,,,"See [https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/hive/]

The Maven artifact IDs in the dependencies snippet are somewhat wrong, thus cannot be resolved. According to my finding in the repository, they should be corrected as follows.
{code:java}
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-hive_2.11</artifactId>
    <version>1.9.0</version>
    <scope>provided</scope>
</dependency>

<!-- Hadoop Dependencies -->

<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-hadoop-compatibility_2.11</artifactId>
    <version>1.9.0</version>
    <scope>provided</scope>
</dependency>

<!-- Hive 1.2.1 is built with Hadoop 2.6.0. We pick 2.6.5 which flink-shaded-hadoop is pre-built with, but users can pick their own hadoop version, as long as it's compatible with Hadoop 2.6.0 -->

<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-shaded-hadoop-2-uber</artifactId>
    <version>2.6.5-7.0</version>
    <scope>provided</scope>
<dependency>{code}",,lmagics,rmetzger,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 19 13:37:56 UTC 2019,,,,,,,,,,"0|z05z2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/19 12:30;chesnay;Could you specify the exact differences, and whether this applies only to the documentation or the maven setup?

The only issue I see at a glance is that the scala suffix is prefixed with 2 underscores, where it should be only 1.;;;","23/Aug/19 13:57;lmagics;[~chesnay] Sorry for the inconvenience, I will try to make it a bit clearer.
 # The artifact ""flink-connector-hive__2.11"" has 2 underscores before the Scala binary version.
 # The artifact ""flink-hadoop-compatibility-1.9.0"" could not be found in Maven repo. Replacing the ""-1.9.0"" suffix with Scala binary version ""_2.11"" and it's all right.
 # The artifact ""flink-shaded-hadoop-2-uber-2.6.5-1.9.0"" / ""flink-shaded-hadoop-2-uber-2.7.5-1.9.0"" could not be found in Maven repo. According to former versions, we should just use ""flink-shaded-hadoop-2-uber"" as artifact ID, and the version number should be prefixed with Hadoop version, like ""2.6.5-7.0"".

The original code and corrections (written in comments) are listed as below.
{code:java}
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-connector-hive__2.11</artifactId>
  <!-- <artifactId>flink-connector-hive_2.11</artifactId> -->
  <version>1.9.0</version>
  <scope>provided</scope>
</dependency>

<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-hadoop-compatibility-1.9.0</artifactId>
  <!-- <artifactId>flink-hadoop-compatibility_2.11</artifactId> -->
  <version>1.9.0</version>
  <scope>provided</scope>
</dependency>

<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-shaded-hadoop-2-uber-2.6.5-1.9.0</artifactId>
  <!-- <artifactId>flink-shaded-hadoop-2-uber</artifactId> -->
  <version>1.9.0</version>
  <!-- <version>2.6.5-7.0</version> -->
  <scope>provided</scope>
</dependency>{code}
 ;;;","19/Nov/19 13:37;rmetzger;This has been resolved in a hotfix here: [https://github.com/apache/flink/commit/614e9b73]
Please reopen if you disagree.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Free Slots / All Slots display error,FLINK-13831,13252629,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yuwang0917@gmail.com,yuwang0917@gmail.com,yuwang0917@gmail.com,23/Aug/19 10:01,02/Oct/19 17:42,13/Jul/23 08:10,23/Aug/19 14:48,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Runtime / Web Frontend,,,,,0,pull-request-available,web-dashboard,,,Free Slots / All Slots display error,,tison,trohrmann,yuwang0917@gmail.com,,,,,,,,,,,"gentlewangyu commented on pull request #9521: [FLINK-13831] Free Slots / All Slots display error
URL: https://github.com/apache/flink/pull/9521
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 10:51;githubbot;600","tillrohrmann commented on pull request #9521: [FLINK-13831] Free Slots / All Slots display error
URL: https://github.com/apache/flink/pull/9521
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 14:48;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/19 10:03;yuwang0917@gmail.com;FLINK-13831.patch;https://issues.apache.org/jira/secure/attachment/12978386/FLINK-13831.patch","23/Aug/19 10:01;yuwang0917@gmail.com;slots.png;https://issues.apache.org/jira/secure/attachment/12978385/slots.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 23 14:51:14 UTC 2019,,,,,,,,,,"0|z05z0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/19 10:16;yuwang0917@gmail.com;[~Tison][~sewen] Please review it , Thanks;;;","23/Aug/19 14:48;trohrmann;Fixed via

1.10.0: 48e13852c70025a9f4e99a4de1350337ab33ebc7
1.9.1: 2d3179a4c382369704304fafeba477ddb98a09d9;;;","23/Aug/19 14:51;tison;Thanks for your contribution [~yuwang0917@gmail.com]. Sorry it is too late for me to start a review :P;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Document about Cluster on yarn have some problems,FLINK-13830,13252626,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,zhangmeng0426,zhangmeng0426,23/Aug/19 09:50,26/Aug/19 02:30,13/Jul/23 08:10,26/Aug/19 02:29,,,,,,,,,,1.9.1,,,,Documentation,,,,,0,documentation,wish,,,"Read the flink 1.9 documentation, YARN Setup section, there have some issues with download the installation package. There is no flink-1.10-SNAPSHOT-bin-hadoop2.tgz package in the https://flink.apache.org/downloads.html download page.

!image-2019-08-23-17-48-22-123.png!",,tzulitai,zhangmeng0426,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13829,,,,,,,,,,,,,,,,,,"23/Aug/19 09:48;zhangmeng0426;image-2019-08-23-17-48-22-123.png;https://issues.apache.org/jira/secure/attachment/12978384/image-2019-08-23-17-48-22-123.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 26 02:29:44 UTC 2019,,,,,,,,,,"0|z05z08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/19 09:58;zhuzh;Hi [~zhangmeng0426], I guess you are reading 1.10 docs, which may be caused by this issue https://issues.apache.org/jira/browse/FLINK-13829.

As a workaround before ISSUE#13829 is fixed, you can manually replace ""flink-docs-master"" in the url to ""flink-docs-release-1.9"" to view 1.9 docs, like [https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/deployment/yarn_setup.html].;;;","26/Aug/19 01:57;zhangmeng0426;Is there have a problem with the menu on the left?    I clicked in the menu on the left side of the 1.9 document and automatically jumped to the master's document.;;;","26/Aug/19 02:21;zhuzh;Actually all the cross page urls incorrectly link to the master docs.

A fix is ongoing at [https://github.com/apache/flink/pull/9528|https://github.com/apache/flink/pull/9528/files].;;;","26/Aug/19 02:29;tzulitai;Fixed for release-1.9: f2ab7dfe2350e5ee0d69a9c11e0d87a66ebd06b1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate ConfigConstants.LOCAL_START_WEBSERVER,FLINK-13828,13252605,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,23/Aug/19 08:41,30/Aug/19 08:30,13/Jul/23 08:10,30/Aug/19 08:30,1.10.0,,,,,,,,,1.10.0,,,,Runtime / Configuration,,,,,0,pull-request-available,,,,"Maybe backport to 1.9.1 because I heard this issue from a user of 1.9.0

Exactly {{ConfigConstants.LOCAL_START_WEBSERVER}} has no power any more but we don't mark it as deprecated or give an investigation to revive it. Since it is part of public interface we cannot remove it between minor versions but deprecate it is necessary.",,tison,twalthr,,,,,,,,,,,,"TisonKun commented on pull request #9541: [FLINK-13828][configuration] Deprecate ConfigConstants.LOCAL_START_WEBSERVER
URL: https://github.com/apache/flink/pull/9541
 
 
   ## What is the purpose of the change
   
   Maybe backport to 1.9.1 because I heard this issue from a user of 1.9.0
   
   Exactly ConfigConstants.LOCAL_START_WEBSERVER has no power any more but we don't mark it as deprecated or give an investigate to revive it. Since it is part of public interface we cannot remove it in minor version but deprecate it is necessary.
   
   
   ## Brief change log
   
   - Remove use points of `ConfigConstants.LOCAL_START_WEBSERVER`
   - Deprecate `ConfigConstants.LOCAL_START_WEBSERVER`
   - hotfix for test style in `WebFrontendITCase`
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes, mark a stale field as deprecated.)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
   cc @twalthr 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 09:39;githubbot;600","zentol commented on pull request #9541: [FLINK-13828][configuration] Deprecate ConfigConstants.LOCAL_START_WEBSERVER
URL: https://github.com/apache/flink/pull/9541
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 08:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 30 08:30:29 UTC 2019,,,,,,,,,,"0|z05yvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/19 03:03;tison;Hi [~twalthr] could you assign this issue to me?;;;","27/Aug/19 09:19;twalthr;Thanks [~Tison]. I assigned it to you.;;;","30/Aug/19 08:30;chesnay;master: ac0799a55914d0021b5baeaa784987efe0ac672d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shell variable should be escaped in start-scala-shell.sh,FLINK-13827,13252599,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,23/Aug/19 08:08,16/Oct/20 10:51,13/Jul/23 08:10,07/Oct/19 12:00,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.2,,,Scala Shell,,,,,0,pull-request-available,,,,"{code:java}
diff --git a/flink-scala-shell/start-script/start-scala-shell.sh b/flink-scala-shell/start-script/start-scala-shell.sh
index b6da81af72..65b9045584 100644
--- a/flink-scala-shell/start-script/start-scala-shell.sh
+++ b/flink-scala-shell/start-script/start-scala-shell.sh
@@ -97,9 +97,9 @@ log_setting=""-Dlog.file=""$LOG"" -Dlog4j.configuration=file:""$FLINK_CONF_DIR""/$LOG
 
 if ${EXTERNAL_LIB_FOUND}
 then
-    $JAVA_RUN -Dscala.color -cp ""$FLINK_CLASSPATH"" $log_setting org.apache.flink.api.scala.FlinkShell $@ --addclasspath ""$EXT_CLASSPATH""
+    $JAVA_RUN -Dscala.color -cp ""$FLINK_CLASSPATH"" ""$log_setting"" org.apache.flink.api.scala.FlinkShell $@ --addclasspath ""$EXT_CLASSPATH""
 else
-    $JAVA_RUN -Dscala.color -cp ""$FLINK_CLASSPATH"" $log_setting org.apache.flink.api.scala.FlinkShell $@
+    $JAVA_RUN -Dscala.color -cp ""$FLINK_CLASSPATH"" ""$log_setting"" org.apache.flink.api.scala.FlinkShell $@
 fi
 
 #restore echo
{code}

otherwise it is error prone when {{$log_setting}} contain arbitrary content. 

For example, if the parent dir contain whitespace, said {{flink-1.9.0 2}}, then {{bin/start-scala-shell.sh local}} will fail with

{{Error: Could not find or load main class 2.log.flink\-\*\-scala\-shell\-local\-\*.log}}",,tison,,,,,,,,,,,,,"TisonKun commented on pull request #9800: [FLINK-13827][script] shell variable should be escaped
URL: https://github.com/apache/flink/pull/9800
 
 
   ## What is the purpose of the change
   
   otherwise it is error prone when `$log_setting` contain arbitrary content.
   
   For example, if the parent dir contain whitespace, said `flink-1.9.0 2`, then `bin/start-scala-shell.sh local` will fail with
   
   ```
   Error: Could not find or load main class 2.log.flink-*-scala-shell-local-*.log
   ```
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
   cc @zentol 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/19 06:51;githubbot;600","TisonKun commented on pull request #9800: [FLINK-13827][script] shell variable should be escaped
URL: https://github.com/apache/flink/pull/9800
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Oct/19 12:11;githubbot;600","TisonKun commented on pull request #9834: [FLINK-13827][script] shell variable should be escaped
URL: https://github.com/apache/flink/pull/9834
 
 
   See also #9800
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Oct/19 12:18;githubbot;600","zentol commented on pull request #9834: [bp-1.9][FLINK-13827][script] shell variable should be escaped
URL: https://github.com/apache/flink/pull/9834
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Oct/19 12:00;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,FLINK-17763,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 07 12:00:36 UTC 2019,,,,,,,,,,"0|z05yug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/19 08:11;tison;See also https://github.com/koalaman/shellcheck/wiki/SC2086;;;","26/Aug/19 03:01;tison;Hi [~chesnay] what do you think? I volunteer to give it a fix.;;;","02/Oct/19 12:12;tison;master via 865cc4c7a39f7aa610a02cc4a0f41424edcd6279;;;","07/Oct/19 12:00;chesnay;1.9: ab7f7d04becde84288df5d9578c390ed569e5fda;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support INSERT OVERWRITE for Hive connector,FLINK-13826,13252598,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,23/Aug/19 08:03,28/Aug/19 22:41,13/Jul/23 08:10,28/Aug/19 22:41,,,,,,,,,,1.10.0,,,,Connectors / Hive,Table SQL / Planner,,,,0,pull-request-available,,,,,,lirui,phoenixjiangnan,,,,,,,,,,,,"lirui-apache commented on pull request #9519: [FLINK-13826][table-planner][hive] Support INSERT OVERWRITE for Hive …
URL: https://github.com/apache/flink/pull/9519
 
 
   …connector
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Support INSERT OVERWRITE Hive tables.
   
   
   ## Brief change log
   
     - Pass whether to overwrite from planner to Hive sink
     - Add test to verify it works
   
   
   ## Verifying this change
   
   Added JUnit test case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? yes
     - If yes, how is the feature documented? docs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 08:08;githubbot;600","asfgit commented on pull request #9519: [FLINK-13826][table-planner][hive] Support INSERT OVERWRITE for Hive …
URL: https://github.com/apache/flink/pull/9519
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 22:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 28 22:41:19 UTC 2019,,,,,,,,,,"0|z05yu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/19 22:41;phoenixjiangnan;merged in master: f38c5c068f072720eac615a4c00826e032bec466;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The original plugins dir is not restored after e2e test run,FLINK-13825,13252595,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,1u0,1u0,1u0,23/Aug/19 07:43,23/Aug/19 08:24,13/Jul/23 08:10,23/Aug/19 07:52,1.9.0,,,,,,,,,1.10.0,1.9.1,,,,,,,,0,pull-request-available,,,,"Previously, the result of Flink distribution build didn't contain {{plugins}} dir.
Instead, for some e2e tests, the directory was created (and removed) by a test's setup steps.

FLINK-12868 has added a pre-created {{plugins}} dir into the Flink distribution build, but without adjusting the e2e tests. As the result, after some e2e tests run, the original directory is removed.",,1u0,pnowojski,,,,,,,,,,,,"pnowojski commented on pull request #9121: [FLINK-13825][e2e] Restore original plugins dir in flink distribution after test run
URL: https://github.com/apache/flink/pull/9121
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 07:49;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 23 07:52:21 UTC 2019,,,,,,,,,,"0|z05ytk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/19 07:52;pnowojski;Merged as 71d4f35 into apache:master
Merged as f47d4ccc19 into release-1.9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect debug log in CompileUtils,FLINK-13823,13252589,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wangsan,wangsan,wangsan,23/Aug/19 07:35,28/Aug/19 02:28,13/Jul/23 08:10,28/Aug/19 02:28,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"There is a typo in `CompileUtils`:

```java
CODE_LOG.debug(""Compiling: %s \n\n Code:\n%s"", name, code);
```

The placeholder  should be `{}` instead of `%s`.",,jark,wangsan,,,,,,,,,,,,"jrthe42 commented on pull request #9518: [FLINK-13823] [Table] Fix incorrect debug log in CompileUtils
URL: https://github.com/apache/flink/pull/9518
 
 
   ## What is the purpose of the change
   
   This pull request fix log message error in `CompileUtils`
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 07:47;githubbot;600","asfgit commented on pull request #9518: [FLINK-13823] [Table] Fix incorrect debug log in CompileUtils
URL: https://github.com/apache/flink/pull/9518
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 02:26;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 28 02:28:40 UTC 2019,,,,,,,,,,"0|z05ys8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/19 02:28;jark;1.10.0: 9d03fbdf7524af6de0e0e99386fff1c1f135d3bf

1.9.1: 6b8ff87fef10030dc6c2d0b6db3b1da3789c83a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website must link to License etc,FLINK-13821,13252403,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,sebb,sebb,22/Aug/19 14:25,08/Sep/19 13:02,13/Jul/23 08:10,06/Sep/19 11:08,,,,,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,,"ASF project websites must have certain links:

Apachecon
License
Thanks
Security
Sponsor/Donat

Please see:

https://whimsy.apache.org/site/project/flink",,fhueske,rmetzger,,,,,,,,,,,,"rmetzger commented on pull request #261: [FLINK-13821] Add missing foundation links & add events section
URL: https://github.com/apache/flink-web/pull/261
 
 
   See https://issues.apache.org/jira/browse/FLINK-13821 for more details.
   
   In addition to that, the PMC has discussed adding an events section to the page. Since a link to the ASF events is part of the whimsy checks, I added a proposal.
   
   ![image](https://user-images.githubusercontent.com/89049/64276219-af2b7800-cf47-11e9-901b-55a3336a8063.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 17:11;githubbot;600","asfgit commented on pull request #261: [FLINK-13821] Add missing foundation links & add events section
URL: https://github.com/apache/flink-web/pull/261
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 13:00;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 08 13:02:00 UTC 2019,,,,,,,,,,"0|z05xmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/19 13:32;rmetzger;Thanks for opening this ticket Sebb. I was wondering the same, but never came across doing it.

I'll fix that soon.;;;","05/Sep/19 15:53;rmetzger;This has been resolved in [https://github.com/apache/flink-web/commit/ce569aecbe33f6cf5697d9e8e6214e2011e04875] by [~fhueske];;;","05/Sep/19 22:10;sebb;Please see:

https://whimsy.apache.org/site/project/flink

There are still two issues listed;;;","06/Sep/19 08:07;fhueske;Thanks [~sebb], I'll have a look.;;;","06/Sep/19 11:08;fhueske;Fixed with https://github.com/apache/flink-web/commit/8c02afc727e2770db11e0698f2de45db46f57fd0

We're all green :-);;;","08/Sep/19 13:02;rmetzger;Very nice, thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSink should strip quotes from partition values,FLINK-13814,13252251,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,22/Aug/19 02:57,30/Aug/19 22:21,13/Jul/23 08:10,30/Aug/19 22:20,1.9.0,,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,lirui,phoenixjiangnan,,,,,,,,,,,,"lirui-apache commented on pull request #9502: [FLINK-13814][hive] HiveTableSink should strip quotes from partition …
URL: https://github.com/apache/flink/pull/9502
 
 
   …values
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Strip quotes from partition value in order to get proper string values.
   
   
   ## Brief change log
   
     - Strip quotes when we get partition spec from the planner
     - Added test to verify string partition values are properly handled
     - Migrated some existing tests to `TableEnvHiveConnectorTest` since the planner has now supported `PartitionableTableSink`
   
   
   ## Verifying this change
   
   Added test case
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 07:01;githubbot;600","asfgit commented on pull request #9502: [FLINK-13814][hive] HiveTableSink should strip quotes from partition …
URL: https://github.com/apache/flink/pull/9502
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 22:20;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 30 22:20:49 UTC 2019,,,,,,,,,,"0|z05wp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/19 22:20;phoenixjiangnan;merged in master: 07dfffec8248788630bff5e99afe9866d8b50487;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
metrics is different between overview ui and metric response,FLINK-13813,13252242,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,ghostmickey,ghostmickey,22/Aug/19 01:11,04/Dec/20 07:24,13/Jul/23 08:10,04/Dec/20 07:24,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,0,,,,,"After a flink task is over, I get metrics by http request. The record count in response is different with job overview.After a flink task is over, I get metrics by http request. The record count in response is different with job overview.
get http://x.x.x.x:8081/jobs/57969f3978edf3115354fab1a72fd0c8 returns:
{ ""jid"": ""57969f3978edf3115354fab1a72fd0c8"", ""name"": ""3349_cjrw_1566177018152"", ""isStoppable"": false, ""state"": ""FINISHED"", ... ""vertices"": [ \{ ""id"": ""d1cdde18b91ef6ce7c6a1cfdfa9e968d"", ""name"": ""CHAIN DataSource (3349_cjrw_1566177018152/SOURCE/0) -> Map (3349_cjrw_1566177018152/AUDIT/0)"", ""parallelism"": 1, ""status"": ""FINISHED"", ... ""metrics"": { ""read-bytes"": 0, ""read-bytes-complete"": true, ""write-bytes"": 555941888, ""write-bytes-complete"": true, ""read-records"": 0, ""read-records-complete"": true, ""write-records"": 5000000, ""write-records-complete"": true } ... } ... ] }}
But, the metrics by http://x.x.x.x:8081/jobs/57969f3978edf3115354fab1a72fd0c8/vertices/d1cdde18b91ef6ce7c6a1cfdfa9e968d/metrics?get=0.numRecordsOut returns
[\{""id"":""0.numRecordsOut"",""value"":""4084803""}]
The overview record count is different with task metrics, please view the apppendix.",Flink 1.8.1 with hdfs,ghostmickey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/19 01:10;ghostmickey;metrics.png;https://issues.apache.org/jira/secure/attachment/12978243/metrics.png","22/Aug/19 01:10;ghostmickey;overview.png;https://issues.apache.org/jira/secure/attachment/12978244/overview.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 04 07:24:44 UTC 2020,,,,,,,,,,"0|z05wn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/19 07:03;chesnay;Looks like the WebUI is rounding values; I don't think it did that in the old UI. [~rmetzger];;;","29/Aug/19 09:29;ghostmickey;This is a counter, so rounding will not affect it's value.;;;","29/Aug/19 09:44;chesnay;there are different kinds of rounding. You can round ""223"" to the nearest multiple of ""100"", which would be ""200"".;;;","30/Aug/19 03:44;ghostmickey;In my experiment, value displayed on UI(5,000,000) is correct. If rounding affected the metric value ,it will not be a number ended with 3(4084803).

I checked some source code and found that the value of UI is from IOMetrics of operator, but the metric is from other metric group. 
If this is the reason, why the value in metrics group is not correct?;;;","04/Dec/20 07:24;ghostmickey;I got metrics by other ways;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-avro unit tests fails if the character encoding in the environment is not default to UTF-8,FLINK-13807,13251933,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tison,ethanli,ethanli,20/Aug/19 21:10,02/Oct/19 17:47,13/Jul/23 08:10,28/Aug/19 09:13,1.8.0,,,,,,,,,1.10.0,1.8.2,1.9.1,,,,,,,0,pull-request-available,,,,"On Flink release-1.8 branch:
{code:java}
[ERROR] Tests run: 12, Failures: 4, Errors: 0, Skipped: 0, Time elapsed: 4.81 s <<< FAILURE! - in org.apache.flink.formats.avro.typeutils.AvroTypeExtractionTest
[ERROR] testSimpleAvroRead[Execution mode = CLUSTER](org.apache.flink.formats.avro.typeutils.AvroTypeExtractionTest)  Time elapsed: 0.438 s  <<< FAILURE!
java.lang.AssertionError: 
Different elements in arrays: expected 2 elements and received 2
files: [/tmp/junit5386344396421857812/junit6023978980792200274.tmp/4, /tmp/junit5386344396421857812/junit6023978980792200274.tmp/2, /tmp/junit5386344396421857812/junit6023978980792200274.tmp/1, /tmp/junit5386344396421857812/junit6023978980792200274.tmp/3]
 expected: [{""name"": ""Alyssa"", ""favorite_number"": 256, ""favorite_color"": null, ""type_long_test"": null, ""type_double_test"": 123.45, ""type_null_test"": null, ""type_bool_test"": true, ""type_array_string"": [""ELEMENT 1"", ""ELEMENT 2""], ""type_array_boolean"": [true, false], ""type_nullable_array"": null, ""type_enum"": ""GREEN"", ""type_map"": {""KEY 2"": 17554, ""KEY 1"": 8546456}, ""type_fixed"": null, ""type_union"": null, ""type_nested"": {""num"": 239, ""street"": ""Baker Street"", ""city"": ""London"", ""state"": ""London"", ""zip"": ""NW1 6XE""}, ""type_bytes"": {""bytes"": ""\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000""}, ""type_date"": 2014-03-01, ""type_time_millis"": 12:12:12.000, ""type_time_micros"": 123456, ""type_timestamp_millis"": 2014-03-01T12:12:12.321Z, ""type_timestamp_micros"": 123456, ""type_decimal_bytes"": {""bytes"": ""\u0007?""}, ""type_decimal_fixed"": [7, -48]}, {""name"": ""Charlie"", ""favorite_number"": null, ""favorite_color"": ""blue"", ""type_long_test"": 1337, ""type_double_test"": 1.337, ""type_null_test"": null, ""type_bool_test"": false, ""type_array_string"": [], ""type_array_boolean"": [], ""type_nullable_array"": null, ""type_enum"": ""RED"", ""type_map"": {}, ""type_fixed"": null, ""type_union"": null, ""type_nested"": {""num"": 239, ""street"": ""Baker Street"", ""city"": ""London"", ""state"": ""London"", ""zip"": ""NW1 6XE""}, ""type_bytes"": {""bytes"": ""\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000""}, ""type_date"": 2014-03-01, ""type_time_millis"": 12:12:12.000, ""type_time_micros"": 123456, ""type_timestamp_millis"": 2014-03-01T12:12:12.321Z, ""type_timestamp_micros"": 123456, ""type_decimal_bytes"": {""bytes"": ""\u0007?""}, ""type_decimal_fixed"": [7, -48]}]
 received: [{""name"": ""Alyssa"", ""favorite_number"": 256, ""favorite_color"": null, ""type_long_test"": null, ""type_double_test"": 123.45, ""type_null_test"": null, ""type_bool_test"": true, ""type_array_string"": [""ELEMENT 1"", ""ELEMENT 2""], ""type_array_boolean"": [true, false], ""type_nullable_array"": null, ""type_enum"": ""GREEN"", ""type_map"": {""KEY 2"": 17554, ""KEY 1"": 8546456}, ""type_fixed"": null, ""type_union"": null, ""type_nested"": {""num"": 239, ""street"": ""Baker Street"", ""city"": ""London"", ""state"": ""London"", ""zip"": ""NW1 6XE""}, ""type_bytes"": {""bytes"": ""\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000""}, ""type_date"": 2014-03-01, ""type_time_millis"": 12:12:12.000, ""type_time_micros"": 123456, ""type_timestamp_millis"": 2014-03-01T12:12:12.321Z, ""type_timestamp_micros"": 123456, ""type_decimal_bytes"": {""bytes"": ""\u0007??""}, ""type_decimal_fixed"": [7, -48]}, {""name"": ""Charlie"", ""favorite_number"": null, ""favorite_color"": ""blue"", ""type_long_test"": 1337, ""type_double_test"": 1.337, ""type_null_test"": null, ""type_bool_test"": false, ""type_array_string"": [], ""type_array_boolean"": [], ""type_nullable_array"": null, ""type_enum"": ""RED"", ""type_map"": {}, ""type_fixed"": null, ""type_union"": null, ""type_nested"": {""num"": 239, ""street"": ""Baker Street"", ""city"": ""London"", ""state"": ""London"", ""zip"": ""NW1 6XE""}, ""type_bytes"": {""bytes"": ""\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000""}, ""type_date"": 2014-03-01, ""type_time_millis"": 12:12:12.000, ""type_time_micros"": 123456, ""type_timestamp_millis"": 2014-03-01T12:12:12.321Z, ""type_timestamp_micros"": 123456, ""type_decimal_bytes"": {""bytes"": ""\u0007??""}, ""type_decimal_fixed"": [7, -48]}]
	at org.apache.flink.formats.avro.typeutils.AvroTypeExtractionTest.after(AvroTypeExtractionTest.java:76)
{code}

Comparing “expected” with “received”, there is really some question mark difference.

For example, in “expected’, it’s
{code:java}
""type_decimal_bytes"": {""bytes"": ""\u0007?”}
{code}

While in “received”, it’s 
{code:java}
""type_decimal_bytes"": {""bytes"": ""\u0007??""}
{code}

The environment I ran the unit tests on uses ANSI_X3.4-1968 

I changed to ""en_US.UTF-8"" and the unit tests passed. ",,ethanli,tison,,,,,,,,,,,,"TisonKun commented on pull request #9510: [FLINK-13807][tests] Read file with UTF-8 charset in TestBaseUtils.getResultReader
URL: https://github.com/apache/flink/pull/9510
 
 
   ## What is the purpose of the change
   
   Read file with UTF-8 charset in `TestBaseUtils.getResultReader`. Otherwise test depends on locale setting.
   
   ## Verifying this change
   
   This change is itself a test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
   cc @zentol 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 12:01;githubbot;600","zentol commented on pull request #9510: [FLINK-13807][tests] Read file with UTF-8 charset in TestBaseUtils.getResultReader
URL: https://github.com/apache/flink/pull/9510
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 09:11;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/19 03:08;tison;patch.diff;https://issues.apache.org/jira/secure/attachment/12978126/patch.diff",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 28 09:13:48 UTC 2019,,,,,,,,,,"0|z05uqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/19 03:08;tison;Thanks for report this issue [~ethanli]!

[~chesnay] I think we can address this issue by always reading the file with UTF-8 charset.

[~ethanli] could you try out the patch I have just attached? I test in my local environment and see no conflict.

I volunteer to follow up this issue.;;;","21/Aug/19 15:47;ethanli;Thanks for fixing it. Sorry I am not able to test it in a short time. If you change your environment to ANSI_X3.4-1968 and if the unit tests pass, it should be good enough. ;;;","22/Aug/19 07:27;chesnay;[~Tison] That's exactly what we should do.;;;","28/Aug/19 09:13;chesnay;master: 59ec116aedab5f4b59fc5a0260b4c741e138de50
1.9: 603c128c364b8f6bb20dce5ce2708b614b284a99
1.8: 70dfb7a8ea52fac7a00ed1d5b28b7ef244defaa6 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metric Fetcher floods the JM log with errors when TM is lost,FLINK-13806,13251919,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,sewen,sewen,20/Aug/19 20:15,22/Aug/19 09:06,13/Jul/23 08:10,22/Aug/19 09:06,1.8.0,,,,,,,,,1.10.0,1.8.2,1.9.1,,Runtime / Metrics,,,,,0,pull-request-available,,,,"When a task manager is lost, the log contains a series of exceptions from the metrics fetcher, making it hard to identify the exceptions from the actual job failure.

The exception below is contained multiple time (in my example eight times) in a simple 4 TM setup after one TM failure.

I would suggest to suppress ""failed asks"" (timeouts) from the metrics fetcher service, because the fetcher has not enough information to distinguish between root cause exceptions and follow-up exceptions. In most cases, these exceptions should be follow-up to a failure that is handled in the scheduler/ExecutionGraph already, and the additional exception logging only add noise to the log.

{code}
2019-08-20 22:00:09,865 WARN  org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl  - Requesting TaskManager's path for query services failed.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-1834666306]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:871)
        at akka.dispatch.OnComplete.internal(Future.scala:263)
        at akka.dispatch.OnComplete.internal(Future.scala:261)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
        at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
        at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
        at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
        at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:644)
        at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
        at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
        at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
        at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
        at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
        at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
        at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
        at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
        at java.lang.Thread.run(Thread.java:748)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-1834666306]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
        at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
        at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
        at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
        ... 9 more
{code}
",,sewen,trohrmann,,,,,,,,,,,,"zentol commented on pull request #9503: [FLINK-13806][metrics] Log all errors on DEBUG
URL: https://github.com/apache/flink/pull/9503
 
 
   This PR sets the log level of all messages in the `MetricFetcher` to debug.
   
   Errors in the MetricFetcher are mostly due the connection with a TaskExecutor breaking down, which are logged in a more appropriate way elsewhere (relevant: FLINK-13805).
   Additionally, the MetricFetcher is one of those components which does not crash when running into exceptions, and thus should special care on what it logs, as any message will likely be logged several times.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 07:51;githubbot;600","zentol commented on pull request #9503: [FLINK-13806][metrics] Log all errors on DEBUG
URL: https://github.com/apache/flink/pull/9503
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 09:05;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 22 09:06:45 UTC 2019,,,,,,,,,,"0|z05unc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/19 12:40;chesnay;Can we follow a similar approach as in FLINK-6440, as in just setting the log level to debug?;;;","22/Aug/19 07:37;sewen;Setting to DEBUG level sounds good.;;;","22/Aug/19 09:06;chesnay;master: 6618cef776cf1c9ce67ff8ee461412110ca5ada7
1.9: d708ab222846735b4ec4562fe8609389cb616da2 
1.8: b837e1ce5b8a6a7578623939ccc8afd676674546 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad Error Message when TaskManager is lost,FLINK-13805,13251897,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,sewen,sewen,20/Aug/19 18:01,29/Aug/19 15:56,13/Jul/23 08:10,29/Aug/19 15:56,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When a TaskManager is lost, the job reports as the failure cause
{code}
org.apache.flink.util.FlinkException: The assigned slot 6d0e469d55a2630871f43ad0f89c786c_0 was removed.
{code}

That is a pretty bad error message, as a user I don't know what that means. Sounds like it could simply refer to internal book keeping, maybe some rebalancing or so.
You need to know a lot about Flink to understand that this means actually ""TaskManager failure"".
",,clevermose,kisimple,sewen,trohrmann,,,,,,,,,,"tillrohrmann commented on pull request #9550: [FLINK-13805] Properly forward cause for slot removal in SlotManager
URL: https://github.com/apache/flink/pull/9550
 
 
   ## What is the purpose of the change
   
   Forwarding the slot removal cause to the ResourceActions allows to notify the JobMaster
   about the allocation failure cause. This improves debuggability and understanding of the
   system.
   
   ## Verifying this change
   
   - Added `SlotManagerImplTest#unregisterTaskManager_withAllocatedSlot_failsAllocationsWithCause`
   - Tested manually that the correct failure cause is forwarded to the `JobMaster`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 13:07;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 29 15:56:59 UTC 2019,,,,,,,,,,"0|z05uig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/19 15:56;trohrmann;Fixed via

1.10.0: 19d40ff6e2b3b4bd9f6595f80d74085177d72285
1.9.1: 9fa6a42fccb2a287eaf08445dedacbfd19557be6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web Job Submit Page displays stream of error message when web submit is disables in the config,FLINK-13799,13251850,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,sewen,sewen,20/Aug/19 14:44,18/Oct/19 09:48,13/Jul/23 08:10,18/Oct/19 09:48,1.9.0,,,,,,,,,1.10.0,1.9.2,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"If you put {{web.submit.enable: false}} into the configuration, the web UI will still display the ""SubmitJob"" page, but errors will continuously pop up, stating

""Unable to load requested file /jars.""",,sewen,tison,trohrmann,vthinkxie,,,,,,,,,,"vthinkxie commented on pull request #9504: [FLINK-13799][web]: fix error message web submit is disables in the c…
URL: https://github.com/apache/flink/pull/9504
 
 
   …onfig
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 08:13;githubbot;600","zentol commented on pull request #9504: [FLINK-13799][web]: fix error message web submit is disables in the c…
URL: https://github.com/apache/flink/pull/9504
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Oct/19 11:44;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/19 08:57;vthinkxie;FireShot Capture 359 - Apache Flink Web Dashboard - localhost.png;https://issues.apache.org/jira/secure/attachment/12978283/FireShot+Capture+359+-+Apache+Flink+Web+Dashboard+-+localhost.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 18 09:48:30 UTC 2019,,,,,,,,,,"0|z05u88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/19 07:52;vthinkxie;I will check this, plz assign this to me;;;","22/Aug/19 08:15;vthinkxie;Hi [~sewen], there is no method to get if web.submit.enable is false in rest API

I hide the jar error message from rest by modifying the frontend interceptor

[https://github.com/apache/flink/pull/9504];;;","22/Aug/19 08:19;tison;Hi [~vthinkxie], thanks for your contribution. If you hide the error then users would still be confused why they saw a ""Job New Submit"" page but cannot do anything on it.

With my knowledge we currently don't have a method to get the status, but I think the right way is to dig out how to pass this information to the front end(maybe we introduce a new method in the backend), instead of just address error message.;;;","22/Aug/19 08:21;sewen;The old web UI hid the ""submit job"" page alltogether, when web submit was disabled.

Can the same approach work for the new UI as well?;;;","22/Aug/19 08:27;chesnay;The old UI was trying to load the submit page, and when it hit an error hid the contents of the page. This is a rather hacky; instead I will adjust the DashboardConfigHandler to return whether web submissions are enabled.

Ideally we also adjust the REST API such that you don't run into a 404 when trying to submit a job under these circumstances, but instead get a not authorized error or similar.;;;","22/Aug/19 09:00;vthinkxie;!FireShot Capture 359 - Apache Flink Web Dashboard - localhost.png|width=456,height=280!

[~sewen] the old UI also have no way to detect if it was enabled, it will hide all the page when any restAPI got 404 error, I don't think it is a good way to do this, since the network problem also will make the whole page disabled(and could not recover until you refresh it mannually).;;;","22/Aug/19 09:01;vthinkxie;[~Tison] [~chesnay] I agree that we should introduce a new method in the backend, the web.submit.enable is not the only problem that will cause restAPI 404 error.;;;","18/Oct/19 09:48;chesnay;The Job Submit page is no longer displayed if web submission are disabled.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Transactional Id Generation fails due to user code impacting formatting string,FLINK-13789,13251671,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,haodang,haodang,haodang,19/Aug/19 19:33,28/Aug/19 11:58,13/Jul/23 08:10,28/Aug/19 11:49,,,,,,,,,,1.10.0,1.8.2,1.9.1,,,,,,,0,pull-request-available,,,,"In [TransactionalIdsGenerator.java|#L94]], prefix contains taskName of the particular task which could ultimately contain user code.  In some cases, user code contains conversion specifiers like %, the string formatting could fail.

For example, in Flink SQL code, user could have a LIKE statement with a % wildcard, the % wildcard will end up in the prefix and get mistreated during formatting, causing task to fail.

Think we should move prefix out of the string formatting.",,haodang,,,,,,,,,,,,,"haodang commented on pull request #9486: [FLINK-13789] move prefix out of the format string
URL: https://github.com/apache/flink/pull/9486
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fixing https://issues.apache.org/jira/browse/FLINK-13789 by moving `prefix` out of the format string.
   
   ## Brief change log
   
   - moving `prefix` out of the format string
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 19:53;githubbot;600","zentol commented on pull request #9486: [FLINK-13789] move prefix out of the format string
URL: https://github.com/apache/flink/pull/9486
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 11:47;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-10478,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 28 11:49:36 UTC 2019,,,,,,,,,,"0|z05t4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/19 11:49;chesnay;master: c038b92a1c90aec0e466e37858a78472ef1f0f10
1.9: c1716b0ed45baf132b2c1eccfe834c996bc0c6ec 
1.8: d22e15cce1b9098244c800b551e80632c0f5c346 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GroupWindowTableAggregateITCase tests fail on Travis,FLINK-13772,13251548,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,trohrmann,trohrmann,19/Aug/19 10:38,23/Aug/19 03:03,13/Jul/23 08:10,23/Aug/19 03:03,1.10.0,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,test-stability,,,,"The {{GroupWindowTableAggregateITCase.testEventTimeSlidingGroupWindowOverTimeNonOverlappingSplitPane}} fails on Travis:

{code}
05:21:41.532 [ERROR] Tests run: 18, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 14.039 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase
05:21:41.532 [ERROR] testEventTimeSlidingGroupWindowOverTimeNonOverlappingSplitPane[StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase)  Time elapsed: 0.573 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase.testEventTimeSlidingGroupWindowOverTimeNonOverlappingSplitPane(GroupWindowTableAggregateITCase.scala:301)
Caused by: java.lang.Exception: Artificial Failure
{code}

https://api.travis-ci.org/v3/job/573615974/log.txt",,1u0,hequn8128,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13740,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 23 01:50:45 UTC 2019,,,,,,,,,,"0|z05sd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/19 17:59;1u0;Other tests failures of the same test class:

1. CI log: https://api.travis-ci.com/v3/job/227426973/log.txt
{code}
16:44:09.568 [ERROR] testEventTimeTumblingWindow[StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase)  Time elapsed: 0.521 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase.testEventTimeTumblingWindow(GroupWindowTableAggregateITCase.scala:151)
Caused by: java.lang.Exception: Artificial Failure
{code}

 2. CI log: https://api.travis-ci.com/v3/job/227403906/log.txt
{code}
16:00:10.526 [ERROR] testEventTimeSessionGroupWindowOverTime[StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase)  Time elapsed: 0.736 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase.testEventTimeSessionGroupWindowOverTime(GroupWindowTableAggregateITCase.scala:109)
Caused by: java.lang.Exception: Artificial Failure
{code}

3. CI log: https://api.travis-ci.com/v3/job/227400854/log.txt
{code}
15:46:18.143 [ERROR] testEventTimeSlidingGroupWindowOverTimeOverlappingSplitPane[StateBackend=HEAP](org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase)  Time elapsed: 0.86 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.table.planner.runtime.stream.table.GroupWindowTableAggregateITCase.testEventTimeSlidingGroupWindowOverTimeOverlappingSplitPane(GroupWindowTableAggregateITCase.scala:266)
Caused by: java.lang.Exception: Artificial Failure
{code};;;","23/Aug/19 01:50;hequn8128;As discussed in FLINK-13740. The serializer logic contains a bug which leads to the test failure in TableAggregate tests.
I will open a hotfix to ignore the tests for the time being. We can recover the tests once the bug is fixed in blink-planner.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchFineGrainedRecoveryITCase.testProgram failed on Travis,FLINK-13769,13251532,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,azagrebin,azagrebin,19/Aug/19 10:01,30/Aug/19 07:27,13/Jul/23 08:10,30/Aug/19 07:27,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"{{BatchFineGrainedRecoveryITCase.testProgram}} failed on Travis.

{code}
23:14:26.860 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 50.007 s <<< FAILURE! - in org.apache.flink.test.recovery.BatchFineGrainedRecoveryITCase
23:14:26.868 [ERROR] testProgram(org.apache.flink.test.recovery.BatchFineGrainedRecoveryITCase)  Time elapsed: 49.469 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.test.recovery.BatchFineGrainedRecoveryITCase.testProgram(BatchFineGrainedRecoveryITCase.java:225)
Caused by: java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@localhost:39333/user/taskmanager_3#-344551647]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@localhost:39333/user/taskmanager_3#-344551647]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.

{code}

[https://travis-ci.org/apache/flink/jobs/573523669]",,aljoscha,azagrebin,trohrmann,,,,,,,,,,,"azagrebin commented on pull request #9513: [FLINK-13769][Coordination] Close RM connection in TaskExecutor.onStop and do not reconnect
URL: https://github.com/apache/flink/pull/9513
 
 
   ## What is the purpose of the change
   
   Fix for the problem that after we merged waiting for all tasks termination in `TaskExecutor.onStop` (#9072). This interrupted the testing mappers earlier in `BatchFineGrainedRecoveryITCase`, causing concurrency problems with the next slot allocation.
   
   The JM got notified about task failure faster and requested quickly a slot from RM which has not realised yet that the slot of the stopping TM cannot be used anymore. This led to deploying the recovered task into the just killed TM. To fix this we need to deregister TM with the RM at the beginning of the `TaskExecutor.onStop`.
   
   ## Brief change log
   
    - Call `closeResourceManagerConnection` in beginning of `TaskExecutor.onStop`.
    - Do not call `reconnectToResourceManager` in `TaskExecutor.disconnectResourceManager` if TM RPC Endpoint is stopping.
   
   ## Verifying this change
   
   loop `BatchFineGrainedRecoveryITCase` on Travis.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 13:00;githubbot;600","azagrebin commented on pull request #9513: [FLINK-13769][Coordination] Close RM connection in TaskExecutor.onStop and do not reconnect
URL: https://github.com/apache/flink/pull/9513
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Aug/19 07:46;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13819,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 30 07:27:38 UTC 2019,,,,,,,,,,"0|z05s9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/19 06:34;trohrmann;Another instance: https://api.travis-ci.org/v3/job/573915348/log.txt;;;","21/Aug/19 05:52;trohrmann;Another instance: https://api.travis-ci.org/v3/job/574335587/log.txt;;;","22/Aug/19 10:18;azagrebin;The problem is that after we merged waiting for all tasks termination in TaskExecutor.onStop() (FLINK-11630),

this interrupted the testing mappers earlier in BatchFineGrainedRecoveryITCase, causing concurrency problems with the next slot allocation.

The JM got notified about task failure faster and requested quickly a slot from RM which has not realised yet that the slot of the stopping TM cannot be used anymore. This led to deploying the recovered task into the just killed TM. To fix this we need to deregister TM with the RM at the beginning of the TaskExecutor.onStop().

Having deregistered itself, TM should stop reconnecting to RM. A preliminary change is required for that to check the stopping state (FLINK-13819) of the TM RpcEndpoint in TaskExecutor.disconnectResourceManager to decide whether to reconnect.;;;","26/Aug/19 08:24;trohrmann;Just for the records: https://api.travis-ci.org/v3/job/576002720/log.txt;;;","29/Aug/19 10:03;aljoscha;It it resolved with the fix?;;;","30/Aug/19 07:27;azagrebin;merged into master by fa12f2de7574b318008d67803d6969b339331867

merged into release-1.9 by 1a837659b71dd02ef6775bd2a4de331aab3ddc2e

[~aljoscha] Yes, I was just waiting for my Travis to be green after the rebase;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Master build is broken because of wrong Maven version,FLINK-13763,13251470,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,trohrmann,trohrmann,18/Aug/19 18:40,18/Aug/19 18:51,13/Jul/23 08:10,18/Aug/19 18:51,1.10.0,,,,,,,,,,,,,Build System,,,,,0,,,,,"Currently, all master builds fail on Travis because Maven {{3.6.0}} is being used instead of Maven {{3.2.5}} (FLINK-3158). Strangely, this only seems to happen for the master branch.

{code}
/home/travis/maven_cache/apache-maven-3.2.5
/home/travis/maven_cache/apache-maven-3.2.5/bin:/home/travis/.rvm/gems/ruby-2.5.3/bin:/home/travis/.rvm/gems/ruby-2.5.3@global/bin:/home/travis/.rvm/rubies/ruby-2.5.3/bin:/home/travis/.rvm/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin:/home/travis/bin:/home/travis/.local/bin:/usr/local/lib/jvm/openjdk11/bin:/opt/pyenv/shims:/home/travis/.phpenv/shims:/home/travis/perl5/perlbrew/bin:/home/travis/.nvm/versions/node/v8.12.0/bin:/home/travis/gopath/bin:/home/travis/.gimme/versions/go1.11.1.linux.amd64/bin:/usr/local/maven-3.6.0/bin:/usr/local/cmake-3.12.4/bin:/usr/local/clang-7.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/home/travis/.phpenv/bin:/opt/pyenv/bin:/home/travis/.yarn/bin
-Dorg.slf4j.simpleLogger.showDateTime=true -Dorg.slf4j.simpleLogger.dateTimeFormat=HH:mm:ss.SSS
Apache Maven 3.6.0 (97c98ec64a1fdfee7767ce5ffb20918da4f719f3; 2018-10-24T18:41:47Z)
Maven home: /usr/local/maven-3.6.0
{code}

https://api.travis-ci.org/v3/job/573427149/log.txt
https://api.travis-ci.org/v3/job/573405515/log.txt",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 18 18:51:11 UTC 2019,,,,,,,,,,"0|z05s00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/19 18:42;trohrmann;Maybe it is caused by a Travis cache inconsistency because it seems that we run {{setup_maven.sh}} to install Maven {{3.2.5}}.;;;","18/Aug/19 18:51;trohrmann;Clearing the cache of the master branch seemed to have solved the problem. Consequently, it must have been a cache inconsistency.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`SplitStream` should be deprecated because `SplitJavaStream` is deprecated,FLINK-13761,13251448,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,izhangzhihao,izhangzhihao,izhangzhihao,18/Aug/19 14:54,01/Nov/22 09:39,13/Jul/23 08:10,22/Aug/19 16:22,1.8.1,,,,,,,,,1.10.0,1.8.2,1.9.1,,API / Scala,,,,,0,pull-request-available,starter,,,h1. `SplitStream` should be deprecated because `SplitJavaStream` is deprecated.,,izhangzhihao,trohrmann,,,,,,,,,,,,"zentol commented on pull request #9505: [FLINK-13761][datastream] Deprecate SplitStream
URL: https://github.com/apache/flink/pull/9505
 
 
   The split() API was deprecated, but the scala SplitStream class was missed when this was done.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 08:19;githubbot;600","zentol commented on pull request #9505: [FLINK-13761][datastream] Deprecate SplitStream
URL: https://github.com/apache/flink/pull/9505
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 10:05;githubbot;600","tillrohrmann commented on pull request #9474: [FLINK-13761][ScalaAPI]`SplitStream` should be deprecated because `SplitJavaStream` is deprecated
URL: https://github.com/apache/flink/pull/9474
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 16:21;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 22 16:22:57 UTC 2019,,,,,,,,,,"0|z05rv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/19 16:22;trohrmann;Fixed via
1.10.0: 8be6ad1d0cc0a2388e88d162b391b8de24d6bb8c
1.9.1: 36366ac477342c289fc54c6b613ad40189be9168
1.8.2: c6b07c7d47036c1e23c2eceaeaf9fd3baca3b311;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix hardcode Scala version dependency in hive connector,FLINK-13760,13251436,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,jark,jark,18/Aug/19 12:49,18/Aug/19 14:15,13/Jul/23 08:10,18/Aug/19 14:14,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,Build System,Connectors / Hive,,,,0,pull-request-available,,,,"FLINK-13688 introduced a {{flink-test-utils}} dependency in {{flink-connector-hive}}. However, the Scala version of the artifactId is hardcoded, this result in recent CRON jobs failed. 

Here is an instance: https://api.travis-ci.org/v3/job/573092374/log.txt


{code}
11:46:09.078 [INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-connector-hive_2.12 ---
11:46:09.134 [WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:
Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21
Found Banned Dependency: com.typesafe.akka:akka-actor_2.11:jar:2.5.21
Found Banned Dependency: com.typesafe:ssl-config-core_2.11:jar:0.3.7
Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0
Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21
Found Banned Dependency: org.apache.flink:flink-clients_2.11:jar:1.10-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.10-SNAPSHOT
Found Banned Dependency: com.typesafe.akka:akka-stream_2.11:jar:2.5.21
Found Banned Dependency: com.github.scopt:scopt_2.11:jar:3.5.0
Found Banned Dependency: org.apache.flink:flink-test-utils_2.11:jar:1.10-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-runtime_2.11:jar:1.10-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-runtime_2.11:test-jar:tests:1.10-SNAPSHOT
Found Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1
Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6
Found Banned Dependency: org.clapper:grizzled-slf4j_2.11:jar:1.3.2
Found Banned Dependency: org.apache.flink:flink-optimizer_2.11:jar:1.10-SNAPSHOT
Use 'mvn dependency:tree' to locate the source of the banned dependencies.
{code}
",,jark,trohrmann,,,,,,,,,,,,"wuchong commented on pull request #9473: [FLINK-13760][hive] Fix hardcode Scala version dependency in hive connector
URL: https://github.com/apache/flink/pull/9473
 
 
   
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   FLINK-13688 introduced a flink-test-utils dependency in flink-connector-hive. However, the Scala version of the artifactId is hardcoded, this result in recent CRON jobs failed.
   
   ## Brief change log
   
   - Fix the hardcoded 2.11 scala version to the property `scala.binary.version`. 
   
   
   ## Verifying this change
   
   This change is a trivial fix without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/19 12:55;githubbot;600","tillrohrmann commented on pull request #9473: [FLINK-13760][hive] Fix hardcode Scala version dependency in hive connector
URL: https://github.com/apache/flink/pull/9473
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/19 14:12;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-13688,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 18 14:14:14 UTC 2019,,,,,,,,,,"0|z05rsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/19 14:14;trohrmann;Fixed via 

1.10.0: 64938e5317cca98054cfd944eb89f9e53f067ae8
1.9.1: c53fada078400ee812d6acd9d1e87ca7ee1c67a7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All builds for master branch are failed during compile stage,FLINK-13759,13251434,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,jark,jark,18/Aug/19 11:59,18/Aug/19 14:14,13/Jul/23 08:10,18/Aug/19 14:13,1.10.0,,,,,,,,,1.10.0,,,,Build System,,,,,0,pull-request-available,,,,"Here is an instance: https://api.travis-ci.org/v3/job/572950228/log.txt

There is an error in the log.


{code}
==============================================================================
find: ‘flink-connectors/flink-connector-elasticsearch/target/flink-connector-elasticsearch*.jar’: No such file or directory
==============================================================================
Previous build failure detected, skipping cache setup.
==============================================================================
{code}

The {{flink-connector-elasticsearch}} is not exist. But recent commits didn't modify this.",,jark,trohrmann,,,,,,,,,,,,"wuchong commented on pull request #9472: [FLINK-13759][builds] Fix builds for master branch are failed during compile stage
URL: https://github.com/apache/flink/pull/9472
 
 
   
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix builds for master branch are failed during compile stage.
   
   ## Brief change log
   
   - `flink-connector-elasticsearch` doesn't exist at all. Fix the travis script to check `flink-connector-elasticsearch6`.
   
   ## Verifying this change
   
   This change is a trivial work without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/19 12:30;githubbot;600","tillrohrmann commented on pull request #9472: [FLINK-13759][builds] Fix builds for master branch are failed during compile stage
URL: https://github.com/apache/flink/pull/9472
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/19 14:12;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 18 14:13:02 UTC 2019,,,,,,,,,,"0|z05rs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/19 14:13;trohrmann;Fixed via 814190e9f2efc067a004bef6af86c2541e33aada;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to submit JobGraph when registered hdfs file in DistributedCache ,FLINK-13758,13251380,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,luoguohao,luoguohao,17/Aug/19 08:55,22/Jun/21 13:55,13/Jul/23 08:10,22/Jan/20 10:34,1.10.0,1.6.3,1.6.4,1.7.2,1.8.0,1.8.1,1.9.0,1.9.1,,1.10.0,1.9.2,,,Command Line Client,,,,,0,pull-request-available,,,,"when using HDFS files for DistributedCache, it would failed to submit jobGraph, we can see exceptions stack traces in log file after a while, but if DistributedCache file is a local file, every thing goes fine.",,arvid heise,dian.fu,djra,dwysakowicz,gjy,kkl0u,liyu,luoguohao,ram_krish,SleePy,wangyang0918,,,"luoguohao commented on pull request #9481: [FLINK-13758] [flink-clients] failed to submit JobGraph when registered hdfs file in DistributedCache.
URL: https://github.com/apache/flink/pull/9481
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 09:46;githubbot;600","luoguohao commented on pull request #9482: [FLINK-13758] [flink-clients] failed to submit JobGraph when registered hdfs file in DistributedCache.
URL: https://github.com/apache/flink/pull/9482
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *make jobGraph submission success when using DistributedCache*
   
   ## Brief change log
   
     - *only local files registered in DistributedCache would be uploaded to jobManager when JobGraph was submitted.*
   
   ## Verifying this change
   This change added tests and can be verified as follows:
     - *Added test for jobGraph submits when using DistributedCache*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 12:51;githubbot;600","zentol commented on pull request #9482: [FLINK-13758] [flink-clients] failed to submit JobGraph when registered hdfs file in DistributedCache.
URL: https://github.com/apache/flink/pull/9482
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 09:55;githubbot;600","wangyang0918 commented on pull request #10871: [FLINK-13758][client] Support to register DFS files as distributed cache
URL: https://github.com/apache/flink/pull/10871
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The the public API `StreamExecutionEnvironment#registerCachedFile` has been broken since the RestClusterClient  has been introduced.
   
   We could not register remote cached files for all session deployments(standalone, Yarn, Kubernetes, Mesos). Since they are using RestClusterClient to submit job.
   
   ## Brief change log
   
   * Skip to upload DFS artifacts
   * Add a test to cover the case 
   
   
   ## Verifying this change
   
   * The changes could be covered by `DistributedCacheDfsTest#testSubmittingJobViaRestClusterClient`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 13:38;githubbot;600","kl0u commented on pull request #10871: [FLINK-13758][client] Support to register DFS files as distributed cache
URL: https://github.com/apache/flink/pull/10871
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jan/20 15:55;githubbot;600",,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,FLINK-14908,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 22 10:34:40 UTC 2020,,,,,,,,,,"0|z05rg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/19 09:09;luoguohao;after reading the code, i found that every files registered in the distributedCache will uploaded to the jobManager, but in fact, only local file should be uploaded to add the file into the blobServer, other file types such as hdfs, is not needed, which only stores the file path in jobManager. When Task inits in TaskManager,  the hdfs file in distributedCache would be retrieved directly from HDFS,but not the blobServer.;;;","19/Aug/19 02:29;wangyang0918;Hi [~luoguohao]

Do you mean to register a cached file located on the hdfs? Just like the code below.
{code:java}
env.registerCachedFile(""hdfs://myhdfs/path/of/file"", ""test_data"", false){code}
I think it could works.;;;","19/Aug/19 06:01;luoguohao;[~fly_in_gis] yes. this works in local mode, because the different code path. but if you deploy the application on the cluster, it would failed after a while (default would be 100 minutes). ;;;","28/Aug/19 09:55;chesnay;It isn't necessarily true that files from hdfs (or generally, from any DFS) should not be uploated to the distributed cache. A basic example is a client-accessible DFS, which isn't accessible from the cluster.;;;","28/Aug/19 10:21;luoguohao;[~chesnay] Thanks for reply, in the current implementation, HDFS files will not stored in the BlobServer, and HDFS files is retrieved directly from HDFS when the Task initialized in TaskManager, so if we want to store all the files into BlobServer, the scope of code adjustment is wider. And in my opinion, as long as the user choose the HDFS file as a DistributeCache file, the user should make sure that the file is accessible from the cluster, but not the cluster itself.;;;","06/Sep/19 14:36;chesnay;ahhhh I may have remembered wrong; it is true that files from a DFS are never downloaded on the client. They are also never added to the blobstore, but only cached locally on the TaskExecutor when a task requests it.

Please expand on how exactly your job is failing; at this time I'm a bit unsure on what this issue is about.;;;","07/Sep/19 02:56;luoguohao;[~chesnay] ok, the key problem is that when i try to register a hdfs file into DistributedCache:
{code:java}
env.registerCachedFile(""hdfs://myhdfs/path/of/file"", ""test_data"", false)
{code}
it will fail to execute job on cluster mode, but local mode is ok. by the way, the hdfs path is accessible from the cluster.

 ;;;","08/Nov/19 10:37;chesnay;Can you provide us with the actual error you are getting, and/or log files of the task executor / job master?;;;","25/Nov/19 07:37;arvid;If FLINK-14908 is indeed a duplicate, then this is also affecting Flink 1.9 and most likely 1.10 .;;;","25/Nov/19 08:08;dwysakowicz;[~arvid heise]Yes, I am pretty sure it affects all versions that use RestClusterClient for job submission;;;","25/Nov/19 09:33;wangyang0918;I go over the current code base and find that it is actually a bug. Because `StreamExecutionEnvironment#registerCachedFile` allows a remote distributed path(for example, hdfs://host:port/and/path). However, the remote path could not properly be processed by `RestClusterClient#submitJob`. It does not differentiate the local and remote path.

I think we need to check whether the artifacts is a remote path and skip to upload in `RestClusterClient#submitJob`. 

This bug has been introduced since RestClusterClient is used to submit job.;;;","15/Jan/20 04:12;wangyang0918;[~luoguohao] Are you still working on this? I am pretty sure it is a bug and it will be better if we could fix this in release 1.10.

Currently, the public API {{StreamExecutionEnvironment#registerCachedFile}} has been broken.  We could not register remote cached files for all session deployments(standalone, Yarn, Kubernetes, Mesos). Since they are using {{RestClusterClient}} to submit job.;;;","16/Jan/20 09:01;wangyang0918;After offline discussion with [~trohrmann], we hope this ticket could be fix in 1.10.

Even this bug has already existed there for so long time, we should definitely fix it since {{StreamExecutionEnvironment#registerCachedFile}} is a public Api for users.

[~luoguohao] Thanks a lot for creating this ticket. If you do not have time to work on this, i will take over.;;;","18/Jan/20 08:33;luoguohao;Yes,it really took long time to fix this issue,I already made a pull request for this issue,but it have not so much progress on it .after all, thanks,@Yang Wang



--
发自我的网易邮箱手机智能版



在 2020-01-16 16:50:00，""Yang Wang (Jira)"" <jira@apache.org> 写道：


Currently, the public API {{StreamExecutionEnvironment#registerCachedFile}} has been broken.  We could not register remote cached files for all session deployments(standalone, Yarn, Kubernetes, Mesos). Since they are using {{RestClusterClient}} to submit job.

[~gjy], [~liyu] Do you think this ticket should be a blocker for 1.10? |
;;;","19/Jan/20 03:35;wangyang0918;[~luoguohao] Thanks for your reply. Yes, you are right. There's no much progress on the current PR and it is outdated. I will take over this ticket. Feel free to let me know if you still want to work on this.;;;","19/Jan/20 04:59;luoguohao;i'm ok to transfer this ticket to you if you have much more time to handle it.[~fly_in_gis];;;","21/Jan/20 09:40;gjy;It would be nice to get the bug fix in. However, considering that the bug has existed since at least 1.6.3, I would not block the 1.10 release on it.;;;","21/Jan/20 09:51;wangyang0918;[~gjy] Yeah, it will not block the 1.10 release. And i will fix it asap.;;;","21/Jan/20 15:55;kkl0u;Merged on master with 8bb83c7fed2142aba5e7f3188a69c32ee04f2761
1.10 with 2f91a24325b52fa224ef7ed1c560e44cdf4e84a3
1.9 with 0ea00eabaa0eca30dc457eff85f7f1a0c0c7d09c;;;","22/Jan/20 00:29;wangyang0918;Hi [~kkl0u], the building of branch release-1.9 is broken. The commit [0ea00ea|https://github.com/apache/flink/commit/0ea00eabaa0eca30dc457eff85f7f1a0c0c7d09c] causes the compile error. Because the code base of release-1.9 is different with master. So it can not be picked directly.

[https://api.travis-ci.org/v3/job/640208939/log.txt];;;","22/Jan/20 10:34;wangyang0918;The release-1.9 compile error has been fixed via [6061372|https://github.com/apache/flink/commit/6061372e8c8b54b759ed3cccc4015cc4edb21757].

 

[https://travis-ci.org/wangyang0918/flink/builds/640336837];;;",,,,,,,,,,,
TaskDeploymentDescriptor cannot be recycled by GC due to referenced by an anonymous function,FLINK-13752,13251279,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,16/Aug/19 16:08,19/Aug/19 23:10,13/Jul/23 08:10,19/Aug/19 11:44,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Coordination,,,,,0,,,,,"When comparing the 1.8 and 1.9.0-rc2 on a test streaming job, we found that the performance on 1.9.0-rc2 is much lower than that of 1.8. By comparing the two versions, we found that the count of Full GC of TaskExecutor process on 1.9.0-rc2 is much more than that on 1.8.

A further analysis found that the difference is due to in _TaskExecutor#setupResultPartitionBookkeeping_, the anonymous function in _taskTermimationWithResourceCleanFuture_ has referenced the _TaskDeploymentDescriptor_, since this function will be kept till the task is terminated,  _TaskDeploymentDescriptor_ will also be kept referenced in the closure and cannot be recycled by GC. In this job, _TaskDeploymentDescriptor_ of some tasks are as large as 10M, and the total heap is about 113M, thus the kept _TaskDeploymentDescriptors_ will cause relatively large impact on GC and performance.",,gaoyunhaii,jark,maguowei,mzuehlke,trohrmann,tzulitai,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 19 23:10:30 UTC 2019,,,,,,,,,,"0|z05qtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/19 11:44;trohrmann;Fixed via

1.9.0:  [~tzulitai] please update when cherry picking for the new RC
1.9.1: 51e50aba23e9d62f66fa3274485d2bad05478841
1.10.0: 9391a70a4362f8a02cdd744a2a7f3ebbd1d577d6;;;","19/Aug/19 23:10;tzulitai;Cherry-picked for 1.9.0: 04e95278777611519f5d14813dec4cbc533e2934;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Flink client respect classloading policy,FLINK-13749,13251208,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Paul Lin,Paul Lin,Paul Lin,16/Aug/19 10:04,29/Jan/20 14:25,13/Jul/23 08:10,25/Nov/19 11:20,,,,,,,,,,1.10.0,1.8.3,1.9.2,,Command Line Client,Runtime / REST,,,,0,pull-request-available,,,,"Currently, Flink client does not respect the classloading policy and uses hardcoded parent-first classloader, while the other components like jobmanager and taskmanager use child-first classloader by default and respect the classloading options. This makes the client more likely to have dependency conflicts, especially after we removed the convenient hadoop binaries (so users need to add hadoop classpath in the client classpath).

So I propose to make Flink client's (including cli and rest handler) classloading behavior aligned with the other components.",,aljoscha,azagrebin,kyledong,Paul Lin,QiLuo,rongr,shixg,tison,wind_ljy,,,,,"link3280 commented on pull request #10113: [FLINK-13749][client] Make PackagedProgram respect classloading policy
URL: https://github.com/apache/flink/pull/10113
 
 
   ## What is the purpose of the change
   Currently, Flink client (PackagedProgram) does not respect the classloading policy and uses hardcoded parent-first classloader, while the other components like jobmanager and taskmanager use child-first classloader by default and respect the classloading options. This makes the client more likely to have dependency conflicts, especially after we removed the convenient hadoop binaries (so users need to add hadoop classpath in the client classpath).
   
   ## Brief change log
   
     - *Add configuration to PackagedProgram construtors.*
     - *Determind PackagedProgram classloader with respect to classloader resolve-order and parent-first pattern in the configuration.*
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - Add tests in ClassLoaderITCase to check classloading orders in PackagedProgram execution.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`:  no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 07:38;githubbot;600","aljoscha commented on pull request #10113: [FLINK-13749][client] Make PackagedProgram respect classloading policy
URL: https://github.com/apache/flink/pull/10113
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Nov/19 08:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-14598,FLINK-14575,FLINK-14037,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 25 11:18:18 UTC 2019,,,,,,,,,,"0|z05qds:",9223372036854775807,"The Flink client now also respects the configured classloading policy, i.e. parent-first or child-first classloading. Previously only cluster components such as the job manager or task manager supported this setting.

This does mean that users might get different behaviour in their programs, in which case they should configure the classloading policy explicity to use parent-first classloading, which was the previous (hard-coded) behaviour.",,,,,,,,,,,,,,,,,,,"17/Aug/19 02:30;Paul Lin;Please assign this issue to me. Thanks!;;;","30/Aug/19 09:56;azagrebin;[~Paul Lin] are you working on the issue? Could you move in progress if you are working on it?;;;","05/Nov/19 10:55;victor-wong;Any progress on this? If the Assignee is busy, I'd love to help:);;;","06/Nov/19 01:37;Paul Lin;[~victor-wong] Thanks for your attention. I postponed a bit the progress because the client was under active development. Now that the client is relatively steady, I'll make a PR in a day or two.;;;","15/Nov/19 08:59;aljoscha;Fixed on master in ae553ac1bc5697eba68fa98a2bb6c434287565b3;;;","19/Nov/19 09:10;aljoscha;Fixed on release-1.8 in 540458e2933e2441847d4813fb6901f08eab346d

Fixed on release-1.9 in 0f30c263eebd2fc3ecbeae69a4ce9477e1d5d774;;;","21/Nov/19 12:10;chesnay;[~aljoscha] This change implies that the client now uses child-first classloading, correct? If so, we may want to mention that in the release notes.;;;","25/Nov/19 11:18;aljoscha;You're right, I'm adding that. 👌;;;","25/Nov/19 11:18;aljoscha;Reopen to add release notes.;;;",,,,,,,,,,,,,,,,,,,,,,,
Streaming File Sink s3 end-to-end test failed on Travis,FLINK-13748,13251190,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kkl0u,trohrmann,trohrmann,16/Aug/19 07:43,09/Oct/19 07:40,13/Jul/23 08:10,27/Sep/19 12:09,1.10.0,,,,,,,,,,,,,Connectors / FileSystem,Tests,,,,0,pull-request-available,test-stability,,,"The {{Streaming File Sink s3 end-to-end test}} failed on Travis because it did not produce any output for 10 minutes.

https://api.travis-ci.org/v3/job/572255913/log.txt",,gjy,kkl0u,liyu,trohrmann,,,,,,,,,,"kl0u commented on pull request #9695: [FLINK-13748][S3][build] Fix jaxb relocation for S3.
URL: https://github.com/apache/flink/pull/9695
 
 
   ## What is the purpose of the change
   
   Fixes shading on Presto and Hadoop S3 FileSystems.
   
   ## Brief change log
   
   Changes the `pom.xml` in the `flink-s3-fs-hadoop` and `flink-s3-fs-presto`.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as the s3 e2e tests and can be verified by running the cron jobs on Travis.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (**yes** although its build process, not the logic / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 07:00;githubbot;600","kl0u commented on pull request #9695: [FLINK-13748][S3][build] Fix jaxb relocation for S3.
URL: https://github.com/apache/flink/pull/9695
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Sep/19 13:50;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,2400,,,,,,,,,,,,,,,,,,,,,FLINK-13893,,FLINK-14195,,,,,,,,,,,"10/Sep/19 17:29;liyu;image-2019-09-11-01-29-25-634.png;https://issues.apache.org/jira/secure/attachment/12979983/image-2019-09-11-01-29-25-634.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 12:09:36 UTC 2019,,,,,,,,,,"0|z05q9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/19 07:54;gjy;Another instance https://api.travis-ci.org/v3/job/581399567/log.txt;;;","06/Sep/19 07:54;gjy;Another instance https://api.travis-ci.org/v3/job/581399573/log.txt;;;","10/Sep/19 06:11;trohrmann;Another instance: https://api.travis-ci.org/v3/job/582875585/log.txt;;;","10/Sep/19 06:12;trohrmann;[~kkl0u] could you take a look at this issue in order to find out what the problem could be?;;;","10/Sep/19 07:10;kkl0u;I will [~till.rohrmann]. I have to mention though that I do not think we changed anything in the {{StreamingFileSink}} recently. ;;;","10/Sep/19 16:22;liyu;Almost all e2e-misc related part failed (except for e2e-misc-jdk11) in this nightly run including the last instance Till posted: https://travis-ci.org/apache/flink/builds/582875546

From the output we could see below line, which possibly indicating some checkpoint related problem:
{noformat}
Waiting for job (xxx) to have at least 3 completed checkpoints ...

No output has been received in the last 10m0s
{noformat};;;","10/Sep/19 17:34;liyu;!image-2019-09-11-01-29-25-634.png|width=634,height=302!
 From the build history, 536ccd7 was the last green cron build and e0759c3 was the first failure, there're only another prometheus related commit between these two, so I think e0759c3 is the most questionable.;;;","10/Sep/19 17:44;kkl0u;Thanks for digging into it [~carp84]! I will also have a look tomorrow to see if I can reproduce it locally.;;;","11/Sep/19 09:10;trohrmann;Another cron job seemed to have failed because of this end to end test: https://travis-ci.org/apache/flink/builds/583357356;;;","11/Sep/19 10:20;kkl0u;This can also be reproduced locally and the cause seems to be:
{code:java}
java.lang.RuntimeException: Error while confirming checkpoint
   at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1205)
   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
   at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/fs/s3hadoop/shaded/javax/xml/bind/JAXBException
   at org.apache.flink.fs.s3base.shaded.com.amazonaws.util.BinaryUtils.toBase64(BinaryUtils.java:59)
   at org.apache.flink.fs.s3base.shaded.com.amazonaws.services.s3.AmazonS3Client.deleteObjects(AmazonS3Client.java:2116)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$deleteObjects$8(S3AFileSystem.java:1406)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:280)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.deleteObjects(S3AFileSystem.java:1402)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeys(S3AFileSystem.java:1662)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.deleteUnnecessaryFakeDirectories(S3AFileSystem.java:2725)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.S3AFileSystem.finishedWrite(S3AFileSystem.java:2691)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$finalizeMultipartUpload$1(WriteOperationHelper.java:234)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:260)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:317)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:256)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.WriteOperationHelper.finalizeMultipartUpload(WriteOperationHelper.java:222)
   at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.s3a.WriteOperationHelper.completeMPUwithRetries(WriteOperationHelper.java:267)
   at org.apache.flink.fs.s3hadoop.HadoopS3AccessHelper.commitMultiPartUpload(HadoopS3AccessHelper.java:84)
   at org.apache.flink.fs.s3.common.writer.S3Committer.commit(S3Committer.java:67)
   at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.onSuccessfulCompletionOfCheckpoint(Bucket.java:300)
   at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.commitUpToCheckpoint(Buckets.java:216)
   at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink.notifyCheckpointComplete(StreamingFileSink.java:399)
   at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:842)
   at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1200)
   ... 5 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.fs.s3hadoop.shaded.javax.xml.bind.JAXBException
   at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
   at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
   at org.apache.flink.util.ChildFirstClassLoader.loadClass(ChildFirstClassLoader.java:60)
   at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
   ... 29 more {code}
 ;;;","13/Sep/19 07:50;trohrmann;Hi [~kkl0u], what is the progress on this issue? I think it would be good to be fixed because it constantly fails our cron jobs.;;;","13/Sep/19 10:09;kkl0u;[~till.rohrmann] not much progress. I was caught up in another thing. I will get back to it soon.

Till then, why not moving it to the end of the list of test to run for that profile so that at least the rest of the tests can run?;;;","13/Sep/19 11:27;trohrmann;If the fix takes longer then it is an option to disable the test for the time being. If we decide to do that then the important bit is to follow up and to fix the instability in order to enable the test again. Can you do this? What I would like to avoid is me manually following up on this issue if you say that you will take care of it. Test instabilities affect every developer and should, hence, be swiftly addressed. That's also why they are marked as critical.;;;","13/Sep/19 13:53;kkl0u;If it does not get solved by the end of the day, I will disable the test without closing the issue here and keep on working on it. For the record it seems to be related to https://issues.apache.org/jira/browse/FLINK-13893;;;","13/Sep/19 15:06;kkl0u;The problem seems to be the relocation that  https://issues.apache.org/jira/browse/FLINK-13893 introduced. 

A possible explanation seems to be that the relocation should be relevant for java 11 only and not for all the profiles. For previous java versions jaxb is part of java itself ([https://stackoverflow.com/questions/43574426/how-to-resolve-java-lang-noclassdeffounderror-javax-xml-bind-jaxbexception-in-j]). In the current state and for java-8, it seems that Flink is looking for a relocated jaxb which maven has not relocated as it is not part of the dependencies, so it cannot find it.

I can open a PR that seems to work for java-8, but I am not sure how to test it for java-11 on Travis. Any ideas?;;;","13/Sep/19 17:06;liyu;One failure observed in the release-1.9 cron job: https://travis-ci.org/apache/flink/jobs/584110139;;;","13/Sep/19 17:09;liyu;[~chesnay] could you help answer Klou's question about how to test against java-11? Thanks.;;;","13/Sep/19 17:57;kkl0u;[~carp84], Chesnay is on holidays currently. So I will try it locally and will merge the fix if it works ;);;;","13/Sep/19 19:46;trohrmann;Thanks for the analysis [~kkl0u]. I think you are right that if we relocate, then we should always include the dependency in the created uber-jar otherwise the relocation pattern should be conditional (Java 11 vs. different Java versions). I think the former approach would be simplest.

I think you can enable the Java 11 profile by providing {{-Djdk11}} to maven. Alternatively you could also setup a cron job in your personal travis account under settings.;;;","14/Sep/19 07:27;kkl0u;Thanks [~till.rohrmann]. I will go with the first one then to see if it works. So far I was trying the second but I had tested it for Java 8 only.;;;","23/Sep/19 11:01;trohrmann;[~chesnay] could you take a look at this problem? At the moment we have disabled this end-to-end test for Java 11 and removed the relocation of jaxb from the filesystem modules (FLINK-14157).;;;","24/Sep/19 11:32;liyu;There're some probably related issues, FYI: FLINK-14186, FLINK-14195, FLINK-14196

What's more, from the cron history, it seems all above issues occur from commit f65ee55 ([https://travis-ci.org/apache/flink/builds/586735914]) and the one before (fedf11c [https://travis-ci.org/apache/flink/builds/586250934]) only has the S3 file sink issue.;;;","24/Sep/19 11:55;trohrmann;I guess that reverting the jaxb dependency addition and only excluding the failing test case did not cover all other Java 11 end-to-end test cases which fail now.;;;","27/Sep/19 12:09;trohrmann;Fixed as part of FLINK-14195.;;;",,,,,,,,
Elasticsearch (v2.3.5) sink end-to-end test fails on Travis,FLINK-13746,13251188,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,alfredlu,trohrmann,trohrmann,16/Aug/19 07:41,24/Sep/19 12:44,13/Jul/23 08:10,24/Sep/19 12:44,1.10.0,1.9.0,,,,,,,,1.10.0,1.8.3,1.9.1,,Connectors / ElasticSearch,Tests,,,,0,pull-request-available,test-stability,,,"The {{Elasticsearch (v2.3.5) sink end-to-end test}} fails on Travis because it logs contain the following line:

{code}
INFO  org.elasticsearch.plugins - [Terror] modules [], plugins [], sites []
{code}

Due to this, the error check is triggered.

https://api.travis-ci.org/v3/job/572255901/log.txt",,alfredlu,trohrmann,,,,,,,,,,,,"TszKitLo40 commented on pull request #9738: [FLINK-13746]Whitelist [Terror] to avoid end to end test failure in e…
URL: https://github.com/apache/flink/pull/9738
 
 
   …s 2.3.5
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Sep/19 05:36;githubbot;600","tillrohrmann commented on pull request #9738: [FLINK-13746][Connectors/ElasticSearch, Tests]Elasticsearch (v2.3.5) sink end-to-end test fails on Travis
URL: https://github.com/apache/flink/pull/9738
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Sep/19 12:43;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 24 12:44:45 UTC 2019,,,,,,,,,,"0|z05q9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/19 08:10;trohrmann;It seems to be caused by {{org.elasticsearch.plugins.PluginsService}}. Somehow the prefix for {{PluginsService.java:180}} is {{[Terror]}}. I see two easy fixes, either suppressing elasticsearch log statements or to whitelist {{[Terror]}}.;;;","17/Sep/19 03:15;alfredlu; Do you mean the test class of Elasticsearch6SinkExample? And do you mean the way to fix this bug is to add the whitelist in the test class?;;;","17/Sep/19 12:58;trohrmann;It is the end-to-end test {{Elasticsearch (v2.3.5) sink end-to-end test}} which run by executing {{flink-end-to-end-tests/test-scripts/test_streaming_elasticsearch.sh}} which I'm referring to. Either we figure out why {{[Terror]}} is logged or suppress it or make it not count by whitelisting it.;;;","18/Sep/19 03:10;alfredlu;In the function *check_logs_for_errors,*  [Terror] is counted. Can we whitelist it in this function? If this idea works, I am happy to fix it.;;;","18/Sep/19 09:31;trohrmann;That would be one idea to solve it. Go ahead an do it [~alfredlu]. I've assigned this ticket to you.;;;","18/Sep/19 10:08;alfredlu;I am planing to add a line in the ""check_logs_for_errors"", grep -v ""[Terror]”, can I whitlist it in this way?[~till.rohrmann];;;","18/Sep/19 16:38;trohrmann;Yes, this should work. Please also test whether it is working like you intend it to work.;;;","21/Sep/19 05:41;alfredlu;I have submitted a pull request [https://github.com/apache/flink/pull/9738] . [~trohrmann]

 ;;;","24/Sep/19 12:44;trohrmann;Fixed via

1.10.0: fe60f996e792680f74efea24e2916d013c24a044
1.9.1: 71ae6d7f073d0ffb308721a1c5e9c909c3022eca
1.8.3: bd677f22044aa567cd691d76da37a28fa88cb6a9;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix code generation when aggregation contains both distinct aggregate with and without filter,FLINK-13742,13251155,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,icshuo,jark,jark,16/Aug/19 03:49,27/Aug/19 04:17,13/Jul/23 08:10,19/Aug/19 02:34,,,,,,,,,,1.10.0,1.9.1,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"The following test will fail when the aggregation contains {{COUNT(DISTINCT c)}} and {{COUNT(DISTINCT c) filter ...}}.

{code:java}
	  @Test
  def testDistinctWithMultiFilter(): Unit = {
    val sqlQuery =
      ""SELECT b, "" +
        ""  SUM(DISTINCT (a * 3)), "" +
        ""  COUNT(DISTINCT SUBSTRING(c FROM 1 FOR 2)),"" +
        ""  COUNT(DISTINCT c),"" +
        ""  COUNT(DISTINCT c) filter (where MOD(a, 3) = 0),"" +
        ""  COUNT(DISTINCT c) filter (where MOD(a, 3) = 1) "" +
        ""FROM MyTable "" +
        ""GROUP BY b""
    val t = failingDataSource(StreamTestData.get3TupleData).toTable(tEnv).as('a, 'b, 'c)
    tEnv.registerTable(""MyTable"", t)
    val result = tEnv.sqlQuery(sqlQuery).toRetractStream[Row]
    val sink = new TestingRetractSink
    result.addSink(sink)
    env.execute()
    val expected = List(
      ""1,3,1,1,0,1"",
      ""2,15,1,2,1,0"",
      ""3,45,3,3,1,1"",
      ""4,102,1,4,1,2"",
      ""5,195,1,5,2,1"",
      ""6,333,1,6,2,2"")
    assertEquals(expected.sorted, sink.getRetractResults.sorted)
  }
{code}
",,jark,,,,,,,,,,,,,"cshuo commented on pull request #9459: [FLINK-13742][table-planner-blink] Fix code generation when aggregation contains both distinct aggregate with and without filter.
URL: https://github.com/apache/flink/pull/9459
 
 
   ## What is the purpose of the change
   
   fix a bug of distinct aggregation code generation when there exists distinct aggregations on same column, and some of them have filter condition.
   
   ## Verifying this change
   add new IT case in 'AggregateITCase' to cover the change.
   
   ## Does this pull request potentially affect one of the following parts:
   * Dependencies (does it add or upgrade a dependency): no
   * The public API, i.e., is any changed class annotated with @Public(Evolving): no
   * The serializers: no
   * The runtime per-record code paths (performance sensitive): no
   * Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, * Yarn/Mesos, ZooKeeper: no
   * The S3 file system connector: no
   
   ## Documentation
   * Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 05:21;githubbot;600","asfgit commented on pull request #9459: [FLINK-13742][table-planner-blink] Fix code generation when aggregation contains both distinct aggregate with and without filter.
URL: https://github.com/apache/flink/pull/9459
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 02:32;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 19 02:34:52 UTC 2019,,,,,,,,,,"0|z05q20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/19 02:34;jark;master: 94fa4ceade57172362e2d35e5aac8383f8f40a40
1.9: fc42fdd9e369ff375bf189238e0edeaf1c683901;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""SHOW FUNCTIONS"" should include Flink built-in functions' names",FLINK-13741,13251110,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,phoenixjiangnan,phoenixjiangnan,phoenixjiangnan,15/Aug/19 22:47,29/Sep/19 07:17,13/Jul/23 08:10,21/Aug/19 22:59,1.9.0,,,,,,,,,1.10.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Currently ""SHOW FUNCTIONS;"" only returns catalog functions and FunctionDefinitions registered in memory, but does not include Flink built-in functions' names.

AFAIK, it's standard for ""SHOW FUNCTIONS;"" to show all available functions for use in queries in SQL systems like Hive, Presto, Teradata, etc, thus it includes built-in functions naturally. Besides, {{FunctionCatalog.lookupFunction(name)}} resolves calls to built-in functions, it's not feeling right to not displaying functions but can successfully resolve to them.

It seems to me that the root cause is the call stack for ""SHOW FUNCTIONS;"" has been a bit messy - it calls {{tEnv.listUserDefinedFunctions()}} which further calls {{FunctionCatalog.getUserDefinedFunctions()}}, and I'm not sure what's the intention of those two APIs. Are they dedicated to getting all functions, or just user defined functions excluding built-in ones?

In the end, I believe ""SHOW FUNCTIONS;"" should display built-in functions. To achieve that, we either need to modify and/or rename existing APIs mentioned above, or add new APIs to return all functions from FunctionCatalog.

cc [~xuefuz] [~lirui] [~twalthr]",,aljoscha,jark,lirui,phoenixjiangnan,Terry1897,twalthr,xuefuz,,,,,,,"bowenli86 commented on pull request #9457: [FLINK-13741][table] FunctionCatalog.getUserDefinedFunctions() should include Flink built-in functions' names
URL: https://github.com/apache/flink/pull/9457
 
 
   ## What is the purpose of the change
   
   FunctionCatalog.getUserDefinedFunctions() only returns catalog functions and FunctionDefinitions registered in memory, but does not include Flink built-in functions' names.
   
   It means currently if users call {{tEnv.listUserDefinedFunctions()}} in Table API or {{show functions;}} thru SQL would not be able to see Flink's built-in functions.
   
   Should be fixed to include Flink built-in functions' names
   
   ## Brief change log
   
   - made FunctionCatalog.getUserDefinedFunctions() to include Flink built-in functions' names
   - dedup function names by replacing list with set in FunctionCatalog.getUserDefinedFunctions()
   - added unit tests
   
   ## Verifying this change
   
   This change added tests and can be verified as follows: `FunctionCatalogTest.testGetBuiltInFunctions()`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (JavaDocs)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 23:09;githubbot;600","asfgit commented on pull request #9457: [FLINK-13741][table] ""SHOW FUNCTIONS"" should include Flink built-in functions' names
URL: https://github.com/apache/flink/pull/9457
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 22:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-14179,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 21 22:59:21 UTC 2019,,,,,,,,,,"0|z05ps0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/19 04:45;phoenixjiangnan;Hi [~Terry1897], I incorporate your comment into the JIRA's description.;;;","16/Aug/19 05:31;lirui;+1 to include built-in functions for {{SHOW FUNCTIONS}}. On the other hand, maybe it also makes sense to provide a method for users to only list user-defined functions?;;;","16/Aug/19 06:05;jark;Regarding to the {{tEnv.listUserDefinedFunctions()}}, I would like to keep it as is. Not only for compatibility purpose, but also it describe clearly from the method name and javadoc. Maybe we can add a new {{tEnv.listAllFunctions()}} to list all functions including builtin ones.

Regarding to the {{SHOW FUNCTIONS;}} , I'm fine with returning all functions. But do we need to provide a command to only return the registered functions? ;;;","16/Aug/19 07:36;Terry1897;[spark sql show function syntax |https://docs.databricks.com/spark/latest/spark-sql/language-manual/show-functions.html]

SHOW [USER|SYSTEM|ALL] FUNCTIONS ([LIKE] regex | [db_name.]function_name)

[presto sql show function syntax|https://prestodb.github.io/docs/current/sql/show-functions.html]

Simply use `SHOW FUNCTIONS` to list all the functions available for use in queries.

[snowflake sql|https://docs.snowflake.net/manuals/sql-reference/sql/show-user-functions.html]
SHOW FUNCTIONS  Lists all the native (i.e. system-defined/built-in) scalar functions provided by Snowflake, as well as any UDFs that have been created for your account.

[snowflake sql|https://docs.snowflake.net/manuals/sql-reference/sql/show-functions.html]
SHOW USER FUNCTIONS
Lists all user-defined functions (UDFs) for which you have access privileges. 

I prefer the spark sql way, and the default behavior is to show all functions, meanwhile, users can also query specific kind functions by indicating the 'show cope'.
What do all your other guys think?;;;","16/Aug/19 09:32;twalthr;I think we should start to align the TableEnvironment with SQL. So a {{listFunctions}} with semantics similar to other SQL vendors makes sense to me. Once FunctionCatalog has been integrated into the catalog API and built-in functions are stored there as well, this should be easily doable. We should also introduce a syntax {{SHOW EXTENDED FUNCTION}}? to list more useful information such as metadata with comment from the catalog.;;;","16/Aug/19 09:53;Terry1897;[~twalthr] I appreciate the idea of introducing a new syntax like SHOW EXTENDED FUNCTION to show details of a specific one function;;;","16/Aug/19 12:29;phoenixjiangnan;Thanks [~Terry1897] for the summary of existing systems!

Hi [~twalthr], w.r.t. ""Once FunctionCatalog has been integrated into the catalog API and built-in functions are stored there as well, this should be easily doable."", currently {{FunctionCatalog}} wraps {{CatalogManager}} and builtin functions can be easily obtained thru {{BuiltInFunctionDefinitions.getDefinitions()}}, thus my understanding is it's already doable, no?

seems to me what need to be done is 1) add {{listFuntions()}} API to TableEnvironment and FunctionCatalog 2) add new {{Executor.listFunctions()}} API (I'll just keep {{Executor.listUserDefinedFunctions()}} API for now in case we want to support showing UDFs only in the future) 3) make {{SHOW FUNCTIONS;}} call {{executor.listFunctions()}}

W.r.t. ""We should also introduce a syntax SHOW EXTENDED FUNCTION"", shall they be {{DESCRIBE FUNCTION <function_name>;}} and {{DESCRIBE FUNCTION EXTENDED <function_name>;}}?;;;","16/Aug/19 17:05;xuefuz;Thanks all for the inputs. I think we are pretty much aligned:

1. SQL ""SHOW FUNCTIONS"" and tableEnv.listFunction() should include built-in functions. It would be nice to have some options for filtering, like what were suggested above, but missing it will not kill us right way. Most users to use this to see what are out there.

2. Showing the details of a function is the job of ""DESCRIBE FUNCTION func"". With ""EXTENDED"", more info can be shown. ""SHOW FUNCTIONS"" just list the function names. This aligns with table/column/database.;;;","16/Aug/19 17:32;xuefuz;Re: Once FunctionCatalog has been integrated into the catalog API and built-in functions are stored there as well.

Hi [~twalthr], thanks for bringing this up. I think we briefly touched upon this before, but there wasn't consensus how we are going to do this. Thus, I look forward to the proposal. To me, certain logic in FunctionCatalog such as function resolution doesn't seem belonging to Catalog API, and I'm also a little dubious about whether built-in functions need to be stored in a catalog. I think we can have more discussions once we get to it.

Thanks.;;;","21/Aug/19 22:59;phoenixjiangnan;merged in master: ae8acd6c53bd925c99185db9d40721603fbf74a5;;;",,,,,,,,,,,,,,,,,,,,,,
TableAggregateITCase.testNonkeyedFlatAggregate failed on Travis,FLINK-13740,13251071,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,trohrmann,trohrmann,15/Aug/19 17:54,17/Jan/20 12:25,13/Jul/23 08:10,07/Nov/19 10:21,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.2,,,Table SQL / Planner,,,,,0,test-stability,,,,"The {{TableAggregateITCase.testNonkeyedFlatAggregate}} failed on Travis with 

{code}
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.table.planner.runtime.stream.table.TableAggregateITCase.testNonkeyedFlatAggregate(TableAggregateITCase.scala:93)
Caused by: java.lang.Exception: Artificial Failure
{code}

https://api.travis-ci.com/v3/job/225551182/log.txt",,dwysakowicz,hequn8128,jark,lzljs3620320,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13822,FLINK-13772,,,,,,FLINK-13702,FLINK-15619,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 07 10:21:01 UTC 2019,,,,,,,,,,"0|z05pjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/19 03:22;jark;Hi [~hequn8128], could you help to have a looks at this issue? ;;;","16/Aug/19 05:07;hequn8128;[~till.rohrmann] Thanks a lot for pointing out the failure.

The test should be restarted after the `Artificial Failure`, as the restart strategy has been set with restartAttempts = 1. It is failed because there is another exception, as it is shown below:
{code:java}
Caused by: java.lang.IllegalStateException: Concurrent access to KryoSerializer. Thread 1: GroupTableAggregate -> Calc(select=[b AS category, f0 AS v1, f1 AS v2]) (1/4) , Thread 2: AsyncOperations-thread-1
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.enterExclusiveThread(KryoSerializer.java:630)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(KryoSerializer.java:285)
	at org.apache.flink.util.InstantiationUtil.serializeToByteArray(InstantiationUtil.java:526)
	at org.apache.flink.table.dataformat.BinaryGeneric.materialize(BinaryGeneric.java:60)
	at org.apache.flink.table.dataformat.LazyBinaryFormat.ensureMaterialized(LazyBinaryFormat.java:92)
	at org.apache.flink.table.dataformat.BinaryGeneric.copy(BinaryGeneric.java:68)
	at org.apache.flink.table.runtime.typeutils.BinaryGenericSerializer.copy(BinaryGenericSerializer.java:63)
	at org.apache.flink.table.runtime.typeutils.BinaryGenericSerializer.copy(BinaryGenericSerializer.java:40)
	at org.apache.flink.table.runtime.typeutils.BaseRowSerializer.copyBaseRow(BaseRowSerializer.java:150)
	at org.apache.flink.table.runtime.typeutils.BaseRowSerializer.copy(BaseRowSerializer.java:117)
	at org.apache.flink.table.runtime.typeutils.BaseRowSerializer.copy(BaseRowSerializer.java:50)
	at org.apache.flink.table.runtime.typeutils.BaseRowSerializer.copyBaseRow(BaseRowSerializer.java:150)
	at org.apache.flink.table.runtime.typeutils.BaseRowSerializer.copy(BaseRowSerializer.java:117)
	at org.apache.flink.table.runtime.typeutils.BaseRowSerializer.copy(BaseRowSerializer.java:50)
	at org.apache.flink.runtime.state.heap.CopyOnWriteStateMap.get(CopyOnWriteStateMap.java:296)
	at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:244)
	at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:138)
	at org.apache.flink.runtime.state.heap.HeapValueState.value(HeapValueState.java:73)
	at org.apache.flink.table.runtime.operators.aggregate.GroupTableAggFunction.processElement(GroupTableAggFunction.java:117)
{code}

And this exception is thrown because of the same KryoSerializer object is used by two threads: one is the table aggregate thread, the other is the async operator thread. The TypeSerializer is not thread safe, to avoid unpredictable side effects, it is recommended to call duplicate() method and use one serializer instance per thread. 

One option to fix the problem is call the duplicate() method when create the {{BinaryGeneric}}. Other option like making the two thread unrelated would also be considered however may need further discussions. 

This looks like a common problem for blink planner. Not sure whether it is a blocker for release-1.9? [~jark] [~lzljs3620320]

Best, Hequn

 ;;;","16/Aug/19 05:36;jark;Thanks [~hequn8128] for looking into this problem. 

It seems that this happens when async heap snapshot is enabled and there's a generic type element in the accumulator,  then the KryoSerializer will be used in two threads. Is that right?

I will not block release-1.9 because blink planner is an experimental feature, and we can mention this problem in the release note. 

;;;","16/Aug/19 05:43;jark;One possible fix is to duplicate the objSerializer in BinaryGeneric when {{BinaryGeneric.copy}}. 
https://github.com/apache/flink/blob/master/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/dataformat/BinaryGeneric.java#L73;;;","16/Aug/19 05:58;hequn8128;[~jark] You are right. Furthermore, we may also have to perform duplicate() in the materialize method in BinaryGeneric.

Good to hear that it is not a blocker. ;;;","23/Aug/19 01:39;hequn8128;Hi, [~jark] [~lzljs3620320] the same problem is reported by another jira FLINK-13822. Would be great if you can take another look and fix it. 
Another option is ignoring the test case for the time being if you find fixing the problem in blink planner needs more time to discuss. (I can do it if it is true.)
Thanks, Hequn;;;","23/Aug/19 01:58;jark;Agree with you [~hequn8128]. Let's ignore the tests first. And discuss the solution under this Jira issue. ;;;","23/Aug/19 03:34;hequn8128;The hotfix(b6645c0abcbbe6a94586916772b15bea15818d95) has been merged into the master.;;;","25/Sep/19 11:50;lzljs3620320;Hi [~hequn8128] and [~jark] , In my opinion, TypeSerializer copy is too heavy for element copy.

I suggest we can remove BinaryGeneric.copy and do these logic in BinaryGenericSerializer.copy (Use serializer in BinaryGenericSerializer instead of serializer in BinaryGeneric), in this way, we can use thread local serializer to do thread local things. What do you think?;;;","25/Sep/19 12:10;jark;Do you mean make BinaryGenericSerializer#serializer as thread local variable ? ;;;","25/Sep/19 12:21;lzljs3620320;No, I mean use this BinaryGenericSerializer, this BinaryGenericSerializer should be exclusive to this thread.;;;","26/Sep/19 02:25;jark;Do you mean moving all the {{{materialize()}} logic to BinaryGenericSerializer?

I think the root problem is the BinaryGeneric object will be serialize/deserialize/copy by multiple threads, but the BinaryGeneric#copy()/materialize()/ensureMaterialized() uses the {{javaObjectSer}} to (de)serializer which is not thread safe. 

So I think even if we move copy logic to BinaryGenericSerializer.copy, there still be problem in other methods. ;;;","26/Sep/19 03:00;lzljs3620320;Yes, A thorough solution is BinaryGeneric should never hold javaObjectSer.;;;","26/Sep/19 03:04;jark;But LazyBinaryFormat BinaryGeneric need the javaObjectSer...;;;","26/Sep/19 03:09;lzljs3620320;1.Let BinaryGeneric not extends LazyBinaryFormat

2.All supported method of BinaryGeneric should pass javaObjectSer to it.

It is a special LazyBinaryFormat, but it has different operation modes.

What do you think?;;;","26/Sep/19 03:37;jark;#1, It will be a great regression for generic types. (most users are using generic type for UDFs)
#2, I'm not sure whether it works. Because the javaObjectSer is mainly used for materialize(), but materialize()  is an interface of LazyBinaryFormat. It's hard to pass serializer for a nested generic object. 

An easy way maybe we can make operations of javaObjectSer in a synchronized block, including:
1) {{InstantiationUtil.serializeToByteArray(javaObjectSer, javaObject)}}  in {{materialize()}}
2) {{javaObjectSer.copy(javaObject)}} in {{copy()}};;;","26/Sep/19 03:49;lzljs3620320;Let me clear the solutions:

1.Let BinaryGeneric not extends LazyBinaryFormat and all supported method of BinaryGeneric should pass javaObjectSer to it.

2.Add sync to materialize(there are many invoking to it), and remove copy in BinaryGeneric.

For performance, #1 is better.

For easy fix and LazyBinaryFormat definition, #2 is better.;;;","26/Sep/19 03:59;jark;I don't think #1 gets better performance, because it forces that we have to materialize/serialize at the beginning of creating BinaryGeneric. ;;;","26/Sep/19 04:06;lzljs3620320;For #1, I mean BinaryGeneric still extends BinaryFormat and has javaObject, it still lazy. The only difference is that we need do these materialize with pass javaObjectSer to it.;;;","26/Sep/19 05:31;jark;If it is still lazy, then in my understanding, BinaryGeneric should override some methods of BinaryFormat (getOffset, getSizeInBytes, getSegments, hashCode) to materialize first. But these methods (getOffset, getSizeInBytes, getSegments, hashCode) don't have a serializer parameter. ;;;","26/Sep/19 05:50;lzljs3620320;It depends on implementation details... BinaryFormat only provide getters, equals and hashCode from java Object, No matter how we implement it, we should have these methods. Whether they are materialized or not should be left to the caller to decide.

Actually, these fields are not final,  they need to be controlled outside.;;;","30/Oct/19 11:10;lzljs3620320;Hi [~jark], Have we reached an agreement? Can you assign this to me?;;;","30/Oct/19 11:47;lzljs3620320;Hi [~dwysakowicz] , are you interested in fixing it? We all think it needs to be fixed before 1.10.;;;","30/Oct/19 12:00;jark;[~lzljs3620320], Let's keep the discussion in FLINK-13702 and figure out a solution. ;;;","30/Oct/19 12:09;dwysakowicz;Yes, I can take this as well. Even though this is slightly different than [FLINK-13702] they both boil down to thread safety of {{LazyBinaryFormat}}. Lets discuss solution in [FLINK-13702].;;;","07/Nov/19 10:21;dwysakowicz;Fixed in 
master: 0b28e830d7366126a91ca9faa38cb19a8f66a9b6
1.9.2: 814b5fd35fd0308f6edeedb8414623a43ba512a7;;;",,,,,,
BinaryRowTest.testWriteString() fails in some environments,FLINK-13739,13251038,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,rmetzger,rmetzger,15/Aug/19 15:12,27/Aug/19 04:17,13/Jul/23 08:10,18/Aug/19 12:53,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Table SQL / Runtime,,,,,0,pull-request-available,test-stability,,," 

 
{code:java}
Test set: org.apache.flink.table.dataformat.BinaryRowTest
-------------------------------------------------------------------------------
Tests run: 26, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.328 s <<< FAILURE! - in org.apache.flink.table.dataformat.BinaryRowTest
testWriteString(org.apache.flink.table.dataformat.BinaryRowTest)  Time elapsed: 0.05 s  <<< FAILURE!
org.junit.ComparisonFailure: expected:<[<E5><95><A6><E5><95><A6><E5><95><A6><E5><95><A6><E5><95><A6><E6><88><91><E6><98><AF>
<E5><BF><AB><E4><B9><90><E7><9A><84><E7><B2><89><E5><88><B7><E5><8C><A0>]> but was:<[?????????????]>
        at org.apache.flink.table.dataformat.BinaryRowTest.testWriteString(BinaryRowTest.java:189)
{code}
 

This error happens on a Google Cloud n2-standard-16 (16 vCPUs, 64 GB memory) machine.

{code}$ lsb_release -a
No LSB modules are available.
Distributor ID:    Debian
Description:    Debian GNU/Linux 9.9 (stretch)
Release:    9.9
Codename:    stretch{code}
"," 

 ",jark,lzljs3620320,rmetzger,,,,,,,,,,,"JingsongLi commented on pull request #9455: [FLINK-13739][table-blink] JDK String to bytes should specify UTF-8 encoding
URL: https://github.com/apache/flink/pull/9455
 
 
   
   ## What is the purpose of the change
   
   The reason should be that the default encoding of the machine is not UTF-8, and the test relies on the default encoding.
   The code should use the specified UTF-8 encoding
   
   ## Verifying this change
   
   This change is already covered by existing tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector:no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 15:42;githubbot;600","asfgit commented on pull request #9455: [FLINK-13739][table-blink] JDK String to bytes should specify UTF-8 encoding
URL: https://github.com/apache/flink/pull/9455
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/19 12:43;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 18 12:53:39 UTC 2019,,,,,,,,,,"0|z05pc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/19 15:31;lzljs3620320;The reason should be that the default encoding of the machine is not UTF-8, and the test relies on the default encoding.

The code should use the specified UTF-8 encoding.

[~rmetzger] Can you assign to me?;;;","15/Aug/19 15:40;rmetzger;Indeed, the machine is using {{US-ASCII}} as the default charset.;;;","18/Aug/19 12:53;jark;Fixed in master: 130f4e85a0dc7498e67e591155aa0b0470b5950d
Fixed in 1.9.1: 0a378bbd04b029ca269b3137f565d02eacdc962e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NegativeArraySizeException in LongHybridHashTable,FLINK-13738,13251036,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,rmetzger,rmetzger,15/Aug/19 15:05,27/Aug/19 04:16,13/Jul/23 08:10,18/Aug/19 10:30,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"Executing this (meaningless) query:
{code:java}
INSERT INTO sinkTable ( SELECT CONCAT( CAST( id AS VARCHAR), CAST( COUNT(*) AS VARCHAR)) as something, 'const' FROM CsvTable, table1  WHERE sometxt LIKE 'a%' AND id = key GROUP BY id ) {code}
leads to the following exception:
{code:java}
Caused by: java.lang.NegativeArraySizeException
 at org.apache.flink.table.runtime.hashtable.LongHybridHashTable.tryDenseMode(LongHybridHashTable.java:216)
 at org.apache.flink.table.runtime.hashtable.LongHybridHashTable.endBuild(LongHybridHashTable.java:105)
 at LongHashJoinOperator$36.endInput1$(Unknown Source)
 at LongHashJoinOperator$36.endInput(Unknown Source)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain.endInput(OperatorChain.java:256)
 at org.apache.flink.streaming.runtime.io.StreamTwoInputSelectableProcessor.checkFinished(StreamTwoInputSelectableProcessor.java:359)
 at org.apache.flink.streaming.runtime.io.StreamTwoInputSelectableProcessor.processInput(StreamTwoInputSelectableProcessor.java:193)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.performDefaultAction(StreamTask.java:276)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:298)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:403)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:687)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:517)
 at java.lang.Thread.run(Thread.java:748){code}
This is the plan:

 
{code:java}
== Abstract Syntax Tree ==
LogicalSink(name=[sinkTable], fields=[f0, f1])
+- LogicalProject(something=[CONCAT(CAST($0):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", CAST($1):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL)], EXPR$1=[_UTF-16LE'const'])
   +- LogicalAggregate(group=[{0}], agg#0=[COUNT()])
      +- LogicalProject(id=[$1])
         +- LogicalFilter(condition=[AND(LIKE($0, _UTF-16LE'a%'), =($1, CAST($2):BIGINT))])
            +- LogicalJoin(condition=[true], joinType=[inner])
               :- LogicalTableScan(table=[[default_catalog, default_database, CsvTable, source: [CsvTableSource(read fields: sometxt, id)]]])
               +- LogicalTableScan(table=[[default_catalog, default_database, table1, source: [GeneratorTableSource(key, rowtime, payload)]]])

== Optimized Logical Plan ==
Sink(name=[sinkTable], fields=[f0, f1]): rowcount = 1498810.6659336376, cumulative cost = {4.459964319978008E8 rows, 1.879799762133187E10 cpu, 4.8E9 io, 8.4E8 network, 1.799524266373455E8 memory}
+- Calc(select=[CONCAT(CAST(id), CAST($f1)) AS something, _UTF-16LE'const' AS EXPR$1]): rowcount = 1498810.6659336376, cumulative cost = {4.444976213318672E8 rows, 1.8796498810665936E10 cpu, 4.8E9 io, 8.4E8 network, 1.799524266373455E8 memory}
   +- HashAggregate(isMerge=[false], groupBy=[id], select=[id, COUNT(*) AS $f1]): rowcount = 1498810.6659336376, cumulative cost = {4.429988106659336E8 rows, 1.8795E10 cpu, 4.8E9 io, 8.4E8 network, 1.799524266373455E8 memory}
      +- Calc(select=[id]): rowcount = 1.575E7, cumulative cost = {4.415E8 rows, 1.848E10 cpu, 4.8E9 io, 8.4E8 network, 1.2E8 memory}
         +- HashJoin(joinType=[InnerJoin], where=[=(id, key0)], select=[id, key0], build=[left]): rowcount = 1.575E7, cumulative cost = {4.2575E8 rows, 1.848E10 cpu, 4.8E9 io, 8.4E8 network, 1.2E8 memory}
            :- Exchange(distribution=[hash[id]]): rowcount = 5000000.0, cumulative cost = {1.1E8 rows, 8.4E8 cpu, 2.0E9 io, 4.0E7 network, 0.0 memory}
            :  +- Calc(select=[id], where=[LIKE(sometxt, _UTF-16LE'a%')]): rowcount = 5000000.0, cumulative cost = {1.05E8 rows, 0.0 cpu, 2.0E9 io, 0.0 network, 0.0 memory}
            :     +- TableSourceScan(table=[[default_catalog, default_database, CsvTable, source: [CsvTableSource(read fields: sometxt, id)]]], fields=[sometxt, id]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 0.0 cpu, 2.0E9 io, 0.0 network, 0.0 memory}
            +- Exchange(distribution=[hash[key0]]): rowcount = 1.0E8, cumulative cost = {3.0E8 rows, 1.68E10 cpu, 2.8E9 io, 8.0E8 network, 0.0 memory}
               +- Calc(select=[CAST(key) AS key0]): rowcount = 1.0E8, cumulative cost = {2.0E8 rows, 0.0 cpu, 2.8E9 io, 0.0 network, 0.0 memory}
                  +- TableSourceScan(table=[[default_catalog, default_database, table1, source: [GeneratorTableSource(key, rowtime, payload)]]], fields=[key, rowtime, payload]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 0.0 cpu, 2.8E9 io, 0.0 network, 0.0 memory}

== Physical Execution Plan ==
Stage 1 : Data Source
	content : collect elements with CollectionInputFormat

	Stage 2 : Operator
		content : CsvTableSource(read fields: sometxt, id)
		ship_strategy : REBALANCE

		Stage 3 : Operator
			content : SourceConversion(table=[default_catalog.default_database.CsvTable, source: [CsvTableSource(read fields: sometxt, id)]], fields=[sometxt, id])
			ship_strategy : FORWARD

			Stage 4 : Operator
				content : Calc(select=[id], where=[(sometxt LIKE _UTF-16LE'a%')])
				ship_strategy : FORWARD

Stage 6 : Data Source
	content : collect elements with CollectionInputFormat

	Stage 7 : Operator
		content : SourceConversion(table=[default_catalog.default_database.table1, source: [GeneratorTableSource(key, rowtime, payload)]], fields=[key, rowtime, payload])
		ship_strategy : FORWARD

		Stage 8 : Operator
			content : Calc(select=[CAST(key) AS key0])
			ship_strategy : FORWARD

			Stage 10 : Operator
				content : HashJoin(joinType=[InnerJoin], where=[(id = key0)], select=[id, key0], build=[left])
				ship_strategy : HASH[id]

				Stage 11 : Operator
					content : Calc(select=[id])
					ship_strategy : FORWARD

					Stage 12 : Operator
						content : HashAggregate(isMerge=[false], groupBy=[id], select=[id, COUNT(*) AS $f1])
						ship_strategy : FORWARD

						Stage 13 : Operator
							content : Calc(select=[(CAST(id) CONCAT CAST($f1)) AS something, _UTF-16LE'const' AS EXPR$1])
							ship_strategy : FORWARD

							Stage 14 : Operator
								content : SinkConversionToRow
								ship_strategy : FORWARD

								Stage 15 : Operator
									content : Map
									ship_strategy : FORWARD

									Stage 16 : Data Sink
										content : Sink: CsvTableSink(f0, f1)
										ship_strategy : FORWARD


{code}
 ",,jark,lzljs3620320,rmetzger,,,,,,,,,,,"JingsongLi commented on pull request #9462: [FLINK-13738][blink-table-planner] Fix NegativeArraySizeException in LongHybridHashTable
URL: https://github.com/apache/flink/pull/9462
 
 
   
   ## What is the purpose of the change
   
   The dense range is computed by (maxKey - minKey + 1) in LongHybridHashTable.
   it may be negative. it mean the range is bigger than Long.MaxValue when range is negative...
   If range is zero, maybe the max is Long.Max, and the min is Long.Min, so we should not use dense mode too.
   
   ## Verifying this change
   
   JoinITCase.testLongJoinWithBigRange
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? yes
     - If yes, how is the feature documented? JavaDocs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 09:18;githubbot;600","asfgit commented on pull request #9462: [FLINK-13738][blink-table-planner] Fix NegativeArraySizeException in LongHybridHashTable
URL: https://github.com/apache/flink/pull/9462
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/19 10:24;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 18 10:30:03 UTC 2019,,,,,,,,,,"0|z05pbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/19 03:22;jark;cc [~lzljs3620320] [~TsReaper];;;","16/Aug/19 09:05;lzljs3620320;Ah... Thanks [~rmetzger] for find this bug.

The range is computed by (maxKey - minKey + 1).

it may be negative. it mean the range is bigger than Long.MaxValue when range is negative...

If range is zero, maybe the max is Long.Max, and the min is Long.Min, so we should not use dense mode too.

I'll fix it.;;;","18/Aug/19 10:30;jark;Fixed in master: b61ea354eb63ff9a09865d4370f4ef67344f4420
Fixed in 1.9.1: fd07573aaf79090fa85ef48ae8fa10d823915f90;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-dist should add provided dependency on flink-examples-table,FLINK-13737,13251018,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,15/Aug/19 13:27,27/Aug/19 04:14,13/Jul/23 08:10,16/Aug/19 15:53,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Examples,,,,,0,pull-request-available,,,,"In FLINK-13558 we changed the `flink-dist/bin.xml` to also include flink-examples-table in the binary distribution. The flink-dist module though does not depend on the flink-examples-table.

If only the flink-dist module is built with its dependencies (this happens in the release scripts). The table examples are not built and thus not included in the distribution",,dwysakowicz,jark,,,,,,,,,,,,"dawidwys commented on pull request #9465: [FLINK-13737][flink-dist] Added examples-table to flink-dist dependencies
URL: https://github.com/apache/flink/pull/9465
 
 
   ## What is the purpose of the change
   
   This PR adds examples-table to flink-dist dependencies. In https://issues.apache.org/jira/browse/FLINK-13558 we added table examples to the distribution package, but forgot to add it to the build dependencies of flink-dist.
   
   
   ## Verifying this change
   
   Run
   
   ```
   mvn clean install -pl flink-dist -am
   ```
   
   and see that the distribution contain table examples.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 14:18;githubbot;600","dawidwys commented on pull request #9466: [FLINK-13737][flink-dist] Added examples-table to flink-dist dependencies
URL: https://github.com/apache/flink/pull/9466
 
 
   ## What is the purpose of the change
   
   This PR adds examples-table to flink-dist dependencies. In https://issues.apache.org/jira/browse/FLINK-13558 we added table examples to the distribution package, but forgot to add it to the build dependencies of flink-dist.
   
   
   ## Verifying this change
   
   Run
   
   ```
   mvn clean install -pl flink-dist -am
   ```
   
   and see that the distribution contain table examples.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 14:19;githubbot;600","dawidwys commented on pull request #9465: [FLINK-13737][flink-dist][bp-1.9] Added examples-table to flink-dist dependencies
URL: https://github.com/apache/flink/pull/9465
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 15:49;githubbot;600","dawidwys commented on pull request #9466: [FLINK-13737][flink-dist] Added examples-table to flink-dist dependencies
URL: https://github.com/apache/flink/pull/9466
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 15:52;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 04:14:51 UTC 2019,,,,,,,,,,"0|z05p7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/19 15:53;dwysakowicz;Fixed:
master: 95e3f88f08b652d01583410f9a8bb73b0d891602
1.9: e598d76418d438bcca9e7b8c50e6adc82347e561;;;","27/Aug/19 04:14;ykt836;I've updated the fix version to 1.9.1 since this is not included in 1.9.0 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaInternalProducerITCase.testHappyPath fails on Travis,FLINK-13733,13250952,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,trohrmann,trohrmann,15/Aug/19 06:46,11/Nov/20 06:00,13/Jul/23 08:10,11/Nov/20 06:00,1.10.0,1.11.0,1.12.0,1.9.0,,,,,,1.11.3,1.12.0,,,Connectors / Kafka,Tests,,,,0,pull-request-available,test-stability,,,"The {{FlinkKafkaInternalProducerITCase.testHappyPath}} fails on Travis with 

{code}
Test testHappyPath(org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase) failed with:
java.util.NoSuchElementException
	at org.apache.kafka.common.utils.AbstractIterator.next(AbstractIterator.java:52)
	at org.apache.flink.shaded.guava18.com.google.common.collect.Iterators.getOnlyElement(Iterators.java:302)
	at org.apache.flink.shaded.guava18.com.google.common.collect.Iterables.getOnlyElement(Iterables.java:289)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase.assertRecord(FlinkKafkaInternalProducerITCase.java:169)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase.testHappyPath(FlinkKafkaInternalProducerITCase.java:70)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
{code}

https://api.travis-ci.org/v3/job/571870358/log.txt",,becket_qin,dian.fu,renqs,rmetzger,sewen,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17851,,,,,,,,,,,,,,,,,,,,,"22/Apr/20 06:09;rmetzger;20200421.13.tar.gz;https://issues.apache.org/jira/secure/attachment/13000792/20200421.13.tar.gz",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 11 05:59:09 UTC 2020,,,,,,,,,,"0|z05osw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/19 07:53;becket_qin;This test itself is quite simple. It might fail because a particular message send to Kafka was unsuccessful and we did not check the status, or the record fetching took longer than expected. This could happen when the testing machine is super busy or experienced a long GC.

In term of the fix, it might make sense to do the following:
 # ensure each step was done successfully, e.g. a message send callback fired successfully.
 # Extend the timeout a little bit, e.g. give it 30 seconds to fetch a record rather than 10 seconds.;;;","24/Dec/19 09:06;becket_qin;It looks that this issue only happened once. I'll close the ticket for now. If this happen again we can enhance the message delivery guarantee.;;;","22/Apr/20 06:09;rmetzger;Another instance: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7851&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20

Attached logs to ticket.;;;","22/May/20 14:58;trohrmann;The issue seems to have reoccurred. [~rmetzger] please re-open closed issues if the problem seems to still exist.;;;","24/May/20 02:04;becket_qin;I am not completely sure what caused the problem. One guess is that the consumer was not able to connect to the broker to begin with. If so, this is more of a connectivity issue of the environment. I am adding some debugging code here to give more information in case this happen again.;;;","10/Jun/20 12:19;trohrmann;Another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3018&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20. This time the {{FlinkKafkaProducerITCase.testHappyPath}} failed but with a very similar exception message.;;;","11/Jun/20 18:43;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3298&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20;;;","16/Jun/20 17:51;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3577&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8;;;","19/Jun/20 06:50;pnowojski;Another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3794&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=684b1416-4c17-504e-d5ab-97ee44e08a20;;;","07/Sep/20 03:03;dian.fu;Another instance on master: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6256&view=logs&j=1fc6e7bf-633c-5081-c32a-9dea24b05730&t=80a658d1-f7f6-5d93-2758-53ac19fd5b19];;;","21/Sep/20 01:51;renqs;Hello everyone, I ran this test case locally with TRACE level log. Records are indeed produced to and logged by Kafka, but the consumer was just closed after it sends consume request and before getting response from the broker. I think we can extend the timeout value of kafkaConsumer.poll() method in assertRecord a little bit. I'll create a PR for this. ;;;","24/Sep/20 02:04;dian.fu;Another instance on 1.11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6871&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=eb5f4d19-2d2d-5856-a4ce-acf5f904a994;;;","03/Nov/20 17:12;sewen;Another instance: https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/168/logs/82;;;","04/Nov/20 01:45;dian.fu;Another instance on 1.11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8925&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=19faa67c-fc43-55af-ba4e-7519fca274b5;;;","10/Nov/20 02:04;dian.fu;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9362&view=logs&j=72d4811f-9f0d-5fd0-014a-0bc26b72b642&t=19faa67c-fc43-55af-ba4e-7519fca274b5;;;","10/Nov/20 06:26;rmetzger;[~renqs] what's the status of the PR increasing the timeout?;;;","10/Nov/20 07:16;renqs;[~rmetzger] Sorry for my late response! I'll create a PR to fix this now. Thanks for your reminder~ ;;;","11/Nov/20 05:59;becket_qin;The patch has been merged to master.
cb2d137adc9ea1e46a67513c1e0f2469bb05bff4

 I am closing the ticket. Hopefully we don't see the unstability again.;;;",,,,,,,,,,,,,,
Fix wrong closing tag order in sidenav,FLINK-13728,13250878,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,14/Aug/19 21:30,14/Nov/19 15:58,13/Jul/23 08:10,26/Aug/19 11:17,1.10.0,1.8.1,1.9.0,,,,,,,1.10.0,1.8.3,1.9.1,,Documentation,,,,,0,pull-request-available,,,,The order of closing HTML tags in the sidenav is wrong: instead of {{</li></ul></div>}} it should be {{</ul></div></li>}},,nkruber,sewen,,,,,,,,,,,,"NicoK commented on pull request #9439: [FLINK-13728][docs] fix wrong closing tag order in sidenav
URL: https://github.com/apache/flink/pull/9439
 
 
   ## What is the purpose of the change
   
   The side navigation was closing some tags incorrectly as `</li></ul></div>` which should rather be `</ul></div></li>`.
   
   ## Brief change log
   
   - fix tag order as proposed
   
   ## Verifying this change
   
   This change is a trivial rework which I checked in the generated HTML pages.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 21:45;githubbot;600","asfgit commented on pull request #9439: [FLINK-13728][docs] fix wrong closing tag order in sidenav
URL: https://github.com/apache/flink/pull/9439
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/19 08:24;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 14 15:58:27 UTC 2019,,,,,,,,,,"0|z05ocg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/19 11:17;sewen;Fixed via b1c2e213302cd68758761c60a1ccff85c5c67203;;;","26/Aug/19 11:25;sewen;Fixed in 1.9.1 via 9ba0a8906e24fa864a89df65edbc95c25ec3f6dd;;;","14/Nov/19 15:58;nkruber;Fixed in 1.8.3 via bd6b2e2eb527392e7b6100089fd83c212e976705;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive array values not properly displayed in SQL CLI,FLINK-13711,13250748,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,14/Aug/19 10:11,21/Aug/19 20:43,13/Jul/23 08:10,21/Aug/19 20:43,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Table SQL / Client,,,,,0,pull-request-available,,,,"Array values are displayed like:
{noformat}
 [Ljava.lang.Integer;@632~
 [Ljava.lang.Integer;@6de~
{noformat}",,lirui,phoenixjiangnan,xuefuz,,,,,,,,,,,"lirui-apache commented on pull request #9450: [FLINK-13711][sql-client] Hive array values not properly displayed in…
URL: https://github.com/apache/flink/pull/9450
 
 
   … SQL CLI
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix the issue that SQL CLI doesn't display arrays properly.
   
   
   ## Brief change log
   
     - Properly handle arrays when converting a Row to String.
     - Add test case.
   
   
   ## Verifying this change
   
   New test case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 07:52;githubbot;600","asfgit commented on pull request #9450: [FLINK-13711][sql-client] Hive array values not properly displayed in…
URL: https://github.com/apache/flink/pull/9450
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 20:29;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 21 20:43:34 UTC 2019,,,,,,,,,,"0|z05njk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/19 20:43;phoenixjiangnan;merged in master: 637bc062bb979bb5b143cdfcb83c782d09c030ce  1.9.1: b58fa92f0ade9475e39b59c58540676748da5b3a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transformations should be cleared because a table environment could execute multiple job,FLINK-13708,13250714,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,godfreyhe,godfreyhe,14/Aug/19 07:01,24/Jan/20 01:11,13/Jul/23 08:10,25/Nov/19 02:44,,,,,,,,,,1.10.0,1.9.2,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"currently, if a table environment execute more than one sql jobs, the following job contains transformations about the previous job. the reason is the transformations is not cleared after execution",,godfreyhe,jark,leonard,lirui,swhelan,trohrmann,txhsj,zhuzh,zjffdu,,,,,"godfreyhe commented on pull request #9433: [FLINK-13708] [table-planner-blink] transformations should be cleared after execution in blink planner
URL: https://github.com/apache/flink/pull/9433
 
 
   
   ## What is the purpose of the change
   
   *transformations should be cleared after execution in blink planner*
   
   
   ## Brief change log
   
     - *clear transformations in ExecutorBase*
   
   
   ## Verifying this change
   
   
   
   This change added tests and can be verified as follows:
   
     - *Added testExecuteTwiceUsingSameTableEnv to verify this bug*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 07:07;githubbot;600","godfreyhe commented on pull request #10290: [FLINK-13708] [table-planner-blink] transformations should be cleared after execution in blink planner
URL: https://github.com/apache/flink/pull/10290
 
 
   ## What is the purpose of the change
   
   *transformations should be cleared after execution in blink planner, cherry pick https://github.com/apache/flink/pull/9433 to release-1.9*
   
   
   ## Brief change log
   
     - *clear transformations in ExecutorBase*
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added testExecuteTwiceUsingSameTableEnv to verify this bug*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Nov/19 09:10;githubbot;600","wuchong commented on pull request #9433: [FLINK-13708] [table-planner-blink] transformations should be cleared after execution in blink planner
URL: https://github.com/apache/flink/pull/9433
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Nov/19 09:21;githubbot;600","wuchong commented on pull request #10290: [FLINK-13708] [table-planner-blink] transformations should be cleared after execution in blink planner
URL: https://github.com/apache/flink/pull/10290
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Nov/19 02:43;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,FLINK-14181,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 25 02:44:08 UTC 2019,,,,,,,,,,"0|z05nc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/19 10:26;trohrmann;Please remove the fixes introduced with fb7beb880ed49538850e3ad740f7423282003243 once this issue has been fixed [~godfreyhe].;;;","22/Nov/19 09:14;jark;Fixed in 1.10.0:

[FLINK-13708][table-planner-blink] Revert changes in fb7beb88 which is a temporary fix
 - f01cdd1f2c676d620c834e2d3acc72764a8260c1

[FLINK-13708][table-planner-blink] Transformations should be cleared after execution in blink planner
 - 23c9b5ac50d04d28a34a87c78eb2d3331c06b74b;;;","25/Nov/19 02:44;jark;FLINK-13708[table-planner-blink] Transformations should be cleared after execution in blink planner

 - Fixed in 1.9.2: 2dd3ba8335d35de7ed59073def4f0093326f1464;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken links in Hive documentation,FLINK-13705,13250628,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sjwiesman,trohrmann,trohrmann,13/Aug/19 18:36,14/Aug/19 21:29,13/Jul/23 08:10,14/Aug/19 21:29,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,Connectors / Hive,,,,,0,pull-request-available,test-stability,,,"Flink's Hive documentation contains broken links which makes the documentation links check end-to-end test fail on Travis:

https://api.travis-ci.org/v3/job/571237828/log.txt

{code}
[2019-08-13 13:32:21] ERROR `/dev/table/catalog.html' not found.
[2019-08-13 13:32:21] ERROR `/zh/dev/table/config.html' not found.
[2019-08-13 13:32:25] ERROR `/dev/table/hive_integration_example.html' not found.
[2019-08-13 13:32:25] ERROR `/dev/table/hive_integration.html' not found.
http://localhost:4000/dev/table/catalog.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/table/config.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/table/hive_integration_example.html:
Remote file does not exist -- broken link!!!
http://localhost:4000/dev/table/hive_integration.html:
Remote file does not exist -- broken link!!!
{code}",,phoenixjiangnan,trohrmann,,,,,,,,,,,,"asfgit commented on pull request #9435: [FLINK-13705][docs] Broken links in Hive documentation
URL: https://github.com/apache/flink/pull/9435
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 21:25;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,FLINK-13517,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 14 21:29:40 UTC 2019,,,,,,,,,,"0|z05mt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/19 18:37;trohrmann;[~phoenixjiangnan] could you fix this issue. Thanks a lot.;;;","13/Aug/19 21:38;phoenixjiangnan;[~sjwiesman] can you take a look? I can help to merge the PR;;;","14/Aug/19 21:29;phoenixjiangnan;merged in master: e5dd9bcd71201d50d68c6932d438e6e196ea466e 1.9.0: c0502ab261c558249de43808ba262fbd4fddc517;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPC-H end-to-end test (Blink planner) fails on Travis,FLINK-13704,13250541,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,trohrmann,trohrmann,13/Aug/19 12:53,15/Aug/19 02:10,13/Jul/23 08:10,14/Aug/19 04:35,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,Table SQL / Runtime,,,,,0,pull-request-available,test-stability,,,"The {{TPC-H}} end-to-end test fails on Travis with the following problem:

{code}
Running query #22...
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Could not submit given SQL update statement to cluster.
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:129)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:105)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:194)
[FAIL] Test script contains errors.
Checking for errors...

org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL update statement.
	at org.apache.flink.table.client.gateway.local.LocalExecutor.applyUpdate(LocalExecutor.java:539)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdateInternal(LocalExecutor.java:432)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdate(LocalExecutor.java:367)
	at org.apache.flink.table.client.cli.CliClient.callInsertInto(CliClient.java:496)
	at org.apache.flink.table.client.cli.CliClient.lambda$submitUpdate$0(CliClient.java:231)
	at java.util.Optional.map(Optional.java:215)
	at org.apache.flink.table.client.cli.CliClient.submitUpdate(CliClient.java:228)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:127)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:105)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:194)
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. From line 13, column 10 to line 13, column 30: No match found for function signature substr(<CHARACTER>, <NUMERIC>, <NUMERIC>)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:125)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:82)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:154)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:89)
	at org.apache.flink.table.planner.delegation.PlannerBase.parse(PlannerBase.scala:130)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:335)
	at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.sqlUpdate(StreamTableEnvironmentImpl.java:299)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$applyUpdate$12(LocalExecutor.java:531)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:216)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.applyUpdate(LocalExecutor.java:529)
	... 9 more
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 13, column 10 to line 13, column 30: No match found for function signature substr(<CHARACTER>, <NUMERIC>, <NUMERIC>)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:809)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4807)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1762)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:273)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:215)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5566)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5553)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1680)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1665)
	at org.apache.calcite.sql.type.InferTypes.lambda$static$0(InferTypes.java:46)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1854)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1862)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1862)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereOrOn(SqlValidatorImpl.java:4006)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereClause(SqlValidatorImpl.java:3998)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3368)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:957)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3111)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3093)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3365)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:957)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:216)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:932)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:639)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:122)
	... 18 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No match found for function signature substr(<CHARACTER>, <NUMERIC>, <NUMERIC>)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:463)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:572)
	... 51 more
{code}

https://api.travis-ci.org/v3/job/570757857/log.txt
https://api.travis-ci.org/v3/job/570757863/log.txt
https://api.travis-ci.org/v3/job/570757869/log.txt",,jark,trohrmann,,,,,,,,,,,,"JingsongLi commented on pull request #9427: [FLINK-13704][table-planner-blink] Re-add substr support in blink-planner to fix tpc-h e2e test failure
URL: https://github.com/apache/flink/pull/9427
 
 
   
   ## What is the purpose of the change
   
   substr is a common function, HIVE/SPARK/MYSQL/ORACLE/SQSERVER all support it. We should not delete it.
   And tpc-h will generate the sql that contains substr. We should re-add it to fix e2e test failure.
   
   ## Brief change log
   
   Add substr in FlinkSqlOperatorTable, and add tests.
   
   ## Verifying this change
   
   ScalarFunctionsTest and SqlExpressionTest.
   run the TPC-H E2E test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/19 14:41;githubbot;600","asfgit commented on pull request #9427: [FLINK-13704][table-planner-blink] Re-add substr support in blink-planner to fix tpc-h e2e test failure
URL: https://github.com/apache/flink/pull/9427
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 04:35;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-13547,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 15 02:10:30 UTC 2019,,,,,,,,,,"0|z05m9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/19 12:59;trohrmann;More instances from the master cron job:

https://api.travis-ci.org/v3/job/570793224/log.txt
https://api.travis-ci.org/v3/job/570793230/log.txt
https://api.travis-ci.org/v3/job/570793236/log.txt;;;","13/Aug/19 18:30;trohrmann;How critical is this for the release [~jark] and [~ykt836]?;;;","14/Aug/19 02:10;jark;Hi [~till.rohrmann], I'm sorry for the issue. We didn't check the tpc-h e2e locally when merging FLINK-13547. The reason of this issue is that tpch 22.q uses {{substr}} function which we just removed in FLINK-13547. We removed {{substr}} because we thought it can be covered by existing {{substring}} function, and don't want to introduce too much functions at once. {{substr}} is a synonyms for {{substring}}, we only need to add back the function definition and reuse the implementation of {{substring}}.

So, this issue is only about we need a new {{substr}} function. It won't break any features we already provided. 
The only concern is the sentence ""full TPC-H coverage for blink batch planner"" in the release note might be questioned. Because it failed in RC2...;;;","14/Aug/19 04:35;jark;Fixed in master: 5a5966ba273b9ceb4ca2a87c77d3c5948e7717cc
Fixed in 1.9.1: 09a6aa96909c1e1f0b618d525a26dae8b69817cb;;;","14/Aug/19 09:01;trohrmann;I'd be ok with documenting this as a limitation. [~jark] could you please update FLINK-13547 with a release note saying that {{substr}} is not supported in 1.9.0. That way we include this note in the release notes which still needs to be created for the 1.9 release.;;;","15/Aug/19 02:10;jark;Oh, I see that you have updated the release note in FLINK-13547. Thanks. [~till.rohrmann];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
BaseMapSerializerTest.testDuplicate fails on Travis,FLINK-13702,13250499,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dwysakowicz,trohrmann,trohrmann,13/Aug/19 10:09,09/May/20 01:45,13/Jul/23 08:10,07/Nov/19 10:20,1.10.0,,,,,,,,,1.10.0,1.9.2,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,"The {{BaseMapSerializerTest.testDuplicate}} fails on Travis with an {{java.lang.IndexOutOfBoundsException}}.

https://api.travis-ci.org/v3/job/570973199/log.txt",,dwysakowicz,jark,klion26,libenchao,lzljs3620320,ram_krish,trohrmann,wind_ljy,,,,,,"dawidwys commented on pull request #10068: [FLINK-13702][FLINK-13740] Fixed issues with generic types and materialization of lazy binary formats in blink planner
URL: https://github.com/apache/flink/pull/10068
 
 
   ## What is the purpose of the change
   This PR fixes a couple of issues related to serialization stack in blink table planner:
   * fixes the digest of GenericRelDataType, before the digest did not take into account generic parameters of TypeInformation. Therefore e.g. `ANY(TypeInformation<Map<Integer, String>>)` and `ANY(TypeInformation<Map<Row, Int>>)` were equal.
   * fixes the way `NullSerializer`, `NullAwareMapSerializer` and `BinaryGenericSerializer` take snapshots. 
   * fixes the type of accumulator of `CollectAggFunction`, before the function was using a `GenericTypeInfo` instead of `PojoTypeInfo`
   * fixes race conditions when calling `LazyBinaryFormat#materialize`
   * fixes `BinaryGenericSerializer#duplicate` method
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (**yes** / no / don't know)
     - The runtime per-record code paths (performance sensitive): (**yes** / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Nov/19 14:21;githubbot;600","dawidwys commented on pull request #10068: [FLINK-13702][FLINK-13740] Fixed issues with generic types and materialization of lazy binary formats in blink planner
URL: https://github.com/apache/flink/pull/10068
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 07:50;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-14430,,,,,,,FLINK-14430,FLINK-16242,,FLINK-13740,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 23 10:11:57 UTC 2020,,,,,,,,,,"0|z05m0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/19 02:23;jark;Could you have a look [~lzljs3620320]? It might because of {{BaseMapSerializer}} is not thread safe.;;;","14/Aug/19 08:08;lzljs3620320;Hi [~jark], TypeSerializer can be not thread safe. CC: [~TsReaper];;;","30/Oct/19 10:29;dwysakowicz;I had a look at this issue and I think the problem is not in the {{BaseMapSerializer}} but in the {{LazyBinaryFormat}} class.

The method {{LazyBinaryFormat#ensureMaterialized}} is not thread safe. Therefore e.g. when two threads call {{getSizeInBytes}}, the first one starts materializing the {{BinaryString}} and sets the {{segments}} first, but before it updates the {{sizeInBytes}} field the second thread calls {{getSizeInBytes}} and assumes it has already been materialized (segments were set) so it just returns the sizeInBytes value which is equal to {{-1}} as it was not yet updated.

A potential solution to the problem would be to synchronize the {{LazyBinaryFormat#ensureMaterialized}} method. Unfortunately, it has a potential performance penalty. Do you think the {{BinaryFormat}} classes should be thread safe? I think they should. Or were they supposed to work only in a single threaded context? If it is the latter we need to update the {{BaseMapSerializerTest.testDuplicate}} to perform a copy of the input item before serializing it (in {{SerializerTestBase.SerializerRunner#run}}).

[~tzulitai] WDYT about the changes to the {{SerializerTestBase}}? Does it make sense to add the copy anyways?

WDYT? [~lzljs3620320] [~jark] ;;;","30/Oct/19 11:04;lzljs3620320;Thanks [~dwysakowicz], nice analysis, you are right.

{{BinaryFormat}} classes are not thread safe now. Like BinaryRow, it is a mutable class, mutable class is difficult to be thread safe. So at the beginning, the design was not thread safe.

I think there is another solution: we can invoke BinaryString.ensureMaterialized to make BinaryString read thread safe (Or consider adding a fromStringAndMaterialize method to BinaryString) in BaseMapSerializerTest.getTestData.;;;","30/Oct/19 11:13;dwysakowicz;What do you mean by ??invoke BinaryString.ensureMaterialized to make BinaryString read thread safe??.

Actually I also spotted [FLINK-13740]. After a bit longer analysis I think adding a {{copy}} in the test is not enough. I am afraid the {{BinaryLazyFormat}} is really flawed in the current state and effectively unusable. I think we should aim to fix it in 1.10.;;;","30/Oct/19 11:24;lzljs3620320;BinaryLazyFormat is designed for single threaded context.

If we add code like this:
{code:java}
@Override
protected BaseMap[] getTestData() {
   Map<Object, Object> first = new HashMap<>();
   BinaryString empty = BinaryString.fromString("""");
   empty.ensureMaterialized();
   first.put(1, empty);
   return new BaseMap[] {
         new GenericMap(first)
   };
}
{code}
This method generate GenericMap with BinaryString, the fields of this empty BinaryString are all be inited. All its methods(except pointTo method that re-assign fields) should be thread safe. So the remaining test should be safe.;;;","30/Oct/19 11:30;lzljs3620320;I think this Jira is not production bug, it is a test bug.

FLINK-13740 is another problem, the copy of BinaryGeneric not consider the serializer. My idea is to get it out of the serializer. That will let BinaryGeneric not extend BinaryLazyFormat (The methods are unsuited).

> I think we should aim to fix it in 1.10.

Yes, In particular, FLINK-13740 is a production bug.

 ;;;","30/Oct/19 11:47;jark;Hi [~dwysakowicz], thanks for looking into this. However, I think they will be called by different threads. I think the root cause is the same with FLINK-13740. 

The reason is that the async snapshot thread (e.g. snapshot the heap CopyOnWriteStateMap to disk) and the process element thread are concurrent. The former one will call {{TypeSerializer#serialize()}}, and the latter one will call {{TypeSerializer#copy()}} (when the state entry is in snapshoting)  on the same object. I think that's why {{SerializerTestBase#testDuplicate()}} is added, and that's why {{LazyBinaryFormat#ensureMaterialized}} will be called concurrently. 

So a simple way to fix this is adding synchronize to {{ensureMaterialized}}, but I'm not sure how much the performance impact is. ;;;","30/Oct/19 11:54;jark;Sorry didn't notice the discussion above. 

Hi [~lzljs3620320], I don't think this is a test bug as I said above. And I think BinaryString has the same problem in FLINK-13740.

> invoke BinaryString.ensureMaterialized to make BinaryString read thread safe
This is really hack and fragile. It is highly possible miss the invoking at somewhere. ;;;","30/Oct/19 13:52;dwysakowicz;I agree with [~jark]. If we ever keep the {{BinaryString/Generic}} in state they will be accessed from different threads. That's why those classes should be thread safe.

I agree, as I said in my first message, the easiest fix would be to make the {{ensureMaterialized}} synchronized, but I share the concerns of performance degradation with [~jark].

Honestly I'm wondering right now if it would make sense to get rid of the {{LazyBinaryFormat}} for now and revisit the problem of long udfs chain. I think correctness is more important than performance. This should also have a lower performance cost than synchronizing {{ensureMaterialized}} as it would only affect long udfs chains (the original reason for introducing the {{LazyBinaryFormat}}) without affecting every record path. One very broad idea of how can we solve the problem of multiple conversions between binary and java object, could be to add a reusable serialization/deserialization statements in the code generator if needed. Similar to {{reusableInputUnboxingExprs}}.;;;","31/Oct/19 03:22;lzljs3620320;Thanks [~jark] 's input. It's true that there are multiple threads to read in this new scenario. I think we can talk about this ticket thoroughly and then look at FLINK-13740. Because even adding synchronize can't solve the problem of FLINK-13740, so I maintain my view that FLINK-13740 is another problem.

Hi [~dwysakowicz], I agree correctness is more important than performance. Not only long udfs chain, but also operator chain. Operator chain make thing more complex. We have tried to use CodeGen to solve the chain problem before, but the solution is very complex, and we can't solve the problem in a long time. We have also questioned whether it is worth introducing such a complex solution to solve this problem, which will lead to increased code complexity.

Back to this problem, the essence of this problem is that we now introduce a set of _*thread unsafe lazy initialization*_.

A simple solution is add synchronize to materialize.

I'll consider the second solution:

Consider our binary segments are _*immutable*_ object and _*idempotent*_, The root cause of thread unsafe is that it has three fields: segments, offset, sizeInBytes. So if we introduce a Binary immutable object to represent it, we can do like this:
{code:java}
public void ensureMaterialized() {
   if (binary == null) {
      binary = materialize();
   }
}

public Binary materialize() {
   byte[] bytes = StringUtf8Utils.encodeUTF8(javaObject);
   return new Binary(new MemorySegment[] {MemorySegmentFactory.wrap(bytes)}, 0, bytes.length);
}

public static final class Binary {
   public final MemorySegment[] segments;
   public final int offset;
   public final int sizeInBytes;

   public Binary(MemorySegment[] segments, int offset, int sizeInBytes) {
      this.segments = segments;
      this.offset = offset;
      this.sizeInBytes = sizeInBytes;
   }
}
{code}
In this case, binary is like the hash field of JDK String, which is a _*thread safe lazy initialization*_ field. The cost is just a java object, it is small.

What do you think? Does it address your concerns? ;;;","31/Oct/19 09:44;jark;Thanks for the approach. I think it can solve the problem of BinaryString.

But I would like to figure out a solution for BinaryGeneric before we proceed this approach (to find out a generic solution). Do you have some ideas about BinaryGeneric? I think the root cause is the {{javaObjSer}}.;;;","31/Oct/19 10:15;dwysakowicz;I agree the solution suggested by [~lzljs3620320] should work, it is very fragile though. It is based on the assumption that {{MemorySegments}} are immutable which in general case is not true. You can mutate MemorySegment via put methods. It is also possible that with this solution different code paths might work against different MemorySegments for the same {{LazyBinaryFormat}} instances. It does improve the current situation, I agree. We can add that solution, but we should keep in mind it is basically a hack and add a proper documentation with the assumptions that we do there.

I was also thinking if it would make sense to make {{MemorySegment[] getSegments}} return {{UnmodifiableMemorySegment}}. This would require more changes, as we would have to extract {{MemorySegment}} interface. Still that would not make the {{MemorySegment}} fully immutable as it still gives direct access to underlying memory array via {{getHeapMemory}}.
I would keep mind open for a better solution in a long run though.

As for the BinaryGeneric I think we could:
* remove the {{copy}} method from the class and move it to the {{Serializer}} as [~lzljs3620320] was suggesting
* in the {{materialize}} method call {{duplicate}}. This would ensure that every caller thread uses its own copy of the serializer.
* Moreover we can simply remove the getter for the serializer. There is no need to ever get it. The only place when it is called will actually never be reached. Because the {{if (input.getSegments() == null) {}}} will materialize the segments and they will always be not null. 
* additionally we can think of keeping {{TypeSerializerSnapshot}} instead of {{Serializer} and call {{restoreSerializer}} in the {{materialize}} method. Which should create a new copy of a Serializer. Unfortunately thats not the case for some of the {{Serializer}}s in Blink planner (this should be fixed independently anyway). To be on the safe side we would still call duplicate. What do you think?;;;","31/Oct/19 11:30;jark;I thought about call {{duplicate}} too, but calling {{duplicate}} for every BinaryGeneric {{materialize()}} is a performance penalty (most of the cases the serializer is KryoSerializer). 

Another idea is exposing {{ensureMaterialized(TypeSerializer<T> javaObjectSer)}} to BinaryGeneric, it will use the serializer parameter to materialize. And in {{BinaryGenericSerializer}}, we should call {{generic.ensureMaterialized(ser)}} before access segments of it. This can make sure one thread holds a serializer, and don't need to duplicate for every records. 

Some pseudo code:

{code:java}
// BinaryGeneric

public void ensureMaterialized(TypeSerializer<T> serializer) {
   if (binary == null) {
      binary = materialize(serializer);
   }
}

public Binary materialize(TypeSerializer<T> serializer) {
    byte[] bytes = InstantiationUtil.serializeToByteArray(serializer, javaObject);
   return new Binary(new MemorySegment[] {MemorySegmentFactory.wrap(bytes)}, 0, bytes.length);
}

@Override
public Binary materialize() {
   return materialize(javaObjectSer.duplicate());
}
{code}

What do you think?

;;;","31/Oct/19 11:38;lzljs3620320;Hi [~dwysakowicz] , I think maybe we don't need to make it immutable. Consider org.apache.flink.types.Row and other data formats, they are not immutable, but it doesn't affect their read-only thread safety. In flink, maybe we just want read-only thread safety objects.;;;","31/Oct/19 11:45;lzljs3620320;About BinaryGeneric:

I have considered pass type serializer to BinaryGeneric instead of it contains serializer too. Further more, I think we can push down the methods(like ensureMaterialized, materialize and others) to BinaryString. So BinaryGeneric can have ensureMaterialized(TypeSerializer) and materialize(TypeSerializer).;;;","31/Oct/19 12:20;dwysakowicz;There is one problem with entirely removing the serializer from {{BinaryGeneric }} that we can not implement {{equals/hashCode}} as they require that both objects have the same representation available.;;;","31/Oct/19 12:30;jark;Regarding removing serializer from BinaryGeneric entirely,
If we want to support {{LazyBinaryFormat}} for {{BinaryMap}} and {{BinaryArray}} in the future, considering the BinaryGeneric is nested in BinaryMap, I think we still need the {{materialize()}} method on {{BinaryGeneric}}. Which mean, we can't remove serializer from BinaryGeneric entirely.;;;","31/Oct/19 12:40;lzljs3620320;> There is one problem with entirely removing the serializer from {{BinaryGeneric that we can not implement equals/hashCode}} as they require that both objects have the same representation available.

Now consider JoinedRow, it can not equals and hashcode too, because we don't know what row is below and how to serialize it to be consistent. (the values serialized by GenericRow and BinaryRow are different)

> If we want to support {{LazyBinaryFormat}} for {{BinaryMap}} and {{BinaryArray}} in the future, considering the BinaryGeneric is nested in BinaryMap, I think we still need the {{materialize()}} method on {{BinaryGeneric}}. Which mean, we can't remove serializer from BinaryGeneric entirely.

About the implementation of future BinaryMap, we need serializer to do these things, we have nested row, nested generic, it is necessary to do these conversion by serializer. So I think there is no need to keep {{materialize().}};;;","31/Oct/19 13:12;dwysakowicz;I don't know exactly how the {{JoinedRow}} is used and therefore if the {{equals/hashCode}} is needed. Correct me if I am wrong but if we remove the {{equals/hashCode}} from {{BinaryGeneric}}, doesn't it mean that we can no longer use generic objects in a {{GroupBy}} clause, no? Isn't that a regression?

Edit: Ok I think the answer is that the key selector always uses {{BinaryRow}}, where the {{BinaryGeneric}} is written as bytes. You never compare the objects.

I would be in favor of removing the {{serializer}} from the {{BinaryGeneric}} then.;;;","01/Nov/19 05:20;lzljs3620320;[~dwysakowicz] yes, it depends on our CodeGen or runtime computation.

[~jark] About removing the serializer from the BinaryGeneric, BinaryArray/BinaryMap should never be LazyBinaryFormat, because them have BaseArray to unify GenericArray and BinaryArray, so we don't need introduce lazy format for them.(Consider the methods of BaseArray, it is really hard to support LazyBinaryFormat). So this is not a problem. Is that look good to you?;;;","01/Nov/19 06:35;jark;As discussed with [~lzljs3620320] offline, we will not have lazy BinaryMap and lazy BinaryArray in the future (we already have an abstraction of BaseMap whose subclass could be BinaryMap or GenericMap). 

So I agree with removing the serizlier from BinaryGeneric, then BinaryGeneric shouldn't extends from {{LazyBinaryFormat}} or we should move some methods of {{LazyBinaryFormat}} to {{BinaryString}}.;;;","01/Nov/19 06:56;lzljs3620320;Hi [~dwysakowicz] [~jark], Let me summarize the current conclusions:

For BinaryLazyFormat: the core issue is we need it read-only thread safe on lazy initialization. Consider our binary segments are idempotent, _*we introduce a Binary class to wrap segments and offset and sizeInBytes*_,  to let the lazy field be a single field, In this way, even if different threads initialize different instances, there will be no conflict. It will become a read-only thread-safe lazy initialization.

For BinaryGeneric: *_We need get it rid of serializer, equals and hashcode should throw unsupport exception_.* One implementation way is push down methods to BinaryString from LazyBinaryFormat,  in this way, BinaryString and BinaryGeneric have different methods, methods of BinaryGeneric should have serializer argument.

Correct me if I am wrong.;;;","01/Nov/19 12:24;jark;Sounds good to me.;;;","07/Nov/19 10:20;dwysakowicz;Fixed in 
master: 0b28e830d7366126a91ca9faa38cb19a8f66a9b6
1.9.2: 814b5fd35fd0308f6edeedb8414623a43ba512a7;;;","23/Feb/20 10:11;wind_ljy;[~dwysakowicz] [~jark] [~lzljs3620320]

The error still exists after I cherry-pick the commit in 1.9.2. Could you take a look at FLINK-16242 ?;;;",,,,,,
Fix TableFactory doesn't work with DDL when containing TIMESTAMP/DATE/TIME types,FLINK-13699,13250410,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,jark,jark,13/Aug/19 03:34,20/Aug/19 01:34,13/Jul/23 08:10,19/Aug/19 16:23,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / API,Table SQL / Planner,,,,0,pull-request-available,,,,"Currently, in blink planner, we will convert DDL to {{TableSchema}} with new type system, i.e. DataTypes.TIMESTAMP()/DATE()/TIME() whose underlying TypeInformation are  Types.LOCAL_DATETIME/LOCAL_DATE/LOCAL_TIME. 

However, this makes the existing connector implementations (Kafka, ES, CSV, etc..) don't work because they only accept the old TypeInformations (Types.SQL_TIMESTAMP/SQL_DATE/SQL_TIME).

A simple solution is encode DataTypes.TIMESTAMP() as ""TIMESTAMP"" when translating to properties. And will be converted back to the old TypeInformation: Types.SQL_TIMESTAMP. This would fix all factories at once.
",,jark,trohrmann,tzulitai,,,,,,,,,,,"wuchong commented on pull request #9423: [FLINK-13699][table-api] Fix TableFactory doesn't work with DDL when containing TIMESTAMP/DATE/TIME types
URL: https://github.com/apache/flink/pull/9423
 
 
   
   The solution is encode DataTypes.TIMESTAMP() as ""TIMESTAMP"" when translating to properties.
   And will be converted back to the old TypeInformation: Types.SQL_TIMESTAMP.
   This would fix all factories at once.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently, in blink planner, we will convert DDL to TableSchema with new type system, i.e. DataTypes.TIMESTAMP()/DATE()/TIME() whose underlying TypeInformation are Types.LOCAL_DATETIME/LOCAL_DATE/LOCAL_TIME.
   
   However, this makes the existing connector implementations (Kafka, ES, CSV, etc..) don't work because they only accept the old TypeInformations (Types.SQL_TIMESTAMP/SQL_DATE/SQL_TIME).
   
   A simple solution is encode DataTypes.TIMESTAMP() as ""TIMESTAMP"" when translating to properties. And will be converted back to the old TypeInformation: Types.SQL_TIMESTAMP. This would fix all factories at once.
   
   ## Brief change log
   
   - Convert Types.LOCAL_DATETIME/LOCAL_DATE/LOCAL_TIME to ""TIMESTAMP""/""DATE""/""TIME"" as well.
   
   
   ## Verifying this change
   
   - Add unit test in `TypeStringUtilsTest` to verify the conversion from LocalDataTime/LocalDate/LocalTime.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/19 03:48;githubbot;600","wuchong commented on pull request #9423: [FLINK-13699][table-api] Fix TableFactory doesn't work with DDL when containing TIMESTAMP/DATE/TIME types
URL: https://github.com/apache/flink/pull/9423
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Aug/19 01:34;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 19 23:09:41 UTC 2019,,,,,,,,,,"0|z05lgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/19 16:23;jark;[FLINK-13699][table-api] Fix TableFactory doesn't work with DDL when containing TIMESTAMP/DATE/TIME types
 - master: b837a589f1bda5d8352e9760af39937f9194c670
 - 1.9.1:dca0879d8e992b92cff22b27fb15f598dd2c36d9

[FLINK-13699][hbase] Add integration test for HBase to verify DDL with TIMESTAMP types
 - master: d20175ee62cd9b3ce8912745240b57c88c5af51c
 - 1.9.1: 95ba5408833fa38aba5624be1fa88fee342cdd1b;;;","19/Aug/19 23:09;tzulitai;Cherry picked for 1.9.0: d8941711e51f3315f543399a1030dbcf2fb07434;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Identifiers are not properly handled in some DDLs,FLINK-13693,13250234,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lirui,lirui,12/Aug/19 09:44,29/Apr/21 09:11,13/Jul/23 08:10,29/Apr/21 09:11,,,,,,,,,,,,,,Table SQL / Client,,,,,0,stale-major,,,,"Fully qualified paths and backtick quoted identifiers are not supported in some DDLs while they're supported in queries. For example:
{noformat}
Flink SQL> describe `default`.tbl;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Table '`default`.tbl' was not found.
{noformat}

One solution is to make all DDLs go through the parser in order to get the identifiers right.",,felixzheng,jark,lirui,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 09:11:44 UTC 2021,,,,,,,,,,"0|z05kdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/21 11:46;flink-jira-bot;This major issue is unassigned and itself and all of its Sub-Tasks have not been updated for 30 days. So, it has been labeled ""stale-major"". If this ticket is indeed ""major"", please either assign yourself or give an update. Afterwards, please remove the label. In 7 days the issue will be deprioritized.;;;","29/Apr/21 09:11;jark;This has been fixed in 1.13. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rest High Level Client for Elasticsearch6.x connector leaks threads if no connection could be established,FLINK-13689,13250201,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,melmoth,rishi55,rishi55,12/Aug/19 07:24,07/Aug/20 06:47,13/Jul/23 08:10,31/Jan/20 12:14,1.8.1,,,,,,,,,1.10.2,1.11.0,,,Connectors / ElasticSearch,,,,,0,pull-request-available,,,,"If the created Elastic Search Rest High Level Client(rhlClient) is unreachable, Current code throws RuntimeException. But, it doesn't close the client which causes thread leak.

 

*Current Code*

*if (!rhlClient.ping()) {*

     *throw new RuntimeException(""There are no reachable Elasticsearch nodes!"");*

*}*

 

*Change Needed*

rhlClient needs to be closed.

 

*Steps to Reproduce*

1. Add the ElasticSearch Sink to the stream. Start the Flink program without starting the ElasticSearch. 

2. Program will give error: ""*Too many open files*"" and it doesn't write even though you start the Elastic Search later.

 ",,aljoscha,fhueske,jark,melmoth,ram_krish,rishi55,rmetzger,,,,,,,"Rishi55 commented on pull request #9468: [FLINK-13689] [Connectors/ElasticSearch] Fix thread leak when elasticsearch6 rest high level cli…
URL: https://github.com/apache/flink/pull/9468
 
 
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(To fix the thread leak when elasticsearch6 rest high level client ping is unsuccessful)*
   
   
   ## Brief change log
   
   *(Added close statement for the rhlClient when ping is unsuccessful)*
   
   
   ## Verifying this change
   
   *(This change is a trivial rework / code cleanup without any test coverage. Tested in local though)*
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 18:15;githubbot;600","aljoscha commented on pull request #9468: [FLINK-13689] [Connectors/ElasticSearch] Fix thread leak when elasticsearch6 rest high level cli…
URL: https://github.com/apache/flink/pull/9468
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/20 16:03;githubbot;600","static-max commented on pull request #10936: [FLINK-13689] [Connectors/ElasticSearch] Fix thread leak in Elasticsearch connector when cluster is down
URL: https://github.com/apache/flink/pull/10936
 
 
   ## What is the purpose of the change
   
   Fixes thread leaks caused by unclosed Elasticsearch clients
   
   ## Brief change log
   
     - Created new method *verifyClientConnection()* in *ElasticsearchApiCallBridge*
     - Moved test calls in existing Elasticsearch connectors to this new method
     - verifyClientConnection() is called in *ElasticsearchSinkBase#open()* after client has been created. If an error occurs, close() in 'ElasticsearchSinkBase' can now clean up the client. This was not possible before as the Exception was thrown before the client variable was assigned.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   I tested the change on our development cluster by applying the change on top of release tag 1.9.1. No problems so far and the threads don't leak anymore. Recovery works fine, I tested different scenarios with the Elasticsearch cluster shut down,.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jan/20 16:07;githubbot;600","aljoscha commented on pull request #10936: [FLINK-13689] [Connectors/ElasticSearch] Fix thread leak in Elasticsearch connector when cluster is down
URL: https://github.com/apache/flink/pull/10936
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/20 12:14;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 07 06:47:34 UTC 2020,,,,,,,,,,"0|z05k68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/19 13:22;rishi55;Hi [~fhueske],

I figured out the bug when I was running my streaming application which pulls data from kafka and puts it in Elasticsearch. I used *Elasticsearch6 connector*. When the application is unable to connect to ElasticSearch it keeps on retrying and leaving all those unestablished clients open which is resulting in ""TOO MANY OPEN FILES"" error.

The close statement is added in previous Elasticsearch connectors(2,5) where Transport client is used. But in ES6 connector, RestHighLevel client is used.

 

*Plan to fix:*
 # Add IOUtils.closeQuietly(rhlClient) statement before throwing the runtime exception in above mentioned code.

 

*Plan to test:*
 # Start the streaming application and stop the ElasticSearch service. Make sure that client is getting closed and the TOO many files error is not being observed.

 ;;;","16/Aug/19 18:19;rishi55;Hi [~fhueske],

I created pull request with the mentioned change.

[https://github.com/apache/flink/pull/9468]

 ;;;","04/Sep/19 05:30;jark;I changed the fix version to 1.8.3 because of releasing 1.8.2. Please let me know if you want it in 1.8.2.;;;","22/Jan/20 17:03;melmoth;Hi there, I would like to contribute the changes proposed by [~aljoscha] at [https://github.com/apache/flink/pull/9468].

I already started working on a fix and testing it at the moment. ;;;","31/Jan/20 12:14;aljoscha;master: 79f2f04ecba3e37e0b761f50348382fa08d7db9d;;;","07/Aug/20 06:47;rmetzger;release-1.10: https://github.com/apache/flink/commit/b717877f69d90b3081bb6a79e8d78196831256a7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalogUseBlinkITCase.testBlinkUdf constantly failed with 1.9.0-rc2,FLINK-13688,13250173,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,ykt836,ykt836,12/Aug/19 03:21,23/Mar/20 07:01,13/Jul/23 08:10,16/Aug/19 02:00,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Connectors / Hive,Tests,,,,0,pull-request-available,,,,"I tried to build flink 1.9.0-rc2 from source and ran all tests in a linux server, HiveCatalogUseBlinkITCase.testBlinkUdf will be constantly fail. 

 

Fail trace:
{code:java}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 313.228 s <<< FAILURE! - in org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase
[ERROR] testBlinkUdf(org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase) Time elapsed: 305.155 s <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
at org.apache.flink.table.catalog.hive.HiveCatalogUseBlinkITCase.testBlinkUdf(HiveCatalogUseBlinkITCase.java:180)
Caused by: org.apache.flink.runtime.resourcemanager.exceptions.UnfulfillableSlotRequestException: Could not fulfill slot request 35cf6fdc1b525de9b6eed13894e2e31d. Requested resource profile (ResourceProfile{cpuCores=0.0, heapMemoryInMB=0, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0, managedMemoryInMB=128}) is unfulfillable.
{code}
 ","Linux server

kernal version: 3.10.0

java version: ""1.8.0_102""

processor count: 96",lzljs3620320,phoenixjiangnan,sewen,trohrmann,xuefuz,,,,,,,,,"JingsongLi commented on pull request #9417: [FLINK-13688][hive] Limit the parallelism of HiveCatalogUseBlinkITCase
URL: https://github.com/apache/flink/pull/9417
 
 
   
   ## What is the purpose of the change
   
   limit the parallelism of HiveCatalogUseBlinkITCase to avoid too many slot requests by default parallelism (use the core size of machine).
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 08:27;githubbot;600","KurtYoung commented on pull request #9417: [FLINK-13688][hive] Limit the parallelism/memory of HiveCatalogUseBlinkITCase
URL: https://github.com/apache/flink/pull/9417
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 01:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-13890,,,,,FLINK-13760,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 04:13:59 UTC 2019,,,,,,,,,,"0|z05k00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/19 08:09;trohrmann;Could this be a misconfiguration of the started TM? It looks as if the slot request is not fulfillable. If this is the case, then this is not a blocker issue but should definitely be fixed.;;;","12/Aug/19 08:12;ykt836;[~till.rohrmann] Yes, I will change it to critical once the reason is confirmed.;;;","12/Aug/19 08:23;lzljs3620320;I think we should limit the parallelism of this it case to avoid too many slot requests.;;;","12/Aug/19 08:24;ykt836;Was it because the mini cluster is divided to too much slots? The machine I used have 96 processors. [~lzljs3620320];;;","12/Aug/19 08:28;lzljs3620320;Yeah, it will use processor size when not config the parallelism.;;;","12/Aug/19 09:42;sewen;This is a common error in many tests.

I would recommend to never just launch a program (implicitly using the local environment), but always use the mini cluster test rule instead, which uses a deterministic test setup.;;;","15/Aug/19 23:32;phoenixjiangnan;[~lzljs3620320] [~ykt836] what's the status of this PR? Though it's not a blocker, it has been blocking our local development and testing. I have not been able to test flink-connector-hive successfully for quite a while.

Can you guys help to fix this ASAP?

cc [~xuefuz] [~lirui] [~Terry1897];;;","16/Aug/19 01:25;ykt836;I'll merge this ASAP;;;","16/Aug/19 02:00;ykt836;merged in master (1.10.0): a194b37d9b99a47174de9108a937f821816d61f5

merged in 1.9.1: 03b3430135a96c8557e0ae64d5c73b1e7d4b2baf;;;","18/Aug/19 14:10;trohrmann;Please don't forget to set the correct fixVersion fields when merging PRs [~ykt836]. I guess we also need to add {{1.10.0}}.;;;","27/Aug/19 04:13;ykt836;[~till.rohrmann] Thanks for the heads up, this was planned. I'm also start to scan all issues that are not included in 1.9.0 release and change the fixVersion now.;;;",,,,,,,,,,,,,,,,,,,,,
SQL Client end-to-end test for modern Kafka failed on Travis,FLINK-13663,13249834,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,1u0,trohrmann,trohrmann,09/Aug/19 07:11,16/Sep/20 02:42,13/Jul/23 08:10,14/Aug/19 10:10,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,Connectors / Kafka,Table SQL / Client,Tests,,,0,pull-request-available,test-stability,,,"The {{SQL Client end-to-end test for modern Kafka}} failed on Travis because it could not download {{https://archive.apache.org/dist/kafka/0.11.0.2/kafka_2.11-0.11.0.2.tgz}}.

Maybe we could add a similar retry logic as with the Kinesis end-to-end test FLINK-13599.

https://api.travis-ci.org/v3/job/569262834/log.txt
https://api.travis-ci.org/v3/job/569262828/log.txt",,1u0,azagrebin,dian.fu,jark,trohrmann,,,,,,,,,"1u0 commented on pull request #9429: [FLINK-13663][e2e] Double curl retries count and total time for Kafka downloads
URL: https://github.com/apache/flink/pull/9429
 
 
   ## What is the purpose of the change
   
   Double retries count and total retries time for Kafka downloads as an attempt to make some e2e tests fail due to failure during tests setup.
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (sure)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (hopefully no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/19 18:53;githubbot;600","tillrohrmann commented on pull request #9429: [FLINK-13663][e2e] Double curl retries count and total time for Kafka downloads
URL: https://github.com/apache/flink/pull/9429
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 10:13;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-13686,,,,,,,,,,FLINK-13599,,,FLINK-13567,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 02:42:55 UTC 2020,,,,,,,,,,"0|z05hww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/19 11:53;azagrebin;From what I see in the test code (bash function setup_kafka_dist in kafka_common.sh), we already have a hotfix introduced by [~1u0] which hardens the test by increasing the timeout of the failing curl download. The hotfix adds curl options which activate an embedded retry algorithm of curl command (5 retries with 1 sec init back-off time then doubling, total retry time is up to 1 min).
{code:java}
KAFKA_URL=""https://archive.apache.org/dist/kafka/$KAFKA_VERSION/kafka_2.11-$KAFKA_VERSION.tgz""
echo ""Downloading Kafka from $KAFKA_URL""
curl ""$KAFKA_URL"" --retry 5 --retry-max-time 60 > $TEST_DATA_DIR/kafka.tgz{code}
 
{code:java}
--retry-max-time <seconds>
 The retry timer is reset before the first transfer attempt. Retries will be done as usual (see --retry) as long as the timer hasn't reached this given limit. Notice that if
 the timer hasn't reached the limit, the request will be made and while performing, it may take longer than this given time period. To limit a single request's maximum time,
 use -m, --max-time. Set this option to zero to not timeout retries.

------------------------------------------------------------------------------------------------------------
--retry <num>
 If a transient error is returned when curl tries to perform a transfer, it will retry this number of times before giving up. Setting the number to 0 makes curl do no retries
 (which is the default). Transient error means either: a timeout, an FTP 4xx response code or an HTTP 5xx response code.
When curl is about to retry a transfer, it will first wait one second and then for all forthcoming retries it will double the waiting time until it reaches 10 minutes which
 then will be the delay between the rest of the retries. By using --retry-delay you disable this exponential backoff algorithm. See also --retry-max-time to limit the total
 time allowed for retries.
{code}
We could either further increase the total curl timeout from 1 min to e.g. 5 or 10 or try the same approach with our custom retry logic proposed in FLINK-13599 but I would not expect that our retry will be much better than the embedded curl retry mechanism.

 ;;;","12/Aug/19 18:02;1u0;FYI, the {{curl}} retries were added as an attempt to address the issue described in FLINK-12773.
We can try to increase number of retries/retry timeout, but it's possible that this won't help much.

Instead (or in parallel), my proposal would be to try to cache and share the downloaded Kafka distribution among multiple e2e test runs.
There is a PR by [~chesnay] (https://github.com/apache/flink/pull/7605) which conceptually does it. Personally, I find the PR as relatively big and it's possible to implement the caching logic as some script with low effort.;;;","13/Aug/19 02:51;ykt836;+1 to find a way to cache downloaded distributions. Not only for multiple e2e runs, but also for local verifying. I found it's impossible to run e2e tests locally in China sine the download is very slow and unstable. ;;;","13/Aug/19 04:02;jark;+1 to cache distributions. Even to share the distribution across build jobs if possible. ;;;","13/Aug/19 10:12;trohrmann;Another issue: https://api.travis-ci.org/v3/job/570757852/log.txt

[~1u0] could you try to increase the timeout/retries of the curl command and see whether the tests stabilize? If this does not work, then I would be in favour of a fix for the download problem (e.g. a simple cache) because I don't believe that we replace these tests any time soon with a different testing framework.;;;","14/Aug/19 10:10;trohrmann;We increased the curl retries and the timeout. If this should not fix the problem permanently, then we need to look into caching of dependencies.

1.10.0: a3157710fe8267f689ae9a6f4f2338b97ae2d8c0
1.9.1: f4af5a8171507b33d4ad19d698db8b21e1090205;;;","15/Aug/19 02:24;jark;I think the idea proposed by [~1u0] to cache external distributions is very nice. It will help to reduce a lot of e2e testing time. And it would be very helpful for Chinese developers to verify e2e locally, because it's very unstable and almost impossible to download them in China.

So I created a JIRA FLINK-13730 to track the work. [~1u0] would you like to continue the work? 
;;;","16/Sep/20 02:42;dian.fu;Another instance on master: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6520&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529;;;",,,,,,,,,,,,,,,,,,,,,,,,
FlinkKinesisProducerTest.testBackpressure failed on Travis,FLINK-13662,13249833,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,trohrmann,trohrmann,09/Aug/19 07:05,24/Dec/19 07:29,13/Jul/23 08:10,24/Dec/19 07:29,1.10.0,1.9.0,,,,,,,,1.10.0,,,,Connectors / Kinesis,Tests,,,,0,pull-request-available,test-stability,,,"The {{FlinkKinesisProducerTest.testBackpressure}} failed on Travis with

{code}
14:45:50.489 [ERROR] Failures: 
14:45:50.489 [ERROR]   FlinkKinesisProducerTest.testBackpressure:298 Flush triggered before reaching queue limit
{code}

https://api.travis-ci.org/v3/job/569262823/log.txt",,azagrebin,dian.fu,fmthoma,roman,thw,trohrmann,zhuzh,,,,,,,"zentol commented on pull request #10551: [FLINK-13662][kinesis] Relax timing requirements
URL: https://github.com/apache/flink/pull/10551
 
 
   Relaxes the timing constraints in `FlinkKinesisProducerTest#testBackpressure` by relying on a `Deadline` instead of `100ms` syncs.
   
   To prevent the `Deadline` usage from obfuscating errors in case `timeLeft()` returns a negative value, a new method `timeLeftIfAny() throws TimeoutException` was introduced, which should be usable as a drop-in replacement for usage that follow this pattern:
   ```
   if (deadline.hasTimeLeft()) {
       <do something with deadline.timeLeft()>
   } else {
       <abort>
   }
   ```
   
   The problem in the above example is that there is no guarantee that `timeLeft()` returns a positive value, because time.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 13:02;githubbot;600","zhuzhurk commented on pull request #10551: [FLINK-13662][kinesis] Relax timing requirements
URL: https://github.com/apache/flink/pull/10551
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Dec/19 06:53;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 24 07:29:28 UTC 2019,,,,,,,,,,"0|z05hwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/19 07:31;trohrmann;Another instance: https://api.travis-ci.org/v3/job/571271887/log.txt;;;","14/Aug/19 07:31;trohrmann;[~thw@apache.org] do you have an idea what the problem could be?;;;","14/Aug/19 17:24;thw;Unfortunately I'm not familiar with this test and the producer overall. Do we see this issue only recently?

[~fmthoma] do you perhaps have an idea what to look for?

 ;;;","15/Aug/19 09:21;trohrmann;I could find another issue, hence I think it is a new problem/instability.;;;","20/Aug/19 12:40;fmthoma;I did a quick check, there don't seem to be any relevant changes that could lead to this behaviour. I'll have a look into it. Does the test fail consistently, or occaisonally?;;;","20/Aug/19 14:21;trohrmann;So far we observed the test case to fail twice (the dark figure is probably a bit higher) [~fmthoma].;;;","16/Oct/19 13:12;azagrebin;another instance [https://api.travis-ci.org/v3/job/598142916/log.txt];;;","15/Nov/19 11:41;fmthoma;The test heavily depends on the {{processElement()}} thread finishing within 100ms. Maybe on a CI system and under load, this is not always the case. We could increase the timeout to 200ms or even more; this should not be a problem for the successful case (since {{trySync()}} will return as soon as the thread is finished).;;;","27/Nov/19 09:14;dian.fu;Another instance: [https://api.travis-ci.org/v3/job/617316742/log.txt];;;","04/Dec/19 10:55;trohrmann;Do you wanna give it a try and see whether it fixes the problem [~fmthoma]?;;;","24/Dec/19 07:29;zhuzh;Fixed via

master:
a62641a0918aaedbac6312293cf8826e4d11f300
20041cafbfe500ed386e11da5d09f116e7a45b81

1.10.0:
a57da33111ba6f9155fef0dde8635ae54e641507
b9b422e2f282e374aba6f207ffbe280bd5f91c9e;;;",,,,,,,,,,,,,,,,,,,,,
Caused by: java.io.IOException: Thread 'SortMerger spilling thread' terminated due to an exception,FLINK-13655,13249652,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,tokielee,tokielee,08/Aug/19 12:43,09/Aug/19 07:36,13/Jul/23 08:10,09/Aug/19 07:36,1.6.3,,,,,,,,,1.8.1,,,,API / Type Serialization System,,,,,0,KryoSerializer,,,,"Symptom:

flink program can sucessfully read and process single ORC file from HDFS,whatever given reading path is file's parent folder or specific file path. However,I put them together in the same folder and program reads that folder, the following error always occurs.

{color:#cc7832}val {color}configHadoop = {color:#cc7832}new {color}org.apache.hadoop.conf.Configuration()
 configHadoop.set({color:#6a8759}""HADOOP_USER_NAME""{color}{color:#cc7832}, {color}{color:#6a8759}""user""{color})
 configHadoop.set({color:#6a8759}""fs.defaultFS""{color}{color:#cc7832}, {color}{color:#6a8759}""xx.xxx.xx.xx""{color})
 {color:#cc7832}val {color}env = ExecutionEnvironment.getExecutionEnvironment
 {color:#cc7832}val {color}bTableEnv = TableEnvironment.getTableEnvironment(env)

{color:#cc7832}val {color}orcTableSource = OrcTableSource.builder()
 {color:#808080}// path to ORC file(s). NOTE: By default, directories are recursively scanned.{color} .path(inPath)
 {color:#808080}// schema of ORC files{color} .forOrcSchema({color:#6a8759}""struct<storage_time:String,storage_source_msg:String,rev_name:String,where2:String,rev_phone:String,bind_email:String,ord_pin:String,ord_tm:String>""{color})
 {color:#808080}// Hadoop configuration{color} .withConfiguration(configHadoop)
 {color:#808080}// build OrcTableSource{color} .build()

 

------------The following is stack info --------------------

Root exception
 Timestamp: 2019-08-08, 20:15:05
 java.lang.Exception: The data preparation for task 'CHAIN GroupReduce (GroupReduce at com.jd.risk.flink.analysis.framework.core.EventOfflineServiceFrameWork.startService(EventOfflineServiceFrameWork.scala:41)) -> Map (Key Extractor)' , caused an error: Error obtaining the sorted input: Thread 'SortMerger spilling thread' terminated due to an exception: Index: 97, Size: 17
  at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:479)
  at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:368)
  at org.apache.flink.runtime.taskmanager.Task.run(Task.java:712)
  at java.lang.Thread.run(Thread.java:748)
 Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger spilling thread' terminated due to an exception: Index: 97, Size: 17
  at org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:650)
  at org.apache.flink.runtime.operators.BatchTask.getInput(BatchTask.java:1108)
  at org.apache.flink.runtime.operators.GroupReduceDriver.prepare(GroupReduceDriver.java:99)
  at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:473)
  ... 3 more
 Caused by: java.io.IOException: Thread 'SortMerger spilling thread' terminated due to an exception: Index: 97, Size: 17
  at org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:831)
 Caused by: java.lang.IndexOutOfBoundsException: Index: 97, Size: 17
  at java.util.ArrayList.rangeCheck(ArrayList.java:653)
  at java.util.ArrayList.get(ArrayList.java:429)
  at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)
  at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:805)
  at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:759)
  at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:315)
  at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:335)
  at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(KryoSerializer.java:350)
  at org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase.copy(TupleSerializerBase.java:99)
  at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:84)
  at org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase.copy(TupleSerializerBase.java:99)
  at org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase.copy(TupleSerializerBase.java:99)
  at org.apache.flink.runtime.operators.sort.NormalizedKeySorter.writeToOutput(NormalizedKeySorter.java:519)
  at org.apache.flink.runtime.operators.sort.UnilateralSortMerger$SpillingThread.go(UnilateralSortMerger.java:1375)
  at org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:827)
 CHAIN GroupReduce (GroupReduce at com.jd.risk.flink.analysis.framework.core.EventOfflineServiceFrameWork.startService(EventOfflineServiceFrameWork.scala:41)) -> Map (Key Extractor) (305/480)
 Timestamp: 2019-08-08, 20:15:05 Location: LF-BCC-POD0-172-21-60-234.hadoop.jd.local:15837
 java.lang.Exception: The data preparation for task 'CHAIN GroupReduce (GroupReduce at com.jd.risk.flink.analysis.framework.core.EventOfflineServiceFrameWork.startService(EventOfflineServiceFrameWork.scala:41)) -> Map (Key Extractor)' , caused an error: Error obtaining the sorted input: Thread 'SortMerger spilling thread' terminated due to an exception: Index: 97, Size: 17
  at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:479)
  at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:368)
  at org.apache.flink.runtime.taskmanager.Task.run(Task.java:712)
  at java.lang.Thread.run(Thread.java:748)
 Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger spilling thread' terminated due to an exception: Index: 97, Size: 17
  at org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:650)
  at org.apache.flink.runtime.operators.BatchTask.getInput(BatchTask.java:1108)
  at org.apache.flink.runtime.operators.GroupReduceDriver.prepare(GroupReduceDriver.java:99)
  at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:473)
  ... 3 more
 Caused by: java.io.IOException: Thread 'SortMerger spilling thread' terminated due to an exception: Index: 97, Size: 17
  at org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:831)
 Caused by: java.lang.IndexOutOfBoundsException: Index: 97, Size: 17
  at java.util.ArrayList.rangeCheck(ArrayList.java:653)
  at java.util.ArrayList.get(ArrayList.java:429)
  at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)
  at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:805)
  at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:759)
  at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:315)
  at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:335)
  at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(KryoSerializer.java:350)
  at org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase.copy(TupleSerializerBase.java:99)
  at org.apache.flink.api.scala.typeutils.TraversableSerializer.copy(TraversableSerializer.scala:84)
  at org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase.copy(TupleSerializerBase.java:99)
  at org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase.copy(TupleSerializerBase.java:99)
  at org.apache.flink.runtime.operators.sort.NormalizedKeySorter.writeToOutput(NormalizedKeySorter.java:519)
  at org.apache.flink.runtime.operators.sort.UnilateralSortMerger$SpillingThread.go(UnilateralSortMerger.java:1375)
  at org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:827)","{color:#e8bf6a}<properties>
{color}{color:#e8bf6a} <project.build.sourceEncoding>{color}UTF-8{color:#e8bf6a}</project.build.sourceEncoding>
{color}{color:#e8bf6a} <flink.version>{color}1.5.6{color:#e8bf6a}</flink.version>
{color}{color:#e8bf6a} <slf4j.version>{color}1.7.7{color:#e8bf6a}</slf4j.version>
{color}{color:#e8bf6a} <log4j.version>{color}1.2.17{color:#e8bf6a}</log4j.version>
{color}{color:#e8bf6a} <scala.binary.version>{color}2.11{color:#e8bf6a}</scala.binary.version>
{color}{color:#e8bf6a} <scala.version>{color}2.11.12{color:#e8bf6a}</scala.version>
{color}{color:#e8bf6a}</properties>{color}

{color:#e8bf6a}_parameters.setBoolean(""recursive.file_{color:#333333}.enumeration"",true){color}{color}",tokielee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 09 07:36:08 UTC 2019,,,,,,,,,,"0|z05gsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/19 07:36;tokielee;I verified this issue in Flink 1.8.1 version. The problem can not be reproduced. so I think it is fixed in latest version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong word used in comments in the class <KeyedStream>,FLINK-13654,13249647,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Eric Lee,Eric Lee,Eric Lee,08/Aug/19 12:27,06/Nov/19 11:30,13/Jul/23 08:10,06/Nov/19 11:29,1.9.0,,,,,,,,,1.9.1,,,,API / DataStream,,,,,0,,,,,"While beginning to learn datastream and want to check the usage of the max function in the source code, just find an incorrect work used in comments in the class <KeyedStream>.

 

!image-2019-08-08-20-23-26-292.png!",,Eric Lee,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/19 12:23;Eric Lee;image-2019-08-08-20-23-26-292.png;https://issues.apache.org/jira/secure/attachment/12977035/image-2019-08-08-20-23-26-292.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 08 13:50:35 UTC 2019,,,,,,,,,,"0|z05grc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/19 12:30;Eric Lee;Hello guys, I just find a little bit mistake in comments. I want to fix this mistake and make little contribution to the community, could someone assign this issue to me? ;;;","08/Aug/19 13:08;jark;Such issue can open a hotfix pull request without creating JIRA issue. ;;;","08/Aug/19 13:50;Eric Lee;I will create a {{[hotfix]}} pull request to fix this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResultStore should avoid using RowTypeInfo when creating a result,FLINK-13653,13249644,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,08/Aug/19 11:27,11/Sep/19 18:35,13/Jul/23 08:10,11/Sep/19 18:35,,,,,,,,,,1.10.0,1.9.1,,,Table SQL / Client,,,,,0,pull-request-available,,,,"Creating a RowTypeInfo from a TableSchema can lose type parameters. As a result, querying a Hive table with decimal column from SQL CLI will hit the following exception:
{noformat}
Caused by: org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink [default_catalog, default_database, default: select * from foo] do not match.
Query result schema: [x: BigDecimal]
TableSink schema:    [x: BigDecimal]
        at org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSink(TableSinkUtils.scala:69)
        at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:179)
        at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:178)
        at scala.Option.map(Option.scala:146)
        at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:178)
        at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:146)
        at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:146)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.Iterator$class.foreach(Iterator.scala:891)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:146)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:439)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.insertInto(TableEnvironmentImpl.java:327)
        at org.apache.flink.table.api.internal.TableImpl.insertInto(TableImpl.java:428)
        at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeQueryInternal$10(LocalExecutor.java:477)
        at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:216)
        at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQueryInternal(LocalExecutor.java:475)
        ... 8 more
{noformat}",,jark,lirui,phoenixjiangnan,twalthr,,,,,,,,,,"lirui-apache commented on pull request #9432: [FLINK-13653][sql-client] ResultStore should avoid using RowTypeInfo …
URL: https://github.com/apache/flink/pull/9432
 
 
   …when creating a result
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix the issue that types with parameters, e.g. decimal, cannot be accessed via SQL client.
   
   
   ## Brief change log
   
     - Avoid the conversion between `DataType` and `TypeInformation` for `CollectBatchTableSink` and `CollectStreamTableSink`.
   
   
   ## Verifying this change
   
   Will add test cases later on.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 02:46;githubbot;600","asfgit commented on pull request #9432: [FLINK-13653][sql-client] ResultStore should avoid using RowTypeInfo …
URL: https://github.com/apache/flink/pull/9432
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Sep/19 18:22;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 11 18:35:32 UTC 2019,,,,,,,,,,"0|z05gqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/19 03:21;lirui;I thought we should just use the new type system for {{CollectBatchTableSink}} and {{CollectStreamTableSink}}. However, according to the JavaDoc of {{TableSink.getOutputType()}} and {{TableSource.getReturnType()}}, user should ""_use either the old or the new type system consistently to avoid unintended behavior_"". And if the table sinks created in SQL client need to support table sources that may use new or old type systems, I'm not sure whether we have to create different sinks for new and old type systems respectively?;;;","14/Aug/19 02:54;lirui;Just created a PoC pull request to get some feedbacks.
It seems we don't have to worry about the type system inconsistency between source and sink, because we create the sink using the table schema of the source table. Therefore I think we're good as long as we avoid the conversion between {{DataType}} and {{TypeInformation}} in the sinks.
[~lzljs3620320] [~xuefuz] [~phoenixjiangnan] could you please share your opinions about this issue? Thanks.;;;","11/Sep/19 18:35;phoenixjiangnan;master: 25534adc2311c0198b0e115d12183e6ea90b4449  1.9: f11cdd3bed438d46583368284afce74bc4ecd703;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in code-gen when using blink planner in scala shell,FLINK-13645,13249561,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zjffdu,zjffdu,zjffdu,08/Aug/19 03:43,09/Aug/19 09:19,13/Jul/23 08:10,09/Aug/19 09:19,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,, !image-2019-08-08-11-43-08-741.png! ,,bencao,jark,zjffdu,,,,,,,,,,,"zjffdu commented on pull request #9389: [FLINK-13645][planner] Error in code-gen when using blink planner in scala shell
URL: https://github.com/apache/flink/pull/9389
 
 
   
   ## What is the purpose of the change
   
   This is a trivial PR to fix the issue when using blink planner in scala shell. The root cause is that we didn't use the right ClassPathLoader. This PR just fix it.
   
   ## Brief change log
   
   Set the ClassPathLoader properly.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage. I verify it manually in apache zeppelin which uses scala shell related code.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: ( no )
     - The runtime per-record code paths (performance sensitive): ( no )
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no )
     - The S3 file system connector: ( no )
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Aug/19 03:47;githubbot;600","wuchong commented on pull request #9389: [FLINK-13645][table-planner] Error in code-gen when using blink planner in scala shell
URL: https://github.com/apache/flink/pull/9389
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 09:14;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/19 03:43;zjffdu;image-2019-08-08-11-43-08-741.png;https://issues.apache.org/jira/secure/attachment/12976981/image-2019-08-08-11-43-08-741.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 09 09:19:59 UTC 2019,,,,,,,,,,"0|z05g88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/19 09:19;jark;master: f827360d3edbc7fbec8a7c26531a0b6f5a593990
1.9: 65b52437588589061e4e7b868bed677a810b3574
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Anchors not working in document(building.md, common.md, queryable_state.md)",FLINK-13637,13249375,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hequn8128,hequn8128,hequn8128,07/Aug/19 14:59,27/Aug/19 04:07,13/Jul/23 08:10,09/Aug/19 14:10,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Documentation,,,,,0,pull-request-available,,,,"Anchors not working in document(building.md, common.md, queryable_state.md).

The format should be:
{code:java}
[create an anchor](#anchors-in-markdown)
{code}

- Add - characters between each word in the heading and wrap the value in parens
- All letters should be lowercase.
",,hequn8128,,,,,,,,,,,,,"hequn8128 commented on pull request #9384: [FLINK-13637][docs] Fix problems of anchors in document(building.md, common.md, queryable_state.md)
URL: https://github.com/apache/flink/pull/9384
 
 
   
   ## What is the purpose of the change
   
   This pull request fixes problems of anchors in documents(building.md, common.md, queryable_state.md). 
   
   The format should be:
   `[create an anchor](#anchors-in-markdown)`
   - Add - characters between each word in the heading and wrap the value in parens
   - All letters should be lowercase.
   
   
   ## Brief change log
   
     - Change letters to lowercase in queryable_state.md and common.md.
     - Add - characters between each word for the anchors in building.md.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Aug/19 15:12;githubbot;600","hequn8128 commented on pull request #9384: [FLINK-13637][docs] Fix problems of anchors in document(building.md, common.md, queryable_state.md)
URL: https://github.com/apache/flink/pull/9384
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 13:57;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 04:02:17 UTC 2019,,,,,,,,,,"0|z05f2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/19 14:07;hequn8128;Fix in
master: 56c6b487ee1bc395d700579f69f9fb8884724243
1.9: 6f3a0d0fb7a92ebd94548fa7793f9810344a5157;;;","27/Aug/19 04:02;ykt836;I've updated the fix version to 1.9.1 since this is not included in 1.9.0 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateDescriptor Loading Error NPE at FlinkKafkaProducer011 with High Concurrency Initialization,FLINK-13612,13249281,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Vow,Vow,07/Aug/19 08:51,03/Sep/19 12:52,13/Jul/23 08:10,03/Sep/19 12:48,1.6.3,1.6.4,1.7.2,shaded-7.0,,,,,,1.9.0,,,,Connectors / Kafka,,,,,0,,,,,"org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011#NEXT_TRANSACTIONAL_ID_HINT_DESCRIPTOR

The NEXT_TRANSACTIONAL_ID_HINT_DESCRIPTOR variable state in FlinkKafkaProducer011 is modified with static

NullPointerException occur in high concurrency when initializeSerializerUnlessSet method is executed

 

java.lang.NullPointerException at org.apache.flink.api.common.state.StateDescriptor.initializeSerializerUnlessSet(StateDescriptor.java:264) at org.apache.flink.runtime.state.DefaultOperatorStateBackend.getListState(DefaultOperatorStateBackend.java:730) at org.apache.flink.runtime.state.DefaultOperatorStateBackend.getUnionListState(DefaultOperatorStateBackend.java:271) at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.initializeState(FlinkKafkaProducer011.java:837) at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:178) at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:160) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:281) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:730) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:295) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:720) at java.lang.Thread.run(Thread.java:748)",,klion26,limbo,Vow,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-12688,,,,,,,,,,,,,,,,,,,,,"03/Sep/19 12:44;Vow;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12979218/screenshot-1.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 03 12:46:59 UTC 2019,,,,,,,,,,"0|z05ei0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/19 13:08;klion26;[~Vow] Does this problem still there on Flink 1.9?;;;","03/Sep/19 02:50;Vow;[~klion26] Yes, it still exists.;;;","03/Sep/19 03:15;klion26;[~Vow] could you please share the exception stack on flink 1.9.  ;;;","03/Sep/19 03:56;Vow;[~klion26] 

java.lang.NullPointerException at org.apache.flink.api.common.state.StateDescriptor.initializeSerializerUnlessSet(StateDescriptor.java:264) at org.apache.flink.runtime.state.DefaultOperatorStateBackend.getListState(DefaultOperatorStateBackend.java:730) at org.apache.flink.runtime.state.DefaultOperatorStateBackend.getUnionListState(DefaultOperatorStateBackend.java:271) at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.initializeState(FlinkKafkaProducer011.java:837) at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:178) at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:160) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:281) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:730) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:295) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:720) at java.lang.Thread.run(Thread.java:748)


This problem is solved after ""NEXT_TRANSACTIONAL_ID_HINT_DESCRIPTOR"" static variables are changed to class variables;;;","03/Sep/19 12:19;klion26;[~Vow] The line 264 of {{StateDescriptor[1]}} on release-1.9 seems an empty line, which showed in your given exception stack. Could you recheck that the exception was thrown on flink 1.9?

 

 [1] https://github.com/apache/flink/blob/1bb602b987908aacdc1e025f8255db9f0479c913/flink-core/src/main/java/org/apache/flink/api/common/state/StateDescriptor.java#L264;;;","03/Sep/19 12:45;Vow;Sorry, this problem has been fixed on Flink 1.9

The line 306
 !screenshot-1.png! ;;;","03/Sep/19 12:46;klion26;[~Vow] As this has been fixed in 1.9, could you please close this issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
AsyncDataStreamITCase.testUnorderedWait failed on Travis,FLINK-13605,13249252,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,SleePy,kkl0u,kkl0u,07/Aug/19 06:04,07/Aug/19 12:49,13/Jul/23 08:10,07/Aug/19 09:21,1.9.0,,,,,,,,,1.9.0,,,,API / DataStream,Tests,,,,0,pull-request-available,,,,An instance of the failure can be found here https://api.travis-ci.org/v3/job/568291353/log.txt,,aljoscha,kkl0u,SleePy,tison,trohrmann,,,,,,,,,"asfgit commented on pull request #9378: [FLINK-13605][tests] Fix unstable case AsyncDataStreamITCase#testUnor…
URL: https://github.com/apache/flink/pull/9378
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Aug/19 09:21;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,FLINK-13635,,,,FLINK-13486,,,,,,,,,,,"07/Aug/19 06:28;tison;0001-FLINK-13605.patch;https://issues.apache.org/jira/secure/attachment/12976895/0001-FLINK-13605.patch",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 07 09:21:52 UTC 2019,,,,,,,,,,"0|z05ebk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/19 06:29;tison;[~kkl0u] you can try out the patch attached if it is allowed to swallow {{InterruptException}} in {{SystemProcessingTimeService}}.;;;","07/Aug/19 07:28;trohrmann;Another instance: https://api.travis-ci.org/v3/job/568526204/log.txt

The test failed with

{code}
04:11:41.683 [ERROR] testUnorderedWait(org.apache.flink.streaming.api.scala.AsyncDataStreamITCase)  Time elapsed: 0.214 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.streaming.api.scala.AsyncDataStreamITCase.executeAndValidate(AsyncDataStreamITCase.scala:80)
	at org.apache.flink.streaming.api.scala.AsyncDataStreamITCase.testAsyncWait(AsyncDataStreamITCase.scala:65)
	at org.apache.flink.streaming.api.scala.AsyncDataStreamITCase.testUnorderedWait(AsyncDataStreamITCase.scala:47)
Caused by: org.apache.flink.streaming.runtime.tasks.TimerException: java.lang.InterruptedException
Caused by: java.lang.InterruptedException
{code};;;","07/Aug/19 07:29;trohrmann;Might be related to FLINK-13486.;;;","07/Aug/19 07:32;SleePy;Oops, it's probably caused by https://issues.apache.org/jira/browse/FLINK-13486. Maybe this case is not fixed completely. I will check it.;;;","07/Aug/19 07:43;trohrmann;Another instance: https://api.travis-ci.org/v3/job/568314793/log.txt;;;","07/Aug/19 07:45;trohrmann;It seems to also affect the {{AsyncDataStreamITCase.testOrderedWait}}.

https://api.travis-ci.org/v3/job/568658124/log.txt;;;","07/Aug/19 08:39;SleePy;[~till.rohrmann], Yes. It's even more unstable than before the fixing. Sorry for that.
I'm discussing with [~kkl0u] to find a completely fixing way.;;;","07/Aug/19 09:21;kkl0u;Merged on master with 8b3430bb288500ed7ff381458a2ff8519876c993
 and on release-1.9 with 7cb3c5f21976f8e811c92db09a21f3c290f97bb7

This fixed the instability but there may be a real underlying issue with the {{AsyncWaitOperator}}. The problem may be that there is no synchronisation/coordination between the {{asyncInvoke()}} and the {{timeout()}} methods.;;;",,,,,,,,,,,,,,,,,,,,,,,,
RegionFailoverITCase is unstable,FLINK-13601,13249130,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunta,aljoscha,aljoscha,06/Aug/19 13:22,18/Oct/19 12:47,13/Jul/23 08:10,18/Oct/19 12:47,1.10.0,1.9.0,,,,,,,,1.10.0,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"Excerpt from https://travis-ci.com/flink-ci/flink/jobs/222711830:

{code}
10:44:31.222 [INFO] Running org.apache.flink.test.checkpointing.RegionFailoverITCase
org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 9e0fbeaa580123e05cfce5554f443d23)
    at org.apache.flink.client.program.MiniClusterClient.submitJob(MiniClusterClient.java:92)
    at org.apache.flink.test.checkpointing.RegionFailoverITCase.testMultiRegionFailover(RegionFailoverITCase.java:132)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
    at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
    at org.apache.flink.client.program.MiniClusterClient.submitJob(MiniClusterClient.java:90)
    ... 13 more
Caused by: java.lang.RuntimeException: Test failed due to unexpected recovered index: 2000, while last completed checkpoint record index: 1837
    at org.apache.flink.test.checkpointing.RegionFailoverITCase$StringGeneratingSourceFunction.initializeState(RegionFailoverITCase.java:300)
    at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:178)
    at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:160)
    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:281)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:862)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:367)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:688)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:518)
    ... 1 more
10:44:39.210 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.983 s <<< FAILURE! - in org.apache.flink.test.checkpointing.RegionFailoverITCase
{code}",,aljoscha,tison,trohrmann,yunta,,,,,,,,,,"Myasuka commented on pull request #9386: [FLINK-13601][tests] Harden RegionFailoverITCase by recording info when checkpoint just completed
URL: https://github.com/apache/flink/pull/9386
 
 
   # What is the purpose of the change
   
   Harden `RegionFailoverITCase` by recording info when checkpoint just completed to avoid the race condition in [FLINK-13601].
   
   
   ## Brief change log
   
     - Harden `RegionFailoverITCase`.
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - We use `TestingCompletedCheckpointStore` to record information of checkpoints instead of previous `notifyCheckpointComplete` invoked on task side. This could avoid the race condition in [FLINK-13601](https://issues.apache.org/jira/browse/FLINK-13601).
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Aug/19 18:07;githubbot;600","zentol commented on pull request #9386: [FLINK-13601][tests] Harden RegionFailoverITCase by recording info when checkpoint just completed
URL: https://github.com/apache/flink/pull/9386
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Oct/19 12:45;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-10712,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 18 12:47:55 UTC 2019,,,,,,,,,,"0|z05dkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 13:24;aljoscha;cc [~yunta], I think you wrote the test.;;;","06/Aug/19 13:25;trohrmann;If this is only a test instability, then we can decrease the priority of this issue to critical.;;;","06/Aug/19 18:34;yunta;[~aljoscha] After looking up the [detail log|https://transfer.sh/bNhvk/1452.8.tar.gz] of [flink-ci-failure|https://travis-ci.com/flink-ci/flink/jobs/222711830]. I am pretty sure that this is a test instability.

Currently, I use {{notifyCheckpointComplete}} interface to record {{snapshotIndicesOfSubTask}} and {{lastCompletedCheckpointId}} on task side. However, the checkpoint 15 completed just after this task failed, and {{notifyCheckpointComplete}} would not invoked on task side anymore since this the task has been failded. In other words, {{lastCompletedCheckpointId}} would only be record as last complete checkpoint 14, which leads to the failure.
 Below is related code:
{code:java}
@Override
public void notifyCheckpointComplete(long checkpointId) {
	if (getRuntimeContext().getIndexOfThisSubtask() == NUM_OF_REGIONS - 1) {
		lastCompletedCheckpointId.set(checkpointId);
		snapshotIndicesOfSubTask.put(checkpointId, lastRegionIndex);
		numCompletedCheckpoints.incrementAndGet();
	}
}
{code}
And below is the logs which record this situation.
{code:java}
10:44:38,755 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering checkpoint 15 @ 1565088278754 for job 9e0fbeaa580123e05cfce5554f443d23.
10:44:38,762 INFO  org.apache.flink.runtime.taskmanager.Task                     - Map (3/3) (fb073c45e552de69f06e8037f581491c) switched from RUNNING to FAILED.
org.apache.flink.test.checkpointing.RegionFailoverITCase$TestException
	at org.apache.flink.test.checkpointing.RegionFailoverITCase$FailingMapperFunction.map(RegionFailoverITCase.java:344)
	at org.apache.flink.test.checkpointing.RegionFailoverITCase$FailingMapperFunction.map(RegionFailoverITCase.java:315)
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processElement(StreamOneInputProcessor.java:164)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:143)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performDefaultAction(StreamTask.java:269)
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:140)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:378)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:688)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:518)
	at java.lang.Thread.run(Thread.java:748)
10:44:38,776 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionStrategy  - Calculating tasks to restart to recover the failed task 5c51e52cde5a1c4df827ddb38fbc8da9_2.
10:44:38,776 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionStrategy  - 3 tasks should be restarted to recover the failed task 5c51e52cde5a1c4df827ddb38fbc8da9_2. 
10:44:38,776 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Custom Source (3/3) (1a7ad9441ead7561b1a92bef8d46e421) switched from RUNNING to CANCELING.
10:44:38,776 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Sink: Unnamed (3/3) (6cdcfb1522f95345e0afbd32e82aaa68) switched from RUNNING to CANCELING.
10:44:38,777 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Discarding the results produced by task execution fb073c45e552de69f06e8037f581491c.
10:44:38,777 INFO  org.apache.flink.runtime.taskmanager.Task                     - Attempting to cancel task Source: Custom Source (3/3) (1a7ad9441ead7561b1a92bef8d46e421).
10:44:38,780 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Custom Source (3/3) (1a7ad9441ead7561b1a92bef8d46e421) switched from RUNNING to CANCELING.
10:44:38,780 INFO  org.apache.flink.runtime.taskmanager.Task                     - Triggering cancellation of task code Source: Custom Source (3/3) (1a7ad9441ead7561b1a92bef8d46e421).
10:44:38,780 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed checkpoint 15 for job 9e0fbeaa580123e05cfce5554f443d23 (35251 bytes in 26 ms).
{code}
I plan to use a customized {{CompletedCheckpointStore}} to record information instead of relaying on the {{notifyCheckpointComplete}} invoked on task side to fix this problem. Pleas assign this ticket to me.;;;","07/Aug/19 07:42;trohrmann;I've assigned you to this ticket [~yunta]. Please move it into ""in progress"" once you start working on it.;;;","18/Oct/19 12:47;chesnay;master: db436d93797ef1422444ff4d92f966865c264cc3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
TableEnvironment.connect() is not usable,FLINK-13600,13249098,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,twalthr,twalthr,twalthr,06/Aug/19 12:08,08/Aug/19 15:37,13/Jul/23 08:10,08/Aug/19 15:36,,,,,,,,,,1.9.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,Connecting to an external system is currently not possible in the unified table environment. Because it returns a wrong descriptor.,,twalthr,,,,,,,,,,,,,"twalthr commented on pull request #9382: [FLINK-13600][table] Rework TableEnvironment.connect() class hierarchy
URL: https://github.com/apache/flink/pull/9382
 
 
   ## What is the purpose of the change
   
   This fixes the currently broken `TableEnvironment.connect()` API by reworking the descriptor class hierarchy. `TableDescriptor` now contains most of the logic and `ConnectTableDescriptor` is the return type of table environments (expect for legacy Batch/StreamTableDescriptors).
   
   The API is still mostly source code compatible.
   
   ## Brief change log
   
   - Return `ConnectTableDescriptor`
   - Merge ConnectorFormatDescriptor, StreamableDescriptor, and TableDescriptor
   - Merge RegistrableDescriptor and ConnectTableDescriptor
   
   ## Verifying this change
   
   A new `TableEnvironmentTest` tests the implementation.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? JavaDocs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Aug/19 14:11;githubbot;600","asfgit commented on pull request #9382: [FLINK-13600][table] Rework TableEnvironment.connect() class hierarchy
URL: https://github.com/apache/flink/pull/9382
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Aug/19 15:37;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 08 15:36:21 UTC 2019,,,,,,,,,,"0|z05ddc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/19 15:36;twalthr;Fixed in 1.10.0: d544d302bdd640edb5432c2df9c424057a5548e9
Fixed in 1.9.0: 88458ebe41d170a43e5621f4af007dc8e25b7115;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis end-to-end test failed on Travis,FLINK-13599,13249073,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,trohrmann,trohrmann,06/Aug/19 10:07,27/Aug/19 04:18,13/Jul/23 08:10,19/Aug/19 12:55,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,Connectors / Kinesis,Tests,,,,0,pull-request-available,test-stability,,,"The {{Kinesis end-to-end test}} failed on Travis with 

{code}
2019-08-06 08:48:20,177 ERROR org.apache.flink.client.cli.CliFrontend                       - Error while running the command.
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unable to execute HTTP request: Connect to localhost:4567 [localhost/127.0.0.1] failed: Connection refused (Connection refused)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:593)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:438)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:274)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:746)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:273)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:205)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1010)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1083)
	at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1083)
Caused by: org.apache.flink.kinesis.shaded.com.amazonaws.SdkClientException: Unable to execute HTTP request: Connect to localhost:4567 [localhost/127.0.0.1] failed: Connection refused (Connection refused)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1116)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1066)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.AmazonKinesisClient.doInvoke(AmazonKinesisClient.java:2388)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.AmazonKinesisClient.invoke(AmazonKinesisClient.java:2364)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.AmazonKinesisClient.executeDescribeStream(AmazonKinesisClient.java:754)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.AmazonKinesisClient.describeStream(AmazonKinesisClient.java:729)
	at org.apache.flink.kinesis.shaded.com.amazonaws.services.kinesis.AmazonKinesisClient.describeStream(AmazonKinesisClient.java:766)
	at org.apache.flink.streaming.kinesis.test.KinesisPubsubClient.createTopic(KinesisPubsubClient.java:63)
	at org.apache.flink.streaming.kinesis.test.KinesisExampleTest.main(KinesisExampleTest.java:57)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:576)
	... 9 more
Caused by: org.apache.flink.kinesis.shaded.org.apache.http.conn.HttpHostConnectException: Connect to localhost:4567 [localhost/127.0.0.1] failed: Connection refused (Connection refused)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:159)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.conn.$Proxy2.connect(Unknown Source)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:381)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:237)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1238)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1058)
	... 27 more
Caused by: java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:666)
	at org.apache.flink.kinesis.shaded.com.amazonaws.http.apache.client.impl.ApacheConnectionManagerFactory$TrustingSocketFactory.connectSocket(ApacheConnectionManagerFactory.java:184)
	at org.apache.flink.kinesis.shaded.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)
	... 43 more
{code}

It seems as if the test failed to create the test topic.

https://api.travis-ci.org/v3/job/568211010/log.txt",,azagrebin,trohrmann,,,,,,,,,,,,"azagrebin commented on pull request #9419: [FLINK-13599][e2e tests] Harden test_streaming_kinesis with kinesalite docker image download/run retries
URL: https://github.com/apache/flink/pull/9419
 
 
   ## What is the purpose of the change
   
   Add retry logic for docker run command to start kinesalite in Kinesis E2E test.
   
   ## Brief change log
   
     - add function `retry_until_passed_and_wait_condition` to `common.sh`
     - call `retry_until_passed_and_wait_condition` in `test_streaming_kinesis.sh`
   
   ## Verifying this change
   
   FLINK_DIR=../build-target ./run-single-test.sh test-scripts/test_streaming_kinesis.sh
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 10:05;githubbot;600","azagrebin commented on pull request #9419: [FLINK-13599][e2e tests] Harden test_streaming_kinesis with kinesalite docker image download/run retries
URL: https://github.com/apache/flink/pull/9419
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 12:52;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13663,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 04:18:28 UTC 2019,,,,,,,,,,"0|z05d7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 12:29;chesnay;The issue is that kinesis could not be setup properly:
{code}
Unable to find image 'instructure/kinesalite:latest' locally
docker: Error response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers).
See 'docker run --help'.
Error: No such container: flink-test-kinesis
{code};;;","07/Aug/19 09:03;trohrmann;I would like to keep the priority of this issue as critical because it is a test instability (independent of the cause). We should try to resolve test instabilities asap since a stitch in time saves nine!

Maybe we need to add a retry loop for setting up end-to-end tests in order to harden against outages of external systems.;;;","19/Aug/19 12:55;azagrebin;I also tried to loop it on Travis but it is hard to reproduce that download hiccup.
The testing of the retry itself should be enough.
We have to see if the original download problem sustains despite the retries then we reopen the issue.

merged into

master 24ab7bb55b8d45ede3466f291f4e60a8917ed291

release-1.9 a893b70b203ffbce1165fd4424b106443b4ab6c8;;;","27/Aug/19 04:18;ykt836;I've updated the fix version to 1.9.1 since this is not included in 1.9.0 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaITCase.testBigRecordJob fails on Travis,FLINK-13595,13249034,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,trohrmann,trohrmann,06/Aug/19 07:34,18/Nov/21 07:18,13/Jul/23 08:10,16/Jan/20 12:10,1.9.0,,,,,,,,,1.10.0,,,,Connectors / Kafka,Tests,,,,0,pull-request-available,test-stability,,,"The {{KafkaITCase.testBigRecordJob}} failed with a {{TestTimedOutException}} on Travis.

{code}
Test testBigRecordJob(org.apache.flink.streaming.connectors.kafka.KafkaITCase) failed with:
org.junit.runners.model.TestTimedOutException: test timed out after 60000 milliseconds
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.kafka.clients.admin.KafkaAdminClient.close(KafkaAdminClient.java:476)
	at org.apache.kafka.clients.admin.AdminClient.close(AdminClient.java:92)
	at org.apache.kafka.clients.admin.AdminClient.close(AdminClient.java:75)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.deleteTestTopic(KafkaTestEnvironmentImpl.java:153)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.deleteTestTopic(KafkaTestBase.java:204)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runBigRecordTestTopology(KafkaConsumerTestBase.java:1336)
	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testBigRecordJob(KafkaITCase.java:121)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
{code}

https://api.travis-ci.org/v3/job/568176170/log.txt",,aljoscha,becket_qin,dian.fu,gjy,hxbks2ks,trohrmann,,,,,,,,"becketqin commented on pull request #10724: [FLINK-13595][connector/kafka][test] Close the KafkaAdminClient with ...
URL: https://github.com/apache/flink/pull/10724
 
 
   …a timeout. Print stacktrace if a long closure happens.
   
   ## What is the purpose of the change
   Fix the test failure caused by long closing time of the the `KafkaAdminClient`.
   
   ## Brief change log
   Use the `close()` method in `KafkaAdminClient` with a timeout. It looks that this does not guarantee the timeout will be honored due to the bug in `KafkaAdminClient`. Therefore we also print the stacktrace of the dangling `KafkaAdminClient` thread in case this happens again. 
   
   ## Verifying this change
   
   This change is already covered by existing tests that delete Kafka topics.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Dec/19 14:28;githubbot;600","becketqin commented on pull request #10724: [FLINK-13595][connector/kafka][test] Close the KafkaAdminClient with ...
URL: https://github.com/apache/flink/pull/10724
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jan/20 12:09;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-24949,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 06:56:01 UTC 2020,,,,,,,,,,"0|z05cz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 08:24;becket_qin;This issue is more of a Kafka issue than a Flink issue. It occurs after all the testing logic has completed while cleaning the testing environment to delete the Kafka testing topics. The delete took long to finish and cause the test to fail.

I'll look into this issue and probably fix this on the Kafka side. Meanwhile, we don't have to make this as a blocker for 1.9 release. Moving to critical instead.;;;","20/Nov/19 14:43;gjy;Another instance https://api.travis-ci.org/v3/job/614530220/log.txt;;;","29/Nov/19 05:14;dian.fu;Another instance: [https://api.travis-ci.org/v3/job/618270925/log.txt];;;","24/Dec/19 09:03;becket_qin;Two out of the three reported failures occurred in the cleanup stage due to the timeout when closing the {{KafkaAdminClient}}. I am not sure if this is an actual issue but for our testing purpose, closing the {{KafkaAdminClient}} with a timeout should avoid the problem.

The logging information from the third case was too vague to debug.

I'll address the timeout problem and close the ticket. If there are further occurrence, let's reopen the ticket.;;;","16/Jan/20 12:10;becket_qin;PR Merged.
master: fff0952a0230ab7271e95ce604356cf211595d02
release-1.10: bf0f968f89061153d0d3a93890eb8f7a202f0c59;;;","09/Dec/20 06:56;hxbks2ks;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10677&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5]

This test occurred timeout in the release-1.12, not when KafkaAdminClient is closed in the cleanup stage, but the job execution timeout.
{code:java}
2020-12-09T02:32:42.0953308Z Test testBigRecordJob(org.apache.flink.streaming.connectors.kafka.KafkaITCase) failed with:
2020-12-09T02:32:42.0953877Z org.junit.runners.model.TestTimedOutException: test timed out after 60000 milliseconds
2020-12-09T02:32:42.0954413Z 	at sun.misc.Unsafe.park(Native Method)
2020-12-09T02:32:42.0954766Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-12-09T02:32:42.0955196Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-12-09T02:32:42.0955654Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-12-09T02:32:42.0956099Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-12-09T02:32:42.0961014Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-12-09T02:32:42.0961793Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1842)
2020-12-09T02:32:42.0962431Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1822)
2020-12-09T02:32:42.0962974Z 	at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:40)
2020-12-09T02:32:42.0963756Z 	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runBigRecordTestTopology(KafkaConsumerTestBase.java:1305)
2020-12-09T02:32:42.0964449Z 	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testBigRecordJob(KafkaITCase.java:120)
2020-12-09T02:32:42.0964886Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-12-09T02:32:42.0965314Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-12-09T02:32:42.0965813Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-12-09T02:32:42.0966261Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-12-09T02:32:42.0966693Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-12-09T02:32:42.0967188Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-12-09T02:32:42.0967775Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-12-09T02:32:42.0968408Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-12-09T02:32:42.0968934Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2020-12-09T02:32:42.0969467Z 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2020-12-09T02:32:42.0969937Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-12-09T02:32:42.0970293Z 	at java.lang.Thread.run(Thread.java:748){code}
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent failing the wrong execution attempt in CheckpointFailureManager,FLINK-13593,13249025,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyu,liyu,liyu,06/Aug/19 05:55,12/Aug/19 09:59,13/Jul/23 08:10,09/Aug/19 12:50,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"Due to the asynchronously handling of checkpoint decline message in {{LegacyScheduler#declineCheckpoint}}, it's possible that the message is handled before job status transition thus {{receiveDeclineMessage}} grabbed the lock in {{CheckpointCoordinator}} before {{pendingCheckpoints}} got cleared by {{stopCheckpointScheduler}} (as triggered by the job status listener {{CheckpointCoordinatorDeActivator}}). And if the job/tasks restarts quickly enough, the {{FailJobCallback}} in {{CheckpointFailureManager}} might unexpectedly fail the job again, as observed in FLINK-13527.

To resolve the issue, we need to add a safe guard when failing the job, passing through the {{ExecutionAttemptID}} and checking against the current executions to make sure the to-be-failed one is still running, so we won't fail the newly restarted one by accident.",,liyu,pnowojski,trohrmann,yanghua,,,,,,,,,,"carp84 commented on pull request #9364: [FLINK-13593][checkpointing] Prevent failing the wrong job in CheckpointFailureManager
URL: https://github.com/apache/flink/pull/9364
 
 
   
   ## What is the purpose of the change
   
   This PR fixes the issue as reported in FLINK_13593, that due to the asynchronously handling of checkpoint decline message in `LegacyScheduler#declineCheckpoint`, it's possible that the message is handled before job status transition thus `receiveDeclineMessage` grabbed the lock in `CheckpointCoordinator` before `pendingCheckpoints` got cleared by `stopCheckpointScheduler` (as triggered by the job status listener `CheckpointCoordinatorDeActivator`). If the job/tasks restarts quickly enough, the FailJobCallback in CheckpointFailureManager might unexpectedly fail the job again.
   
   ## Brief change log
   
   Add a safe guard when failing the job, passing through the `ExecutionAttemptID` and checking against the current executions to make sure the to-be-failed one is still running, so we won't fail the newly restarted one by accident.
   
   
   ## Verifying this change
   
   The issue is revealed by the unstable failure of `KafkaProducerExactlyOnceITCase` thus is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/19 06:07;githubbot;600","tillrohrmann commented on pull request #9364: [FLINK-13593][checkpointing] Prevent failing the wrong execution attempt in CheckpointFailureManager
URL: https://github.com/apache/flink/pull/9364
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 12:52;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-13527,,,,,,FLINK-13695,,FLINK-13527,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 09 12:50:05 UTC 2019,,,,,,,,,,"0|z05cx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 06:12;yanghua;[~carp84] I think there are several contributors discussed the solution in FLINK-13497. We do not need to create a new issue to describe the issue again. And besides the reason you described, we also need to think about how to stop the new checkpoint been triggered.

IMO, it's better to keep more discussion in FLINK-13497. It just needs more opinion from [~pnowojski]. And FLINK-13497 is not a blocker issue, IMO FLINK-13527 also could not be a blocker issue.;;;","06/Aug/19 07:57;liyu;Hi [~yanghua], the issue described here is dedicated to resolve FLINK-13527 and relatively a different issue from FLINK-13497. And sure will move to FLINK-13497 later for further discussion.;;;","06/Aug/19 08:02;pnowojski;Creating a separate ticket for this might make sense, since this bug can cause multiple test failures, not only this reported in FLINK-13527.

I think we can treat this as a release blocker, since the hot fix should be fairly simple. Proper fix would come with removing multi threading here (as I described [here|https://issues.apache.org/jira/browse/FLINK-13497?focusedCommentId=16900732&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16900732]);;;","09/Aug/19 12:50;trohrmann;Fixed via

1.10.0: 7a1222f354499a78e7047db47b403f73460207b1
1.9.0: 1e22e641e4185e1964459d2486e676526990d719;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_tpch.sh should not hardcode flink version,FLINK-13592,13249014,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,ykt836,ykt836,ykt836,06/Aug/19 03:39,08/Aug/19 01:01,13/Jul/23 08:10,06/Aug/19 14:06,1.9.0,,,,,,,,,1.9.0,,,,Tests,,,,,0,pull-request-available,,,,"in flink-end-to-end-tests/test-scripts/test_tpch.sh, there are two hardcoded `flink-tpch-test-1.10-SNAPSHOT.jar`",,jark,twalthr,,,,,,,,,,,,"KurtYoung commented on pull request #9368: [FLINK-13592][e2e] Fix hardcoded flink version in tpch end-to-end test.
URL: https://github.com/apache/flink/pull/9368
 
 
   ## What is the purpose of the change
   
   - Fix hardcoded flink version in tpch end-to-end test.
   
   ## Brief change log
   
   - Use auto fetched flink version in tpch end-to-end test.
   - Changes test parallelism to 2 to cover the situation that parallelism is higher than the slot number, since the testing cluster only have one task manager and contains only one slot.
   
   
   ## Verifying this change
   
   Tested on a linux server and passed. 
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/19 06:36;githubbot;600","KurtYoung commented on pull request #9368: [FLINK-13592][e2e] Fix hardcoded flink version in tpch end-to-end test.
URL: https://github.com/apache/flink/pull/9368
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/19 14:05;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-13607,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 06 14:06:50 UTC 2019,,,,,,,,,,"0|z05cuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 14:06;ykt836;merged in master (1.10.0): 648fc72253e73e22593eeab117105b59a2c3df90

merged in 1.9.0: 13de7c332b9004ec83da65d375166a60707a31d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Completed Job List' in Flink web doesn't display right when job name is very long,FLINK-13591,13248999,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,vthinkxie,ykt836,ykt836,06/Aug/19 02:57,06/Sep/19 10:43,13/Jul/23 08:10,06/Sep/19 03:07,1.9.0,,,,,,,,,1.10.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,!10_57_07__08_06_2019.jpg!,,vthinkxie,,,,,,,,,,,,,"vthinkxie commented on pull request #9621: [FLINK-13591][web]: fix job list display when job name is too long
URL: https://github.com/apache/flink/pull/9621
 
 
   ## What is the purpose of the change
   
   fix Job List in Flink web doesn't display right when the job name is very long
   
   ## Brief change log
   
   add max-width to the td of job name column table to 40%
   
   ## Verifying this change
   
     - *Submit a job with long name (more than 200 char)*
     - *Visit the overview page*
   
   The job name column is not more than 40% width of table
   
   before
   ![FireShot Capture 361 - Apache Flink Web Dashboard - localhost](https://user-images.githubusercontent.com/1506722/64306606-f8221180-cfc5-11e9-8971-36b2ffe63554.png)
   
   after fix
   ![FireShot Capture 362 - Apache Flink Web Dashboard - localhost](https://user-images.githubusercontent.com/1506722/64306634-0ec86880-cfc6-11e9-81fb-d2b5e04c19ac.png)
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 02:16;githubbot;600","KurtYoung commented on pull request #9621: [FLINK-13591][web]: fix job list display when job name is too long
URL: https://github.com/apache/flink/pull/9621
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 03:06;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-13816,,,,,,,,,,,,,,,,,,,,,"06/Aug/19 02:57;ykt836;10_57_07__08_06_2019.jpg;https://issues.apache.org/jira/secure/attachment/12976774/10_57_07__08_06_2019.jpg",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 06 03:07:04 UTC 2019,,,,,,,,,,"0|z05crk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 03:00;vthinkxie;Hi [~ykt836]

plz assign this Jira to me;;;","06/Aug/19 03:01;ykt836;[~vthinkxie] Thanks, I will assign this to you.;;;","06/Sep/19 03:07;ykt836;merged in master: 60cf41ab6b9309979849e3310b4c32cf528a1568;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelimitedInputFormat index error on multi-byte delimiters with whole file input splits,FLINK-13589,13248963,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,arvid,aeckstein,aeckstein,05/Aug/19 20:58,22/Jun/21 14:04,13/Jul/23 08:10,17/Dec/19 02:18,1.8.1,,,,,,,,,1.10.0,1.9.2,,,Connectors / FileSystem,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,"The DelimitedInputFormat can drops bytes when using input splits that have a length of -1 (for reading the whole file).  It looks like this is a simple bug in handing the delimiter on buffer boundaries where the logic is inconsistent for different split types.

Attached is a possible patch with fix and test.

 ",,aeckstein,arvid heise,zjwang,,,,,,,,,,,"AHeise commented on pull request #10573: [FLINK-13589] Fixing DelimitedInputFormat for whole file input splits.
URL: https://github.com/apache/flink/pull/10573
 
 
   The DelimitedInputFormat drops bytes when using whole file input splits.
   This commit adjusts the whole file input split logic to regular input
   splits.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   The DelimitedInputFormat drops bytes when using whole file input splits. This commit adjusts the whole file input split logic to regular input splits.
   
   The bug was reported and a fix was provided by Adric Eckstein.
   
   ## Brief change log
   
     - Adjusts the whole file input split logic to regular input splits.
   
   ## Verifying this change
   
     - Replicated testDelimiterOnBufferBoundary for whole file input splits.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 14:29;githubbot;600","AHeise commented on pull request #10574: [FLINK-13589] Fixing DelimitedInputFormat for whole file input splits.
URL: https://github.com/apache/flink/pull/10574
 
 
   Unchanged backport from https://github.com/apache/flink/pull/10573.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 14:37;githubbot;600","zhijiangW commented on pull request #10573: [FLINK-13589] Fixing DelimitedInputFormat for whole file input splits.
URL: https://github.com/apache/flink/pull/10573
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 14:24;githubbot;600","AHeise commented on pull request #10599: [FLINK-13589][1.10] Fixing DelimitedInputFormat for whole file input splits.
URL: https://github.com/apache/flink/pull/10599
 
 
   1.10 Backport of https://github.com/apache/flink/pull/10573 . No changes.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 15:35;githubbot;600","zhijiangW commented on pull request #10599: [FLINK-13589][1.10] Fixing DelimitedInputFormat for whole file input splits.
URL: https://github.com/apache/flink/pull/10599
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 02:14;githubbot;600","zhijiangW commented on pull request #10574: [FLINK-13589][1.9] Fixing DelimitedInputFormat for whole file input splits.
URL: https://github.com/apache/flink/pull/10574
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 02:15;githubbot;600",,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/19 20:57;aeckstein;delimiter-bug.patch;https://issues.apache.org/jira/secure/attachment/12976756/delimiter-bug.patch",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 17 02:18:38 UTC 2019,,,,,,,,,,"0|z05cjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/19 14:17;arvid;Thank you very much for the bug report and the patch. I will incorporate it for the upcoming release.;;;","17/Dec/19 02:18;zjwang;Merged in master: 0bd083e5eeb5eb5adeddfbe3a9928860f3b4a6eb

Merged in release-1.9: db531e79807acba1ba28d9922bfed912fd78dd03

Merged in release-1.10: 1e716e4a43018caeb77beaa5d8f16cedfedbd887;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTask.handleAsyncException throws away the exception cause,FLINK-13588,13248949,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,johnlon,johnlon,johnlon,05/Aug/19 18:58,12/Nov/21 22:47,13/Jul/23 08:10,21/Aug/19 06:14,1.8.1,,,,,,,,,1.10.0,1.9.1,,,Runtime / Task,,,,,0,pull-request-available,,,,"Code below throws the reason 'message' away making it hard to diagnose why a split has failed for instance.

 
{code:java}

https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L909

@Override
	public void handleAsyncException(String message, Throwable exception) {
	if (isRunning) {
	// only fail if the task is still running
	getEnvironment().failExternally(exception);
	}
}{code}
 

Need to pass the message through so that we see it in logs please.

 ",,trohrmann,yanghua,,,,,,,,,,,,"Johnlon commented on pull request #9456: FLINK-13588 flink-streaming-java don't throw away exception info in logging 
URL: https://github.com/apache/flink/pull/9456
 
 
   Previously the async error handler threw away the descriptive text provided by the call site. This makes diagnosis of errors really difficult. 
   
   ## Brief change log
   
   This change wraps the error message and cause exception into a wrapper exception that properly conveys the descriptive text to the logs.
   
   ## Verifying this change
   
   Test added to StreamTaskTest to verify that objects are passed correctly to the Environment object and also to verify that the toString rendering includes the given text.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no )
     - The runtime per-record code paths (performance sensitive): (no )
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 22:46;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 21 06:14:24 UTC 2019,,,,,,,,,,"0|z05cgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 12:00;yanghua;[~johnlon] I think your opinion is reasonable. I'd like to fix this issue. cc [~till.rohrmann];;;","07/Aug/19 09:30;trohrmann;I think the rationale is that exceptions which occur after the task has stopped running should no longer be relevant and hence don't need to be logged. Exceptions could occur when shutting down the task, for example. This is expected and should not be reported. Does this make sense [~johnlon]?;;;","08/Aug/19 20:04;johnlon;Don't agree.

If there is context then the code should not throw it away. Principal.

Without the exception message I cannot discover why the split failed.

For example we had a failure because of a zero byte avro file in hdfs. The
error message had the filename in it but the code throws it away.

As a result we had to write and run a separate trivial job that brute
forced reading all the files (100k) without flinks help.

The change is justified.
I don't think it's reasonable to throw this away. It looks like the error
handling /logging is a bit inconsistent for sure.

We are now running with a modified version of this class that wraps the
original exception into a runtime exception that includes the cause text.







;;;","14/Aug/19 09:12;trohrmann;Ah I think I misunderstood the problem. You are proposing to do the following:

{code}
public void handleAsyncException(String message, Throwable exception) {
  if (isRunning) {
    getEnvironment().failExternally(new AsyncFlinkException(message, exception));
  }
}
{code}
?

If that is the case, then I can assign you to this issue and you could then open a PR to fix it.;;;","14/Aug/19 09:50;johnlon;Hi yes that's how we""fixed"" it because it limits the change to a one liner.



;;;","14/Aug/19 10:13;trohrmann;Great, I've assigned you to this issue [~johnlon]. Now you could open a PR for this fix. Ping me and I'll review it. If you don't have time for this, then let me know and I'll do it.;;;","15/Aug/19 22:48;johnlon;See [https://github.com/apache/flink/pull/9456]

Hi done the work including test - trivial change.

Unfortunately I cannot verify the test as I couldn't work out how to make the existing build including tests on master run to completion without tests hanging for ages, and loads of errors.

I am using Java 8 221

Tried maven 3.1.1 and 3.2.5

No idea how to fix.

The following works but doesn't run tests ....

{{mvn clean package -DskipTests # this will take up to 10 minutes}}

----

Also couldn't run test in IntelliJ getting error 

Error:java: invalid flag: --add-exports=java.base/sun.net.util=ALL-UNNAMED;;;","16/Aug/19 08:20;trohrmann;Thanks for opening ta PR [~johnlon]. I'll take a look at the PR.

Just curious, where was the build hanging exactly?;;;","21/Aug/19 06:14;trohrmann;Fixed via

1.10.0: 8935ebf1b1f36109e3b1c48a5aba8e63f9df263a
1.9.1: 6f7d62528eba6387dae20f8e1678af2fd36a46c6;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix some transformation names are not set in blink planner,FLINK-13587,13248892,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,05/Aug/19 14:36,09/Aug/19 06:41,13/Jul/23 08:10,09/Aug/19 06:41,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Currently, there are some transformation names are not set in blink planner. For example, LookupJoin transformation uses ""LookupJoin"" directly which loses a lot of informatoion.",,jark,,,,,,,,,,,,,"wuchong commented on pull request #9363: [FLINK-13587][table-planner-blink] Fix some operator names are not set in blink planner
URL: https://github.com/apache/flink/pull/9363
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently, there are some transformation names are not set in blink planner. For example, LookupJoin transformation uses ""LookupJoin"" directly which loses a lot of informatoion.
   
   ## Brief change log
   
   1. Introduces a RelWriter `RelDisplayNameWriterImpl` to reuse code of ""explainTerms"" to generate operator names
   2. Fix some operator names are not set in blink planner
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 14:41;githubbot;600","wuchong commented on pull request #9363: [FLINK-13587][table-planner-blink] Fix some operator names are not set in blink planner
URL: https://github.com/apache/flink/pull/9363
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 06:40;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 09 06:41:12 UTC 2019,,,,,,,,,,"0|z05c3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/19 06:41;jark;[table-planner-blink] Fix some operator names are not set in blink planner
 - master: bf37130d34d762d4ebbfc594ad1a52e63feae71a
 - 1.9: 4517949700b0401923eaa0bd6ec6704f05b7bdf3

[table-planner-blink] Introduces a framework to reuse code of ""explainTerms"" to generate operator names
 - master: c649c8bef5a457f49508382e77b923b2e0da643c
 - 1.9: 0c5d0a112e8c0e5a6c5bef00ca90f263cbabe902
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Method ClosureCleaner.clean broke backward compatibility between 1.8.0 and 1.8.1,FLINK-13586,13248889,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aljoscha,gael,gael,05/Aug/19 14:29,02/Oct/19 17:42,13/Jul/23 08:10,04/Sep/19 10:01,1.10.0,1.8.1,1.9.0,,,,,,,1.8.2,,,,API / DataStream,,,,,0,pull-request-available,,,,"Method clean in org.apache.flink.api.java.ClosureCleaner received a new parameter in Flink 1.8.1. This class is noted as internal, but is used in the Kafka connectors (in org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerBase).

The Kafka connectors library is not provided by the server, and must be set up as a dependency with compile scope (see https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kafka.html#usage, or the Maven project template). Any project using those connectors and built with 1.8.0 cannot be deployed on a 1.8.1 Flink server, because it would target the old method.

=> This methods needs a fallback with the original two arguments (setting a default value of RECURSIVE for the level argument).",,aitozi,aljoscha,andrew_lin,david.artiga,gael,gyfora,jark,peizhouyu,sewen,trohrmann,,,,"aljoscha commented on pull request #9595: [FLINK-13586] Make ClosureCleaner.clean() backwards compatible with 1.8.0
URL: https://github.com/apache/flink/pull/9595
 
 
   ## What is the purpose of the change
   
   We revert earlier changes that make (among other things) connectors incompatible between different 1.8.x versions. See https://issues.apache.org/jira/browse/FLINK-13586 for a more thorough description
   
   
   ## Brief change log
   
    - introduce a new method `ClosureCleaner.clean()` that has the same signature as the 1.8.0 method.
    - change call sites in connectors to use the ""new"" method
   
   
   ## Verifying this change
   
    - This is covered by existing tests, the default of `RECURSIVE` is still in place
    - actually verifying that we don't break binary compatibility in this way again is prohibitively difficult, IMO
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Technically, this does affect public API but only because the connectors internally use `ClosureCleaner`
   
   ## Documentation
   
     - No new feature, and no documentation
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 08:44;githubbot;600","asfgit commented on pull request #9595: [FLINK-13586] Make ClosureCleaner.clean() backwards compatible with 1.8.0
URL: https://github.com/apache/flink/pull/9595
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 10:00;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-12297,,,,,,,FLINK-13696,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 04 10:01:44 UTC 2019,,,,,,,,,,"0|z05c34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/19 15:12;victor-wong;Hi Gaël, could you assign this ticket to me, so I can come up with a PR following the suggestion in the ticket.;;;","05/Aug/19 15:35;gael;Hi Victor, I don't seem to have the permissions for that, unfortunately.;;;","06/Aug/19 10:00;aljoscha;I think we first need to discuss whether this is breaking backwards compatibility. Going down this possible rabbit hole, where code that is internally used by public code has to be considered we could get into complicated territory. ;;;","06/Aug/19 10:02;sewen;Can we add another method that supplies the old default value for the level, just so that linking from old code works again?;;;","06/Aug/19 12:17;gael;[~aljoscha] As far as I know, the Flink connectors are maintained by the Flink team as well, am I wrong ? If this is the case, it's not exactly being used by public code as much as having a compatibility problem with another part of Flink, which is packaged with existing jobs. From an outside point of view, I'm using the connectors as intended, and trying to update the server to 1.8.1 broke existing jobs. It's pretty harsh for a patch version.;;;","06/Aug/19 14:14;victor-wong;I think it would be helpful to attach [mail discussion|https://lists.apache.org/thread.html/56f137a4c0b90eb94a73b7a2ab95453282293cf9c81107e9bedc658a@%3Cuser.flink.apache.org%3E] here.  

A user ran into `java.lang.NoSuchMethodError` when trying to submit an application packaged with flink 1.8.1 to flink cluster 1.8.0.  
{code:java}
...
Caused by: java.lang.NoSuchMethodError: org.apache.flink.api.java.ClosureCleaner.clean(Ljava/lang/Object;Lorg/apache/flink/api/common/ExecutionConfig$ClosureCleanerLevel;Z)V
at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011.<init>(FlinkKafkaProducer011.java:494)
...{code};;;","08/Aug/19 08:57;sewen;Looking at the changes in more detail, I think there was a mistake in the PR that the additional parameter was forced into every call site.

Call sited should have stayed the same, by keeping the original method (and semantics) and only adding the level in a new method.

We can do the following now:
  -  Add the method {{ClosureCleaner.clean(Object, boolean)}} and change all call sites in libraries and connectors to call that method.
  - As for which value we supply for the level there, we have two options:
   1.  {{TOP_LEVEL}} which makes this compatible with 1.8.0
   2.  {{RECURSIVE}} which makes this compatible with 1.8.1

Tough choice.

I don't understand how this got merged into 1.8.1 in the first place, with changing the semantics from flat cleaning to recursive cleaning.
That is a change in behavior that should not happen in a bugfix release.
 ;;;","08/Aug/19 09:29;sewen;My first thought would be to go with option (2).;;;","02/Sep/19 08:44;aljoscha;[~sewen] I cut a PR for this, I think it implements your proposal.;;;","04/Sep/19 10:01;aljoscha;Resolved on release-1.8 in
655c78f31b45ec3a96e53e269fca64b4682025ff;;;",,,,,,,,,,,,,,,,,,,,,,
Fix sporadical deallock in TaskAsyncCallTest#testSetsUserCodeClassLoader(),FLINK-13585,13248873,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,SleePy,gjy,gjy,05/Aug/19 12:50,29/Sep/19 09:37,13/Jul/23 08:10,14/Aug/19 14:56,1.10.0,,,,,,,,,1.10.0,1.9.1,,,Runtime / Task,Tests,,,,0,test-stability,,,,"{{TaskAsyncCallTest#testSetsUserCodeClassLoader() deadlocks}} sporadically. Commit 1ad16bc252f1d3502a29ddb2081fdfdf3436cc55.

*Stacktrace:*
{noformat}

""main"" #1 prio=5 os_prio=0 tid=0x00007f5e6000b800 nid=0x49bd in Object.wait() [0x00007f5e67e0b000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008f450af8> (a java.lang.Object)
	at java.lang.Object.wait(Object.java:502)
	at org.apache.flink.core.testutils.OneShotLatch.await(OneShotLatch.java:63)
	- locked <0x000000008f450af8> (a java.lang.Object)
	at org.apache.flink.runtime.taskmanager.TaskAsyncCallTest.testSetsUserCodeClassLoader(TaskAsyncCallTest.java:201)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",,gjy,SleePy,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/19 12:47;gjy;log.txt;https://issues.apache.org/jira/secure/attachment/12976694/log.txt",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 14 14:56:21 UTC 2019,,,,,,,,,,"0|z05bzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/19 08:31;SleePy;Another instance, https://travis-ci.com/flink-ci/flink/jobs/223867020.;;;","14/Aug/19 13:13;SleePy;Hi [~gjy], I think I have found the cause of the unstable case. I have attached a PR to fix it. That would be nice if you could assign this ticket to me and review the PR :);;;","14/Aug/19 14:56;trohrmann;Fixed via

1.10.0: f5fb2c036e69870c45c3733fe5ea4ead1a52d918
1.9.1: c87dcac6fb6ed99faae38108023f8633e7a0f255;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RankLikeAggFunctionBase should respect type to construct literal,FLINK-13584,13248863,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,05/Aug/19 11:35,06/Aug/19 07:25,13/Jul/23 08:10,06/Aug/19 04:20,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Should invoke ""valueLiteral(value, fromLogicalTypeToDataType(orderType))"" instead of valueLiteral(value). Previous way will lose precision information of decimal type.",,jark,lzljs3620320,,,,,,,,,,,,"JingsongLi commented on pull request #9360: [FLINK-13584][table-planner-blink] RankLikeAggFunctionBase should respect type to construct literal
URL: https://github.com/apache/flink/pull/9360
 
 
   
   ## What is the purpose of the change
   
   Should invoke ""valueLiteral(value, fromLogicalTypeToDataType(orderType))"" instead of valueLiteral(value). Previous way will lose precision information of decimal type.
   
   ## Verifying this change
   
   OverWindowITCase.testRankByDecimal
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 12:40;githubbot;600","asfgit commented on pull request #9360: [FLINK-13584][table-planner-blink] RankLikeAggFunctionBase should respect type to construct literal
URL: https://github.com/apache/flink/pull/9360
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/19 04:22;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 06 04:20:47 UTC 2019,,,,,,,,,,"0|z05bxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 04:20;jark;Fixed in 1.10.0: 57f1ba2aaaff67a2d606928cb489ed43014a5318
Fixed in 1.9.0: 1983e6f8cae2b2a5ca0c52f129f9756d0ecd91b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchFineGrainedRecoveryITCase failed on Travis,FLINK-13581,13248840,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,azagrebin,azagrebin,05/Aug/19 09:08,08/Aug/19 12:13,13/Jul/23 08:10,08/Aug/19 12:12,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,[https://travis-ci.com/flink-ci/flink/jobs/221567908],,azagrebin,kkl0u,trohrmann,,,,,,,,,,,"azagrebin commented on pull request #9374: [FLINK-13581][coordination][tests] Harden BatchFineGrainedRecoveryITCase
URL: https://github.com/apache/flink/pull/9374
 
 
   ## What is the purpose of the change
   
   If counting of mapper restarts in `BatchFineGrainedRecoveryITCase` is based on the open method of user function, the fact of the restart depends on internal implementation of the local Task and whether the open method is eventually called.
   
   If execution attempt numbers are used instead, the test behaviour is more stable because it depends only on coordination. The execution attempt numbers can be queried from the REST client of the testing mini cluster.
   
   ## Brief change log
   
     - Introduce `MiniClusterClient` for `BatchFineGrainedRecoveryITCase` to query task attempts
     - Use mapper task attempt numbers instead of user function open method call counters
   
   ## Verifying this change
   
   Run `BatchFineGrainedRecoveryITCase`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/19 15:01;githubbot;600","tillrohrmann commented on pull request #9374: [FLINK-13581][coordination][tests] Harden BatchFineGrainedRecoveryITCase
URL: https://github.com/apache/flink/pull/9374
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Aug/19 12:13;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 08 12:12:39 UTC 2019,,,,,,,,,,"0|z05bs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 05:47;trohrmann;Any insights on why this test is failing [~azagrebin]?;;;","06/Aug/19 06:35;azagrebin;[~till.rohrmann]

According to logs, I think the reason is the following. Task managers of the mini cluster are started with a random data port. All TMs have the same local host.

In this particular run the first TM and the second one were started accidentally with the same port. When the failure of the first TM was generated and the second TM was started, the partition request from the redeployed consuming mapper came to the second TM because of the accidental port match. This led to a partition-not-found response as the partition resided on the first TM and subsequently led to an async re-triggering of the partition request. In this case task proceeds with the user code. Therefore, this was counted as one extra mapper restart in the open method.

Usually, the random data ports always differ for each subsequently started TM after failing the previous one. This always leads to a connection failure of the first partition request and fails the whole Task without entering the user code. The user code is not entered because the partition request was recently moved to gate setup before invoking the user code. Eventually mapper restart is not counted in normal case.;;;","06/Aug/19 08:48;trohrmann;So if we counted the restart attempts not inside the user code but via the exposed REST handler, this problem would not occur?;;;","06/Aug/19 12:14;azagrebin;Yes, this problem would be solved.

The execution attempts are incremented a lot w/o running user code. The final counters will of course differ comparing to the whole job restart w/o fine grained recovery. My concern here was always that we do not check what the user observes. On the other hand, test should be more stable and will not depend on internal details of when the the user code is touched.

The other solutions around stabilising the port difference among all TMs are probably more involved with relying on the mini cluster internals and might be fragile.;;;","06/Aug/19 16:08;kkl0u;I assume this is another instance of the same issue https://api.travis-ci.org/v3/job/568356591/log.txt;;;","08/Aug/19 07:14;trohrmann;Unblocking this issue since it is a test instability and not a bug of the feature.;;;","08/Aug/19 12:12;trohrmann;Fixed via

1.10.0: c24b9a371658c2829f0ff1cedb9876592dc15b8d
1.9.0: 331878aef276ad388c8a9ed55a4eb8c4cc7e9adc;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Failed launching standalone cluster due to improper configured irrelevant config options for active mode.,FLINK-13579,13248825,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,xtsong,xtsong,05/Aug/19 07:54,05/Aug/19 19:03,13/Jul/23 08:10,05/Aug/19 19:02,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,,,tison,trohrmann,xtsong,,,,,,,,,,,"tillrohrmann commented on pull request #9357: [FLINK-13579] Only set managed memory when starting an active RM
URL: https://github.com/apache/flink/pull/9357
 
 
   ## What is the purpose of the change
   
   Introduce ActiveResourceManagerFactory which encapsulates the logic to set relevant
   configuration values for the active ResourceManager implementations (e.g. managed
   memory).
   
   Let existing active ResourceManager factory implementations extend ActiveResourceManagerFactory.
   
   Add ActiveResourceManagerFactoryTest to ensure that active ResourceManager relevant
   configuration values are set.
   
   Add StandaloneResourceManagerFactoryTest to ensure that a standalone ResourceManager can be
   started if the memory size is configured to be less than the containered min cutoff size.
   
   ## Verifying this change
   
   - Added `ActiveResourceManagerFactoryTest` and `StandaloneResourceManagerFactoryTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 10:02;githubbot;600","tillrohrmann commented on pull request #9357: [FLINK-13579] Only set managed memory when starting an active RM
URL: https://github.com/apache/flink/pull/9357
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 19:03;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-13241,,FLINK-13489,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 05 19:02:36 UTC 2019,,,,,,,,,,"0|z05bow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/19 08:19;tison;[~xintongsong] are there any more details? From the title it's hard to see what happened exactly.;;;","05/Aug/19 09:01;trohrmann;The problem is that we always try to update the {{ResourceManager}} configuration independent whether we are trying to start the active or the standalone/reactive mode. The update method fails if some configuration parameters are not set which is the case for the standalone case: https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/entrypoint/component/AbstractDispatcherResourceManagerComponentFactory.java#L171;;;","05/Aug/19 09:02;trohrmann;I think we should fix the problem by moving the {{ResourceManager}} update procedure into an {{ActiveResourceManagerFactory}} which is extended by the {{YarnResourceManagerFactory}} and {{MesosResourceManagerFactory}}.;;;","05/Aug/19 19:02;trohrmann;Fixed via

1.10.0: 634a634bb21b50649b3a819bf73f179d5cbe7931
1.9.0: 789c605d16b97222f2e0213ac5ad60da50901d09;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
DDL create table doesn't allow STRING data type,FLINK-13568,13248790,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,danny0405,xuefuz,xuefuz,04/Aug/19 22:16,02/Sep/19 11:44,13/Jul/23 08:10,02/Sep/19 11:44,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Table SQL / API,,,,,0,pull-request-available,,,,"Creating a table with ""string"" data type fails with tableEnv.sqlUpdate().",,jark,lirui,twalthr,xuefuz,,,,,,,,,,"danny0405 commented on pull request #9354: [FLINK-13568][sqk-parser] DDL create table doesn't allow STRING data …
URL: https://github.com/apache/flink/pull/9354
 
 
   ## What is the purpose of the change
   
   After FLINK-13335, we have supported full data types in SQL CREATE TABLE, but the `SqlCreateTable` validation breaks this, this patch remove the `SqlColumnType` because we would check the data types in the `SqlValidator`.
   
   ## Brief change log
   
     - Remove the `SqlColumnType`
     - Change the tests of DDL varchar type to string
   
   
   ## Verifying this change
   
   See tests in CatalogTableITCase
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (no),
     - Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 04:33;githubbot;600","asfgit commented on pull request #9354: [FLINK-13568][sql-parser] DDL create table doesn't allow STRING data …
URL: https://github.com/apache/flink/pull/9354
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 11:43;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-13571,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 02 11:44:06 UTC 2019,,,,,,,,,,"0|z05bh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/19 11:44;jark;1.10.0: 6094a52abea4caa3914c807dee40cf9eadbe63a0
1.9.1: 1f0c67087f19b10d1218af30282414f8b032f2a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avro Confluent Schema Registry nightly end-to-end test failed on Travis,FLINK-13567,13248781,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,trohrmann,trohrmann,04/Aug/19 20:36,27/Dec/19 12:35,13/Jul/23 08:10,19/Dec/19 12:29,1.10.0,1.8.2,1.9.0,,,,,,,1.10.0,,,,Connectors / Kafka,Tests,,,,0,pull-request-available,test-stability,,,"The {{Avro Confluent Schema Registry nightly end-to-end test}} failed on Travis with

{code}
[FAIL] 'Avro Confluent Schema Registry nightly end-to-end test' failed after 2 minutes and 11 seconds! Test exited with exit code 1

No taskexecutor daemon (pid: 29044) is running anymore on travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.
No standalonesession daemon to stop on host travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.
rm: cannot remove '/home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/plugins': No such file or directory
{code}

https://api.travis-ci.org/v3/job/567273939/log.txt",,aljoscha,dwysakowicz,gjy,klion26,liyu,tison,trohrmann,,,,,,,"zentol commented on pull request #10544: [FLINK-13567][e2e] Harden schema registry test
URL: https://github.com/apache/flink/pull/10544
 
 
   Hardens the schema registry test by retrying the setup of kafka/ZK/registry . For this the existing `retry_times` function was extended to optionally include a cleanup command; in this case for shutting down previously started processes.
   Additionally, if kafka isn't running when shutting it down we now dump the kafka logs to ease debugging.
   
   The last commit in this PR flags the schema test as a pre-commit tests for demonstration purposes.
   
   The exact failure cause of the test is still unknown; what we do know however is that kafka broke down after being successfully started.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 10:27;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,FLINK-13663,FLINK-10885,FLINK-15428,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 26 08:46:13 UTC 2019,,,,,,,,,,"0|z004b8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/19 08:24;dwysakowicz;I just checked that this a transient problem with downloading kafka 0.10 from apache archives. I think we can unblock the release from this issue. We can hopefully get rid of those kind of issues when we migrate to the new e2e framework with a proper caching mechanism.;;;","06/Aug/19 05:55;trohrmann;Another instance where we cannot download the kafka binaries: https://api.travis-ci.org/v3/job/567832405/log.txt;;;","09/Aug/19 07:14;trohrmann;Another instance: https://api.travis-ci.org/v3/job/569297594/log.txt;;;","09/Aug/19 07:15;trohrmann;Might be the same problem. [~azagrebin] would the retry logic which you develop for FLINK-13599 help here as well?;;;","13/Sep/19 07:58;trohrmann;Could be another instance in the {{release-1.8}} branch: https://api.travis-ci.org/v3/job/584114230/log.txt;;;","09/Oct/19 07:46;liyu;Another two instances:
https://api.travis-ci.org/v3/job/594564466/log.txt
https://api.travis-ci.org/v3/job/595083514/log.txt;;;","10/Oct/19 02:59;liyu;Another instance: https://api.travis-ci.org/v3/job/595592185/log.txt

It seems to happen stably recently.;;;","10/Oct/19 14:52;trohrmann;Would it make sense to disable this test for the moment and make this issue a blocker for 1.10? Given the frequency of the test failure I would be in favour of this.;;;","11/Oct/19 07:22;liyu;bq. Would it make sense to disable this test for the moment and make this issue a blocker for 1.10?
+1, marking this as a blocker makes sense according to the failure frequency.;;;","11/Oct/19 09:30;trohrmann;I've disabled the test in {{master}} via 2c2095bdad3d47f27973a585112ed820f457de6f until the problem has been fixed.;;;","16/Oct/19 16:25;aljoscha;These are actually two separate issues, the original log (and one other log) show failures because Kafka could not be downloaded. The later failures happen because the schema registry could not be started successfully.;;;","23/Oct/19 08:35;gjy;[~aljoscha] Did you open a new ticket?;;;","23/Oct/19 09:25;aljoscha;No, since both might be because of network problems or some such. I haven't investigated further.;;;","10/Dec/19 09:20;aljoscha;This didn't happen in quite a while so I'm closing it for now, please re-open if it re-appears.;;;","10/Dec/19 09:36;gjy;The test should be uncommented if we are confident that it works, [~aljoscha]. See 2c2095bdad3d47f27973a585112ed820f457de6f;;;","10/Dec/19 13:35;aljoscha;Ah damn, I didn't remember that they're actually disabled...;;;","19/Dec/19 12:29;trohrmann;Hardened via

master:
bdd432a09a776fec88f465771101893a8151dc1d
d2f3b27d0625e39092a49db1895a6fca4522eb7c
feb68d8863bb043597b8ffa1f3800ca0c6a6fc71
3d8fe9c9bf98d21446c8e7646b2db59fbceca83b

1.10.0:
e5e6211516a478849404d18fc366503abb0e7cc9
03e8b847e17d76a50682ae1a5bb1dd90e0ee5d35
1af3a022849ad9e30c656bccad84879809be24b8
fe76d10573e6c12701f2eae2685a12d73090c981;;;","26/Dec/19 08:46;liyu;It seems the issue occurs again: https://api.travis-ci.com/v3/job/270219446/log.txt;;;",,,,,,,,,,,,,,
blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows ,FLINK-13564,13248707,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,03/Aug/19 09:41,27/Aug/19 04:20,13/Jul/23 08:10,22/Aug/19 06:34,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"just as [FLINK-11017|https://issues.apache.org/jira/browse/FLINK-11017], blink planner should also throw exception if constant with YEAR TO MONTH resolution was used for group windows ",,godfreyhe,jark,,,,,,,,,,,,"godfreyhe commented on pull request #9349: [FLINK-13564] [table-planner-blink] throw exception if constant with YEAR TO MONTH resolution was used for group windows
URL: https://github.com/apache/flink/pull/9349
 
 
   
   
   ## What is the purpose of the change
   
   *Checking if a proper resolution was used for group window parameter. Without this check INTERVAL '2-10' YEAR TO MONTH would be a valid window size that would be translated to 34 milliseconds window.*
   
   
   ## Brief change log
   
     - *add check in StreamLogicalWindowAggregateRule*
   
   
   ## Verifying this change
   
   org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest#testWindowWrongWindowParameter
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Aug/19 09:54;githubbot;600","asfgit commented on pull request #9349: [FLINK-13564] [table-planner-blink] throw exception if constant with YEAR TO MONTH resolution was used for group windows
URL: https://github.com/apache/flink/pull/9349
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 06:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 22 06:34:02 UTC 2019,,,,,,,,,,"0|z05ayo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/19 09:42;godfreyhe;i would like to fix this;;;","22/Aug/19 06:34;jark;master: 845587232736b01bcad3c7a87f94a570842f011b

1.9.1: a106738721edd2f7853605ac68f6bb16e1d817b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TumblingGroupWindow should implement toString method,FLINK-13563,13248697,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,03/Aug/19 05:12,27/Aug/19 04:07,13/Jul/23 08:10,12/Aug/19 02:58,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"{code:scala}
  @Test
  def testAllEventTimeTumblingGroupWindowOverTime(): Unit = {
    val util = streamTestUtil()
    val table = util.addDataStream[(Long, Int, String)](
      ""T1"", 'long, 'int, 'string, 'rowtime.rowtime)

    val windowedTable = table
      .window(Tumble over 5.millis on 'rowtime as 'w)
      .groupBy('w)
      .select('int.count)
    util.verifyPlan(windowedTable)
  }
{code}

currently, it's physical plan is 

{code:java}
HashWindowAggregate(window=[TumblingGroupWindow], select=[Final_COUNT(count$0) AS EXPR$0])
+- Exchange(distribution=[single])
   +- LocalHashWindowAggregate(window=[TumblingGroupWindow], select=[Partial_COUNT(int) AS count$0])
      +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])
{code}

we know nothing about the TumblingGroupWindow except its name. the expected plan is

{code:java}
HashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Final_COUNT(count$0) AS EXPR$0])
+- Exchange(distribution=[single])
   +- LocalHashWindowAggregate(window=[TumblingGroupWindow('w, long, 5)], select=[Partial_COUNT(int) AS count$0])
      +- TableSourceScan(table=[[default_catalog, default_database, Table1, source: [TestTableSource(long, int, string)]]], fields=[long, int, string])

{code}

",,godfreyhe,jark,twalthr,,,,,,,,,,,"godfreyhe commented on pull request #9347: [FLINK-13563] [table-planner-blink] TumblingGroupWindow should implement toString method to explain more info
URL: https://github.com/apache/flink/pull/9347
 
 
   
   
   ## What is the purpose of the change
   
   *TumblingGroupWindow should implement toString method to explain more info*
   
   
   ## Brief change log
   
     - *add toString method for TumblingGroupWindow*
   
   
   ## Verifying this change
   
   
   
   This change is already covered by existing tests
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Aug/19 05:18;githubbot;600","asfgit commented on pull request #9347: [FLINK-13563] [table-planner-blink] TumblingGroupWindow should implement toString method to explain more info
URL: https://github.com/apache/flink/pull/9347
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 02:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 15 09:55:41 UTC 2019,,,,,,,,,,"0|z05awg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/19 05:13;godfreyhe;i would like to fix this;;;","12/Aug/19 02:58;jark;master: 0cda582487372d3f2717cb4fc7b0f8f818e18d03
1.9.1:  d74f35216a95d502a942e7492098e0663931e3a3;;;","14/Aug/19 14:24;twalthr;[~jark] we should watch out what we currently merge to the 1.9 branch as it looks like the release is getting closer. This contribution changes the plan representation and thus could cause user programs to fail if upgraded to Flink 1.9.1. We should not merge things that change the plan representation.;;;","15/Aug/19 02:36;jark;Hi [~twalthr], thanks for the reminder. I don't quite understand why this will cause user programs to fail. This doesn't change the structure or content of physical/logical plan. This only changes the explanation. I think it is compatible with 1.9.0.;;;","15/Aug/19 09:33;twalthr;Hi [~jark], I just saw that this PR touches also plan tests and wanted to note that to be on the safe-side. Thanks for the explanation.;;;","15/Aug/19 09:55;jark;OK. I checked the plan tests and this change only affected the plan displaying. The {{testReturnTypeInferenceForWindowAgg}} changes in XML is because we re-generate the XML and the entry position is re-organized.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
throws exception when FlinkRelMdColumnInterval meets two stage stream group aggregate,FLINK-13562,13248696,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,godfreyhe,godfreyhe,godfreyhe,03/Aug/19 04:46,27/Aug/19 04:07,13/Jul/23 08:10,12/Aug/19 02:02,,,,,,,,,,1.10.0,1.9.1,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"test case:

{code:scala}
  @Test
  def testTwoDistinctAggregateWithNonDistinctAgg(): Unit = {
    util.addTableSource[(Int, Long, String)](""MyTable"", 'a, 'b, 'c)
    util.verifyPlan(""SELECT c, SUM(DISTINCT a), SUM(a), COUNT(DISTINCT b) FROM MyTable GROUP BY c"")
  }
{code}



org.apache.flink.table.api.TableException: Sum aggregate function does not support type: ''VARCHAR''.
Please re-check the data type.

	at org.apache.flink.table.planner.plan.utils.AggFunctionFactory.createSumAggFunction(AggFunctionFactory.scala:191)
	at org.apache.flink.table.planner.plan.utils.AggFunctionFactory.createAggFunction(AggFunctionFactory.scala:74)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$$anonfun$9.apply(AggregateUtil.scala:285)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$$anonfun$9.apply(AggregateUtil.scala:279)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$.transformToAggregateInfoList(AggregateUtil.scala:279)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$.getOutputIndexToAggCallIndexMap(AggregateUtil.scala:154)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getAggCallIndexInLocalAgg$1(FlinkRelMdColumnInterval.scala:504)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.estimateColumnIntervalOfAggregate(FlinkRelMdColumnInterval.scala:526)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMdColumnInterval.getColumnInterval(FlinkRelMdColumnInterval.scala:417)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval_$(Unknown Source)
	at GeneratedMetadataHandler_ColumnInterval.getColumnInterval(Unknown Source)
	at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.getColumnInterval(FlinkRelMetadataQuery.java:122)",,godfreyhe,jark,,,,,,,,,,,,"godfreyhe commented on pull request #9346: [FLINK-13562] [table-planner-blink] fix incorrect input type for local stream group aggregate in FlinkRelMdColumnInterval
URL: https://github.com/apache/flink/pull/9346
 
 
   …l stream group aggregate in FlinkRelMdColumnInterval
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
   ## What is the purpose of the change
   
   *fix incorrect input type for local stream group aggregate in FlinkRelMdColumnInterval*
   
   
   ## Brief change log
   
     - *fix incorrect input type for local stream group aggregate in FlinkRelMdColumnInterval*
   
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
     - *add new tests to verify the bug*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Aug/19 05:04;githubbot;600","asfgit commented on pull request #9346: [FLINK-13562] [table-planner-blink] fix incorrect input type for local stream group aggregate in FlinkRelMdColumnInterval
URL: https://github.com/apache/flink/pull/9346
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 02:02;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 12 02:02:35 UTC 2019,,,,,,,,,,"0|z05aw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/19 04:47;godfreyhe;i would like to fix this;;;","12/Aug/19 02:02;jark;master: 0b65aeaede36f0bc706dfdb82f039352b15069e9
1.9.1: bca6330fce03bc177ca48f142d786ce2e79c6c45;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to query Hive table with char or varchar columns,FLINK-13549,13248526,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lirui,lirui,02/Aug/19 08:10,14/Aug/19 05:44,13/Jul/23 08:10,14/Aug/19 05:44,,,,,,,,,,,,,,Connectors / Hive,,,,,0,,,,,,,lirui,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13534,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 14 05:39:49 UTC 2019,,,,,,,,,,"0|z059ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/19 08:24;lzljs3620320;Hi [~lirui]

In flink planner, do not support varchar(non-max)

In blink planner, It's supportive.;;;","14/Aug/19 05:39;lirui;Seems FLINK-13534 also fixed the issue here. Closing this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JoinToMultiJoinRule should not match SEMI/ANTI LogicalJoin,FLINK-13545,13248474,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,02/Aug/19 03:31,08/Aug/19 13:22,13/Jul/23 08:10,08/Aug/19 13:22,1.10.0,1.9.0,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"run tpcds 14.a on blink planner, an exception will thrown

java.lang.ArrayIndexOutOfBoundsException: 84

	at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:564)
	at org.apache.calcite.rel.rules.JoinToMultiJoinRule$InputReferenceCounter.visitInputRef(JoinToMultiJoinRule.java:555)
	at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112)
	at org.apache.calcite.rex.RexVisitorImpl.visitCall(RexVisitorImpl.java:80)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:191)
	at org.apache.calcite.rel.rules.JoinToMultiJoinRule.addOnJoinFieldRefCounts(JoinToMultiJoinRule.java:481)
	at org.apache.calcite.rel.rules.JoinToMultiJoinRule.onMatch(JoinToMultiJoinRule.java:166)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:560)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:419)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:284)
	at org.apache.calcite.plan.hep.HepInstruction$RuleCollection.execute(HepInstruction.java:74)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:215)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:202)


the reason is {{JoinToMultiJoinRule}} should match SEMI/ANTI LogicalJoin. before calcite-1.20, SEMI join is represented by {{SemiJoin}} which is not matched {{JoinToMultiJoinRule}}.",,godfreyhe,jark,,,,,,,,,,,,"godfreyhe commented on pull request #9329: [FLINK-13545] [table-planner-blink] JoinToMultiJoinRule should not match SEMI/ANTI LogicalJoin
URL: https://github.com/apache/flink/pull/9329
 
 
   
   
   ## What is the purpose of the change
   
   *fix ""JoinToMultiJoinRule should not match SEMI/ANTI LogicalJoin""*
   
   
   ## Brief change log
   
     - *copy JoinToMultiJoinRule from Calcite to blink-planner, and does not match SEMI/ANTI LogicalJoin in new rule*
   
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
     - *Added FlinkJoinToMultiJoinRuleTest to verify the bug*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 03:54;githubbot;600","asfgit commented on pull request #9329: [FLINK-13545] [table-planner-blink] JoinToMultiJoinRule should not match SEMI/ANTI LogicalJoin
URL: https://github.com/apache/flink/pull/9329
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Aug/19 13:08;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 08 13:22:00 UTC 2019,,,,,,,,,,"0|z059iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/19 03:31;godfreyhe;i would like to fix this bug;;;","08/Aug/19 13:22;jark;master: 1b1e31944a6c62aad7e3a5854ee00af812702cf9
1.9: 47d0fed51999ccea847b3fa6b6645a6adad54843;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set parallelism of table sink operator to input transformation parallelism,FLINK-13544,13248473,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,jark,jark,02/Aug/19 02:57,06/Aug/19 07:23,13/Jul/23 08:10,02/Aug/19 12:06,,,,,,,,,,1.9.0,,,,Connectors / Common,Table SQL / Planner,,,,0,pull-request-available,,,,"Currently, there are a lot of {{TableSink}} connectors uses {{dataStream.addSink()}} without {{setParallelism}} explicitly. This will use default parallelism of the environment. However, the parallelism of input transformation might not be env.parallelism, for example, global aggregation has 1 parallelism. In this case, it will lead to data reorder, and result in incorrect result.",,jark,,,,,,,,,,,,,"wuchong commented on pull request #9332: [FLINK-13544][connectors] Set parallelism of table sink operator to input transformation parallelism
URL: https://github.com/apache/flink/pull/9332
 
 
   
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently, there are a lot of TableSink connectors uses `dataStream.addSink()` without `.setParallelism()` explicitly. This will use default parallelism of the environment. However, the parallelism of input transformation might not be `env.parallelism`, for example, global aggregation has 1 parallelism. In this case, it will lead to data reorder, and result in incorrect result.
   
   ## Brief change log
   
   Checks all the implementation of `StreamTableSink#emitDataStream/consumeDataStream`, and set parallelism as input parallelism. 
   
   ## Verifying this change
   
   This is covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 06:24;githubbot;600","asfgit commented on pull request #9332: [FLINK-13544][connectors] Set parallelism of table sink operator to input transformation parallelism
URL: https://github.com/apache/flink/pull/9332
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 12:02;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 12:06:35 UTC 2019,,,,,,,,,,"0|z059io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/19 12:06;jark;Merged in 1.10.0: 5254126244b07719d695c26f10bd25c1dddccb96
Merged in 1.9.0: 9e4de1eaabdc958826b96733564753ae60b86a5a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State Processor Api sets the wrong key selector when writing savepoints,FLINK-13541,13248427,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sjwiesman,sjwiesman,sjwiesman,01/Aug/19 20:17,02/Aug/19 17:02,13/Jul/23 08:10,02/Aug/19 17:02,,,,,,,,,,1.9.0,,,,API / DataStream,Runtime / State Backends,,,,0,pull-request-available,,,,"The state processor api is setting the wrong key selector for its StreamConfig when writing savepoints. It uses two key selectors internally that happen to output the same value for integer keys but not in general. 


{noformat}
Caused by: java.lang.RuntimeException: Exception occurred while setting the current key context.
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setCurrentKey(AbstractStreamOperator.java:641)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setKeyContextElement(AbstractStreamOperator.java:627)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setKeyContextElement1(AbstractStreamOperator.java:615)
	at org.apache.flink.state.api.output.BoundedStreamTask.performDefaultAction(BoundedStreamTask.java:83)
	at org.apache.flink.streaming.runtime.tasks.mailbox.execution.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:140)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:378)
	at org.apache.flink.state.api.output.BoundedOneInputStreamTaskRunner.mapPartition(BoundedOneInputStreamTaskRunner.java:76)
	at org.apache.flink.runtime.operators.MapPartitionDriver.run(MapPartitionDriver.java:103)
	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:504)
	at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:369)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:688)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:518)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String
	at org.apache.flink.api.common.typeutils.base.StringSerializer.serialize(StringSerializer.java:33)
	at org.apache.flink.contrib.streaming.state.RocksDBSerializedCompositeKeyBuilder.serializeKeyGroupAndKey(RocksDBSerializedCompositeKeyBuilder.java:159)
	at org.apache.flink.contrib.streaming.state.RocksDBSerializedCompositeKeyBuilder.setKeyAndKeyGroup(RocksDBSerializedCompositeKeyBuilder.java:96)
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.setCurrentKey(RocksDBKeyedStateBackend.java:303)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setCurrentKey(AbstractStreamOperator.java:639)
	... 12 more

{noformat}
",,sewen,sjwiesman,,,,,,,,,,,,"sjwiesman commented on pull request #9324: [FLINK-13541][state-processor-api] State Processor Api sets the wrong key selector when writing savepoints
URL: https://github.com/apache/flink/pull/9324
 
 
   ## What is the purpose of the change
   
   The state processor api is setting the wrong key selector for its StreamConfig when writing savepoints. It uses two key selectors internally that happen to output the same value for integer keys but not in general.
   
   ## Brief change log
   
   *(for example:)*
     - set the correct key selector
   
   ## Verifying this change
   
   UT
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 20:25;githubbot;600","asfgit commented on pull request #9324: [FLINK-13541][state-processor-api] State Processor Api sets the wrong key selector when writing savepoints
URL: https://github.com/apache/flink/pull/9324
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 16:58;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 17:02:00 UTC 2019,,,,,,,,,,"0|z0598g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/19 17:02;sewen;Fixed in
  - 1.9.0 via 92eb0b80ed3e761f51825f4e56329085436f39e3
  - 1.10.0 via 61352fb69b24ea8c2de5e2c8840cabb3acc2202e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DDL do not support key of properties contains number or ""-""",FLINK-13540,13248370,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,danny0405,lzljs3620320,lzljs3620320,01/Aug/19 15:27,06/Aug/19 12:09,13/Jul/23 08:10,06/Aug/19 12:09,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"But many connector have key of properties contains number or ""-"", like kafka..

So as long as we don't solve this problem, it's hard for users to use these connectors.",,danny831,jark,lzljs3620320,trohrmann,twalthr,,,,,,,,,"danny0405 commented on pull request #9361: [FLINK-13540] DDL do not support properties key that contains number …
URL: https://github.com/apache/flink/pull/9361
 
 
   ## What is the purpose of the change
   
   This patch changes the table option keys format to string literal, they are as sql identifiers before.
   
   
   ## Brief change log
   
     - Change the table option key to string literal
   
   
   ## Verifying this change
   
   See tests in FlinkSqlParserImplTest.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`:  no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 12:53;githubbot;600","asfgit commented on pull request #9361: [FLINK-13540] DDL do not support properties key that contains number …
URL: https://github.com/apache/flink/pull/9361
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/19 12:07;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13539,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 06 12:09:09 UTC 2019,,,,,,,,,,"0|z058vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/19 08:28;lzljs3620320;For this reason, DDL is completely useless, can not use to CSV or Kafka...

[~ykt836] [~jark] [~twalthr] [~danny831] What do you think? Is it a blocker for 1.9?;;;","02/Aug/19 08:31;jark;I just set it as a blocker. Because I find that almost all the existing sql connectors has properties keys with ""-"" . This lead to DDL can't work for almost all the connectors. cc [~till.rohrmann]

Btw, I think we should also test DDL in sql e2e tests. We can do that later.;;;","02/Aug/19 09:33;twalthr;I'm also fine with marking it as a blocker.;;;","02/Aug/19 09:39;trohrmann;How long would it take to fix this issue?;;;","02/Aug/19 15:26;danny831;Not sure why this is assigned to me.  But I am not working on this.  Maybe it was assigned by mistake. 

;;;","02/Aug/19 16:17;lzljs3620320;Sorry [~danny831], should be [~danny0405] ...;;;","05/Aug/19 02:58;jark;Sorry [~danny831], I mis-assigned to you... ;;;","06/Aug/19 12:09;jark;master: 14dba3a29400f377cd924047ac8ce8cf31eb44c3
1.9: cf07492f6ba9f244cf3f98e4e41d9cfab58951cd;;;",,,,,,,,,,,,,,,,,,,,,,,,
DDL do not support CSV tableFactory because CSV require format.fields,FLINK-13539,13248367,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,lzljs3620320,lzljs3620320,01/Aug/19 15:25,23/Oct/19 02:54,13/Jul/23 08:10,23/Oct/19 02:54,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"(Now DDL do not support key of properties contains number or ""-"".)

And old csv validator require ""format.fields.#.type"". So there is an validation exception now.",,aljoscha,dwysakowicz,godfreyhe,jark,lzljs3620320,,,,,,,,,"wuchong commented on pull request #9421: [FLINK-13539][table-api] Improve CSV table factory doesn't need to require format.fields
URL: https://github.com/apache/flink/pull/9421
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently, the CSV table factory requires `format.fields.`, which makes it hard to use. This PR makes CSV table factory supports `deriveSchema` property to derive schema from DDL body part and avoid to pass `format.fields.` properties.
   
   ## Brief change log
   
   - Add a `deriveSchema` property to derive schema from `schema.*` properties.
   
   
   ## Verifying this change
   
   - Add a `CsvTableSinkFactoryTest` to verify source and sink table factory. 
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 14:45;githubbot;600","wuchong commented on pull request #9421: [FLINK-13539][table-api] Improve CSV table factory doesn't need to require format.fields
URL: https://github.com/apache/flink/pull/9421
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Oct/19 02:54;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-13540,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 23 02:54:42 UTC 2019,,,,,,,,,,"0|z058v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 07:37;godfreyhe;i would like to fix this;;;","06/Aug/19 07:50;dwysakowicz;Isn't it just duplicate of [FLINK-13540]?;;;","06/Aug/19 08:02;godfreyhe;this issue want to let {{OldCsvValidator}} support {{format.derive-schema}} mode like {{CsvValidator}}.;;;","06/Aug/19 08:07;lzljs3620320;> Isn't it just duplicate of FLINK-13540?

No, I think use DDL with CSV format should not required format.fields, because user has defined the fields in DDL.;;;","06/Aug/19 09:55;jark;Hi [~godfreyhe], I think this issue is aiming to fix CsvTableFactory which requires `format.fields`. This has nothing to do with `OldCsv` because CscTableFactory doesn't depends on `OldCsv` or `Csv` currently. ;;;","07/Aug/19 13:27;lzljs3620320;Be careful with time types:

When there are some timestamp type fields, not work anyway.

Because TimestampType from DDL(Use DataType), the conversion class is LocalDateTime.

And format.fields in DescriptorProperties use Types.TIMESTAMP (TypeInformation), the conversion class is java.sql.Timestamp.

In CsvTableSinkFactoryBase, there should be a  ""Encodings that differ from the schema are not supported yet for CsvTableSink."" exception.;;;","12/Aug/19 14:37;jark;As discussed with [~godfreyhe], I will take this issue. ;;;","23/Oct/19 02:54;jark;1.10.0: 5135cea708939bacd98539d669d274af91f996e6;;;",,,,,,,,,,,,,,,,,,,,,,,,
Unable to query Hive table with decimal column,FLINK-13534,13248327,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,01/Aug/19 12:28,27/Aug/19 04:09,13/Jul/23 08:10,13/Aug/19 21:39,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Hit the following exception when access a Hive table with decimal column:

{noformat}

Caused by: org.apache.flink.table.api.TableException: TableSource of type org.apache.flink.batch.connectors.hive.HiveTableSource returned a DataSet of data type ROW<`x` LEGACY(BigDecimal)> that does not match with the data type ROW<`x` DECIMAL(10, 0)> declared by the TableSource.getProducedDataType() method. Please validate the implementation of the TableSource.
 at org.apache.flink.table.plan.nodes.dataset.BatchTableSourceScan.translateToPlan(BatchTableSourceScan.scala:118)
 at org.apache.flink.table.api.internal.BatchTableEnvImpl.translate(BatchTableEnvImpl.scala:303)
 at org.apache.flink.table.api.internal.BatchTableEnvImpl.translate(BatchTableEnvImpl.scala:281)
 at org.apache.flink.table.api.internal.BatchTableEnvImpl.writeToSink(BatchTableEnvImpl.scala:117)
 at org.apache.flink.table.api.internal.TableEnvImpl.insertInto(TableEnvImpl.scala:564)
 at org.apache.flink.table.api.internal.TableEnvImpl.insertInto(TableEnvImpl.scala:516)
 at org.apache.flink.table.api.internal.BatchTableEnvImpl.insertInto(BatchTableEnvImpl.scala:59)
 at org.apache.flink.table.api.internal.TableImpl.insertInto(TableImpl.java:428)

{noformat}",,lirui,lzljs3620320,phoenixjiangnan,,,,,,,,,,,"lirui-apache commented on pull request #9390: [FLINK-13534][hive] Unable to query Hive table with decimal column
URL: https://github.com/apache/flink/pull/9390
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix the issue that Flink cannot access Hive table with decimal columns.
   
   
   ## Brief change log
   
     - Avoid conversion between `DataType` and `TypeInformation` in several places. Because such conversions can lose type parameters.
     - Add a test for decimal type.
   
   
   ## Verifying this change
   
   New test case.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Aug/19 04:07;githubbot;600","asfgit commented on pull request #9390: [FLINK-13534][hive] Unable to query Hive table with decimal column
URL: https://github.com/apache/flink/pull/9390
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/19 21:00;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,FLINK-13549,,,,,,,,,,,,,,,,,,,FLINK-13495,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 13 21:39:54 UTC 2019,,,,,,,,,,"0|z058m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/19 13:01;lirui;It seems in {{BatchTableSourceScan.translateToPlan}} the produced type gets converted back and forth between {{TypeInformation}} and {{DataType}}, and somehow there's a type mismatch. Similarly the following experimental test would fail. Not sure whether this is intended behavior.
{code}
DataType dataType = DataTypes.DECIMAL(10, 0);
TypeInformation typeInfo = TypeConversions.fromDataTypeToLegacyInfo(dataType);
DataType convertedDataType = TypeConversions.fromLegacyInfoToDataType(typeInfo);
assertEquals(dataType, convertedDataType);
{code}

[~lzljs3620320] would you mind share your thoughts about this issue? Thanks.

And here's {{HiveTableSource.getProducedDataType}}, in case there's something wrong with this method.
{code}
	@Override
	public DataType getProducedDataType() {
		TableSchema tableSchema = catalogTable.getSchema();
		DataTypes.Field[] fields = new DataTypes.Field[tableSchema.getFieldCount()];
		for (int i = 0; i < fields.length; i++) {
			fields[i] = DataTypes.FIELD(tableSchema.getFieldName(i).get(), tableSchema.getFieldDataType(i).get());
		}
		return DataTypes.ROW(fields);
	}
{code};;;","01/Aug/19 13:33;lzljs3620320;you can try rebase the PR of https://issues.apache.org/jira/browse/FLINK-13495 , and see whether it be fixed.;;;","02/Aug/19 09:32;lirui;Thanks [~lzljs3620320]. FLINK-13495 seems to help, but I think there's still some issue with Hive connector code. We did some conversions between {{DataType}} and {{TypeInformation}} in Hive connector. My question is what's the recommended way to do such conversions? And is the conversion planner-specific?;;;","02/Aug/19 09:51;lzljs3620320;It should be OK now.

The better is using blink planner-specific conversion.. But in this way, user must run job with blink-planner dependent.;;;","02/Aug/19 11:48;lirui;[~lzljs3620320] I'm afraid we need to avoid planner-specific code, as long as we still want Hive connector to work with both planners. I'll see whether we can reduce our usage of {{TypeConversions}}.;;;","06/Aug/19 04:08;lirui;[~lzljs3620320] Just verified latest PR of FLINK-13495 fixes the issues of reading decimal/char/varchar types in Hive connector (with some changes to Hive source/sink). I'll submit a PR for this ticket once FLINK-13495 is in.;;;","13/Aug/19 21:39;phoenixjiangnan;merged in master: f33a6f0fd6d46d666d95707ac320115ada21cb0f  1.9.0: 3b04db5c9c6efda58883d4b11f3834b39491612d;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Broken links in documentation,FLINK-13532,13248309,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,SleePy,chesnay,chesnay,01/Aug/19 10:48,06/Aug/19 07:23,13/Jul/23 08:10,05/Aug/19 07:56,1.10.0,1.9.0,,,,,,,,1.9.0,,,,Documentation,,,,,0,pull-request-available,,,,"{code:java}
[2019-07-31 15:58:08] ERROR `/zh/dev/table/hive_integration_example.html' not found.
[2019-07-31 15:58:10] ERROR `/zh/dev/table/types.html' not found.
[2019-07-31 15:58:10] ERROR `/zh/dev/table/hive_integration.html' not found.
[2019-07-31 15:58:14] ERROR `/zh/dev/restart_strategies.html' not found.
http://localhost:4000/zh/dev/table/hive_integration_example.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/table/types.html:
Remote file does not exist -- broken link!!!
http://localhost:4000/zh/dev/table/hive_integration.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/restart_strategies.html:
Remote file does not exist -- broken link!!!{code}",,jark,SleePy,trohrmann,xuefuz,,,,,,,,,,"ifndef-SleePy commented on pull request #9344: [FLINK-13532][docs] Fix broken links of zh docs
URL: https://github.com/apache/flink/pull/9344
 
 
   ## What is the purpose of the change
   
   * Fix broken links of zh docs
   
   ## Brief change log
   
   * Just copy the missing docs from english version
   
   ## Verifying this change
   
   * Run docs/check_links.sh
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 18:11;githubbot;600","asfgit commented on pull request #9344: [FLINK-13532][docs] Fix broken links of zh docs
URL: https://github.com/apache/flink/pull/9344
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 07:56;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 05 07:56:20 UTC 2019,,,,,,,,,,"0|z058i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/19 17:20;xuefuz;cc [~sjwiesman] for visibility.;;;","02/Aug/19 09:29;trohrmann;[~xuefuz] can you take care of the broken links in the {{release-1.9}} and {{master}} branches?;;;","02/Aug/19 09:31;trohrmann;Another instance: https://api.travis-ci.org/v3/job/566420653/log.txt;;;","02/Aug/19 17:46;xuefuz;Sure. Will do. Documentation work for Hive is currently blocked by FLINK-13501. Will resume the work after that's fixed.;;;","02/Aug/19 18:27;SleePy;It seems that there are several documents added recently without zh version. I'm not sure whether there must be a zh version for each document. I have left a message to [~jark].
Just add these missing docs to pass the checking. 
[~xuefuz] you could override these docs when the other hive ticket finished;;;","03/Aug/19 13:15;jark;I will look into this issue too. ;;;","05/Aug/19 07:56;jark;Fixed in 1.10.0: da8a91c2d3769d84a13d7556e601d8258e5128e1
Fixed in 1.9.0: 8186f18f9e149bfca87b8f42397745f0a6bf7767;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Do not print log and call 'release' if no requests should be evicted from the shared slot,FLINK-13531,13248307,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,01/Aug/19 10:43,28/Aug/19 09:09,13/Jul/23 08:10,28/Aug/19 09:09,1.9.0,,,,,,,,,1.10.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"After adding the logic to bookkeeping the resource used in the shared slots, the resource requests will be recorded inside the MultiTaskSlot and when the underlying slot is allocated, all the resource requests will be checked if there is over-subscription, if so, some requests will be failed.

In the current implementation, the code does not check the amount to fail before printing the over-allocated debug log and tries to fail them. This should not cause actual errors, but it will 
 # Print a debug log saying some requests will be failed even if no one to fail.
 # If the total number of requests is 0 (This is possible if there already AllocatedSlot before the first request), the _release_ method will be called. Although it will do nothing with the current implementation (the slot is still being created and not added to any other data structure), it may cause error if the release logic changes in the future.

To fix this issue, we should add a explicit check on the number of requests to fail.

 ",,gaoyunhaii,,,,,,,,,,,,,"gaoyunhaii commented on pull request #9320: [FLINK-13531] Do not print log and call 'release' if no requests should be evicted from the shared slot
URL: https://github.com/apache/flink/pull/9320
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   This PR fixes the code problem for checking the over-reserved requests when the underlying slots are  allocated. This is a part of the bookkeeping logic for sharing slot introduced in #8841 .
   
   The current implementation does not check the amount to fail before printing the over-reserved and tries to fail them. Although it should not cause actual error, it will print a over-reserved log wrongly. Besides, if the slot has been already allocated when constructing the` MultiTaskSlot`, the `released` method will be called. Although `release` will do nothing due to the slot is still being constructed, it may cause error if the `release` logic changes in the future.
   
   To fix the above issue, we need to add the explicit checking for the number of requests to evict.
   
   For the tests, it is hard to add a test for the fix since currently it will not cause actual errors. It will print a wrong debug log and call `release`, but `release` will do nothing actually. Therefore, to add a test, I have to make some assumption on the detailed implementation of `SlotShareManager` and use some hack methods to detect if `release` is called. 
   
   ## Brief change log
   - 187030df57d4fc62a2d121ddda3a4047a390f576 add the explicit check.
   
   ## Verifying this change
   
   - Manually checked that the debug log does not been printed, and `release` does not get called if the underlying slot has already been allocated when creating MultiTaskSlot.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 11:03;githubbot;600","zentol commented on pull request #9320: [FLINK-13531] Do not print log and call 'release' if no requests should be evicted from the shared slot
URL: https://github.com/apache/flink/pull/9320
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 09:09;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 28 09:09:58 UTC 2019,,,,,,,,,,"0|z058hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/19 09:09;chesnay;master: f9b7467c12d6f7f17d198fc4c9ee454c29739216;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractServerTest failed on Travis,FLINK-13530,13248296,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,01/Aug/19 09:42,15/Aug/19 06:12,13/Jul/23 08:10,15/Aug/19 06:12,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Runtime / Queryable State,Tests,,,,0,pull-request-available,,,,"Likely just a port conflict (the range used in the test only covers 3 ports)
{code:java}
09:21:38.371 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.091 s <<< FAILURE! - in org.apache.flink.queryablestate.network.AbstractServerTest
09:21:38.371 [ERROR] testPortRangeSuccess(org.apache.flink.queryablestate.network.AbstractServerTest)  Time elapsed: 0.062 s  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<1>
	at org.apache.flink.queryablestate.network.AbstractServerTest.testPortRangeSuccess(AbstractServerTest.java:125){code}",,,,,,,,,,,,,,,"zentol commented on pull request #9317: [FLINK-13530][qs][tests] Increase port range size
URL: https://github.com/apache/flink/pull/9317
 
 
   Increase the size of the port range used in `AbstractServerTest#testPortRangeSuccess` by 4000%, which should make it quite likely that we find an open port; should the test fail again for the same reason we can conclude that there is an actual problem in the code.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 10:11;githubbot;600","zentol commented on pull request #9317: [FLINK-13530][qs][tests] Increase port range size
URL: https://github.com/apache/flink/pull/9317
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 06:10;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 15 06:12:32 UTC 2019,,,,,,,,,,"0|z058fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/19 06:12;chesnay;master: 0bd5c531f9be963b7b4e94539d78dd8e90bcb5e3
1.9: 2a2296717e292fcdca775039fe8b8fb9d1fc75b0 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switching to a non existing catalog or database crashes sql-client,FLINK-13526,13248266,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lirui,lirui,01/Aug/19 07:25,27/Aug/19 04:08,13/Jul/23 08:10,12/Aug/19 18:13,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Table SQL / Client,,,,,0,pull-request-available,,,,"sql-client crashes if user tries to switch to a non-existing DB:
{noformat}
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:206)
Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: A database with name [foo] does not exist in the catalog: [myhive].
	at org.apache.flink.table.catalog.CatalogManager.setCurrentDatabase(CatalogManager.java:286)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.useDatabase(TableEnvironmentImpl.java:398)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$useDatabase$5(LocalExecutor.java:258)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:216)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.useDatabase(LocalExecutor.java:256)
	at org.apache.flink.table.client.cli.CliClient.callUseDatabase(CliClient.java:434)
	at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:282)
	at java.util.Optional.ifPresent(Optional.java:159)
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:200)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:123)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:105)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:194)
{noformat}",,lirui,phoenixjiangnan,,,,,,,,,,,,"lirui-apache commented on pull request #9399: [FLINK-13526][sql-client] Switching to a non existing catalog or data…
URL: https://github.com/apache/flink/pull/9399
 
 
   …base crashes sql-client
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Avoid crashing sql-client when switching to non-existing catalog or database.
   
   
   ## Brief change log
   
     - Catch `CatalogException` thrown by TableEnvironment and throw a `SqlExecutionException` instead.
     - Add test cases.
   
   
   ## Verifying this change
   
   New test cases.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 03:50;githubbot;600","asfgit commented on pull request #9399: [FLINK-13526][sql-client] Switching to a non existing catalog or data…
URL: https://github.com/apache/flink/pull/9399
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 18:08;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 04:01:53 UTC 2019,,,,,,,,,,"0|z0588o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/19 18:13;phoenixjiangnan;merged in master: a8fb572054c5f2294b46200d64729bc3e20c301b  1.9.0: 39d17e27bf687a71fdb3b11a500e87fe7add51c9;;;","27/Aug/19 04:01;ykt836;I've updated the fix version to 1.9.1 since this is not included in 1.9.0 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in Builder method name from Elasticsearch example,FLINK-13524,13248258,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,aromero,aromero,aromero,01/Aug/19 06:38,13/Apr/21 20:40,13/Jul/23 08:10,07/Oct/19 17:57,1.8.1,,,,,,,,,1.10.0,,,,Connectors / ElasticSearch,Documentation,,,,0,pull-request-available,,,,"The Builder method name from class ElasticsearchSink has got a typo (missing 'd') in the Elasticsearch connector section for Scala (ES version 6.x).
{code:java}
new ElasticsearchSink.Builer[String]({code}
should be:
{code:java}
new ElasticsearchSink.Builder[String]({code}",,aromero,,,,,,,,,,,,,"a-romero commented on pull request #9311: FLINK-13524 [docs] Fixed typo in Builder method name from Elasticsearch example
URL: https://github.com/apache/flink/pull/9311
 
 
   
   ## What is the purpose of the change
   
   Fix typo in Builder method name from Elasticsearch example
   
   ## Brief change log
   
   The Builder method name from class ElasticsearchSink has got a typo (missing 'd') in the Elasticsearch connector section for Scala (ES version 6.x).
   
   `new ElasticsearchSink.Builer[String](`
   
   should be:
   
   `new ElasticsearchSink.Builder[String](`
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 06:47;githubbot;600","zentol commented on pull request #9311: FLINK-13524 [docs] Fixed typo in Builder method name from Elasticsearch example
URL: https://github.com/apache/flink/pull/9311
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Oct/19 17:56;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 07 17:57:04 UTC 2019,,,,,,,,,,"0|z0586w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/19 17:57;chesnay;master: 2e3e27b14be22083c159fea788355698a0338864;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch Connector sample code for Scala on version 6.x will not work,FLINK-13519,13248218,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aromero,aromero,aromero,31/Jul/19 21:50,23/Oct/19 10:04,13/Jul/23 08:10,23/Oct/19 10:04,1.7.2,1.8.0,1.8.1,,,,,,,1.10.0,,,,Connectors / ElasticSearch,Documentation,,,,0,pull-request-available,,,,"The Scala example in the documentation for the Elasticsearch Connector, version 6.x, will not work. The class ElasticsearchSinkFunction[String] requires a RuntimeContext and a RequestIndexer, which the example omits.

Also, *_type_* needs to be in inverse quotes as it's a Scala keyword.

It should look like the following:
{code:java}
def process(element: String, ctx: RuntimeContext, indexer: RequestIndexer) { 
   val json = new java.util.HashMap[String, String] 
   json.put(""data"", element) 

   val rqst: IndexRequest = Requests.indexRequest
      .index(""testarindex"")
      .`type`(""_doc"")
      .source(json) 

   indexer.add(rqst) 
}{code}
 ",,aromero,,,,,,,,,,,,,"a-romero commented on pull request #9309: FLINK-13519 [docs] Rewrite Elasticsearch 6.x example for Scala to use RuntimeContext and RequestIndexer
URL: https://github.com/apache/flink/pull/9309
 
 
   
   ## What is the purpose of the change
   
   Rewrite Elasticsearch 6.x example for Scala to use RuntimeContext and RequestIndexer
   
   ## Brief change log
   
   The Scala example in the documentation for the Elasticsearch Connector, version 6.x, will not work. The class ElasticsearchSinkFunction[String] requires a RuntimeContext and a RequestIndexer, which the example omits.
   
   Also, type needs to be in inverse quotes as it's a Scala keyword.
   
   ```
   new ElasticsearchSinkFunction[String] {
       def process(element: String, ctx: RuntimeContext, indexer: RequestIndexer) {
       ...
       val rqst: IndexRequest = Requests.indexRequest
           .index(""my-index"")
           .`type`(""my-type"")
           .source(json)
   
       indexer.add(rqst)
       }
   }
   ```
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 22:05;githubbot;600","zentol commented on pull request #9309: [FLINK-13519] [docs] Rewrite Elasticsearch 6.x example for Scala to use RuntimeContext and RequestIndexer
URL: https://github.com/apache/flink/pull/9309
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Oct/19 10:03;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 23 10:04:19 UTC 2019,,,,,,,,,,"0|z057y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/19 10:04;chesnay;master: 1cf261e64ff24165f5758263a1e86f38571e32b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTaskTest.testAsyncCheckpointingConcurrentCloseAfterAcknowledge unstable,FLINK-13514,13248122,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,chesnay,chesnay,31/Jul/19 14:07,30/Aug/19 08:06,13/Jul/23 08:10,30/Aug/19 08:06,1.10.0,1.9.0,,,,,,,,1.10.0,1.9.1,,,Runtime / Task,Tests,,,,0,pull-request-available,test-stability,,,"Somewhat reproducible by running the test locally a few hundred times.
{code:java}
org.mockito.exceptions.verification.NeverWantedButInvoked:
keyedStateHandle.discardState();
Never wanted here:
-> at org.apache.flink.streaming.runtime.tasks.StreamTaskTest.testAsyncCheckpointingConcurrentCloseAfterAcknowledge(StreamTaskTest.java:543)
But invoked here:
-> at org.apache.flink.runtime.state.SnapshotResult.discardState(SnapshotResult.java:82)


    at org.apache.flink.streaming.runtime.tasks.StreamTaskTest.testAsyncCheckpointingConcurrentCloseAfterAcknowledge(StreamTaskTest.java:543)
    at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
    at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.lang.Thread.run(Thread.java:748){code}",,aljoscha,pnowojski,trohrmann,,,,,,,,,,,"aljoscha commented on pull request #9563: [FLINK-13514] Fix instability in StreamTaskTest.testAsyncCheckpointingConcurrentCloseAfterAcknowledge
URL: https://github.com/apache/flink/pull/9563
 
 
   ## What is the purpose of the change
   
   Before, thread pool shutdown would interrupt our waiting method.
   Production code cannot throw an InterruptedException here and would also
   not be correct if one is thrown.
   
   We now swallow interrupted exceptions and wait until we successfully
   return from await().
   
   ## Brief change log
   
    - we swallow interrupted exceptions in test code
   
   ## Verifying this change
   
    - this is covered by the existing test that we fix
   
   ## Does this pull request potentially affect one of the following parts:
   
     - this affects checkpointing (only test code is changed, though)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Aug/19 14:39;githubbot;600","aljoscha commented on pull request #9563: [FLINK-13514] Fix instability in StreamTaskTest.testAsyncCheckpointingConcurrentCloseAfterAcknowledge
URL: https://github.com/apache/flink/pull/9563
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 08:04;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 30 08:05:50 UTC 2019,,,,,,,,,,"0|z057co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/19 14:12;chesnay;This is a timing issue:
{code:java}
streamTask.cancel();

// enable this to reliably reproduce the issue
// Thread.sleep(5 * 1000);

completeAcknowledge.trigger();{code};;;","26/Aug/19 08:18;trohrmann;Another instance: https://api.travis-ci.org/v3/job/576620244/log.txt;;;","28/Aug/19 09:20;trohrmann;Another instance: https://api.travis-ci.org/v3/job/577647784/log.txt;;;","29/Aug/19 10:05;aljoscha;Is someone working on this issue? It was reported quite a while back and we have quite recent occurrences of it.;;;","29/Aug/19 11:09;aljoscha;The problem seems to be this: https://github.com/apache/flink/blob/04b5cbf07775c086b1df33f94b77a99acc3f4615/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L557.

Shutting down the thread pool will interrupt the workers, which will lead us to this: https://github.com/apache/flink/blob/04b5cbf07775c086b1df33f94b77a99acc3f4615/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java#L1015.

And this, in turn, will dispose the state handles.;;;","29/Aug/19 11:09;aljoscha;So it seems it's not actually a test instability but an actual bug, I think.;;;","29/Aug/19 11:47;trohrmann;Disposing the state handles is per se not a problem as long as the checkpoint is not acknowledged but failed if I'm not mistaken. However, if we acknowledge the checkpoint, then this is clearly a bug.;;;","29/Aug/19 14:38;pnowojski;In the ned this is not a production issue. The test is interruptibly awaiting on a latch, which can throw an {{InterruptedException}} during task cancelation and this tests assumes that this doesn't fail the checkpoint. However production code interprets this exception as a failure during reporting completed checkpoint, and correctly cleans up the checkpoint files. After discussing this with [~aljoscha] and [~till.rohrmann] we concluded that this {{InterruptedException}} should never be thrown in this test.;;;","30/Aug/19 08:04;aljoscha;Fixed on master in 60b65c42ca2709d20cb59f1617d96a80ec870b6c;;;","30/Aug/19 08:05;aljoscha;Fixed on release-1.9 in 3d71518ef9c96bc8fe0add3b4c25bf141aa599db;;;",,,,,,,,,,,,,,,,,,,,,,
CommonTestUtils#waitUntilCondition() may attempt to sleep with negative time,FLINK-13508,13248059,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gjy,gjy,gjy,31/Jul/19 09:11,05/Aug/19 10:08,13/Jul/23 08:10,05/Aug/19 09:08,,,,,,,,,,1.8.2,1.9.0,,,Tests,,,,,0,pull-request-available,,,,"The test utility {{CommonTestUtils#waitUntilCondition(SupplierWithException<Boolean, Exception>, Deadline, long)}} may attempt to call {{Thread.sleep(long)}} with a negative argument.",,gjy,,,,,,,,,,,,,"GJL commented on pull request #9291: [FLINK-13508][tests] CommonTestUtils#waitUntilCondition() may attempt to sleep with negative time
URL: https://github.com/apache/flink/pull/9291
 
 
   ## What is the purpose of the change
   
   *This fixes that `CommonTestUtils#waitUntilCondition()` may invoke `Thread.sleep()` with a negative argument.*
   
   
   ## Brief change log
   
     - *See commits*
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *tests using this method*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 11:46;githubbot;600","GJL commented on pull request #9341: [FLINK-13508][1.9][tests] CommonTestUtils#waitUntilCondition() may attempt to sleep with negative time
URL: https://github.com/apache/flink/pull/9341
 
 
   See #9291
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 14:37;githubbot;600","GJL commented on pull request #9343: [FLINK-13508][1.8] CommonTestUtils#waitUntilCondition() may attempt to sleep with negative time
URL: https://github.com/apache/flink/pull/9343
 
 
   See #9291 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 17:14;githubbot;600","GJL commented on pull request #9341: [FLINK-13508][1.9][tests] CommonTestUtils#waitUntilCondition() may attempt to sleep with negative time
URL: https://github.com/apache/flink/pull/9341
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Aug/19 18:52;githubbot;600","GJL commented on pull request #9291: [FLINK-13508][tests] CommonTestUtils#waitUntilCondition() may attempt to sleep with negative time
URL: https://github.com/apache/flink/pull/9291
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 09:03;githubbot;600","asfgit commented on pull request #9343: [FLINK-13508][1.8][tests] CommonTestUtils#waitUntilCondition() may attempt to sleep with negative time
URL: https://github.com/apache/flink/pull/9343
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 09:04;githubbot;600",,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 05 09:08:21 UTC 2019,,,,,,,,,,"0|z056yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/19 09:08;gjy;1.8: a0d236fba7c6abdabb461aa504b1e088a3982c31
1.9: d609917d706e6928d6eee1535c9d12b90b6ae6f8
1.10: 1ad16bc252f1d3502a29ddb2081fdfdf3436cc55;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchFieldError when executing DDL via tEnv.sqlUpdate in application project,FLINK-13504,13248017,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,jark,jark,31/Jul/19 06:50,01/Oct/19 15:39,13/Jul/23 08:10,02/Aug/19 13:35,,,,,,,,,,1.9.0,,,,Table SQL / Legacy Planner,Table SQL / Planner,,,,0,pull-request-available,,,,"When we create a quickstart project to try flink 1.9/1.10, a NoSuchFieldError is thrown.

The dependencies (the flink 1.0 is installed locally for commit 70fe6aa747ad021bbb8dd8cdc0beecc863f010be, flink 1.9 has the same problem):


{code:xml}
  <dependencies>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java</artifactId>
            <version>1.10-SNAPSHOT</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner_2.11</artifactId>
            <version>1.10-SNAPSHOT</version>
        </dependency>
    </dependencies>
{code}

The program code:


{code:java}
package com.github.wuchong;

import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.TableEnvironment;

public class DDLTest {

    public static void main(String[] args) {
        EnvironmentSettings settings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();
        TableEnvironment tEnv = TableEnvironment.create(settings);
        tEnv.sqlUpdate(""CREATE TABLE MyTable (\n"" +
                ""    a int, \n"" +
                ""    b bigint, \n"" +
                ""    c varchar \n"" +
                "")\n comment 'table comment'\n"" +
                ""partitioned by (b)\n"" +
                ""with (\n"" +
                ""    connector = 'csv', \n"" +
                ""    csv.path = '/tmp/path'\n"" +
                "")"");

    }
}
{code}

The exception:


{code}
Exception in thread ""main"" java.lang.NoSuchFieldError: names
	at org.apache.flink.sql.parser.ddl.SqlCreateTable.fullTableName(SqlCreateTable.java:326)
	at org.apache.flink.table.sqlexec.SqlToOperationConverter.convertCreateTable(SqlToOperationConverter.java:140)
	at org.apache.flink.table.sqlexec.SqlToOperationConverter.convert(SqlToOperationConverter.java:86)
	at org.apache.flink.table.planner.StreamPlanner.parse(StreamPlanner.scala:115)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:335)
	at com.github.wuchong.DDLTest.main(DDLTest.java:29)
{code}


",,1u0,dwysakowicz,godfreyhe,jark,nicholasjiang,,,,,,,,,"dawidwys commented on pull request #9297: [FLINK-13504][table] Fixed shading issues in table modules.
URL: https://github.com/apache/flink/pull/9297
 
 
   ## What is the purpose of the change
   
   Moved the common calcite specific dependencies configuration to
   flink-table in order to have them unified across flink-table-planner,
   flink-table-planner-blink, flink-table-runtime-blink & flink-sql-parser.
   
   Shading configuration is also done in the table parent pom so that all
   modules have common relocation policies.
   
   Enabled shading for flink-sql-parser.
   
   ## Verifying this change
   
   * For verification of sql-parser module e.g. run the query attached to jira issue.
   * Verify contents of flink-table-planner, flink-table-planner-blink, flink-table-runtime-blink, flink-sql-parser, flink-table-uber, flink-table-uber-blink do not contain unwanted classes.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no /** don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 12:36;githubbot;600","dawidwys commented on pull request #9313:  [FLINK-13504][table] Fixed shading issues in table modules. 
URL: https://github.com/apache/flink/pull/9313
 
 
   ## What is the purpose of the change
   
   This PR properly includes the flink-sql-parser into flink-planner* modules shaded
   jars. It also removes the flink-sql-parser module from flink-table-uber* jars as
   it is already included in the flink-planner* modules.
   
   
   ## Brief change log
   
     - Fixed shading of `flink-sql-parser`
     - Removed unnecessary dependency on guava in `flink-sql-parser`
   
   Verifying this change
   
     * For verification of sql-parser module e.g. run the query attached to jira issue.
     * Verify contents of flink-table-planner, flink-table-planner-blink, flink-table-runtime-blink, flink-sql-parser, flink-table-uber, flink-table-uber-blink do not contain unwanted classes.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (**yes** / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 09:17;githubbot;600","dawidwys commented on pull request #9297: [FLINK-13504][table] Fixed shading issues in table modules.
URL: https://github.com/apache/flink/pull/9297
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 09:18;githubbot;600","dawidwys commented on pull request #9313:  [FLINK-13504][table] Fixed shading issues in table modules. 
URL: https://github.com/apache/flink/pull/9313
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 13:21;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 13:35:44 UTC 2019,,,,,,,,,,"0|z04whg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/19 06:51;jark;cc [~danny831] [~godfreyhe] [~twalthr];;;","31/Jul/19 07:46;godfreyhe;a more simple way to repeat the NoSuchFieldError: 

{code:java}
import org.apache.flink.sql.parser.ddl.SqlCreateTable;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.TableEnvironment;

import org.apache.calcite.sql.SqlIdentifier;
import org.apache.calcite.sql.SqlNodeList;
import org.apache.calcite.sql.parser.SqlParserPos;


public class DDLTest {

	public static void main(String[] args) {
		SqlIdentifier id = new SqlIdentifier(""a"", SqlParserPos.QUOTED_ZERO);
		System.out.println(id.names);

		SqlCreateTable createTable = new SqlCreateTable(
				SqlParserPos.QUOTED_ZERO, id, new SqlNodeList(SqlParserPos.QUOTED_ZERO), null, null, null, null, null);
		// throw exception: java.lang.NoSuchFieldError: names
		System.out.println(createTable.fullTableName());
	}
}
{code}
;;;","31/Jul/19 08:04;dwysakowicz;I will have a look at it.;;;","31/Jul/19 08:08;1u0;I have the same issue on my local build.
I think this is due to problematic shading:

in the resulting {{flink-table*.jar}} s the {{org.apache.calcite.sql.SqlIdentifier}} dependency on {{com.google.common.collect.ImmutableList}} is shaded (relocated as {{org.apache.flink.calcite.shaded.com.google.common.collect.ImmutableList}}).

But the code in {{org.apache.flink.sql.parser.ddl.SqlCreateTable}} still has old original type: {{SqlIdentifier.names:Lcom/google/common/collect/ImmutableList;}}.;;;","01/Aug/19 09:12;nicholasjiang;[~dwysakowicz]I removed sql package of calcite of table-planner dependency jar and add calcite-core in pom.xml,this question was solved.So I have some suggestion that calcite-core class is unnecessary to include in table-planner dependency jar.;;;","02/Aug/19 13:35;dwysakowicz;Fixed in:
master via: 3fa4898a4d1b2d69ef64f48b5a5aefafa2e03974
1.9: 854d9e19a0e8e2b124418f0dfdc9bf7a2e70830c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
CatalogTableStatisticsConverter & TreeNode should be in correct package,FLINK-13502,13247978,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,31/Jul/19 01:38,04/Dec/19 01:37,13/Jul/23 08:10,04/Dec/19 01:37,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"currently, {{CatalogTableStatisticsConverter}} is in {{org.apache.flink.table.util}}, {{TreeNode}} is in {{org.apache.flink.table.planner.plan}}. {{CatalogTableStatisticsConverter}} should be in {{org.apache.flink.table.planner.utils}}, {{TreeNode}} should be in {{org.apache.flink.table.planner.expressions}}.",,godfreyhe,jark,,,,,,,,,,,,"KurtYoung commented on pull request #9284: [FLINK-13502] move CatalogTableStatisticsConverter & TreeNode to correct package
URL: https://github.com/apache/flink/pull/9284
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Dec/19 01:36;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 04 01:37:11 UTC 2019,,,,,,,,,,"0|z056gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/19 01:43;godfreyhe;i will fix this;;;","31/Jul/19 01:50;jark;Currently, TreeNode is in {{org.apache.flink.table.planner.plan}}. ;;;","31/Jul/19 01:51;jark;There are no class conflicts for these classes. I did't set it as a blocker. ;;;","31/Jul/19 01:54;godfreyhe;yes, it's just a code cleanup;;;","04/Dec/19 01:37;ykt836;master: d59c3171dee21ea89c366deb5e700d2d9055770e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes a few issues in documentation for Hive integration,FLINK-13501,13247958,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Terry1897,xuefuz,xuefuz,30/Jul/19 22:30,27/Aug/19 04:11,13/Jul/23 08:10,14/Aug/19 21:21,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Connectors / Hive,Table SQL / API,,,,0,pull-request-available,,,,"Going thru existing Hive doc I found the following issues that should be addressed:
1. Section ""Hive Integration"" should come after ""SQL client"" (at the same level).
2. In Catalog section, there are headers named ""Hive Catalog"". Also, some information is duplicated with that in ""Hive Integration""
3. ""Data Type Mapping"" is Hive specific and should probably move to ""Hive integration""",,lirui,phoenixjiangnan,Terry1897,xuefuz,,,,,,,,,,"zjuwangg commented on pull request #9437: [FLINK-13501][doc]Fixes a few issues in documentation for Hive integration
URL: https://github.com/apache/flink/pull/9437
 
 
   ## What is the purpose of the change
   
   *Fixes a few issues in documentation for Hive integration*
   
   
   ## Brief change log
     - *Fixes a few issues in documentation for Hive integration*
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: ( no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 14:36;githubbot;600","asfgit commented on pull request #9437: [FLINK-13501][doc]Fixes a few issues in documentation for Hive integration
URL: https://github.com/apache/flink/pull/9437
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 21:21;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/19 22:51;xuefuz;Screen Shot 2019-07-30 at 3.21.25 PM.png;https://issues.apache.org/jira/secure/attachment/12976278/Screen+Shot+2019-07-30+at+3.21.25+PM.png","30/Jul/19 22:51;xuefuz;Screen Shot 2019-07-30 at 3.25.13 PM.png;https://issues.apache.org/jira/secure/attachment/12976279/Screen+Shot+2019-07-30+at+3.25.13+PM.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 04:10:54 UTC 2019,,,,,,,,,,"0|z056cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/19 22:32;xuefuz;cc: [~tiwalter], [~Terry1897];;;","31/Jul/19 02:43;Terry1897;Hi [~xuefuz] and [~tiwalter]. I'd like to fix these problems. Please feel free to assign this Jira to me.;;;","31/Jul/19 07:49;lirui;[~xuefuz] Regarding the catalog page, the hierarchy was fixed as part of FLINK-13352. Seems your screenshot doesn't reflect the latest web page?;;;","01/Aug/19 04:11;xuefuz;Hi [~lirui],OK, I will take a new look. Thanks.;;;","01/Aug/19 04:16;xuefuz;Yes. my screen shot is a little obsolete, so some of the issues might not be applicable. Nevertheless, let's wait until FLINK-13517 is fixed to revisit these issues.;;;","14/Aug/19 21:21;phoenixjiangnan;merged in master: 7b4c23255b900ced0afe060194bbb737981405e2   1.9.0: d895dfec7b451358f9e83d1614f48e124938fc58;;;","27/Aug/19 04:10;ykt836;I've updated the fix version to 1.9.1 since this is not included in 1.9.0 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Remove dependency on MapR artifact repository,FLINK-13499,13247879,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sewen,sewen,sewen,30/Jul/19 15:30,02/Oct/19 17:45,13/Jul/23 08:10,31/Jul/19 08:31,1.9.0,,,,,,,,,1.8.2,1.9.0,,,Build System,,,,,0,pull-request-available,,,,"The MapR artifact repository causes some problems. It does not reliably offer a secure (https://) access.

We should change the MapR FS connector to work based on reflection and avoid a hard dependency on any of the MapR vendor-specific artifacts. That should allow us to get rid of the dependency without regressing on the support for the file system.",,sewen,trohrmann,,,,,,,,,,,,"StephanEwen commented on pull request #9282: [FLINK-13499][maprfs] Remove dependency on MapR artifact repository
URL: https://github.com/apache/flink/pull/9282
 
 
   ## What is the purpose of the change
   
   This change removes the MapR FS connector to work based on reflection and avoid a hard dependency on any of the MapR vendor-specific artifacts. That allows us to get rid of the dependency without regressing on the support for the file system.
   
   The MapR artifact repository causes some problems. Access to the a secure (https://) endpoint is very flaky.
   
   ## Verifying this change
   
   Verified by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **No**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **No**
     - The serializers: **No**
     - The runtime per-record code paths (performance sensitive): **No**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **No**
     - The S3 file system connector: **No**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **No**
     - If yes, how is the feature documented? **No**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 16:06;githubbot;600","zentol commented on pull request #9282: [FLINK-13499][maprfs] Remove dependency on MapR artifact repository
URL: https://github.com/apache/flink/pull/9282
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 10:55;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 10 02:53:31 UTC 2019,,,,,,,,,,"0|z055v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/19 18:35;trohrmann;Please move the issue to ""In Progress"" [~sewen].;;;","31/Jul/19 08:31;sewen;Fixed in
  - 1.9.0 via 2a9e18589e70fcff79044a572ea67f4c0660fafb
  - master via 8bdcb001fd7bf8582691b5625cbc5b261ff00cb1
;;;","10/Sep/19 02:53;ykt836;also in 1.8: 80452f19e1712fd09d3b6b9a99c82d954f471a88;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce Kafka producer startup time by aborting transactions in parallel,FLINK-13498,13247871,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,30/Jul/19 14:46,27/Feb/20 12:55,13/Jul/23 08:10,06/Aug/19 12:42,1.8.1,1.9.0,,,,,,,,1.10.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"When a Flink job with a Kafka producer starts up without previous state, it currently starts 5 * kafkaPoolSize number of Kafka producers (per sink instance) to abort potentially existing transactions from a first run without a completed snapshot.
Apparently, this is quite slow and it is also done sequentially. Until there is a better way of aborting these transactions with Kafka, we could do this in parallel quite easily and at least make use of lingering CPU resources.",,mjedynak,nkruber,,,,,,,,,,,,"NicoK commented on pull request #9287: [FLINK-13498][kafka] abort transactions in parallel
URL: https://github.com/apache/flink/pull/9287
 
 
   ## What is the purpose of the change
   
   This makes `FlinkKafkaProducer` abort transactions, e.g. during a first startup, in parallel making use of lingering CPU resources (using at most `kafkaProducersPoolSize` producers at once each, just like during runtime).
   
   Especially during that first startup (and thus also in tests), a lot of producers `(5*poolSize)` are being created at each sink instance to abort potentially existing previous transactions (in most cases, they don't exist).
   
   With this change, Kafka test times (on my PC) change like this:
   ```
   FlinkKafkaProducerMigrationTest        :  36.689s -> 29.438s
   FlinkKafkaProducerMigrationOperatorTest:  33.288s -> 26.232s
   ------
   FlinkKafkaProducerITCase               : 289.475s -> 228.469s
   KafkaITCase                            : 139.7s   -> 127.595s
   FlinkKafkaInternalProducerITCase       :  34.436s ->  35.391s
   KafkaProducerAtLeastOnceITCase         : 391.627s -> 391.473s
   KafkaProducerExactlyOnceITCase         :  84.366s ->  72.608s
   ```
   
   ## Brief change log
   
   - change `FlinkKafkaProducer`:
     - abort transactions using a parallel stream
     - use a new thread pool with `size = kafkaProducersPoolSize`
     - do not use `#initTransactionalProducer()` but rather only instantiate the parts needed for aborting transactions, i.e. a `FlinkKafkaInternalProducer`
   - do the same for `FlinkKafkaProducer011`
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as Kafka unit tests and ITCases.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **JavaDocs**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 07:06;githubbot;600","NicoK commented on pull request #9287: [FLINK-13498][kafka] abort transactions in parallel
URL: https://github.com/apache/flink/pull/9287
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/19 12:41;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,FLINK-16262,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 06 12:42:10 UTC 2019,,,,,,,,,,"0|z055tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/19 12:42;nkruber;fixed on master via d774fea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoints can complete after CheckpointFailureManager fails job,FLINK-13497,13247868,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,SleePy,trohrmann,trohrmann,30/Jul/19 14:34,08/Apr/20 10:21,13/Jul/23 08:10,08/Apr/20 10:20,1.10.0,1.9.0,,,,,,,,1.11.0,,,,Runtime / Checkpointing,,,,,0,,,,,I think that we introduced with FLINK-12364 an inconsistency wrt to job termination a checkpointing. In FLINK-9900 it was discovered that checkpoints can complete even after the {{CheckpointFailureManager}} decided to fail a job. I think the expected behaviour should be that we fail all pending checkpoints once the {{CheckpointFailureManager}} decides to fail the job.,,felixzheng,kisimple,klion26,liyu,pnowojski,SleePy,tison,trohrmann,yanghua,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16945,,,,,,,,,FLINK-13698,FLINK-16945,FLINK-9900,,FLINK-5960,,,FLINK-13527,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 10:21:24 UTC 2020,,,,,,,,,,"0|z055so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/19 18:31;yunta;One quick fix to this problem is to let {{CheckpointFailureManager}} call {{CheckpointCoordinator}} to {{stopCheckpointScheduler}} just before it decided to fail the job.;;;","31/Jul/19 03:44;yanghua;Currently, the {{CheckpointFailureManager}} choose a simple counting mechanism to fail the job, so there is the possibility like [~till.rohrmann] said. It is also another issue(FLINK-12514) to track a better counting mechanism.

The solution proposed by [~yunta] may fix this issue temporarily. But it may cause another risk. The {{stopCheckpointScheduler}} also called {{CheckpointFailureManager#handleCheckpointException}}. It will make counting more complex in the future.

Maybe we need to call a pure method just fails all pending checkpoints when failing the job?;;;","31/Jul/19 04:23;yunta;[~yanghua], there still exists a gap between fail all pending checkpoints to fail the job. And if we could not stop the checkpoint scheduler, some checkpoint could still be triggered and completed unexpectedly. We might simplify the logic of aborting pending checkpoints without {{CheckpointFailureManager}} involved.;;;","31/Jul/19 04:41;yanghua;[~yunta] I have no objection to stopping the checkpoint scheduler. I just explained that calling {{CheckpointCoordinator#stopCheckpointScheduler}} directly is not a good choice in the long run. I am just wondering if we need a pure cleanup mechanism that doesn't involve counting. Because it indirectly calls {{CheckpointFailureManager#handleCheckpointException}}. It will become more complicated when we change the failure count logic in the future.;;;","31/Jul/19 09:36;yanghua;When I thought deeply, even if we did not introduce FLINK-12364, the issue described by [~till.rohrmann] also exists. When job failing, the pending checkpoints and checkpoint coordinator can also work normally. What's more, some failed instances in FLINK-9900 happened before merging FLINK-12364.

But there is a fact that after FLINK-12364 was merged, the number of failures increased significantly.;;;","01/Aug/19 09:04;SleePy;There is another problem. The {{failGlobal}} might fail the job in an unexpected way. If the job has already restarted between the {{declineCheckpoint}} and {{failGlobal}} (due to a task failure). The job would restart again when this asynchronous {{failGlobal}} arrived. I think we should not use {{failGlobal}} directly. Canceling a task (or tasks) with specific {{ExecutionAttemptID}} might be better. It's also more friendly with other failover strategy. For example if under ""region failover"" strategy, we could avoid a global failover.
;;;","01/Aug/19 09:10;yanghua;[~SleePy] Your comment is reasonable, I am discussing with [~yunta] about the scenarios you mentioned.;;;","02/Aug/19 10:32;SleePy;The thread model of {{CheckpointCoordinator}} seems to be a bit messy. Here we missed two necessary synchronizations.
 # Synchronization between different checkpoints. That's the reason of why {{CheckpointFailureManager}} has already decided to {{failGlobal}} but other checkpoints could succeed at the same time. We might need to re-think the thread model here. [~yunta] gave a work-around way.
 # Synchronization between {{CheckpointCoordinator}} and {{ExecutionGraph}}. That's caused by asynchronous {{failGlobal}}. So I suggest using a work-around way canceling task with {{ExecutionAttemptID}} instead. That's a kind of weak synchronization.;;;","02/Aug/19 11:44;yanghua;[~SleePy] I discussed with [~yunta]. Yes, your analysis is correct.

My thought contains two points:
 # Accept your point 2, just fail(cancel) an Execution instead of failing globally;
 # Bring state management for {{CheckpointFailureManager}} , define a field to represent its state, let it has a lifecycle and start/stop with checkpoint scheduler.

WDYT? cc [~till.rohrmann];;;","05/Aug/19 08:30;trohrmann;[~pnowojski] I think this issue would benefit from your attention.;;;","06/Aug/19 07:57;pnowojski;As far as we (me, [~carp84] and [~sewen]) understand this, root problem here is a race condition on the JobManager between failing a checkpoint and completing another checkpoint, caused by {{FailJobCallback CheckpointFailureManager::failureCallback}} executing asynchronous operation:
{code:java}
cause -> getJobMasterMainThreadExecutor().execute(() -> failGlobal(cause)
{code}
Possible race condition between:

Decline checkpoint
# JM receives {{JobMaster#declineCheckpoint}}
# In {{LegacyScheduler#declineCheckpoint}} we asynchronously schedule {{checkpointCoordinator.receiveDeclineMessage}} on {{ioExecutor}}
# This eventually reaches {{CheckpointFailureManager#handleCheckpointException}}, which decides again to asynchronously fail job ({{ExecutionGraph#failGlobal}} on {{getJobMasterMainThreadExecutor}}.
# {{failGlobal}} cancels all pending checkpoints

Acknowledge checkpoint
# JM receives {{JobMaster#acknowledgeCheckpoint}}
# In {{LegacyScheduler#acknowledgeCheckpoint}} we asynchronously schedule {{checkpointCoordinator.receiveAcknowledgeMessage}} on {{ioExecutor}}
# This completes some pending checkpoint

If  ""Acknowledge checkpoint""  path executes and completes while {{failGlobal}} from  ""Decline checkpoint"" step 4 is awaiting execution, this lead to a completed checkpoint AFTER we failed a job.

There might be more that kind of race conditions introduced in by this {{failureCallback}}.

I'm not sure if there is some easy fix for that, because whole design of {{CheckpointCoordinator}} seems strange: like why {{checkpointCoordinator}} methods are executed on the {{ioExecutor}}? I think the proper solution would need to clean up the threading model here.

{{CheckpointCoordinator}} should mostly single threaded, executed only on the JobMasters's main thread executor (not on {{ioExecutor}}). {{ioExecutor}} should be only used for IO operations, like deleting/moving/touching files. Same applies to {{CheckpointFailureManager}}. That should remove/limit the concurrency issues.

Some minor point for refactoring, is that we probably could cut the cyclic dependency between {{CheckpointFailureManager}} and {{CheckpointCoordinator}}, by removing {{CheckpointFailureManager::failureCallback}} and changing for example
{code:java}
void CheckpointFailureManager::handleCheckpointException(ex)
{code} 
to
{code:java}
// return true if an exception should fail job
boolean CheckpointFailureManager::shouldCheckpointExceptionFailJob(ex)
{code}

However as far as we know, this is not a blocker issue. However it might be surprising that a checkpoint has completed after a job has failed. For now, I'm removing the fix for this from 1.9 release. I hope that this issues is not a tip of an iceberg and that we are not missing some other bugs/problems here.;;;","06/Aug/19 08:21;SleePy;To [~pnowojski], I can't agree more.

I have another concern, the reason of why {{CheckpointCoordinator}} is multi-threaded currently. Is there ever a performance issue with single threaded? 

Executing the {{CheckpointCoordinator}} in main thread executor makes things easier. But I'm afraid there is too much burden of main thread executor. Using an independent thread is another alternative.;;;","06/Aug/19 08:26;trohrmann;The reason why we don't run the {{CheckpointCoordinator}} from the {{RpcEndpoint's}} main thread is that the {{CheckpointCoordinator}} contains blocking calls. Hence, there is a possibility that we block the {{RpcEndpoint's}} main thread which must never happen. This actually dates back to FLINK-5960. Hence the proper solution as Piotr said is to encapsulate all blocking operations in the {{ioExecutor}} of the {{CheckpointCoordinator}} and then use the {{RpcEndpoint's}} main thread to run the {{CheckpointCoordinator}}.;;;","06/Aug/19 08:39;SleePy;To [~till.rohrmann], thanks for explanation of history.

I just noticed that I might not express the alternative clearly. We should definitely make meta manipulation of {{CheckpointCoordinator}} single threaded. And using the {{ioExecutor}} to execute the time-consuming operator. I'm just wondering whether the main thread executor is the best place to do it or not. A benchmark might be helpful here. :);;;","06/Aug/19 08:39;pnowojski;[~SleePy] we are also not aware of any performance problems in this area. My gut feeling tells me that we would need hundred of thousands of operations per second on JobManager main executor thread before we overload it and as far as I know, we haven't observed this to be an issue. If it becomes an issue, we can analyse it and then make an informative decision what to do: optimise code (those code was never written with performance in mind, so there is definitely lots of room for improvement) or spread the work load on more threads.

Would you like [~SleePy] to work on this refactoring (changing threading model here) for 1.10?

edit: benchmark would be nice. There was even an jira issue were someone was proposing to add JM benchmarks.;;;","06/Aug/19 08:51;SleePy;To [~pnowojski], yes, my pleasure :)
WRT the performance concern, your suggestion makes sense to me. ;;;","08/Apr/20 10:21;pnowojski;The bug was finally fixed by combination of FLINK-16945 and FLINK-13698.

Thanks [~SleePy] for all of your work in this topic :);;;",,,,,,,,,,,,,,,
Blink planner changes source parallelism which causes stream SQL e2e test fails,FLINK-13494,13247808,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,julien1987,docete,docete,30/Jul/19 09:41,06/Aug/19 07:22,13/Jul/23 08:10,02/Aug/19 07:56,1.10.0,1.9.0,,,,,,,,1.9.0,,,,Table SQL / Planner,Tests,,,,0,pull-request-available,,,,,,jark,julien1987,twalthr,,,,,,,,,,,"XuPingyong commented on pull request #9277: [FLINK-13494][table-planner-blink] Only use env parallelism for sql job
URL: https://github.com/apache/flink/pull/9277
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Only use env parallelism for sql job, not to calculate parallelism of table source and sink.
   
   
   ## Brief change log
   
   Remove parallelism calculator in blink planner.
   
   ## Verifying this change
   
   UT
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)
     - The serializers: ( no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no )
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 11:49;githubbot;600","asfgit commented on pull request #9277: [FLINK-13494][table-planner-blink] Only use env parallelism for sql job
URL: https://github.com/apache/flink/pull/9277
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 07:57;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,FLINK-13439,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 07:56:28 UTC 2019,,,,,,,,,,"0|z055fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/19 09:57;twalthr;[~docete] But this should only apply if the e2e uses the Blink planner, right? Or does the Blink planner affect it without being selected?;;;","30/Jul/19 10:07;docete;Yes, it's only apply if e2e uses Blink planner.;;;","02/Aug/19 07:56;jark;Fixed in 1.10.0:  8266707502889e279256d09645376c665ab0cdcb
Fixed in 1.9.0: 70475d3e13fd3072a3c20fa3f594090d36efb275;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedOutOfOrderTimestamps cause Watermark's timestamp leak,FLINK-13492,13247800,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,simonss,simonss,30/Jul/19 08:48,02/Aug/19 06:03,13/Jul/23 08:10,30/Jul/19 09:02,1.9.0,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,,,,,"{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(1, conf);

// Use eventtime， default autoWatermarkInterval is 200ms
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
Kafka kafka = new Kafka() 
.version(""0.11"") 
.topic(topic) 
.startFromLatest() 
.properties(properties);

Schema schema = new Schema();
for (int i = 0; i < names.length; i++) { 
    if (""timestamp"".equalsIgnoreCase(names[i])) {
        // set latency to 1000ms
        schema.field(""rowtime"", types[i]).rowtime(new     Rowtime().timestampsFromField(""timestamp"").watermarksPeriodicBounded(1000)); }    
    else { 
        schema.field(names[i], types[i]); 
}

/** ..... */
tableEnv 
.connect(kafka) 
.withFormat(new Protobuf().protobufName(""order_sink"")) 
.withSchema(schema) 
.inAppendMode() 
.registerTableSource(""orderStream"");{code}
Register up stream table, then use a 10s Tumble window on this table, we input a sequence of normal data, but there is not result output.

Then we start to debug to see if the watermark is normally emitted, finally we found the issue.
 # maxTimestamp will be initialized in BoundedOutOfOrderTimestamps to Long.MIN_VALUE.
 # nextTimestamp method will extract timestamp from source and set to maxTimestamp.
 # getWatermark() method will calculate the watermark's timestamp based on maxTimestamp and delay.



When  +{color:#205081}TimestampsAndPeriodicWatermarksOperator{color}+ {color:#333333}initialize and call open method, it will start to register a SystemTimeService to generate watermark based on watermarkInterval, so that's the problem, the thread initialize and call BoundedOutOfOrderTimestamps${color}getCurrentWatermark, it will cause a Long Value leak. {color:#d04437}(Long.MIN_VALUE - delay). which cause all of the watermark will be dropped because apparently there are less then ( Long.MIN_VALUE - delay ).

{color}

{color:#d04437}A workaround is to set a large autoWatermarkInterval to make SystemTimeService Thread a long start delay.{color}

 
{code:java}
public void onProcessingTime(long timestamp) throws Exception {
...
getProcessingTimeService().registerTimer(now + watermarkInterval, this);
...
}
{code}
 

 
{code:java}
public ScheduledFuture<?> registerTimer(long timestamp, ProcessingTimeCallback target) {
...
long delay = Math.max(timestamp - getCurrentProcessingTime(), 0) + 1;
...
}
{code}
 

{color:#d04437} {color}

{color:#d04437}Actually, I think we can fix it by add the delay in BoundedOutOfOrderTimestamps's constructor which can avoid the calculation leak ...{color}

{color:#d04437} {color}

 

 ",,dwysakowicz,jark,simonss,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13429,,,,,,,,,,,,,,,,,,,,,"30/Jul/19 08:27;simonss;Watermark_timestamp_leak.diff;https://issues.apache.org/jira/secure/attachment/12976186/Watermark_timestamp_leak.diff",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 06:03:23 UTC 2019,,,,,,,,,,"0|z055dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/19 09:06;dwysakowicz;Hi [~simonss] which commit did you use to reproduce this issue? I think it duplicates: [FLINK-13429].;;;","30/Jul/19 09:23;jark;Yes, I think this might be fixed in this commit: https://github.com/apache/flink/commit/b670b11f303657aa8175ec933ee29b377cb9e087.
Or for release-1.9: https://github.com/apache/flink/commit/0224d9bc0a773633943282f5268770a8063a87a2;;;","02/Aug/19 06:03;simonss;Yes, I have marked it as duplicated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncWaitOperator doesn't handle endInput call properly,FLINK-13491,13247797,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,SleePy,pnowojski,pnowojski,30/Jul/19 08:20,01/Aug/19 10:17,13/Jul/23 08:10,01/Aug/19 10:17,1.9.0,,,,,,,,,1.9.0,,,,API / DataStream,,,,,0,pull-request-available,,,,"This is the same issue as for {{ContinousFileReaderOperator}} in https://issues.apache.org/jira/browse/FLINK-13376. {{AsyncWaitOperator}} will propagate {{endInput}} notification immediately, even if it has some records buffered.

Side note, this also shows that the current {{BoundedOneInput#endInput}} API is easy to mishandle if an operator buffers some records internally. Maybe we could redesign this API somehow [~aljoscha] [~sunhaibotb]?",,aljoscha,maguowei,pnowojski,SleePy,sunhaibotb,trohrmann,,,,,,,,"ifndef-SleePy commented on pull request #9298: [FLINK-13491][datastream] AsyncWaitOperator supports BoundedOneInput
URL: https://github.com/apache/flink/pull/9298
 
 
   ## What is the purpose of the change
   
   * `AsyncWaitOperator` should respect semantics of `BoundedOneInput`
   
   ## Brief change log
   
   * `AsyncWaitOperator` supports `BoundedOneInput`
   * Wait the in-flight inputs in `AsyncWaitOperator`.`endInput`
   * I keep this waiting operation a bit redundant both in `endInput` and `close` because we probably re-think the `endInput` semantics in the near future
   * BTW, I think the resource releasing of `AsyncWaitOperator`(endInput/close/dispose) is a bit messy. However I would like to discuss it in another thread.
   
   ## Verifying this change
   
   * This change added tests in `AsyncWaitOperatorTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 12:51;githubbot;600","pnowojski commented on pull request #9298: [FLINK-13491][datastream] AsyncWaitOperator supports BoundedOneInput
URL: https://github.com/apache/flink/pull/9298
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 10:16;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13486,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 01 10:17:45 UTC 2019,,,,,,,,,,"0|z055cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/19 08:41;aljoscha;I feel it's the same problem as {{cancel()}} and {{prepareSnapshotPreBarrier()}}, i.e. users have to make sure to flush data and wait. I don't know if there is a way around that actually.;;;","30/Jul/19 08:46;SleePy;Oops! Is there any more operators buffering data? 

+1 to rethink the {{endInput}} API. But I guess there is no enough time to do it in 1.9.;;;","30/Jul/19 09:08;maguowei;Currently, Operatorchain calls all operator's ""endInput"" together.

An alternative way likes 
{code:java}
operator1.endInput()
operator1.close()
operator2.endInput
operator2.close()
{code}
 
 this could flush the buffer data in the operator.

 ;;;","30/Jul/19 09:36;sunhaibotb;So for one-input operator, {{endInput()}} repeats {{close()}} semantically. Maybe we don't need {{BoundedOneInput}}, but we need to call {{close()}} immediately when the input is finished?;;;","30/Jul/19 09:51;sunhaibotb;For two-input operator, {{endInput(int inputId)}} is semantically different from {{close()}}, but should we call {{close()}} immediately after the last input is finished?;;;","30/Jul/19 09:54;pnowojski;Re API changes, I was afraid of such answer.

[~SleePy], I'm not aware of more of such operators. 

However there is also one more potential issue. Processing time timers, they can be still firing after {{endInput()}} was called, which might mean that more records will be produced. I briefly discussed this with [~aljoscha] and we think that at least for now, we can say that {{endInput}} combined with processing time timers have undefined behaviour. Probably we should document this undefined behaviour in the java doc of {{BoundedOneInput}} and {{BoundedMultiInput}}.;;;","30/Jul/19 12:35;SleePy;[~pnowojski][~aljoscha], I guess the next step is keeping the interface {{endInput}} and fixing {{AsyncWaitOperator}}. Am I right?;;;","30/Jul/19 12:40;pnowojski;[~SleePy] at least for 1.9 release: yes we should keep the current interfaces. Will you be able to work on this issue [~SleePy]?

For the future, we might want to re-think the API and/or revisit {{BoundedOneInput}} case as [~sunhaibotb] suggested, but that's a separate topic.

;;;","30/Jul/19 12:44;SleePy;[~pnowojski], I agree we should keep it at least for 1.9.
I will fix it. Could you assign this ticket to me?;;;","01/Aug/19 10:17;pnowojski;Merged as cf763d51a8941f48733c08e924c340573d537f3f to master
Merged as bd4328dbcea908191b669e53b57b790a4e627ecd to release-1.9;;;",,,,,,,,,,,,,,,,,,,,,,
Heavy deployment end-to-end test fails on Travis with TM heartbeat timeout,FLINK-13489,13247779,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kevin.cyj,tzulitai,tzulitai,30/Jul/19 04:27,02/Oct/19 17:45,13/Jul/23 08:10,13/Aug/19 19:43,,,,,,,,,,1.10.0,1.9.1,,,Test Infrastructure,,,,,0,pull-request-available,,,,"https://api.travis-ci.org/v3/job/564925128/log.txt

{code}
------------------------------------------------------------
 The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: Job failed. (JobID: 1b4f1807cc749628cfc1bdf04647527a)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:250)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:338)
	at org.apache.flink.streaming.api.environment.StreamContextEnvironment.execute(StreamContextEnvironment.java:60)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1507)
	at org.apache.flink.deployment.HeavyDeploymentStressTestProgram.main(HeavyDeploymentStressTestProgram.java:70)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:576)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:438)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:274)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:746)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:273)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:205)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1010)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1083)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1083)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:247)
	... 21 more
Caused by: java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id ea456d6a590eca7598c19c4d35e56db9 timed out.
	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyHeartbeatTimeout(JobMaster.java:1149)
	at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl$HeartbeatMonitor.run(HeartbeatManagerImpl.java:318)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,gaoyunhaii,kevin.cyj,liyu,sewen,trohrmann,tzulitai,zjwang,,,,,,,"wsry commented on pull request #9345: [FLINK-13489]Fix the broken heavy deployment e2e test by adjusting config values
URL: https://github.com/apache/flink/pull/9345
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Aug/19 06:53;githubbot;600","wsry commented on pull request #9391: [FLINK-13489]Fix akka timeout problem of the heavy deployment e2e test.
URL: https://github.com/apache/flink/pull/9391
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   This pull request increases the config value of akka.ask.timeout and web.timeout to fix akka timeout problem of the heavy deployment e2e test.
   
   
   ## Brief change log
    Two config values are changed for the heavy deployment e2e test.
    - Increase the config value of akka.ask.timeout from default (10s) to 60s
    - Increase the config value of web.timeout form default (10,000ms) to 60,000ms
   
   
   
   ## Verifying this change
   
   Manually verified the change by running ./flink-end-to-end-tests/run-single-test.sh ./flink-end-to-end-tests/test-scripts/test_heavy_deployment.sh 500 times locally and modified the .travis.yml config file to trigger heavy deployment test when pushing code and ran the test for more than 200 times on Travis. No akka timeout problem occurred any more.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Aug/19 07:14;githubbot;600","tillrohrmann commented on pull request #9391: [FLINK-13489]Fix akka timeout problem of the heavy deployment e2e test.
URL: https://github.com/apache/flink/pull/9391
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/19 19:45;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,FLINK-13579,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 13 19:43:56 UTC 2019,,,,,,,,,,"0|z05594:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/19 11:13;kevin.cyj;[~tzulitai] I wonder how this test was triggered? I used both master branch and release-1.9 branch, but there seems problem with the configuration of heavy deployment test. The test cluster can not be started, the following is the exception:

 
{code:java}

2019-08-01 18:42:29,289 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Could not start cluster entrypoint StandaloneSessionClusterEntrypoint.
org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneSessionClusterEntrypoint.
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:182)
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:501)
at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:65)
Caused by: org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
at org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.create(AbstractDispatcherResourceManagerComponentFactory.java:259)
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:210)
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:164)
at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:163)
... 2 more
Caused by: java.lang.IllegalArgumentException: The configuration value 'containerized.heap-cutoff-min'='600' is larger than the total container memory 512
at org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParameters.calculateCutoffMB(ContaineredTaskManagerParameters.java:133)
at org.apache.flink.runtime.util.ResourceManagerUtil.getResourceManagerConfiguration(ResourceManagerUtil.java:34)
at org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.create(AbstractDispatcherResourceManagerComponentFactory.java:171)
... 6 more
{code}
I set the config option containerized.heap-cutoff-min to 100 (default is 600) to make the test cluster start successfully. 

BTW, I Triggered the heavy deployment test by running: ./flink-end-to-end-tests/run-single-test.sh test-scripts/test_heavy_deployment.sh.

 

After the config option containerized.heap-cutoff-min was set, I run more than 300 times of the heavy deployment test, all tests passed and each test took about 20 seconds.

After that, I hacked the travis config file and make the heavy deployment test run each time I push code to my github repository. However, the test did not pass because of timeout (more than 10min and killed by Travis). I am now still trying to figure out why the  test is that slow on Travis.

 ;;;","02/Aug/19 02:42;kevin.cyj;I tested different setting on Travis for the heavy deployment test and find the running time is relevant to state size and taskmanager memory size. When the original setting is used, the test encountered akka timeout problem sometimes, but after the taskmanager memory size was set to 1G (the original value is 512M), the problem did not occur anymore. I think 512M memory for 10 slot is a little small, so I suggest to increase the taskmanager memory size. What do you think [~tzulitai]?;;;","02/Aug/19 09:32;trohrmann;Yes let's increase it [~kevin.cyj]. Can you open a PR for the fix? Please also move this issue into ""In Progress"" if possible.

Another instance: https://api.travis-ci.org/v3/job/566420659/log.txt;;;","02/Aug/19 13:19;kevin.cyj;I'd like to open a PR and fix it.;;;","02/Aug/19 13:40;sewen;[~kevin.cyj] I am wondering whether there is more hidden here.

Can you share some details why the Akka timeout occurs? Why does the smaller TM heap size lead to that timeout? GC pressure? Some OOM in some threads?

Also, why is the test sensitive to the value for {{containerized.heap-cutoff-min}} ? The test runs with the standalone resource manager, no container / heap cutoff should ever be used there.;;;","03/Aug/19 03:32;kevin.cyj;[~sewen] I run the test for many times, but only encountered the akka timeout problem only once, and nerve encountered the heartbeat timeout problem. But unfortunately, I did not get the JM/TM log of that failure. Latter, I modified the test script to print gc and JM/TM log out and run the test for many times, but the timeout problem did not occur. I noticed the gc time is a little long, many 2, 3, 4 seconds (these are for successfully finished job). I guess the previous failure may result by GC. Another problem is that the Travis test platform seems not stable, the test time varies.

As for containerized.heap-cutoff-min, it is because it was used for memory calculation. If the default value (600) is used, the standalone cluster can not start up. I agree with you that this config option should not be considered by standalone mode, but it seems reusing the same code (I think it also should be fixed). The flowing is the exception stack:

{code}
 2019-08-01 18:42:29,289 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Could not start cluster entrypoint StandaloneSessionClusterEntrypoint.
 org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint StandaloneSessionClusterEntrypoint.
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:182)
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:501)
 at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:65)
 Caused by: org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
 at org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.create(AbstractDispatcherResourceManagerComponentFactory.java:259)
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:210)
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:164)
 at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
 at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:163)
 ... 2 more
 Caused by: java.lang.IllegalArgumentException: The configuration value 'containerized.heap-cutoff-min'='600' is larger than the total container memory 512
 at org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParameters.calculateCutoffMB(ContaineredTaskManagerParameters.java:133)
 at org.apache.flink.runtime.util.ResourceManagerUtil.getResourceManagerConfiguration(ResourceManagerUtil.java:34)
 at org.apache.flink.runtime.entrypoint.component.AbstractDispatcherResourceManagerComponentFactory.create(AbstractDispatcherResourceManagerComponentFactory.java:171)
 ... 6 more
{code};;;","05/Aug/19 03:51;gaoyunhaii;I think the cut-off configuration problem might be introduced by the recent PR [#9246|https://github.com/apache/flink/pull/9246], which is cherry-pick of [#9105|https://github.com/apache/flink/pull/9105];;;","05/Aug/19 07:51;liyu;Please mark the status as ""In-Progress"" since you've already started debugging here [~kevin.cyj], thanks.;;;","05/Aug/19 07:57;kevin.cyj;[~carp84] Thanks for reminding, I have marked the status as ""In-Progress"".;;;","05/Aug/19 08:56;trohrmann;Yes [~gaoyunhaii], the cut-off configuration problem has been introduced with FLINK-13241. [~xintongsong] opened FLINK-13579 to fix this issue.

That leaves now the problem of the Akka timeout issues to be figured out. Maybe it's solely because of Travis but we should make sure that this is the case.;;;","05/Aug/19 08:57;trohrmann;This is issue is partially caused by FLINK-13579. The Akka timeouts might not be explained by this issue, though.;;;","05/Aug/19 09:49;kevin.cyj;The GC time is still high after I Increased the heap size of TM and JM (though lower than before, there are still many 1 ~ 2 seconds entries). And I notice that the real time if much longer than user plus sys time. Usually, the user and sys time are longer than real time because of multi thread GC. Longer real time usually means that the GC threads are waiting for CPU or IO. Like the GC thread, the RPC main thread  may also need to wait for CPU or IO. If so, may be the proper solution for this case is just to increase the akka and heartbeat timeout config value.  I will collect more information about this case.;;;","05/Aug/19 10:05;trohrmann;Thanks for the update [~kevin.cyj]. Here is a PR for the problem with the container cutoff min value: https://github.com/apache/flink/pull/9357. You could rebase your work on this to see whether the increase in container size is no longer necessary.;;;","06/Aug/19 05:47;trohrmann;FLINK-13579 has been merged.;;;","07/Aug/19 07:33;trohrmann;-[~kevin.cyj] are you still working on this issue? I've seen the test still fail after FLINK-13579.-

In the latest {{release-1.9}} cron job the tests weren't executed because the tpch end-to-end test failed consistently.;;;","07/Aug/19 07:42;kevin.cyj;[~till.rohrmann] I am still working on this issue, could you share the latest failure log, I wonder if the failures we encountered are resulted by the same cause. I am now focusing on the akka timeout problem, but the probability of failure is low, about 1%-2%.;;;","07/Aug/19 07:49;kevin.cyj;[~till.rohrmann] Is there a Jira for the tpch end-to-end test  failure, or we solve the problem under this Jira?;;;","07/Aug/19 09:21;trohrmann;Thanks for the update [~kevin.cyj]. The corresponding JIRA issue for the failing tpch end-to-end test is FLINK-13607.

I would suggest that we unblock this issue if we see that the cron jobs are passing a couple of times (given that the akka timeout issue happens rarely).;;;","07/Aug/19 10:49;kevin.cyj;[~till.rohrmann] I agree with you, this should not be a blocker.;;;","08/Aug/19 07:16;trohrmann;The latest [release-1.9 cron job|https://travis-ci.org/apache/flink/builds/568771238] passed. Hence I assume that only the Akka timeout problem is left. Assuming that this is most likely a test instability and not a fundamental problem, I'll decrease the priority of this issue to critical. We should nevertheless try to fix the test instability [~kevin.cyj].;;;","08/Aug/19 07:17;kevin.cyj;I ran the heavy deployment test for many times and found the akka timeout problem should be purely resulted by the test environment. The resources of the test VM is limited (two cpu cores and less than 8G memory, both the network and disk are not fast enough), but the test needs to start 10 TM and more than 100 tasks. When the akka timeout config value is decreased, the akka timeout failures occurred more frequently, and when the akka timeout config value was set to 60s, akka timeout failure did not occur after the heavy deployment test ran for more than 200 times (if the default timeout is used, there will be one or two times of failure when the heavy deployment test ran for 100 times).

There are two config options relevant to the akka timeout, one is akka.ask.timeout and most rpc calls use this, the other is web.timeout and the rpc calls from rest client use this. The default value for both config options are 10s and in my tests, both can cause the akka timeout failure. (To be honest, I think we should use the same akka rpc timeout configuration for both rest client and other components like TM, JM and RM, when the a user encounters the akka time out problem, he/she is easy to find the akka.ask.timeout config options and increase it but it is not easy to establish the relationship with web.timeout ).

I have create a pr which increases the config value of both  akka.ask.timeout and web.timeout, which should make the heavy deployment test more stable (other tests are not affected).;;;","08/Aug/19 12:52;trohrmann;Thanks [~kevin.cyj] for the update. I'll take a look at your PR.;;;","09/Aug/19 07:09;trohrmann;Another instance of this test failed on Travis: https://api.travis-ci.org/v3/job/569262833/log.txt. Could be a different reason though.;;;","09/Aug/19 09:41;kevin.cyj;[~till.rohrmann] I'll try to reproduce the problem and figure out why the test ran more than 10min.;;;","09/Aug/19 15:50;tzulitai;Since this is no longer a blocker for 1.9.0 and we're not proceeding with a voting RC, I'll now move the fix version to 1.10.0.;;;","13/Aug/19 19:43;trohrmann;Fixed via

1.10.0: 64738aa0bdba41718518bf1e2fb2a6e2ebdeeaef
1.9.1: 4bdb3132da127fc1d7b7f2b57c62b45677570948;;;",,,,,,
flink-python fails to build on Travis due to PackagesNotFoundError,FLINK-13488,13247778,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sunjincheng121,tzulitai,tzulitai,30/Jul/19 04:24,01/Oct/19 15:38,13/Jul/23 08:10,30/Jul/19 13:09,,,,,,,,,,1.9.0,,,,API / Python,Test Infrastructure,,,,0,pull-request-available,,,,"https://api.travis-ci.org/v3/job/564925115/log.txt

{code}
install conda ... [SUCCESS]
install miniconda... [SUCCESS]
installing python environment...
installing python2.7...
install python2.7... [SUCCESS]
installing python3.3...

PackagesNotFoundError: The following packages are not available from current channels:

  - python=3.3

Current channels:

  - https://repo.anaconda.com/pkgs/main/linux-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/linux-64
  - https://repo.anaconda.com/pkgs/r/noarch
{code}",,dian.fu,dwysakowicz,jark,sunjincheng121,tzulitai,yunta,,,,,,,,"sunjincheng121 commented on pull request #9278:  [FLINK-13488][Python] remove python 3.3/3.4 support
URL: https://github.com/apache/flink/pull/9278
 
 
   ## What is the purpose of the change
   
   Remove python 3.3/3.4 support, due to python3.3/3.4 is pretty old（see more detail here https://devguide.python.org/#branchstatus). and the 3.3 will be drop support some project,such as conda-forge and mypy https://github.com/python/mypy/issues/4036,  The 3.3 and 3.4 is almost no use.
   ## Verifying this change
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: ( no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? ( no)
     
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 12:00;githubbot;600","sunjincheng121 commented on pull request #9278:  [FLINK-13488][Python] remove python 3.3/3.4 support
URL: https://github.com/apache/flink/pull/9278
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 13:05;githubbot;600","GJL commented on pull request #9430: [1.8][FLINK-13488][tests] Harden ConnectedComponents E2E test
URL: https://github.com/apache/flink/pull/9430
 
 
   See #9425
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/19 20:17;githubbot;600","GJL commented on pull request #9431: [FLINK-13488][tests] Harden ConnectedComponents E2E test
URL: https://github.com/apache/flink/pull/9431
 
 
   See #9425
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/19 20:19;githubbot;600","asfgit commented on pull request #9430: [1.8][FLINK-13488][tests] Harden ConnectedComponents E2E test
URL: https://github.com/apache/flink/pull/9430
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 07:39;githubbot;600","GJL commented on pull request #9431: [FLINK-13488][tests] Harden ConnectedComponents E2E test
URL: https://github.com/apache/flink/pull/9431
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 07:51;githubbot;600",,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 27 05:47:47 UTC 2019,,,,,,,,,,"0|yi0g1w:w",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/19 06:26;yunta;Another instance: [https://api.travis-ci.com/v3/job/220732162/log.txt] . It seems very easy to come across.;;;","30/Jul/19 07:23;dwysakowicz;[~sunjincheng121] [~dian.fu] Could you have a look at it. I think this is quite pressing as effectively all travis builds are failing because of this.;;;","30/Jul/19 07:42;sunjincheng121;Thanks [~tzulitai] [~dwysakowicz] ,  the reason is conda-forge do not support python 3.3 recently, I'll fix this !;;;","30/Jul/19 07:45;tzulitai;Please move the ticket to ""In Progress"". Thanks [~sunjincheng121].;;;","30/Jul/19 07:46;sunjincheng121;Sure...;;;","30/Jul/19 08:00;sunjincheng121;Due to python3.3/3.4 is pretty old（see more detail here [https://devguide.python.org/#branchstatus]). and the 3.3 will be drop support some project,such as conda-forge and mypy [https://github.com/python/mypy/issues/4036],  The 3.3 and 3.4 is almost no use, the latest version has reached 3.8, so I suggest to remove the support of 3.3/3.4, add the support of 3.8.

 

What do you think? 

 ;;;","30/Jul/19 08:37;dian.fu;[~sunjincheng121] +1 to drop the support for python 3.3 and 3.4. Python 3.3 has reached EOF on Sep 27, 2017 and Python 3.4 has reached EOF on Mar 18, 2019 and are not officially supported any more.;;;","30/Jul/19 12:40;sunjincheng121;Thanks [~dian.fu], we will ad the 3.8 support, after 3.8 are released. for now we only remove 3.3 and 3.4 for this JIRA.;;;","30/Jul/19 12:42;dian.fu;Make sense to me.;;;","30/Jul/19 13:09;sunjincheng121;Fixed in master: 536d321724a310a0a1825497e82895e8de8360b3
Fixed in release-1.9: 8751899a6e055e8e82203951d3c85a7e5af75f9c;;;","27/Aug/19 04:09;ykt836;I've updated the fix version to 1.9.1 since this is not included in 1.9.0 release.;;;","27/Aug/19 04:55;dian.fu;Hi [~ykt836], I guess this fix is actually contained in [1.9.0|https://github.com/apache/flink/commits/release-1.9.0?after=9c32ed989c0178a2bf3e059e897927c451188700+190]. Pls correct me if I missed something:) Do you mean the PR [https://github.com/apache/flink/pull/9431] ? I guess it refers a wrong Jira number and linked to this Jira by mistake. [~gjy];;;","27/Aug/19 05:47;ykt836;Yeah, you're right. I will revert the change, thanks.;;;",,,,,,,,,,,,,,,,,,,
TaskExecutorPartitionLifecycleTest.testPartitionReleaseAfterReleaseCall failed on Travis,FLINK-13487,13247777,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,tzulitai,tzulitai,30/Jul/19 04:21,02/Oct/19 17:50,13/Jul/23 08:10,31/Jul/19 11:43,,,,,,,,,,1.9.0,,,,Runtime / Task,Tests,,,,0,pull-request-available,,,,"https://api.travis-ci.org/v3/job/564925114/log.txt

{code}
21:14:47.090 [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.754 s <<< FAILURE! - in org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest
21:14:47.090 [ERROR] testPartitionReleaseAfterReleaseCall(org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest)  Time elapsed: 0.136 s  <<< ERROR!
java.util.concurrent.ExecutionException: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: Could not submit task because there is no JobManager associated for the job 2a0ab40cb53241799b71ff6fd2f53d3d.
	at org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.testPartitionRelease(TaskExecutorPartitionLifecycleTest.java:331)
	at org.apache.flink.runtime.taskexecutor.TaskExecutorPartitionLifecycleTest.testPartitionReleaseAfterReleaseCall(TaskExecutorPartitionLifecycleTest.java:201)
Caused by: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: Could not submit task because there is no JobManager associated for the job 2a0ab40cb53241799b71ff6fd2f53d3d.
{code}",,gaoyunhaii,SleePy,trohrmann,tzulitai,zjwang,,,,,,,,,"gaoyunhaii commented on pull request #9283: [FLINK-13487][tests] Fix unstable partition lifecycle tests by ensuring TM has registered to JM before submitting task
URL: https://github.com/apache/flink/pull/9283
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   This pull request targets at fixing the unstable cases in TaskExecutorPartitionLifecycleTest. As described in the corresponding [JIRA](https://issues.apache.org/jira/browse/FLINK-13487), the tests may submit task before TM has successfully registered to JM, which will cause the tests fail since the checking whether the corresponding job id exists in the TaskExecutor#jobManagerTables can not be passed.
   
   The above issue does not exist for the real cases. For real cases, JM will not submit tasks before TM has offered slots, and the successful slot offering ensures TM has registered to JM. To fix the tests, we also simulate this process by ensuring slot has been offered to JM before the task gets submitted.
   
   
   ## Brief change log
   
   1. b94855a7695c7a649a174820b8cf85bb38edbd6d fix the unstable tests by ensure TM has registered to JM before submitting task.
   
   ## Verifying this change
   
   This change fixes tests and can be verified as follows:
     - Add Thread.sleep(2000) in RetryingRegistration#register, before `completionFuture.complete(Tuple2.of(gateway, success));` which is executed in the RPC executors. With this modification, the original tests will fail stably.
     - Add this PR, and above failure will be fixed.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 01:23;githubbot;600","zentol commented on pull request #9283: [FLINK-13487][tests] Fix unstable partition lifecycle tests by ensuring TM has registered to JM before submitting task
URL: https://github.com/apache/flink/pull/9283
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 11:43;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/19 14:58;gaoyunhaii;error_log.png;https://issues.apache.org/jira/secure/attachment/12976228/error_log.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 31 12:56:44 UTC 2019,,,,,,,,,,"0|i3ztt4:k",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/19 12:34;zjwang;Thanks for the cooperation from [~SleePy] and [~gaoyunhaii] to trace the root cause. It is mainly caused by unfinished registration while submitting tasks and we could reoccur it when adjust the codes to sleep for a while during registration.  We should make the submitting task happen only after registration done.

[~gaoyunhaii] would fix for it.;;;","30/Jul/19 17:25;gaoyunhaii;Very thanks [~SleePy] and [~zjwang] for the help at this issue! A more detailed cause of this issue is described as follows.

From the logs recorded in the _error_log_  attached, we can see that:
 # The TM registers to JM for two times.
 # When the task is submitted, the jobManagerTable does not contains the corresponding JobId, and thus cause the test failed.

The basic process of the test is:

 
{code:java}
// Step 1 
taskManagerServices.getJobLeaderService().addJob(jobId, jobMasterAddress); 

// Step 2
jobManagerLeaderRetriever.notifyListener(jobMasterAddress, UUID.randomUUID());

// Step 3 
taskExecutorGateway.requestSlot() 

// Step 4 
taskExecutorGateway.submitTask(){code}
 

The two registration is triggered by step 1/2 and step 3 respectively. However, since the registration is asynchronous, the submit task is executed before the two registrations get finished. To fix this problem, we need to postpone the submitting till the slot is offered to JM. In real case this is guaranteed by JM not start deploying the task before the slot offer succeed.

 

A more detailed description is as follows for reference:

In step 1, the job id is added to the JobLeaderService of the TM side, and it triggers the leader retrieve process. Since the leader retrieve service is an instance of SettableLeaderRetrieveService and by now the address has not been set, the leader retrieve will not take effects.

In step 2, the SettableLeaderRetrieveService is notified with the actual JobMaster address and then TM starts to register to JM. Part of the registration should be done asynchronously by the execution thread pool of the RpcService. However, in this case the asynchronous part might takes longer time than usual.

In step 3, the request slot allocates the slot first, then TM found that the registration has not succeeded. Then is call jobLeaderService#addJob again. Since now the JM address has been set, it will trigger the second registration. The registration is also executed asynchronously and the request will return successfully.

In step 4, the submit started before the registration succeeded. Then the error occurred.

The the finally part in the test code executes and shutdown the TM, the the tests end and the original submit failing exception was thrown. This is the reason that the exception is printed after the TM stopped. At this time, the Akka thread of TM continued to run since the exception in the akka thread is caught, and one of the registration finally succeed and it triggers the slot offering again. This is why we can also see the offering slot event in the error log. 

 The above exception could reoccur stably if we add a sleep in the RetryingRegistration#register, before the call of ""completionFuture.complete(Tuple2.of(gateway, success))"" in the asynchronous executor thread. We can also see the two jobLeaderService#addJob calls in this case. If we also add sleep to the finally part of the test method, we can also see the offer slot event after the submitting failure.

 

 ;;;","31/Jul/19 08:19;trohrmann;Please move the issue into ""In Progress"" [~yungao.gy].

[~chesnay] will take a look at the PR.;;;","31/Jul/19 11:43;chesnay;master: 3958cec0d1be3c2e4e9c48ec675085557384b4cd

1.9: e5e1c11f2fe9ed9de88488e5344033264cad03de ;;;","31/Jul/19 12:56;gaoyunhaii;Very thanks [~chesnay] [~till.rohrmann] [~zjwang], and sorry for not changing the status. I will pay attention the next time. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncDataStreamITCase.testOrderedWaitUsingAnonymousFunction instable on Travis,FLINK-13486,13247776,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,SleePy,tzulitai,tzulitai,30/Jul/19 04:13,07/Aug/19 07:29,13/Jul/23 08:10,06/Aug/19 10:08,,,,,,,,,,1.9.0,,,,API / DataStream,Tests,,,,0,pull-request-available,,,,"https://api.travis-ci.org/v3/job/562526494/log.txt

{code}
15:09:27.608 [ERROR] testOrderedWaitUsingAnonymousFunction(org.apache.flink.streaming.api.scala.AsyncDataStreamITCase)  Time elapsed: 1.315 s  <<< ERROR!
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.streaming.api.scala.AsyncDataStreamITCase.executeAndValidate(AsyncDataStreamITCase.scala:81)
	at org.apache.flink.streaming.api.scala.AsyncDataStreamITCase.testAsyncWaitUsingAnonymousFunction(AsyncDataStreamITCase.scala:135)
	at org.apache.flink.streaming.api.scala.AsyncDataStreamITCase.testOrderedWaitUsingAnonymousFunction(AsyncDataStreamITCase.scala:92)
Caused by: java.lang.Exception: An async function call terminated with an exception. Failing the AsyncWaitOperator.
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Async function call has timed out.
Caused by: java.util.concurrent.TimeoutException: Async function call has timed out.
{code}",,kkl0u,SleePy,trohrmann,tzulitai,,,,,,,,,,"ifndef-SleePy commented on pull request #9321: [FLINK-13486][tests] Optimize AsyncDataStreamITCase to alleviate the …
URL: https://github.com/apache/flink/pull/9321
 
 
   …pain of timeout
   
   ## What is the purpose of the change
   
   * Alleviate the timeout problem of `AsyncDataStreamITCase`
   
   ## Brief change log
   
   * Make a normal timeout threshold from 1s to 10s
   * Avoid time-consuming testing
   * Simplify redundant testing codes
   
   ## Verifying this change
   
   * This change is already covered by existing tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 12:17;githubbot;600","kl0u commented on pull request #9321: [FLINK-13486][tests] Optimize AsyncDataStreamITCase to alleviate the …
URL: https://github.com/apache/flink/pull/9321
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/19 10:08;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-13491,,,FLINK-13605,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 06 10:08:12 UTC 2019,,,,,,,,,,"0|z0558g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/19 11:07;chesnay;[~tzulitai] How sure are we that this is related to FLINK-13491?;;;","01/Aug/19 12:10;SleePy;Hi [~tzulitai], I believe this issue is not caused by FLINK-13491. Maybe the timeout of this case is too small (1 second). There is no any log outputted in that second. There might be a hiccup of testing machine I guess.

I will try to alleviate the timeout problem. Could you assign this ticket to me?;;;","02/Aug/19 08:13;trohrmann;[~kkl0u] will take a look at the PR.;;;","05/Aug/19 10:58;kkl0u;[~SleePy] Could you please move this issue to ""in-progress""?;;;","06/Aug/19 10:08;kkl0u;Merged on master with 9605cf4d41aab2a63b2a1e9902b762b438600323
and on release-1.9 with f77f4c20ae77027c32ffb70ba01a08b16e36cdcb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectedComponents end-to-end test instable with NoResourceAvailableException,FLINK-13484,13247774,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gjy,tzulitai,tzulitai,30/Jul/19 04:10,14/Aug/19 07:50,13/Jul/23 08:10,14/Aug/19 07:40,,,,,,,,,,1.8.2,1.9.1,,,Test Infrastructure,,,,,0,pull-request-available,,,,"The {{ConnectedComponents iterations with high parallelism}} e2e test seems to fail sporadically with {{NoResourceAvailableException}}.

https://api.travis-ci.org/v3/job/564894454/log.txt

{code}
2019-07-29 18:10:37,294 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Map (Map at main(HighParallelismIterationsTestProgram.java:50)) (9/25) (84f306767dabaa104d215bb429797833) switched from SCHEDULED to FAILED.
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate enough slots to run the job. Please make sure that the cluster has enough resources.
	at org.apache.flink.runtime.executiongraph.Execution.lambda$scheduleForExecution$0(Execution.java:459)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:190)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:694)
	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:482)
	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:378)
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:943)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:517)
	at akka.actor.Actor.aroundReceive$(Actor.scala:515)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2019-07-29 18:10:37,299 INFO  org.apache.flink.runtime.executiongraph.failover.AdaptedRestartPipelinedRegionStrategyNG  - Fail to pass the restart strategy validation in region failover. Fallback to fail global.
2019-07-29 18:10:37,299 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Flink Java Job at Mon Jul 29 18:05:26 UTC 2019 (2fd5c4d1583d85fb81ad98ea1176f9b4) switched from state RUNNING to FAILING.
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate enough slots to run the job. Please make sure that the cluster has enough resources.
	at org.apache.flink.runtime.executiongraph.Execution.lambda$scheduleForExecution$0(Execution.java:459)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:190)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:694)
	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:482)
	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:378)
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
	at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:943)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:517)
	at akka.actor.Actor.aroundReceive$(Actor.scala:515)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,f2005870@gmail.com,gjy,trohrmann,tzulitai,,,,,,,,,,"GJL commented on pull request #9425: [WIP][1.9][FLINK-13484]
URL: https://github.com/apache/flink/pull/9425
 
 
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/19 12:23;githubbot;600","GJL commented on pull request #9425: [1.9][FLINK-13484][tests] ConnectedComponents E2E test
URL: https://github.com/apache/flink/pull/9425
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 07:50;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 14 07:40:10 UTC 2019,,,,,,,,,,"0|z05580:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/19 07:08;trohrmann;Another instance of the test failure. Might be a different reason though: https://api.travis-ci.org/v3/job/569262839/log.txt;;;","09/Aug/19 07:08;trohrmann;Another instance: https://api.travis-ci.org/v3/job/569262845/log.txt;;;","09/Aug/19 07:13;trohrmann;Another instance: https://api.travis-ci.org/v3/job/569297593/log.txt;;;","09/Aug/19 07:56;gjy;I ran the test a couple of hundred times and it failed. See https://github.com/GJL/flink/commit/cc37367600f292a5b7a8bd25eaf50014087d52b7;;;","13/Aug/19 12:40;gjy;In my experiments, the test frequently fails due to {{AskTimeoutException}}.

The test attempts to run the job with a parallelism of 25 on 25 individual TaskManager processes, each configured with 44 mb of heap space.

It seems to me that the VM on Travis CI cannot handle this number of simultaneous TMs. Here is the relevant part of the JM log:
{noformat}
2019-08-12 13:11:34,185 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Join (Join at main(HighParallelismIterationsTestProgram.java:60)) -> Combine (MIN(1), at main(HighParallelismIterationsTestProgram.java:62) (10/25) (19811ac3816e2e2283596ecdff034321) switched from CREATED to SCHEDULED.
2019-08-12 13:11:37,694 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Join (Join at main(HighParallelismIterationsTestProgram.java:60)) -> Combine (MIN(1), at main(HighParallelismIterationsTestProgram.java:62) (10/25) (19811ac3816e2e2283596ecdff034321) switched from SCHEDULED to DEPLOYING.
2019-08-12 13:11:37,710 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Join (Join at main(HighParallelismIterationsTestProgram.java:60)) -> Combine (MIN(1), at main(HighParallelismIterationsTestProgram.java:62) (10/25) (attempt #0) to dadc9c817c5218f48538c3eaf1172788 @ localhost (dataPort=39211)
2019-08-12 13:11:47,735 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Join (Join at main(HighParallelismIterationsTestProgram.java:60)) -> Combine (MIN(1), at main(HighParallelismIterationsTestProgram.java:62) (10/25) (19811ac3816e2e2283596ecdff034321) switched from DEPLOYING to FAILED.
java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@127.0.0.1:46641/user/taskmanager_0#925859671]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
        at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:871)
        at akka.dispatch.OnComplete.internal(Future.scala:263)
        at akka.dispatch.OnComplete.internal(Future.scala:261)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
        at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
        at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
        at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
        at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
        at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:644)
        at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
        at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
        at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
        at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
        at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
        at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
        at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
        at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
        at java.lang.Thread.run(Thread.java:748)
{noformat}
As can be seen, the job fails due to a task deployment failure. The task is deployed at 13:11:37 and fails at 13:11:47, which matches the default akka ask timeout of 10s.
 However, on TM side the task submission is received at 13:11:47:
{noformat}
2019-08-12 13:11:47,989 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Received task CHAIN Join (Join at main(HighParallelismIterationsTestProgram.java:60)) -> Combine (MIN(1), at main(HighParallelismIterationsTestProgram.java:62) (10/25).
2019-08-12 13:11:48,404 INFO  org.apache.flink.runtime.taskmanager.Task                     - CHAIN Join (Join at main(HighParallelismIterationsTestProgram.java:60)) -> Combine (MIN(1), at main(Hig
hParallelismIterationsTestProgram.java:62) (10/25) (19811ac3816e2e2283596ecdff034321) switched from CREATED to DEPLOYING.
2019-08-12 13:11:48,422 INFO  org.apache.flink.runtime.taskmanager.Task                     - Creating FileSystem stream leak safety net for task CHAIN Join (Join at main(HighParallelismIterationsT
estProgram.java:60)) -> Combine (MIN(1), at main(HighParallelismIterationsTestProgram.java:62) (10/25) (19811ac3816e2e2283596ecdff034321) [DEPLOYING]
2019-08-12 13:11:48,422 INFO  org.apache.flink.runtime.taskmanager.Task                     - Loading JAR files for task CHAIN Join (Join at main(HighParallelismIterationsTestProgram.java:60)) -> Combine (MIN(1), at main(HighParallelismIterationsTestProgram.java:62) (10/25) (19811ac3816e2e2283596ecdff034321) [DEPLOYING].
2019-08-12 13:11:50,128 INFO  org.apache.flink.runtime.taskmanager.Task                     - Attempting to cancel task Partition (25/25) (32294ea5064d9bb8b67fa7f848909a52).
2019-08-12 13:11:50,128 INFO  org.apache.flink.runtime.taskmanager.Task                     - Partition (25/25) (32294ea5064d9bb8b67fa7f848909a52) switched from DEPLOYING to CANCELING.
{noformat}
No other exceptions can be found in the TM logs.

In addition, I have enabled verbose gc logging for TM and JM processes – nothing suspicious can be found there.

Another problem is that if the Flink test job is not completed successfully, the test script exits with code 0. We then assert that no exceptions are logged in the JM and TM logs. It would be cleaner to exit the test script with the same code that the Flink CLI returns, and maybe skip checking for exceptions in the logs.

All in all, I would propose the following changes:
 - Test should fail if Flink CLI exits with a code other than 0
 - Start 2 TMs with 13 slots each instead of 25 TMs with 1 slot;;;","14/Aug/19 07:40;gjy;1.8: 481332e240188ce0d0e8e2074ff452e0cbcad5ee
1.9: b40dba4e55f8d2d3663107ecc28dae43299d3701
1.10: b5e64e7ea950c3da7cc643f2bda198603ca24129
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
PrestoS3FileSystemITCase.testDirectoryListing fails on Travis,FLINK-13483,13247772,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,qqibrow,tzulitai,tzulitai,30/Jul/19 04:05,22/Jun/21 13:55,13/Jul/23 08:10,30/Mar/20 11:21,1.10.0,,,,,,,,,1.10.1,1.11.0,,,FileSystems,,,,,0,pull-request-available,test-stability,,,"https://api.travis-ci.org/v3/job/564894421/log.txt

{code}
PrestoS3FileSystemITCase>AbstractHadoopFileSystemITTest.testDirectoryListing:144->AbstractHadoopFileSystemITTest.checkPathExistence:61 expected:<false> but was:<true>
{code}

",,AHeise,dian.fu,kkl0u,liyu,qqibrow,tzulitai,,,,,,,,"qqibrow commented on pull request #11516:  [WIP] [FLINK-13483] Retry delete when checking path existence in AbstractHadoopFileSyste…
URL: https://github.com/apache/flink/pull/11516
 
 
   …mITTest
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   ## What is the purpose of the change
   
   This change fixes unit test 
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change fixes unit test 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Kubernetes/Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   no to all 
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Mar/20 23:53;githubbot;600","kl0u commented on pull request #11516:  [WIP] [FLINK-13483] Retry delete when checking path existence in AbstractHadoopFileSyste…
URL: https://github.com/apache/flink/pull/11516
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Mar/20 11:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 11:21:52 UTC 2020,,,,,,,,,,"0|z0557k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/20 03:21;dian.fu;Another instance: [https://api.travis-ci.org/v3/job/659586424/log.txt];;;","13/Mar/20 10:29;liyu;Another instance in release-1.10 crone build: https://api.travis-ci.org/v3/job/661626100/log.txt;;;","18/Mar/20 02:34;qqibrow;Could you assign it to me if no one's working on it? I am a presto contributor and want to help here. ;;;","18/Mar/20 02:57;liyu;Assigned, thanks for volunteering [~qqibrow]!;;;","19/Mar/20 18:41;qqibrow;[~liyu] thanks! Unfortunately I cannot reproduce the failure in my local setup. Do you know how often this happens? Does it start happening most recently? and how can i use travis to reproduce the failure? thanks! ;;;","23/Mar/20 02:09;dian.fu;Another instance of release-1.10 cron job: [https://api.travis-ci.org/v3/job/665248749/log.txt];;;","24/Mar/20 03:44;liyu;[~qqibrow] The failure should be intermittent and occurs every couple of days. Try searching for ""tar.gz"" in the crone job link and you could download the test output of the failure case from link like ""https://transfer.sh/A5PD6/42880.7.tar.gz"". You could also refer to this [private commit|https://github.com/GJL/flink/commit/7b342269d8e26e61c1a050cc4b9663e4754a775c] to enable e2e tests in your local branch.;;;","26/Mar/20 00:38;qqibrow;[~liyu] thanks for the reply. I find the problem might be PrestoS3FileSystem doesn't throw exception when fail to delete the file. I just come up with the walkaround. [https://github.com/apache/flink/pull/11516] . Since it's an intermittent error, it's hard to test such change. questions:
 # Do I need to patch the diff you mentioned ( [private commit|https://github.com/GJL/flink/commit/7b342269d8e26e61c1a050cc4b9663e4754a775c]) to run e2e test? I notice i can comment ""[@flinkbot|https://github.com/flinkbot] run travis"" in the PR to run travis
 # Is there a way to automatically run e2e test with this fix? ;;;","26/Mar/20 14:29;arvid;Azure pipeline now runs most e2e tests on each PR. So it's the easiest way to run that. Unfortunately, unless you also add AWS credentials to your Azure profile, quite a few tests are skipped.;;;","30/Mar/20 11:21;kkl0u;Merged on master with ff491bdbd9f4f6fe935bbe1485afacaf729ddd88
and on release-1.10 with b17a597dec80e590db2beedda446aa3cae9920dd;;;",,,,,,,,,,,,,,,,,,,,,,
Release partitions for FINISHED tasks if they are cancelled/suspended,FLINK-13476,13247662,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,trohrmann,trohrmann,29/Jul/19 13:20,04/Aug/19 07:37,13/Jul/23 08:10,04/Aug/19 07:37,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"With FLINK-12615 we removed that partitions are being explicitly released from the JM if an {{Execution}} which is in state {{FINISHED}} or {{FAILED}} is being cancelled. In order to not have resource leak when using pipelined result partitions whose consumers fail before start consuming, we should re-introduce the deleted else branch (removed via 408f6b67aefaccfc76708b2d4772eb7f0a8fd984).

Once we properly wait that a {{Task}} does not finish until its produced results have been either persisted or sent to a consumer, then we should be able to remove this branch again.",,azagrebin,gaoyunhaii,maguowei,trohrmann,tzulitai,zhuzh,zjwang,,,,,,,"zentol commented on pull request #9296: [FLINK-13476][coordination] Release pipelined partitions on vertex restart / job failure
URL: https://github.com/apache/flink/pull/9296
 
 
   Based on #9250.
   
   Only commits tagged for FLINK-13476 are relevant.
   
   Re-introduces a partition release call for pipelined partitions in case `Execution#cancel` is called after a vertex has been FINISHED. Since a task producing a pipelined partition may finish before any consumer ever attempted to consume the partition, it can happen that said partition is never release until the TE shuts down.
   That is because the partition lifecycle management only cares about blocking partitions.
   
   Note that cancel() is not only called for actually canceling the execution, but _always_ when a job fails or if the containing failover region is being restarted. This covers all cases where a pipelined partition can be produced, but may never be consumed.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 12:34;githubbot;600","zentol commented on pull request #9296: [FLINK-13476][coordination] Release pipelined partitions on vertex restart / job failure
URL: https://github.com/apache/flink/pull/9296
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Aug/19 07:36;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 04 07:37:41 UTC 2019,,,,,,,,,,"0|yi0g1v:a",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/19 04:42;tzulitai;Since this is a blocker for 1.9.0, we should remove the 1.10.0 tag as fix version. I will remove it.;;;","30/Jul/19 04:43;tzulitai;Is this the cause of https://issues.apache.org/jira/browse/FLINK-13487?;;;","31/Jul/19 10:23;chesnay;[~tzulitai] No, that's a separate issue in the testing setup.;;;","01/Aug/19 08:04;azagrebin;I am reviewing it;;;","04/Aug/19 07:37;chesnay;master: 572efd14205232595799addb1919d0dfa13312a1
1.9: a318c4053b5b26f91caf5940827c9a8af5d29a67;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource used by StateMapSnapshot can not be released if snapshot fails,FLINK-13469,13247634,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,banmoy,banmoy,banmoy,29/Jul/19 11:37,06/Nov/19 20:45,13/Jul/23 08:10,06/Nov/19 20:45,1.9.0,,,,,,,,,1.10.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"Currently, resource used by {{StateMapSnapshot}} is released in {{AbstractStateTableSnapshot#writeStateInKeyGroup}} as follows  
{code:java}
public void writeStateInKeyGroup(@Nonnull DataOutputView dov, int keyGroupId) throws IOException {
    StateMapSnapshot<K, N, S, ? extends StateMap<K, N, S>> stateMapSnapshot = getStateMapSnapshotForKeyGroup(keyGroupId);
    stateMapSnapshot.writeState(localKeySerializer, localNamespaceSerializer, localStateSerializer, dov, stateSnapshotTransformer);
    stateMapSnapshot.release();
}
{code}
If exception happens in {{StateMapSnapshot#writeState}}, resources used by this and left {{StateMapSnapshot}} s can not be released.",,banmoy,liyu,rmetzger,sewen,,,,,,,,,,"banmoy commented on pull request #9301: [FLINK-13469][state] Ensure resource used by StateMapSnapshot will be…
URL: https://github.com/apache/flink/pull/9301
 
 
   ## What is the purpose of the change
   
   Currently, resource used by `StateMapSnapshot` may not be released by `StateTableSnapshot` if snapshot fails. This PR will fix this problem.
   
   ## Brief change log
     
   - For `NestedStateMapSnapshot`, there is no resource to release, so  remove `stateMapSnapshot.release()` from `AbstractStateTableSnapshot#writeStateInKeyGroup()`
   - For `CopyOnWriteStateMapSnapshot`, we hope to release the resource as soon as possible after state in a key group is output successfully, so `CopyOnWriteStateTableSnapshot` overrides the method `writeStateInKeyGroup()`, and call `StateMapSnapshot.release()` after state is output
   - `CopyOnWriteStateTableSnapshot` overrides the method `release()` to ensure that resources used by all state map snapshots can be released if checkpoint is cancelled or exception happens where resource can't be released in `writeStateInKeyGroup()`
   - `CopyOnWriteStateMapSnapshot#release()` maybe called twice both in `release()` and `writeStateInKeyGroup()` of `CopyOnWriteStateTableSnapshot`, so we use a flag `CopyOnWriteStateMapSnapshot#released` to ensure the resource will not be released repeatedly
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - Added test `CopyOnWriteStateTableTest#testReleaseSnapshotResource` to validates that resources can be released no matter snapshot is successful or failed
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 13:30;githubbot;600","asfgit commented on pull request #9301: [FLINK-13469][state] Ensure resource used by StateMapSnapshot will be released if snapshot fails
URL: https://github.com/apache/flink/pull/9301
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/19 20:42;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 06 20:45:35 UTC 2019,,,,,,,,,,"0|z054cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/19 12:09;banmoy;[~tzulitai] Hi, sorry for introduing this bug in [FLINK-12693|https://issues.apache.org/jira/browse/FLINK-12693] which has been included in release-1.9. I have prepared the PR, can we fix it in 1.9?;;;","30/Jul/19 07:19;banmoy;[~srichter] Hi, I have prepared the PR, and can you help to assign this issue to me?;;;","31/Jul/19 12:38;rmetzger;After talking to [~carp84] offline, I assigned the ticket to you.;;;","06/Nov/19 20:45;sewen;Fixed via 02c4f202bf0a5b9a40cedc480e9a9d927092080d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL VALUES might fail for Blink planner,FLINK-13463,13247608,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,twalthr,twalthr,twalthr,29/Jul/19 09:24,05/Aug/19 09:16,13/Jul/23 08:10,05/Aug/19 09:16,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,,,,,"Executing the following statement in SQL Client of FLINK-13458:
{code}
SELECT name, COUNT(*) AS cnt FROM (VALUES ('Bob'), ('Alice'), ('Greg'), ('Bob')) AS NameTable(name) GROUP BY name;
{code}

Leads to:
{code}
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:206)
Caused by: org.apache.flink.table.api.TableException: Unsupported conversion from data type 'VARCHAR(5) NOT NULL' (conversion class: java.lang.String) to type information. Only data types that originated from type information fully support a reverse conversion.
	at org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter.toLegacyTypeInfo(LegacyTypeInfoDataTypeConverter.java:242)
	at org.apache.flink.table.types.utils.TypeConversions.fromDataTypeToLegacyInfo(TypeConversions.java:49)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545)
	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
	at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:438)
	at org.apache.flink.table.types.utils.TypeConversions.fromDataTypeToLegacyInfo(TypeConversions.java:55)
	at org.apache.flink.table.api.TableSchema.getFieldTypes(TableSchema.java:129)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.removeTimeAttributes(LocalExecutor.java:609)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQueryInternal(LocalExecutor.java:465)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQuery(LocalExecutor.java:316)
	at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:469)
	at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:291)
	at java.util.Optional.ifPresent(Optional.java:159)
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:200)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:123)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:105)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:194)
{code}

A solution needs some investigation.",,jark,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 05 09:16:01 UTC 2019,,,,,,,,,,"0|z05474:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/19 09:16;twalthr;[FLINK-13463][table-planner-blink] Add test case for VALUES with char literal
Fixed in 1.10.0: 80e81c7e34b48e1e0d8cf4f2e282307744c0dc2f
Fixed in 1.9.0: ae0dba54746e7bedf72ed71c36c8a9f48b593744

[FLINK-13463][table-common] Relax legacy type info conversion for VARCHAR literals
Fixed in 1.10.0: 63e9a167fba59be2addaedc74e5d235ec6739832
Fixed in 1.9.0: 2f4e5eab3ee983f76952a4867403eced7bdd32de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pipelined region failover strategy does not recover Job if checkpoint cannot be read,FLINK-13452,13247542,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,gjy,gjy,28/Jul/19 18:13,02/Oct/19 17:50,13/Jul/23 08:10,07/Aug/19 07:56,1.10.0,1.9.0,,,,,,,,1.9.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The job does not recover if a checkpoint cannot be read and {{jobmanager.execution.failover-strategy}} is set to _""region""_. 

*Analysis*

The {{RestartCallback}} created by {{AdaptedRestartPipelinedRegionStrategyNG}} throws a \{{RuntimeException}} if no checkpoints could be read. When the restart is invoked in a separate thread pool, the exception is swallowed. See:

[https://github.com/apache/flink/blob/21621fbcde534969b748f21e9f8983e3f4e0fb1d/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/failover/AdaptedRestartPipelinedRegionStrategyNG.java#L117-L119]

[https://github.com/apache/flink/blob/21621fbcde534969b748f21e9f8983e3f4e0fb1d/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/restart/FixedDelayRestartStrategy.java#L65]

*Expected behavior*
 * Job should restart

 ",,gjy,liyu,maguowei,yunta,zhuzh,,,,,,,,,"Myasuka commented on pull request #9268: [FLINK-13452] Ensure to fail global when exception happens during reseting tasks of regions
URL: https://github.com/apache/flink/pull/9268
 
 
   ## What is the purpose of the change
   
   After [FLINK-13060](https://issues.apache.org/jira/browse/FLINK-13060), we would run `createResetAndRescheduleTasksCallback` within another runnable `resetAndRescheduleTasks`. Unfortunately, any exception happened in `createResetAndRescheduleTasksCallback` would cause the thread terminated silently but record the exception in `outcome` of `FutureTask`. We should change the code back to previously that would `failGlobal` within the `createResetAndRescheduleTasksCallback` runnable.
   
   
   ## Brief change log
   
     - Let runnable `createResetAndRescheduleTasksCallback` fail global if come across any exception.
     - Refine `RegionFailoverITCase` to mock the exception that checkpoint store would failed when recover from checkpoint for the 1st time.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Refine `RegionFailoverITCase` to mock the exception that checkpoint store would failed when recover from checkpoint for the 1st time.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 04:30;githubbot;600","Myasuka commented on pull request #9268: [FLINK-13452] Ensure to fail global when exception happens during reseting tasks of regions
URL: https://github.com/apache/flink/pull/9268
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 10:32;githubbot;600","Myasuka commented on pull request #9268: [FLINK-13452] Ensure to fail global when exception happens during reseting tasks of regions
URL: https://github.com/apache/flink/pull/9268
 
 
   ## What is the purpose of the change
   
   After [FLINK-13060](https://issues.apache.org/jira/browse/FLINK-13060), we would run `createResetAndRescheduleTasksCallback` within another runnable `resetAndRescheduleTasks`. Unfortunately, any exception happened in `createResetAndRescheduleTasksCallback` would cause the thread terminated silently but record the exception in `outcome` of `FutureTask`. We should change the code back to previously that would `failGlobal` within the `createResetAndRescheduleTasksCallback` runnable.
   
   
   ## Brief change log
   
     - Let runnable `createResetAndRescheduleTasksCallback` fail global if come across any exception.
     - Refine `RegionFailoverITCase` to mock the exception that checkpoint store would failed when recover from checkpoint for the 1st time.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Refine `RegionFailoverITCase` to mock the exception that checkpoint store would failed when recover from checkpoint for the 1st time.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 09:42;githubbot;600","GJL commented on pull request #9376: [1.9][FLINK-13452][runtime] Ensure to fail global when exception happens during reseting tasks of regions
URL: https://github.com/apache/flink/pull/9376
 
 
   Backport to 1.9 of #9268 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Aug/19 04:53;githubbot;600","asfgit commented on pull request #9376: [1.9][FLINK-13452][runtime] Ensure to fail global when exception happens during reseting tasks of regions
URL: https://github.com/apache/flink/pull/9376
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Aug/19 07:54;githubbot;600","GJL commented on pull request #9268: [FLINK-13452] Ensure to fail global when exception happens during reseting tasks of regions
URL: https://github.com/apache/flink/pull/9268
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Aug/19 13:18;githubbot;600",,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,FLINK-13060,,,,,,,,,,,,,,,"28/Jul/19 18:03;gjy;jobmanager.log;https://issues.apache.org/jira/secure/attachment/12976070/jobmanager.log",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 07 07:56:22 UTC 2019,,,,,,,,,,"0|z053sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/19 18:08;yunta;Before FLINK-13060, the process to restart tasks within {{AdaptedRestartPipelinedRegionStrategyNG}} is:
{code:java}
cancelTasks --> resetTasks --> handleExcpetion
{code}
After FLINK-13060, the process changed to:
{code:java}
                   resetTasks
                       |^|
                       | |
cancelTasks --> strategy restart --> handleExcpetion                       
{code}
The bug happened due to the future task of {{resetTasks}} failed but die silently. And as you can see, this bug has no direct relationship between region failover to load checkpointed state.

Previously I try to catch any exception within {{resetTasks}} itself to {{failGlobal}} in my [PR-9268|https://github.com/apache/flink/pull/9268]. However, this would meet another problem if {{failGlobal}} come across an exception and cannot be caught by {{FatalExitExceptionHandler}}.

I still have not found any graceful solution to this bug, [~gjy], [~chesnay] do you have any suggestions for this?

 ;;;","31/Jul/19 08:26;zhuzh;I think this happens because current implemented RestartStrategies do not handle exceptions from the restart callback. If it is by design, all _RestartCallback_ implementations should catch all exceptions in _triggerFullRecovery_ and handle them to recover the job (via failGlobal maybe).

For this case, fix the _AdaptedRestartPipelinedRegionStrategyNG#createResetAndRescheduleTasksCallback_ to catch all exceptions and invoke failGlobal to recover would be a proper choice.

As for _failGlobal_, IMO, any exception directly thrown from it is unexpected. You can explicitly use _FatalExitExceptionHandler_ to handle this unexpected case, which is aligned with what in _AdaptedRestartPipelinedRegionStrategyNG#restartTasks_. ;;;","07/Aug/19 07:56;gjy;1.9: b931e9c10b231ed1823fe6a97bccf73bb835dbc2
1.10: 9828f5317cd4130d8518df5762bdd479b294b272;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support date type in Hive,FLINK-13438,13247264,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,TsReaper,TsReaper,26/Jul/19 09:58,09/Dec/19 12:29,13/Jul/23 08:10,09/Dec/19 12:29,,,,,,,,,,1.10.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Similar to JDBC connectors, Hive connectors communicate with Flink framework using TableSchema, which contains DataType. As the time data read from and write to Hive connectors must be java.sql.* types and the default conversion class of our time data types are java.time.*, we have to fix Hive connector with DataTypes.DATE/TIME/TIMESTAMP support.

But currently when reading tables from Hive, the table schema is created using Hive's schema, so the time types in the created schema will be sql time type not local time type. If user specifies a local time type in the table schema when creating a table in Hive, he will get a different schema when reading it out. This is undesired.",,jark,lirui,lzljs3620320,Terry1897,TsReaper,twalthr,xuefuz,,,,,,,"TsReaper commented on pull request #9342: [FLINK-13438][hive] Fix DataTypes.DATE/TIME/TIMESTAMP support for hive connectors
URL: https://github.com/apache/flink/pull/9342
 
 
   ## What is the purpose of the change
   
   Similar to JDBC connectors, Hive connectors communicate with Flink framework using TableSchema, which contains DataType. As the time data read from and write to Hive connectors must be `java.sql.*` types and the default conversion class of our time data types are `java.time.*`, we have to fix Hive connector with DataTypes.DATE/TIME/TIMESTAMP support.
   
   This PR fixes `DataTypes.DATE/TIME/TIMESTAMP` support for hive connectors, and is similar to PR #9236 .
   
   ## Brief change log
   
    - Fix `DataTypes.DATE/TIME/TIMESTAMP` support for hive connectors.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows: run the newly added `HiveFullTest`. This new test is to test if the time data can correctly flow from hive source to flink system and finally to hive sink.
   
   There seems to be other bugs in hive connectors. When running this tests along with other hive tests in idea, `HiveTableSinkTest` will fail; bug when running this tests along with others in maven the tests will all pass. @zjuwangg please take a look into this issue.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 16:38;githubbot;600","TsReaper commented on pull request #9342: [FLINK-13438][hive] Fix DataTypes.DATE/TIME/TIMESTAMP support for hive connectors
URL: https://github.com/apache/flink/pull/9342
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Dec/19 08:21;githubbot;600","lirui-apache commented on pull request #10494: [FLINK-13438][hive] Support date type in Hive
URL: https://github.com/apache/flink/pull/10494
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix our support for date type in Hive.
   
   
   ## Brief change log
   
     - Add shim method to convert date instances between Flink and Hive
     - Add tests to verify read/write date type
   
   
   ## Verifying this change
   
   New test cases
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Dec/19 09:14;githubbot;600","KurtYoung commented on pull request #10494: [FLINK-13438][hive] Support date type in Hive
URL: https://github.com/apache/flink/pull/10494
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Dec/19 12:29;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,FLINK-15020,,,,,,,,,,,"29/Jul/19 01:05;TsReaper;0001-hive.patch;https://issues.apache.org/jira/secure/attachment/12976082/0001-hive.patch",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 09 12:29:35 UTC 2019,,,,,,,,,,"0|z0522o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/19 10:00;jark;cc [~lirui], [~xuefuz];;;","26/Jul/19 11:52;lirui;[~TsReaper] Not sure if I understand the issue correctly. When creating TableSchema for a Hive table, we map Hive's {{TIMESTAMP}} type to {{DataTypes.TIMESTAMP()}} in Flink. Do you mean there's something wrong with this type mapping? Or do you mean the records we get for a timestamp column are of wrong type? If it's the latter, we convert objects between Hive and Flink in {{HiveInspectors}}, and we can add some conversion for date/timestamp type.;;;","26/Jul/19 12:32;jark;Hi [~lirui], by default, {{DataTypes.TIMESTAMP()}} bridge to the {{java.time.LocalDateTime}} class. However, we are returning a {{java.sql.Timestamp}} from Hive table?;;;","28/Jul/19 04:06;lirui;[~jark] yes, Hive returns `java.sql.Timestamp` for timestamp columns (at least for the Hive versions we currently support). We can do some conversion between `Timestamp` and `LocalDateTime `. But I suppose there might be issues due to different semantics of these two. E.g. DST has an impact on the string presentation of `Timestamp`, but not `LocalDateTime`. I'll do some investigation to verify.;;;","28/Jul/19 12:46;lzljs3620320;Hi [~lirui] , I think should add tests to DATE/TIME/TIMESTAMP type in Hive source, sink and udx using blink-planner (and maybe using flink-planner too).

Hi [~TsReaper], I think you should explain which case and code will lead to this bug in Jira to let the problem more understandable.;;;","29/Jul/19 01:05;TsReaper;Hi [~lirui] [~jark] and [~lzljs3620320], sorry for the late response. I actually tried to add support for DataTypes.DATE/TIME/TIMESTAMP before submitting this issue, and my patch is provided in the attachment. In this patch, when running `HiveCatalogDataTypeTest`, two tests will fail due to this issue. Please take a look.;;;","29/Jul/19 07:42;jark;Hi [~lirui], will you help to fix this issue? If yes, I can assign this to you.;;;","29/Jul/19 10:34;jark;[~lzljs3620320] will help to review this. ;;;","29/Jul/19 21:51;xuefuz;Hi [~TsReaper], Thank you for reporting the issue and providing a fix. I think I understand the problem after reading the whole discussion and your patch. 

To help others understand the problem: Yes, Hive represents Date/TimeStamp data with  classes are in java.sql.*, while Flink expects java.time.LocalDate objects (by default). For this, conversions are needed. However, the current code assumes an identity conversion.

Since you have already provided a patch here, I'd suggest you convert it to a PR at github as that's how code is contributed in Flink. You might also want to rephrase the description here to better outline the problem. Your fix seems reasonable at my first glance. As critical as it can be, we'd like to include a fix for this in 1.9.

BTW, the new test cases about those types are missing in your patch (possibly in the new file HiveCatalogDataTypeTest.java which is not included).;;;","09/Dec/19 12:29;ykt836;master: 2994f0e44b53c85535f2f29fb43d320ace91f6f8;;;",,,,,,,,,,,,,,,,,,,,,,
NameNode HA configuration was not loaded when running HiveConnector on Yarn,FLINK-13431,13247247,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hongtao12310,hongtao12310,hongtao12310,26/Jul/19 08:54,31/Jul/19 07:38,13/Jul/23 08:10,31/Jul/19 07:38,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Reading the Hive table (version 1.2.1) and write into the hdfs file

 

launch the job from SQL Client

 

 

*StackTrace:*

 
Caused by: java.lang.IllegalArgumentException: [java.net.UnknownHostException|http://java.net.unknownhostexception/]: idc-nn
        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:374)
        at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310)
        at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:668)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:604)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2598)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2632)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2614)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:97)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205)
        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
        at org.apache.flink.batch.connectors.hive.HiveTableInputFormat.createInputSplits(HiveTableInputFormat.java:164)
        at org.apache.flink.batch.connectors.hive.HiveTableInputFormat.createInputSplits(HiveTableInputFormat.java:67)
        at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.<init>(ExecutionJobVertex.java:256)
        ... 22 more
Caused by: [java.net.UnknownHostException|http://java.net.unknownhostexception/]: idc-nn
 ","CDH 5.13

Hive 1.2.1

Flink 1.9.0",hongtao12310,Terry1897,xuefuz,,,,,,,,,,,"hongtao12310 commented on pull request #9237: [FLINK-13431][hive] NameNode HA configuration was not loaded when running HiveConnector on Yarn
URL: https://github.com/apache/flink/pull/9237
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 11:09;githubbot;600","hongtao12310 commented on pull request #9237: [FLINK-13431][hive] NameNode HA configuration was not loaded when running HiveConnector on Yarn
URL: https://github.com/apache/flink/pull/9237
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jul/19 07:36;githubbot;600","hongtao12310 commented on pull request #9237: [FLINK-13431][hive] NameNode HA configuration was not loaded when running HiveConnector on Yarn
URL: https://github.com/apache/flink/pull/9237
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 04:36;githubbot;600","KurtYoung commented on pull request #9237: [FLINK-13431][hive] NameNode HA configuration was not loaded when running HiveConnector on Yarn
URL: https://github.com/apache/flink/pull/9237
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 07:34;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 31 07:38:03 UTC 2019,,,,,,,,,,"0|z051yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/19 09:09;hongtao12310;I have identified the issue and will launch a PR;;;","31/Jul/19 07:38;ykt836;merged in 1.9.0: f057286da1ae454cda2ee820e171aad936c294bc

merged in master (1.10.0): a9d393167cc3927f55b196baa918deea29695225;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client end-to-end test fails,FLINK-13429,13247070,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,twalthr,twalthr,25/Jul/19 13:51,30/Jul/19 09:02,13/Jul/23 08:10,29/Jul/19 08:06,,,,,,,,,,1.9.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"The SQL Client test does not work on the current master and hangs when executing CEP SQL. We reproduced this on two machines.

At commit 475c30cd4064a7bc2e32c963b6ca58e7623251c6 it was working.",,twalthr,,,,,,,,,,,,,"twalthr commented on pull request #9241: [FLINK-13429][table-common] Fix BoundedOutOfOrderTimestamps watermark strategy
URL: https://github.com/apache/flink/pull/9241
 
 
   ## What is the purpose of the change
   
   Fixes a bug that was introduced with FLINK-13315 in de1a8a0444c231df6c57a70cefb689aa7126a502 while porting watermark strategies to Java. This ensures the old behavior and is also in sync with the DataStream API timestamp extractor.
   
   ## Brief change log
   
   BoundedOutOfOrderTimestamps updated.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as SQL Client e2e test.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 13:18;githubbot;600","asfgit commented on pull request #9241: [FLINK-13429][table-common] Fix BoundedOutOfOrderTimestamps watermark strategy
URL: https://github.com/apache/flink/pull/9241
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 08:04;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,FLINK-13492,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 29 08:06:19 UTC 2019,,,,,,,,,,"0|z04lx0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/19 08:06;twalthr;Fixed in 1.10.0: b670b11f303657aa8175ec933ee29b377cb9e087
Fixed in 1.9.0: 0224d9bc0a773633943282f5268770a8063a87a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalog's createFunction fails when function name has upper-case characters,FLINK-13427,13247053,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lzljs3620320,lzljs3620320,25/Jul/19 12:20,06/Aug/19 07:22,13/Jul/23 08:10,02/Aug/19 04:53,,,,,,,,,,1.9.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,," 
{code:java}
hiveCatalog.createFunction(
      new ObjectPath(HiveCatalog.DEFAULT_DB, ""myUdf""),
      new CatalogFunctionImpl(TestSimpleUDF.class.getCanonicalName(), new HashMap<>()),
      false);

hiveCatalog.getFunction(new ObjectPath(HiveCatalog.DEFAULT_DB, ""myUdf""));
{code}
There is an exception now:
{code:java}
org.apache.flink.table.catalog.exceptions.FunctionNotExistException: Function default.myUdf does not exist in Catalog test-catalog.

at org.apache.flink.table.catalog.hive.HiveCatalog.getFunction(HiveCatalog.java:1030)
at org.apache.flink.table.catalog.hive.HiveCatalogITCase.testGenericTable(HiveCatalogITCase.java:146)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
at org.junit.rules.RunRules.evaluate(RunRules.java:20)
at org.apache.flink.batch.connectors.hive.FlinkStandaloneHiveRunner.runTestMethod(FlinkStandaloneHiveRunner.java:170)
at org.apache.flink.batch.connectors.hive.FlinkStandaloneHiveRunner.runChild(FlinkStandaloneHiveRunner.java:155)
at org.apache.flink.batch.connectors.hive.FlinkStandaloneHiveRunner.runChild(FlinkStandaloneHiveRunner.java:93)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
at org.junit.rules.RunRules.evaluate(RunRules.java:20)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: NoSuchObjectException(message:Function default.myUdf does not exist)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_function_result$get_function_resultStandardScheme.read(ThriftHiveMetastore.java)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_function_result$get_function_resultStandardScheme.read(ThriftHiveMetastore.java)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_function_result.read(ThriftHiveMetastore.java)
{code}
Seems there are some bugs in HiveCatalog when use upper.

Maybe we should normalizeName in createFunction...",,jark,lirui,lzljs3620320,xuefuz,,,,,,,,,,"lirui-apache commented on pull request #9254: [FLINK-13427][hive] HiveCatalog's createFunction fails when function …
URL: https://github.com/apache/flink/pull/9254
 
 
   …name has upper-case characters
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To fix the issue that function name is not normalized in HiveCatalog.
   
   
   ## Brief change log
   
     - Normalize function name when a Hive function is created.
     - Added a common base class for `HiveCatalogHiveMetadataTest` and `HiveCatalogGenericMetadataTest`. And a new test case to make sure HiveCatalog is case-insensitive when creating a function.
   
   
   ## Verifying this change
   
   New test case added.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 05:32;githubbot;600","asfgit commented on pull request #9254: [FLINK-13427][hive] HiveCatalog's createFunction fails when function …
URL: https://github.com/apache/flink/pull/9254
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 04:52;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 04:53:15 UTC 2019,,,,,,,,,,"0|z050rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/19 12:25;lzljs3620320;CC: [~xuefuz] [~phoenixjiangnan] ;;;","26/Jul/19 04:12;lirui;This is a bug of Hive metastore API. Name is not normalized when a function is created. But it's normalized to lower case when getting a function from HMS. Thus any function created with a name containing upper case characters cannot be retrieved later on.
Since HiveCatalog is case-insensitive, I think we can normalize the function name on our side.;;;","29/Jul/19 01:49;ykt836;[~lzljs3620320] , [~lirui] are you guys working on this?;;;","29/Jul/19 02:37;lirui;I'll work on it if nobody's working on it yet.;;;","29/Jul/19 03:30;ykt836;Thanks [~lirui], I will assign this to you.;;;","02/Aug/19 04:53;jark;Fixed in 1.10.0: 36fdbefc098421781a7cd56ed3440336d29f5811
Fixed in 1.9.0: 32dd1b17eee257ff2e7f757c15a3ee54ff6dee2d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalog should add hive version in conf,FLINK-13424,13247002,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,lirui,lirui,25/Jul/19 08:46,06/Aug/19 07:22,13/Jul/23 08:10,01/Aug/19 23:31,,,,,,,,,,1.9.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,{{HiveTableSource}} and {{HiveTableSink}} retrieve hive version from conf. Therefore {{HiveCatalog}} has to add it.,,lirui,phoenixjiangnan,xuefuz,,,,,,,,,,,"lirui-apache commented on pull request #9232: [FLINK-13424][hive] HiveCatalog should add hive version in conf
URL: https://github.com/apache/flink/pull/9232
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To avoid overriding the hive version users specify in the yaml file.
   
   
   ## Brief change log
   
     - Add hive version to hive conf in `HiveCatalog`
     - Hive table factory and source/sink will retrieve hive version from hive conf. And errors out if hive version is not present.
   
   
   ## Verifying this change
   
   Existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 03:22;githubbot;600","asfgit commented on pull request #9232: [FLINK-13424][hive] HiveCatalog should add hive version in conf
URL: https://github.com/apache/flink/pull/9232
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 23:28;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 01 23:31:03 UTC 2019,,,,,,,,,,"0|z050gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/19 23:31;phoenixjiangnan;merged in master: b20ac47258228aed51be0d0e6fc958ac303ce4a1  1.9.0: 4b8d35c7f9eb6573e3b099fd2ca8d4ca42ccae72;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to find function in hive 1,FLINK-13423,13247000,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lirui,zjffdu,zjffdu,25/Jul/19 08:27,01/Aug/19 10:58,13/Jul/23 08:10,01/Aug/19 04:09,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"I hit the following error when I try to use count in sql on hive1
{code:java}
btenv.sqlQuery(""select count(1) from date_dim"").toDataSet[Row].print(){code}
{code:java}
org.apache.flink.table.api.ValidationException: SQL validation failed. Failed to get function tpcds_text_2.COUNT
at org.apache.flink.table.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:127)
at org.apache.flink.table.api.internal.TableEnvImpl.sqlQuery(TableEnvImpl.scala:427)
... 30 elided
Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to get function tpcds_text_2.COUNT
at org.apache.flink.table.catalog.hive.HiveCatalog.getFunction(HiveCatalog.java:1033)
at org.apache.flink.table.catalog.FunctionCatalog.lookupFunction(FunctionCatalog.java:167)
at org.apache.flink.table.catalog.FunctionCatalogOperatorTable.lookupOperatorOverloads(FunctionCatalogOperatorTable.java:74)
at org.apache.calcite.sql.util.ChainedSqlOperatorTable.lookupOperatorOverloads(ChainedSqlOperatorTable.java:73)
at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1183)
at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1198)
at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1168)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:925)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:639)
at org.apache.flink.table.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:123)
... 31 more{code}",,jark,lirui,Terry1897,xuefuz,zjffdu,,,,,,,,,"asfgit commented on pull request #9279: [FLINK-13423][hive] Unable to find function in hive 1
URL: https://github.com/apache/flink/pull/9279
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 04:09;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 01 04:09:12 UTC 2019,,,,,,,,,,"0|z050g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/19 08:29;zjffdu;\cc [~lirui] [~xuefuz];;;","25/Jul/19 08:57;lirui;I think I hit the same issue when worked on FLINK-13303. I'll extract the fix and submit a separate PR for it.;;;","29/Jul/19 01:45;ykt836;I will assign this issue to you [~lirui];;;","01/Aug/19 04:09;jark;Fixed in 1.10.0:  63010ca20da4c4d5da50b811f32ca15f58df8f6b
Fixed in 1.9.0: f0220de465cf320a925d4d936afbd51fd0f9991e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected ConcurrentModificationException when RM notifies JM about allocation failure,FLINK-13421,13246991,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,trohrmann,zhuzh,zhuzh,25/Jul/19 07:59,02/Aug/19 13:16,13/Jul/23 08:10,02/Aug/19 08:00,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When a TM lost and RM identified it first, it will notify JM about it through JobMaster#notifyAllocationFailure.

We observed unexpected ConcurrentModificationException in this process, stack as below:

 

Caused by: java.util.ConcurrentModificationException

        at java.util.HashMap$HashIterator.nextNode(HashMap.java:1437)

        at java.util.HashMap$ValueIterator.next(HashMap.java:1466)

        at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:477)

        at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:149)

        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.tryFailingAllocatedSlot(SlotPoolImpl.java:712)

        at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.failAllocation(SlotPoolImpl.java:692)

        at org.apache.flink.runtime.jobmaster.JobMaster.internalFailAllocation(JobMaster.java:538)

        at org.apache.flink.runtime.jobmaster.JobMaster.notifyAllocationFailure(JobMaster.java:664)

        ... 26 more

 

This can cause an allocated slot to be removed from SlotPool#allocatedSlots but not all of its payload tasks get failed. Tasks may hang in scheduled forever, not able to fail or timeout, as in the attached log.

 

 ",,gaoyunhaii,hwanju,maguowei,trohrmann,xtsong,zhuzh,,,,,,,,"zhuzhurk commented on pull request #9288: [FLINK-13421] [runtime] Make MultiTaskSlot not available for allocation when it’s releasing c…
URL: https://github.com/apache/flink/pull/9288
 
 
   …hildren to avoid ConcurrentModificationException
   
   ## What is the purpose of the change
   
   *ConcurrentModificationException happened in the children release loop in MultiTaskSlot#release.*
   
   *It happened because new slot allocations happened, adding a new child to the MultiTaskSlot, thus modified the children field.*
   
   *New slot allocations happened because a task failover is caused by a child slot's payload releasing. The failover canceled tasks and returned allocated slots to SlotPool. The returned allocated slots were assigned to some tasks that were not canceled yet. The slot assignment satisfies the input location preference constraints of some downstream tasks, and triggered the slot allocation of these tasks.*
   
   *To fix this, this PR changes MultiTaskSlot to be not available for slot allocation when it is in releasingChildren state. The change is in SlotSharingManager#listResolvedRootSlotInfo.(SlotSharingManager#getUnresolvedRootSlot is not necessary for a change as unresolved multi-task slot is simple and does not lead to a slot allocation.) *
   
   
   ## Brief change log
   
   *(for example:)*
     - *in SlotSharingManager#listResolvedRootSlotInfo*
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added uint test SlotSharingManagerTest#testResolvedSlotInReleasingIsNotAvailable*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**yes** / no / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 07:07;githubbot;600","tillrohrmann commented on pull request #9288: [FLINK-13421] [runtime] Do not allocate slots in a MultiTaskSlot when it’s releasing children
URL: https://github.com/apache/flink/pull/9288
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 08:01;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/19 08:34;zhuzh;jm_concurrentmodification.log;https://issues.apache.org/jira/secure/attachment/12975771/jm_concurrentmodification.log",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 08:00:37 UTC 2019,,,,,,,,,,"0|z04lwy:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/19 09:10;zhuzh;All job modification is happening in the main thread.

Therefore the ConcurrentModificationException seems to happen due to MultiTaskSlot children getting changed in the MultiTaskSlot's releasing iteration loop.;;;","25/Jul/19 13:58;zhuzh;I think I find the cause.

ConcurrentModificationException happened in the children release loop in _*MultiTaskSlot#release*_.

It happened because new slot allocations happened, adding a new child to the MultiTaskSlot, thus modified the *_children_* field.

New slot allocations happened because a task failover is caused by a child slot's payload releasing. The failover canceled tasks and returned allocated slots to SlotPool. The returned allocated slots were assigned to some tasks that were not canceled yet. The slot assignment satisfies the _*input location preference constraints*_ of some downstream tasks, and triggered the slot allocation of these tasks.

To fix this, I think we may change _*MultiTaskSlot*_ to be not available for slot allocation(in SlotSharingManager#listResolvedRootSlotInfo and SlotSharingManager#getUnresolvedRootSlot) when it is in _*releasingChildren*_ state. Besides, changing _*MultiTaskSlot#release*_ to not iterate on its _*children*_ field directly would be better to avoid ConcurrentModificationException to happen in any case.

 

Hi [~till.rohrmann], do you have any suggestion for it?;;;","26/Jul/19 07:17;trohrmann;That is a good finding [~zhuzh]. I need to take a closer look to verify your solution proposal.;;;","26/Jul/19 07:19;trohrmann;Did you observe this while testing Flink or was it a test which failed?;;;","26/Jul/19 07:26;zhuzh;We observed it when running stability test on Flink 1.9. The log is attached.;;;","26/Jul/19 09:16;trohrmann;Thanks a lot for the investigation [~zhuzh]. I've assigned the ticket to you.;;;","29/Jul/19 06:05;zhuzh;Work in progress.;;;","30/Jul/19 09:01;trohrmann;Hi [~zhuzh], what's the state of this issue?;;;","31/Jul/19 07:11;zhuzh;Hi [~till.rohrmann], the PR is opened.

The solution is a bit different from what was proposed.

Only the SlotSharingManager#listResolvedRootSlotInfo is changed. SlotSharingManager#getUnresolvedRootSlot and MultiTaskSlot#release are not changed as they are not necessary or may cause some other inconsistency issues.;;;","02/Aug/19 08:00;trohrmann;Fixed via

1.10.0: 38411298a2fa71194cd7d81405ee8f6b069f377f
1.9.0: ed38718a9b6b1fd55cf2107821813272083a4b63;;;",,,,,,,,,,,,,,,,,,,,,,
Remove Hive and Hadoop dependencies from SQL Client,FLINK-13400,13246759,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,twalthr,twalthr,24/Jul/19 09:00,08/Jun/23 01:48,13/Jul/23 08:10,08/Jun/23 01:48,,,,,,,,,,,,,,Table SQL / Client,,,,,0,auto-deprioritized-critical,auto-deprioritized-major,pull-request-available,stale-assigned,"340/550 lines in the SQL Client {{pom.xml}} are just around Hive and Hadoop dependencies.  Hive has nothing to do with the SQL Client and it will be hard to maintain the long list of  exclusion there. Some dependencies are even in a {{provided}} scope and not {{test}} scope.

We should remove all dependencies on Hive/Hadoop and replace catalog-related tests by a testing catalog. Similar to how we tests source/sinks.",,frank wang,fsk119,godfreyhe,hackergin,jark,lincoln.86xy,lirui,martijnvisser,twalthr,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30660,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 09 10:43:49 UTC 2021,,,,,,,,,,"0|z04yyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/19 09:35;chesnay;I agree, the functionality of the SQL client should be tested independently of any specific (non-testing) catalog implementation.

Whether Hive work with the SQL client should either be verified by an IT case placed in the hive-connector module or as an e2e test.;;;","16/Apr/21 11:06;flink-jira-bot;This issue is assigned but has not received an update in 7 days so it has been labeled ""stale-assigned"". If you are still working on the issue, please give an update and remove the label. If you are no longer working on the issue, please unassign so someone else may work on it. In 7 days the issue will be automatically unassigned.;;;","27/Apr/21 23:02;flink-jira-bot;This issue was marked ""stale-assigned"" and has not received an update in 7 days. It is now automatically unassigned. If you are still working on it, you can assign it to yourself again. Please also give an update about the status of the work.;;;","20/May/21 10:54;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 7 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","22/May/21 10:54;frank wang;Does the issue need to do anything else? if need, mayby i can help to something,

anyway, i just find one has not change from provided to test about hadoop or hive
{code:java}
<groupId>org.apache.hadoop</groupId>
<artifactId>hadoop-mapreduce-client-core</artifactId>
<version>${hivemetastore.hadoop.version}</version>
<scope>provided</scope>
{code}
if need to modify, maybe you can assign to me, [~twalthr] ;;;","25/May/21 07:40;twalthr;Thanks for offering your help [~frank wang]. Ideally, the SQL Client should simply use a simple test catalog instead of a big Hive catalog. I can assign you to this ticket if you are still interested? Maybe [~fsk119] can help reviewing a PR?;;;","25/May/21 08:44;jark;Hi [~twalthr], I think this will be a large PR/task if we want to fix the hive dependency. Currently, we tested some Hive connector features in SQL Client. On the other hand, SQL Client also relies on Hive catalog/module to testing statements about catalog and modules. The tests are mainly located in {{flink-table/flink-sql-client/src/test/resources/sql/}} and {{DependencyTest}} , etc... 

If we want to remove Hive dependency, we may first need to:
1. migrate the Hive-related tests into Hive connector (Hive connector will have a flink-sql-client dependency with test scope).
2. support testing catalog and modules, e.g. FLINK-17909, and replace them with the hive-related tests in flink-sql-client. 
3. remove hive dependency in flink-sql-client. 

So this would be a huge work and would be better break it into sub-tasks.

I'm fine with this refactoring. However, I just have some concerns that this would cost us a lot of time but the benefit maybe small. From my point of view, the problem of this issue is just the mess Hive dependencies in flink-sql-client. Could we just update all Hive dependencies to test scope, so we don't worry about packaging them into sql client jar by mistake. And all the ""exclusions"" list can be removed then (ideally). In this way, the list of Hive dependencies will be small and easy to maintain. What do you think [~twalthr]?

cc [~lirui]


;;;","25/May/21 13:09;lirui;The reason why we introduced hive deps into sql-client was because we don't have e2e tests for hive. So I guess we should figure out a way to add hive e2e test (perhaps using docker?) then we can remove these dependencies from sql-client.;;;","25/May/21 14:31;twalthr;I agree with [~lirui]. We should add dedicated end-to-end tests that combines SQL Client and Hive. The SQL Client module should be an independent entity. Actually, we have infrastructure to do this nowadays, please have a look at the newer e2e tests. We are doing this already for Kafka + SQL Client. Those are perfectly suited to combine multiple Flink modules for testing. 

Regarding ""cost us a lot of time but the benefit maybe small"", I'm currently fixing several vulnerability issues for customers that are pulled in via Hadoop and Hive. Every module without a Hadoop and Hive dependency is saving us time in the long run. +1 for splitting this issue into subtasks and staring with the simply ""everything in test scope"" fix.;;;","01/Jun/21 23:29;flink-jira-bot;This issue was labeled ""stale-critical"" 7 ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","02/Jul/21 22:40;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 30 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","09/Jul/21 02:54;frank wang;hi [~lirui] maybe you can assign this to me;;;","09/Jul/21 09:29;lirui;[~frank wang] Thanks for offering to help with this. Assigning to you.;;;","16/Jul/21 15:51;frank wang;[~lirui] i have submit pr about this issue [FLINK-13400|https://github.com/apache/flink/pull/16532],
I just removed the hive/hadoop related packages and some codes in sql-client, create a new module about flink-end-to-end-tests-hive, support catalog test on sql-client and hive-connector, i didnot implements e2e tests about sql-client and hive, because there is not hive test container, like [SQLClientHBaseITCase |https://issues.apache.org/jira/browse/FLINK-21519];;;","30/Jul/21 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","09/Nov/21 10:43;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,
Hive connector doesn't compile on Java 9,FLINK-13398,13246755,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,24/Jul/19 08:40,24/Jul/19 12:31,13/Jul/23 08:10,24/Jul/19 11:01,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Hive,,,,,0,,,,,"recent dependency exclusions re-introduced a {{jdk.tools}} dependency which cannot be resolved on Java 9.
{code:java}
Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.1.1:shade (shade-flink) on project flink-connector-hive_2.11: Error creating shaded jar: Could not resolve following dependencies: [jdk.tools:jdk.tools:jar:1.7 (system)]: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.11:jar:1.10-SNAPSHOT: Could not find artifact jdk.tools:jdk.tools:jar:1.7 at specified path /home/travis/openjdk9/../lib/tools.jar -> [Help 1]{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13366,,,FLINK-13310,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 24 11:01:25 UTC 2019,,,,,,,,,,"0|z04yxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/19 08:52;chesnay;For some reason the removal of the shade-plugin configuration causes this issue.;;;","24/Jul/19 11:01;chesnay;master: bc16d7322b7b58946aa8e23caa3df9a8401097aa 

1.9: ac27f951c213de5f97b5e72c9a20d0fba0f9352b ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use fallback unsafe secure MapR in nightly.sh,FLINK-13394,13246728,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,docete,docete,24/Jul/19 05:12,06/Aug/19 11:25,13/Jul/23 08:10,24/Jul/19 07:56,1.7.3,1.8.2,1.9.0,,,,,,,1.7.3,1.8.2,1.9.0,,Tests,Travis,,,,0,,,,,"[FLINK-12578|http://example.com/] intros https URL for MapR, but this causes fails on Travis for some reason. travis_watchdog.sh and travis_controller.sh are fixed by unsafe-mapr-repo profile, but nightly.sh is not fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 06 11:24:58 UTC 2019,,,,,,,,,,"0|z04yrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/19 07:56;chesnay;master: 01c06dae21006b363b80bcb7b0071578916eb057

1.9: d2f5914ae674b8db3892da0db444c07d826a0a0c ;;;","06/Aug/19 11:24;chesnay;1.8: a76b9e9fdca68808045bf808f4422081291b0edb 
1.7: 73caa7bfb59b97cf790a9f528405db57bf553e8a ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not download taskmanger & jobmanager's logs in the old UI,FLINK-13387,13246586,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,dwysakowicz,dwysakowicz,23/Jul/19 14:25,02/Oct/19 17:50,13/Jul/23 08:10,29/Jul/19 08:10,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"It is not possible to download the taskmanager & jobmanager logs via the old UI.

The exception is: ""Unable to load requested file /old-version/taskmanagers/1234ddfcc0b1d06d2615d5431a08c7b8/stdout.""",,dwysakowicz,yanghua,,,,,,,,,,,,"yanghua commented on pull request #9225: [FLINK-13387] Can not download taskmanger & jobmanagers logs in the old UI
URL: https://github.com/apache/flink/pull/9225
 
 
   
   ## What is the purpose of the change
   
   *This pull request fixs the problem about taskmanger & jobmanagers logs in the old UI can not be downloaded*
   
   
   ## Brief change log
   
     - *Fixs the problem about taskmanger & jobmanagers logs in the old UI can not be downloaded*
   
   
   ## Verifying this change
   
   This change is hard to be verified with the test cases, the changes were verified in the local env*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jul/19 08:04;githubbot;600","zentol commented on pull request #9238: [FLINK-13387][WebUI] Fix log download for old UI
URL: https://github.com/apache/flink/pull/9238
 
 
   Corrects the URLs used by the WebUI when downloading the logs. When using the WebUI the URL contains a `old-version` prefix, which leaks into the relative path defined in the `.html` files. The JS files are unaffected as they ignore the prefix and have their own way of assembling URLs.
   
   This PR simply modifies the `href` to go up one layer, which effectively removes the prefix from the call.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 11:43;githubbot;600","zentol commented on pull request #9225: [FLINK-13387] Can not download taskmanger & jobmanagers logs in the old UI
URL: https://github.com/apache/flink/pull/9225
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 11:43;githubbot;600","zentol commented on pull request #9238: [FLINK-13387][WebUI] Fix log download for old UI
URL: https://github.com/apache/flink/pull/9238
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 08:09;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 29 08:10:49 UTC 2019,,,,,,,,,,"0|z04xw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/19 01:51;yanghua;Hi [~dwysakowicz] I'd like to take this ticket, WDYT?;;;","24/Jul/19 06:59;dwysakowicz;I think that's fine. I assigned the ticket to you.;;;","26/Jul/19 11:44;chesnay;[~yanghua] I've assigned myself as I've found and verified a simpler solution.;;;","27/Jul/19 01:27;yanghua;[~chesnay] OK, Glad to sound this message.;;;","29/Jul/19 08:10;chesnay;master: 12118b9589606deddc4e5bd60f39ebb04f2568c7

1.9: 5077f186c5fc0644f06145bb5fa7796f54be8900;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix some frictions in the new default Web UI,FLINK-13386,13246581,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,vthinkxie,dwysakowicz,dwysakowicz,23/Jul/19 14:19,01/Oct/19 15:38,13/Jul/23 08:10,27/Sep/19 15:38,1.9.0,,,,,,,,,1.10.0,1.9.1,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"While manually testing the new WebUI I found a few frictions.

* when using the UI the left panel hides unexpectedly at random moments
* mouse wheel does not work on the logs (taskmanager, jobmanager) pane
* the jobmanager configuration is not sorted
* different sorting of the operators (the old UI showed the sources first)
* the drop-down list for choosing operator/tasks metrics is not sorted, which makes it super hard to screen through available metrics
* arrow does not touch the rectangles in Chrome (see attached screenshot)

There are also some views missing in the new UI that I personally found useful in the old UI:
* can't see watermarks for all operators at once
* no numeric metrics (only graphs)
",,aljoscha,dwysakowicz,jark,rmetzger,trohrmann,vthinkxie,,,,,,,,"vthinkxie commented on pull request #9247: [FLINK-13386][web]: Fix frictions in the new default Web Frontend
URL: https://github.com/apache/flink/pull/9247
 
 
   Fix frictions in the new default Web Frontend
   
   ## What is the purpose of the change
   - [ ] the jobmanager configuration is not sorted
   
   - [ ] different sorting of the operators (the old UI showed the sources first)
   
   - [ ] the drop-down list for choosing operator/tasks metrics is not sorted, which makes it super hard to screen through available metrics
   
   There are also some views missing in the new UI that I personally found useful in the old UI:
   
   - [ ] can't see watermarks for all operators at once
   
   - [ ] no numeric metrics (only graphs)
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jul/19 07:09;githubbot;600","dawidwys commented on pull request #9247: [FLINK-13386][web]: Fix frictions in the new default Web Frontend
URL: https://github.com/apache/flink/pull/9247
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Sep/19 15:12;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/19 14:18;dwysakowicz;bug.png;https://issues.apache.org/jira/secure/attachment/12975523/bug.png","27/Jul/19 07:51;vthinkxie;repro.png;https://issues.apache.org/jira/secure/attachment/12976044/repro.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 15:38:25 UTC 2019,,,,,,,,,,"0|z04xv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/19 13:24;trohrmann;[~vthinkxie] could you maybe help with these issues?;;;","27/Jul/19 06:39;vthinkxie;[~till.rohrmann] sure, I will check this, thanks;;;","27/Jul/19 06:54;vthinkxie;Hi [~dwysakowicz] 
 * when using the UI the left panel hides unexpectedly at random moments

the left panel will collapse when you first enter the job detail page, I can remove this behavior if it confuses the users
 * mouse wheel does not work on the logs (taskmanager, jobmanager) pane

I can not reproduce it, could you provide more info about your browser version and operation system(windows or mac)
 * the jobmanager configuration is not sorted

I will fix this
 * different sorting of the operators (the old UI showed the sources first)

I will fix this
 * the drop-down list for choosing operator/tasks metrics is not sorted, which makes it super hard to screen through available metrics

I will fix this
 * arrow does not touch the rectangles in Chrome (see attached screenshot)

I can not reproduce it, could you provide more info about your browser version and operation system(windows or mac) see the repro.png

There are also some views missing in the new UI that I personally found useful in the old UI:
 * can't see watermarks for all operators at once

I will fix this
 * no numeric metrics (only graphs)

I will fix this;;;","27/Jul/19 10:02;vthinkxie;Hi [~dwysakowicz] and [~till.rohrmann]

I have fixed the bugs below
 *  the jobmanager configuration is not sorted
 *  different sorting of the operators (the old UI showed the sources first)
 *  the drop-down list for choosing operator/tasks metrics is not sorted, which makes it super hard to screen through available metrics

and add these two features
 *  can't see watermarks for all operators at once
 *  no numeric metrics (only graphs)

in [https://github.com/apache/flink/pull/9247];;;","30/Jul/19 02:48;vthinkxie;and the scroll bug in firefox is caused by monaco-editor

it is fixed in monaco-editor master branch, but not release yet

[https://github.com/Microsoft/monaco-editor/issues/1353];;;","30/Jul/19 11:16;dwysakowicz;Hi [~vthinkxie]
Sorry for late response.
I think you figured out the setup for the scrolling bug.
As for the arrow not touching the block, maybe it was some temporary problem of my setup. I can't reproduce it now.

I also assigned the issue to you.;;;","30/Jul/19 11:27;vthinkxie;Hi [~dwysakowicz]

I have fixed some bugs in this pr [https://github.com/apache/flink/pull/9247]

it will be very helpful if you could help to check if the bug was fixed;;;","27/Sep/19 15:38;dwysakowicz;Implemented in 
master: dbe883c57e689ed544de09423192843c758bfa54 - 89e02cdd75bc8fec6bf03e26ec9bd26b8b231cda
1.9.1: 21a8e8915aa68f2f9fe9c5ad52c63f7befe908ed - 883d9cdb6873e506a688a64f0cb6f122c33ed092;;;",,,,,,,,,,,,,,,,,,,,,,,,
Align Hive data type mapping with FLIP-37,FLINK-13385,13246572,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Terry1897,twalthr,twalthr,23/Jul/19 13:41,06/Aug/19 07:22,13/Jul/23 08:10,04/Aug/19 05:42,1.10.0,1.9.0,,,,,,,,1.9.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"By looking at the Hive data type mapping of:
https://ci.apache.org/projects/flink/flink-docs-master/dev/table/catalog.html#data-type-mapping

Based on the information available in:
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types

It seems that the type are not mapped correctly. The following changes should be performed (indicated by {{>>...<<}}):

{code}
CHAR(p)	char(p)*
VARCHAR(p)	varchar(p)**
STRING	string
BOOLEAN	boolean
>>TINYINT<<	tinyint
>>SMALLINT<<	smallint
INT	int
BIGINT	long
FLOAT	float
DOUBLE	double
DECIMAL(p, s)	decimal(p, s)
DATE	date
TIMESTAMP_WITHOUT_TIME_ZONE	TIMESTAMP
TIMESTAMP_WITH_TIME_ZONE	N/A
TIMESTAMP_WITH_LOCAL_TIME_ZONE	N/A
INTERVAL	>>INTERVAL?<<
BINARY	>>N/A<<
VARBINARY(p)	>>N/A<<
>>BYTES		BINARY<<
>>ARRAY<E>	ARRAY<E><<
>>MAP<K, V>	MAP<K, V>* we support more than primitives<<
ROW	struct
MULTISET	N/A
{code}",,phoenixjiangnan,Terry1897,twalthr,xuefuz,,,,,,,,,,"zjuwangg commented on pull request #9239: [FLINK-13385]Align Hive data type mapping with FLIP-37
URL: https://github.com/apache/flink/pull/9239
 
 
   ## What is the purpose of the change
   Align Hive data type mapping with FLIP-37
   
   
   ## Brief change log
     - *align Hive data type mapping with FLIP-37*
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *HiveCatalogDataTypeTest.java*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no )
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable )
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 12:34;githubbot;600","zjuwangg commented on pull request #9239: [FLINK-13385]Align Hive data type mapping with FLIP-37
URL: https://github.com/apache/flink/pull/9239
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 07:37;githubbot;600","zjuwangg commented on pull request #9239: [FLINK-13385]Align Hive data type mapping with FLIP-37
URL: https://github.com/apache/flink/pull/9239
 
 
   ## What is the purpose of the change
   Align Hive data type mapping with FLIP-37. 
   
   
   ## Brief change log
     - *Align Hive data type mapping with FLIP-37*
     - *Not support interval type mapping for now. It's not type mapping related only, also related to data converting, maybe we can do it in the future.*
   
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *HiveCatalogDataTypeTest.java*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): ( no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no )
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable )
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 07:37;githubbot;600","asfgit commented on pull request #9239: [FLINK-13385][hive]Align Hive data type mapping with FLIP-37
URL: https://github.com/apache/flink/pull/9239
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Aug/19 05:35;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,FLINK-13443,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 04 05:42:13 UTC 2019,,,,,,,,,,"0|z04xt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/19 23:00;xuefuz;Hi [~twalthr], thanks for pointing this out. I'm not sure if I fully understand your change request regarding the following:
{code}
BINARY	>>N/A<<
VARBINARY(p)	>>N/A<<
>>BYTES		BINARY<<
{code}

The first two lines are currently mapped to Hive binary type, as shown in HiveTypeUtil.java. In addition, BINARY and VARBINARY are defined in LogicalTypeRoot while BYTES are defined in DataTypes. I'm not sure why we should put them together.

Please clarify.;;;","24/Jul/19 08:07;twalthr;As far as I understand the Hive documentation, the Hive's {{BINARY}} type is equivalent to Flink's {{BYTES}} type. In other words: a variable-length data type with maximum length. The other types don't match because they have either a fixed length or potentially truncate the data.;;;","24/Jul/19 08:55;Terry1897;Could you assign this Jira to me, I'd like to fix it. [~twalthr];;;","24/Jul/19 10:47;twalthr;I assigned you [~Terry1897]. Please make sure that also the `HiveTypeUtil` is in sync with the documentation.;;;","24/Jul/19 12:09;Terry1897;ok, I'll check it both in doc and code.;;;","04/Aug/19 05:42;phoenixjiangnan;merged in master: aa14e7e3f33cbebdfaeb7e00f7280255cd5fb1f9 . 1.9.0: 89169eee70797b9bff591dabddbffa04cd013e3c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
The back pressure monitoring does not work for StreamSources,FLINK-13384,13246555,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gjy,dwysakowicz,dwysakowicz,23/Jul/19 12:27,16/Aug/19 14:03,13/Jul/23 08:10,05/Aug/19 10:06,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Metrics,Runtime / Task,,,,0,pull-request-available,,,,"I think it is caused by: [FLINK-12483]. The reason is that the {{BackPressureStatsTrackerImpl}} samples only the main thread. FLINK-12483 introduced a separate thread for executing the sources in the mailbox model. It is similar to other old bug that concerned only Kafka source: [FLINK-3456]
",,1u0,aljoscha,dwysakowicz,gaoyunhaii,gjy,pnowojski,trohrmann,,,,,,,"GJL commented on pull request #9271: [FLINK-13384][runtime] Fix back pressure sampling for SourceStreamTask
URL: https://github.com/apache/flink/pull/9271
 
 
   ## What is the purpose of the change
   
   *This fixes back pressure sampling for sources.*
   
   cc: @pnowojski 
   
   ## Brief change log
   
     - *Fix back pressure sampling for SourceStreamTask*
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added unit tests to `TaskTest`*
     - *Manually verified the change back pressure sampling for sources*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 07:42;githubbot;600","GJL commented on pull request #9271: [FLINK-13384][1.9][runtime] Fix back pressure sampling for SourceStreamTask
URL: https://github.com/apache/flink/pull/9271
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 09:39;githubbot;600","GJL commented on pull request #9340:  [FLINK-13384][runtime] Fix back pressure sampling for SourceStreamTask
URL: https://github.com/apache/flink/pull/9340
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/19 10:07;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,FLINK-12483,,,,,,,FLINK-3456,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 05 10:06:28 UTC 2019,,,,,,,,,,"0|yi0g1t:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/19 12:49;dwysakowicz;I'm still investigating the issue. It's not a problem of the WebUI, but of the backpressure tracker. 

The backpressure metrics are displayed correctly for a map operator(not a source) if I increase the BackPressureStatsTrackerImpl#MAX_STACK_TRACE_DEPTH to 10.;;;","23/Jul/19 14:06;dwysakowicz;After further investigation I found out that it does not work only for {{StreamSource}} s. I think it is caused by: [FLINK-12483]. The reason is that the {{BackPressureStatsTrackerImpl}} samples only the main thread. FLINK-12483 introduced a separate thread for executing the sources in the mailbox model. It is similar to other old bug that concerned only Kafka source: [FLINK-3456]

[~pnowojski] [~1u0] Could you verify if this is the case?;;;","23/Jul/19 14:15;trohrmann;Would it work to fix the issue by increasing the sampling size and leave the proper fix for the sources for later if this is a bigger endeavor? I think this issue might resolve itself once we have the new source interface.;;;","23/Jul/19 14:20;dwysakowicz;This would not solve it. The problem is that we sample wrong thread.;;;","24/Jul/19 08:49;1u0;> FLINK-12483 introduced a separate thread for executing the sources in the mailbox model.

I confirm, for the streaming source tasks, the source function (and other chained operators) is now running in a separate thread.
For such tasks, the main task thread now mostly waiting for termination of that thread and does task shutdown/cleanup.

I'm not aware of any explicit changes/adjustments related to backpressure tracking.

Quick look into {{BackPressureStatsTrackerImpl}} source code, points to tracking {{LocalBufferPool.requestBufferBlocking}} method calls in stack traces. It's possible that this method is not used anymore in the production (due to some other changes to make inputs non-blocking).;;;","26/Jul/19 15:26;gjy;Currently, we sample the stacktrace in [TaskStackTraceSampleableTaskAdapter|https://github.com/apache/flink/blob/bb82aacf46565968359f5ee6d2503c3293d71d6e/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskStackTraceSampleableTaskAdapter.java#L50]. As a quick fix, I see the following options:
* We could somehow expose the {{LegacySourceFunctionThread}} instance from {{Task}} so that we sample the correct thread in {{TaskStackTraceSampleableTaskAdapter}}.
* Using [ThreadGroup|https://docs.oracle.com/javase/7/docs/api/java/lang/ThreadGroup.html], we could enumerate (all) threads in the JVM and find the correct thread to sample from.

Maybe [~pnowojski] can chime in here.;;;","30/Jul/19 07:54;pnowojski;From your two proposals, as a hotfix I think the first one (exposing {{LegacySourceFunctionThread}} via some {{Task.getMainExecutionThread()}} or sth like that) sounds better to me.

However this is not the first time that backpressure monitor has been broken. Also I think it's inherently broken with the current threading model, since it only detects if the task is backpressured when a record was produced from the main thread. That's not always true: processing time timers, {{AsyncWaitOperator}}, {{ContinousFileReaderOperator}}, custom sources that emit records from independent threads are all braking this assumption. Sometime ago I was proposing a more general fix for that: use output buffer usage metrics. If at least one output buffer is full, we assume there is a backpressure. However that would have been probably a bigger change.;;;","05/Aug/19 10:06;gjy;1.9:
e2eb6d41a0abcc206966249d83ceb0f450b2cf6b 
62448ec70575176905b177cac0976959a4bd2d05

1.10:
331d40ff7cc669b614c668b9f0d8160d15685626
fffe9db6b591d2316a4a2aa6ae717063133425a2;;;",,,,,,,,,,,,,,,,,,,,,,,,
BinaryHashTableTest and BinaryExternalSorterTest  is crashed on Travis,FLINK-13381,13246523,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,jark,jark,23/Jul/19 09:02,20/Dec/19 08:57,13/Jul/23 08:10,20/Dec/19 08:57,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,,"Here is an instance of master: https://api.travis-ci.org/v3/job/562437128/log.txt
Here is an instance of 1.9: https://api.travis-ci.org/v3/job/562380020/log.txt",,jark,trohrmann,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 14 09:49:57 UTC 2019,,,,,,,,,,"0|i3ztt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/19 13:58;trohrmann;Another instance: https://api.travis-ci.org/v3/job/562526189/log.txt;;;","25/Jul/19 07:18;jark;It seems that we didn't find this problem in the recent builds. It happens by chance. So I will change the priority to Major or Critical.;;;","30/Jul/19 04:17;tzulitai;Another few instances:

https://api.travis-ci.org/v3/job/562437489/log.txt
https://api.travis-ci.org/v3/job/562437489/log.txt
https://api.travis-ci.org/v3/job/562380020/log.txt

But these have appeared for quite a while already, and I also haven't seen any so far in recent builds.;;;","30/Jul/19 04:18;tzulitai;Some other similar occurences, by for {{OuterJoinITCase}} and {{JoinITCase}}:
https://api.travis-ci.org/v3/job/562437488/log.txt
https://api.travis-ci.org/v3/job/562437488/log.txt
https://api.travis-ci.org/v3/job/562437492/log.txt
https://api.travis-ci.org/v3/job/562437492/log.txt

All appeared only on June 18th, though.;;;","14/Aug/19 09:49;trohrmann;Any progress [~lzljs3620320]?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SINGLE_VALUE is not correctly supported in blink planner,FLINK-13378,13246504,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,LouisXu777,LouisXu777,23/Jul/19 07:42,01/Aug/19 10:57,13/Jul/23 08:10,24/Jul/19 12:26,1.10.0,1.9.0,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"There some problem in SingleValueAggFunction:

1.The comment in the method is ""value = count == 0 ? exception : operand(0)"", but actually it need to be ""value = count > 0 ? exception : operand(0)"" according to the code and logic.

2.The throwException expression should call(THROW_EXCEPTION, literal(msg), typeLiteral(type)).

And there are some bugs to support it in blink-planner:

1.RexNodeConverter lack converter for THROW_EXCEPTION

2.PlannerExpressions lack expr to support type inference for THROW_EXCEPTION",,aljoscha,jark,LouisXu777,lzljs3620320,TsReaper,,,,,,,,,"JingsongLi commented on pull request #9208: [FLINK-13378][table-planner-blink] Fix bug: Blink-planner not support SingleValueAggFunction
URL: https://github.com/apache/flink/pull/9208
 
 
   
   ## What is the purpose of the change
   
   There some problem in SingleValueAggFunction:
   1.The comment in the method is ""value = count == 0 ? exception : operand(0)"", but actually it need to be ""value = count > 0 ? exception : operand(0)"" according to the code and logic.
   2.The throwException expression should call(THROW_EXCEPTION, literal(msg), typeLiteral(type)).
   
   And there are some bugs to support it in blink-planner:
   1.RexNodeConverter lack converter for THROW_EXCEPTION
   2.PlannerExpressions lack expr to support type inference for THROW_EXCEPTION
   
   ## Verifying this change
   
   ScalarQueryITCase
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jul/19 13:31;githubbot;600","asfgit commented on pull request #9208: [FLINK-13378][table-planner-blink] Fix bug: Blink-planner not support SingleValueAggFunction
URL: https://github.com/apache/flink/pull/9208
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jul/19 12:24;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 24 12:26:32 UTC 2019,,,,,,,,,,"0|z04xe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/19 07:43;LouisXu777;[~jark] [~lzljs3620320] What do you think? Can you post some material describe this?;;;","23/Jul/19 08:28;lzljs3620320;[~LouisXu777] Thanks for your nice catch. There are bugs now in blink-planner.

I agree point 1 and point 3. About point 3, you can see SqlThrowExceptionFunction. It should be two args.

The singleValue Agg is for sql: select * from A where A.f0 < (select B.f0 from B); But there is no cases now.

If we want to support it, we need solve bugs in RexNodeConverter and PlannerExpressions. Do you want solve them before 1.9 release?;;;","23/Jul/19 09:09;LouisXu777;[~lzljs3620320] I looked at the related code，found it may be relating to code generation. But now I had not readed the code about code generation. So, if you don't mind, how about delay it to 1.10?;;;","23/Jul/19 09:36;lzljs3620320;[~LouisXu777] TPCH also have these sql pattern, I think it is a bug that we must solve it in 1.9.0. If you don't mind, I'll handle it?;;;","23/Jul/19 11:31;LouisXu777;OK，it's good that you will resolve it.;;;","23/Jul/19 12:09;aljoscha;Could you please change the title of the Issue to reflect what it is actually about? This is not about a wrong comment now, is it?;;;","23/Jul/19 12:38;LouisXu777;Yeah，would you please help to change the title as you are more familiar about it. [~lzljs3620320];;;","23/Jul/19 12:48;lzljs3620320;OK, [~LouisXu777] Thank you again for your attentiveness.;;;","23/Jul/19 13:23;lzljs3620320;[~jark] Can you assign this one to me?;;;","24/Jul/19 12:26;jark;Fixed in 1.10.0: 759b1d4bcde5d22761caee120f74054c6a4e2325
Fixed in 1.9.0: 353dd8d8087f12fe155537acc8887831cc776c9b;;;",,,,,,,,,,,,,,,,,,,,,,
Scala compiler causes StackOverflowError,FLINK-13374,13246453,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,docete,docete,23/Jul/19 02:42,18/Oct/21 07:31,13/Jul/23 08:10,01/Aug/19 13:00,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Here is a instance:

[https://api.travis-ci.org/v3/job/562043336/log.txt]",,jark,twalthr,,,,,,,,,,,,"zentol commented on pull request #9315: [FLINK-13374][scala][build] Set -Xss2m when compiling scala
URL: https://github.com/apache/flink/pull/9315
 
 
   This PR bumps the ThreadStackSize to 2MB when compiling scala. This appears to be the common approach for handling StackOverflowErrors by the scala compiler.
   
   see https://www.scala-lang.org/old/node/10931, https://github.com/scala/bug/issues/10604
   
   I haven't observed an increase in compile time.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 10:05;githubbot;600","zentol commented on pull request #9315: [FLINK-13374][scala][build] Set -Xss2m when compiling scala
URL: https://github.com/apache/flink/pull/9315
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 12:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24480,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 01 13:00:06 UTC 2019,,,,,,,,,,"0|z04x2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/19 08:18;twalthr;[~docete] Does this only affect the 1.10 branch or also the 1.9? If 1.9, we should mark this as a blocker for the 1.9 release.;;;","25/Jul/19 07:50;docete;[~twalthr] Seems it happens by accident for 1.10. I will loop it on my travis to reproduce for 1.9 and 1.10 and update here if have further conclusions.;;;","25/Jul/19 11:00;chesnay;We've seen this issue before for 1.9; it happens rarely. I don't think this is necessarily a problem in Flink; A search on google found _similar_ issue that could be fixed by increasing Xss IIRC.;;;","25/Jul/19 13:17;jark;Here is another instance: https://api.travis-ci.org/v3/job/563507048/log.txt happened in the recent CRON build.;;;","01/Aug/19 13:00;chesnay;master: 1dd64058985afd44cae6a5119c4843b2dd85873d
1.9: a7197009d8bd391fc7032994defa5c8d8a032d6d ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Release partitions in JM if producer restarts,FLINK-13371,13246357,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,azagrebin,azagrebin,22/Jul/19 15:40,02/Aug/19 07:43,13/Jul/23 08:10,02/Aug/19 07:43,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Coordination,Runtime / Network,,,,0,pull-request-available,,,,"As discussed in FLINK-13245, there can be a case that producer does not even detect any consumption attempt if consumer fails before the connection is established. It means we cannot fully rely on shuffle service for the release on consumption in case of consumer failure. When producer restarts it will leak partitions from the previous attempt. Previously we had an explicit release call for this case in Execution.cancel/suspend. Basically JM has to explicitly release all partitions produced by the previous task execution attempt in case of producer restart, including `released on consumption` partitions. For this change, we might need to track all partitions in PartitionTrackerImpl.",,azagrebin,maguowei,trohrmann,zhuzh,zjwang,,,,,,,,,"zentol commented on pull request #9250: [FLINK-13371][coordination] Prevent leaks of blocking partitions 
URL: https://github.com/apache/flink/pull/9250
 
 
   If force-release-on-consumption is enabled it is possible for blocking partitions to be leaked. For these partitions we rely on the consumer sending notifications for having consumed the partition, however the consumer may never be deployed successfully. In this case the partition is neither released by the task, nor by any other cleanup procedure since they all ignore partitions that are released on consumption.
   
   Note that a similar issue can occur for pipelined partitions that are buffered in the producers side before a consumer was actually scheduled. This issue is not addressed by this commit.
   
   The TaskExecutor now tracks all blocking partitions, to ensure they are cleaned up when the JM connection terminates or the TE shuts down.
   
   The PartitionTracker now tracks all blocking partitions, to ensure they are cleaned up on job termination and vertex resets.
   
   The execution now separately issues release calls for all produced partitions in case of a state reconciliation, where an execution was CANCELING but receives the notitification for being FINISHED. Since we arrive in a CANCELED state we release all partitions. At this point the PartitionTracker is not yet tracking these partitions (since we never officially reached a state FINISHED in the EG), hence the execution is sending these through separate RPC logic.
   Additionally, the execution no longer issues release calls through the PartitionTracker if it reached a terminal state, but just removes the partitions from the tracker.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Jul/19 15:31;githubbot;600","zentol commented on pull request #9250: [FLINK-13371][coordination] Prevent leaks of blocking partitions 
URL: https://github.com/apache/flink/pull/9250
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 07:39;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-13245,FLINK-4256,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 07:43:52 UTC 2019,,,,,,,,,,"0|z04whc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/19 15:41;azagrebin;discovered in FLINK-13245;;;","23/Jul/19 14:38;trohrmann;Please don't forget to add the proper fix version if the issue is release relevant [~azagrebin].;;;","24/Jul/19 12:34;azagrebin;I will review the PR;;;","02/Aug/19 07:43;chesnay;master: 7b95f32d01730bcc75ded42e41d3668a1802a69b
1.9: b5ab84c6238ff1f69f2151ed580410ae4c63acd7 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recursive closure cleaner ends up with stackOverflow in case of circular dependency,FLINK-13369,13246311,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,22/Jul/19 12:39,07/Feb/20 17:56,13/Jul/23 08:10,23/Jul/19 13:26,1.8.1,1.9.0,,,,,,,,1.8.2,1.9.0,,,,,,,,0,pull-request-available,,,,,,aroberts@fuze.com,dmvk,kkl0u,txhsj,,,,,,,,,,"dmvk commented on pull request #9200: [FLINK-13369] Add failing test case for cleaning circular dependencie…
URL: https://github.com/apache/flink/pull/9200
 
 
   …s using ClosureCleaner
   
   ## What is the purpose of the change
   
   Another issue I've run into while cleaning Beam's [AvroCoder](https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/coders/AvroCoder.java). In case of [circular dependency](https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/coders/AvroCoder.java#L271), cleaner ends up with StackOverflow.
   
   Providing a failing test case. Patch needs to track references of already accessed object before descending.
   
   ## Brief change log
   
     - *Added failing test case*
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
    
   - *Added test that tries to clean self-referencing closure
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (yes)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 12:46;githubbot;600","kl0u commented on pull request #9200: [FLINK-13369] Add failing test case for cleaning circular dependencie…
URL: https://github.com/apache/flink/pull/9200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jul/19 12:56;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 07 17:56:58 UTC 2020,,,,,,,,,,"0|z04w74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/19 13:26;kkl0u;Merged on master with e568639f97cb2c8be3ed85edf9c7dad3a36c4b73
on release-1.9 with 56a2742c5d0b482170b21236419c5da1bb3a8565
and on release-1.8 with 616d1b8f41eb61be31302719d0163aac0887643a;;;","28/Jan/20 20:51;aroberts@fuze.com;I'm still seeing this behavior in Flink 1.9.1.

I'm not sure what ""in case of circular dependency"" means, but I have a recursive call in a job, and ClosureCleaner explodes with a stack overflow.;;;","29/Jan/20 07:29;chesnay;[~aroberts@fuze.com] Can you provide us with a minimal example that fails for you?;;;","29/Jan/20 19:23;aroberts@fuze.com;{code:scala}
import org.apache.flink.api.common.typeinfo.TypeInformation
import org.apache.flink.api.scala._
import org.apache.flink.streaming.api.functions.sink.SinkFunction
import org.apache.flink.streaming.api.functions.source.SourceFunction
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment

import scala.annotation.tailrec


object TestJob {

  val jobName = ""TestJob""

  def input(n: Int) = {
    @tailrec def inner(current: Int, acc: List[String]): List[String] = {
      if (current > n) acc.reverse
      else inner(current + 1, current.toString :: acc)
    }
    inner(0, Nil)
  }

  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    job(env, input(10000))

    env.execute(jobName)
  }

  def job[Record: TypeInformation](
      env: StreamExecutionEnvironment,
      input: List[Record]
  ) = {
    val source = env.addSource(new Src(input))
    source.addSink(new Sink[Record])
  }

  class Sink[Record] extends SinkFunction[Record] {
    override def invoke(value: Record, context: SinkFunction.Context[_]) = ()
  }

  class Src[Record](in: List[Record]) extends SourceFunction[Record] {
    def run(ctx: SourceFunction.SourceContext[Record]) =
      in.foreach(ctx.collect)

    def cancel() = ()
  }
}
{code};;;","07/Feb/20 17:56;aroberts@fuze.com;[~chesnay] can you please reopen this ticket? I have provided you a failing example for scala 2.11

cc [~dmvk];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Make ClosureCleaner detect writeReplace serialization override,FLINK-13367,13246302,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,22/Jul/19 11:53,22/Aug/19 08:50,13/Jul/23 08:10,25/Jul/19 16:02,1.8.1,1.9.0,,,,,,,,1.8.2,1.9.0,,,,,,,,0,pull-request-available,,,,"Nested ClosureCleaner introduced in FLINK-12297 does not respect [writeReplace serialization overrides.|https://docs.oracle.com/javase/8/docs/api/java/io/Serializable.html] This is a problem for Apache Beam, that takes advantage of this while serializing avro schemas.

 ",,dmvk,mxm,,,,,,,,,,,,"dmvk commented on pull request #9201: [FLINK-13367] Add support for writeReplace in nested ClosureCleaner.
URL: https://github.com/apache/flink/pull/9201
 
 
   ## What is the purpose of the change
   
   Support for writeReplace in ClosureCleaner for Apache Beam [Avro Coder](https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/coders/AvroCoder.java#L215).
   
   ## Brief change log
   
     - *Added failing test case*
     - *Make usesCustomSerialiazation method search for writeReplace method.*
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added failing test case that uses writeReplace method for serializing payload*
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (yes)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 13:54;githubbot;600","asfgit commented on pull request #9201: [FLINK-13367] Add support for writeReplace in nested ClosureCleaner.
URL: https://github.com/apache/flink/pull/9201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jul/19 15:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,BEAM-8034,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 25 16:02:56 UTC 2019,,,,,,,,,,"0|z04w54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/19 16:02;mxm;Merged to master, release-1.8, and release-1.9 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove 2 args constructor in REPLACE expression,FLINK-13353,13246264,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,LouisXu777,LouisXu777,LouisXu777,22/Jul/19 08:52,01/Aug/19 10:57,13/Jul/23 08:10,25/Jul/19 09:05,1.10.0,1.9.0,,,,,,,,1.9.0,,,,Table SQL / Legacy Planner,Table SQL / Planner,,,,0,pull-request-available,,,,"Replace definition in stringExpression.scala has another constructor with 2 arguments.

According to source code, the args' meaning are str, begin. And it call other constructor with 3 args adding the 3rd arg which is the length of str.

But its expectTypes is (String, String, String), but actually is (String, int, int).

So I think the 2 args defined constructor means search and replacement is """" default, not begin and length of str. ",,jark,LouisXu777,twalthr,,,,,,,,,,,"AjaxXu commented on pull request #9218: [FLINK-13353][table-planner]wrong definition of Replace planner expre…
URL: https://github.com/apache/flink/pull/9218
 
 
   …ssion
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *Remove the 2 args constructor in Replace expression in flink-table-planner and flink-table-blink-planner*
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jul/19 11:50;githubbot;600","asfgit commented on pull request #9218: [FLINK-13353][table-planner] Remove 2 args constructor in REPLACE expression
URL: https://github.com/apache/flink/pull/9218
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jul/19 08:53;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 25 09:05:19 UTC 2019,,,,,,,,,,"0|z04vwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/19 08:53;LouisXu777;[~jark] [~lzljs3620320] What do you think? And how about assign this issue to me? Thanks.;;;","22/Jul/19 09:25;jark;The  definition of {{Replace}} planner expression is the same between blink planner and flink planner. And it seems that we don't have tests to cover {{replace(str, begin)}} function. 

Hi [~twalthr], do you have any idea about this? I find that {{replace(str, begin)}} is introduce in your commit: https://github.com/apache/flink/commit/939038fc2a5bfff5a1fc9347a2c21f5876910e0f.;;;","22/Jul/19 11:30;twalthr;[~jark] The commit you mentioned was not introducing this. My commit was just refactoring names. I think the 2nd arg constructor is just wrong and apparently the tests are not good enough.;;;","23/Jul/19 05:14;jark;So, let's remove the 2nd arg constructor in this issue? ;;;","23/Jul/19 11:28;twalthr;Yes, sounds reasonable.;;;","24/Jul/19 10:26;LouisXu777;According to conversation above, we just need to remove the 2 args constructor. So can you guys assign the issue to me? Thanks.;;;","24/Jul/19 11:08;twalthr;[~LouisXu777] I assigned it to you.;;;","25/Jul/19 09:05;jark;Fixed in 1.10.0: 817da8a87b8d6171666b3cbfa5f2d68965765bb2
Fixed in 1.9.0: 729468b983a8b04ca299de60a25a4880b4481edb;;;",,,,,,,,,,,,,,,,,,,,,,,,
Using hive connector with hive-1.2.1 needs libfb303 jar,FLINK-13352,13246261,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lirui,lirui,lirui,22/Jul/19 08:47,01/Aug/19 10:57,13/Jul/23 08:10,29/Jul/19 03:21,,,,,,,,,,1.9.0,,,,Connectors / Hive,Documentation,,,,0,pull-request-available,,,,Should mention that libfb303 jar is needed in {{catalog.md}},,lirui,,,,,,,,,,,,,"lirui-apache commented on pull request #9223: [FLINK-13352][hive] Using hive connector with hive-1.2.1 needs libfb3…
URL: https://github.com/apache/flink/pull/9223
 
 
   …03 jar
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   To document that libfb303 is needed when using hive connector  with hive-1.2.1.
   
   ## Brief change log
   
     - Updated the dependencies part for hive-1.2.1.
   
   
   ## Verifying this change
   
   Trivial doc change
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jul/19 02:42;githubbot;600","KurtYoung commented on pull request #9223: [FLINK-13352][hive] Using hive connector with hive-1.2.1 needs libfb3…
URL: https://github.com/apache/flink/pull/9223
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 03:08;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 29 03:21:15 UTC 2019,,,,,,,,,,"0|z04vw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/19 01:47;ykt836;[~lirui] I will assign this issue to you since you already opened a PR.;;;","29/Jul/19 02:24;lirui;Thanks [~ykt836]. Would you mind also review the PR? This is trivial doc change and [~xuefuz] has already approved the PR.;;;","29/Jul/19 03:21;ykt836;merged in 1.10.0: dbcbc09134e9baa1d8e7d3693afc1402e797dd1e

merged in 1.9.0: 4788e4807f725e8c8ffeae3ecf18ac1f5ab8b39c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
duplicate case ROW match in FlinkTypeFactory.toLogicalType,FLINK-13351,13246258,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,LouisXu777,LouisXu777,LouisXu777,22/Jul/19 08:37,01/Aug/19 10:57,13/Jul/23 08:10,24/Jul/19 10:15,1.10.0,1.9.0,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"FlinkTypeFactory.toLogicalType() in flink-table-planner-blink module has duplicate case Row match in line 482-489.

I also look in FlinkTypeFactory in flink-table-planner, it also have 2 case Row match. But one of match types is 'CompositeRelDataType' which defined only in flink-table-planner module.",,jark,LouisXu777,lzljs3620320,,,,,,,,,,,"AjaxXu commented on pull request #9196: [FLINK-13351][table-blink-planner]duplicate case ROW match in FlinkTy…
URL: https://github.com/apache/flink/pull/9196
 
 
   …peFactory.toLogicalType
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This pull request remove duplicate case Row match in FlinkTypeFactory in table-blink-planner*
   
   
   ## Brief change log
   
    - *remove duplicate case Row match in FlinkTypeFactory in table-blink-planner*
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 09:03;githubbot;600","asfgit commented on pull request #9196: [FLINK-13351][table-blink-planner]duplicate case ROW match in FlinkTy…
URL: https://github.com/apache/flink/pull/9196
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jul/19 10:15;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 24 10:15:10 UTC 2019,,,,,,,,,,"0|z04vvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/19 08:41;LouisXu777;[~jark] [~lzljs3620320] What do you think about this improvement? Remove duplicate code or add CompositeRelDataType to blink-planner. And how about assign this issue to me?;;;","22/Jul/19 08:47;lzljs3620320;[~LouisXu777] Nice catch! Jark help to assign.;;;","24/Jul/19 10:15;jark;Fixed in 1.10.0: 64ca5e4d41722fab109eb4883ce6817c265488aa
Fixed in 1.9.0: d684a72c9f137dcd8262fa5e047dc6fb13d42af3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
should handle new JoinRelType(SEMI/ANTI) in switch case,FLINK-13347,13246224,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,22/Jul/19 03:39,19/Sep/19 19:18,13/Jul/23 08:10,30/Jul/19 04:20,,,,,,,,,,1.9.0,,,,Table SQL / Legacy Planner,Table SQL / Planner,,,,0,pull-request-available,,,,"Calcite 1.20 introduces {{SEMI}} & {{ANTI}} to {{JoinRelType}}, blink planner & flink planner should handle them in each switch case",,godfreyhe,jark,,,,,,,,,,,,"godfreyhe commented on pull request #9227: [FLINK-13347] [table-planner] should handle SEMI/ANTI JoinRelType in switch case
URL: https://github.com/apache/flink/pull/9227
 
 
   
   
   ## What is the purpose of the change
   
   *should handle SEMI/ANTI JoinRelType in switch case*
   
   
   ## Brief change log
   
     - *handle SEMI/ANTI JoinRelType in switch case*
   
   ## Verifying this change
   
   all tests should pass
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not **documented)**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jul/19 09:08;githubbot;600","asfgit commented on pull request #9227: [FLINK-13347] [table-planner] should handle SEMI/ANTI JoinRelType in switch case
URL: https://github.com/apache/flink/pull/9227
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 03:38;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 01 04:20:58 UTC 2019,,,,,,,,,,"0|z04vns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/19 09:07;godfreyhe;i will fix it;;;","30/Jul/19 04:20;jark;Fixed in 1.10.0: ef29f305cd3d907d7c445c271b314ea643baaeeb
Fixed in 1.9.0: 50c957adcff228f1390faa6823a72d93cf0cf92e;;;","30/Jul/19 04:49;rongr;oops. forgot to backport to 1.9.0 and operate on JIRA. thanks for fixing it [~jark]. much appreciated the follow up.;;;","31/Jul/19 16:08;rongr;Hi [~jark] seems like 3f5b1f80bf0551ba2b59d72c48002b1ed5bf65f1 is not found on the release-1.9 branch. should I rebase and commit to it?;;;","01/Aug/19 02:01;jark;Oops... I missed to push it to ASF... Thanks for the reminder.. ;;;","01/Aug/19 03:43;jark;[~walterddr], I have updated the commit id. Sorry for the mistake. ;;;","01/Aug/19 04:20;rongr;no prob at all... It was me first forgetting to backport to 1.9 lol.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Documentation link on Flink Website points to wrong snapshot documentation,FLINK-13342,13246158,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aljoscha,hanfeio,hanfeio,21/Jul/19 05:19,25/Jul/19 07:27,13/Jul/23 08:10,25/Jul/19 07:27,1.9.0,,,,,,,,,,,,,,,,,21/Jul/19 00:00,0,pull-request-available,,,,"link:https://flink.apache.org/
description:Document menu flink1.9(Snapshot) jump to flink1.10(Snapshot)
bug:Flink1.9 document number error",,aljoscha,hanfeio,jark,,,,,,,,,,,"aljoscha commented on pull request #234:  [FLINK-13342] Update master documentation link to say ""master""
URL: https://github.com/apache/flink-web/pull/234
 
 
   At least one user was confused by the fact that during releases the ""master"" documentation does point to the wrong snapshot, see description in the issue.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jul/19 12:20;githubbot;600","aljoscha commented on pull request #234:  [FLINK-13342] Update master documentation link to say ""master""
URL: https://github.com/apache/flink-web/pull/234
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jul/19 07:24;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 25 07:27:16 UTC 2019,,,,,,,,,,"0|z04v94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/19 10:34;aljoscha;This is a temporary issue, because of the ongoing release. Master branch is already at 1.10-SNAPSHOT but 1.9 is not yet released and those links are outdated. We could just change that link to say ""Latest Snapshot"" or something like it.;;;","25/Jul/19 07:27;aljoscha;Fixed on asf-site in
cf5d24b835afbfc297aa16a41c924459433ebb37;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sql conformance is hard to config in TableConfig,FLINK-13338,13246012,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,danny0405,danny0405,danny0405,19/Jul/19 14:19,02/Aug/19 11:15,13/Jul/23 08:10,02/Aug/19 11:15,1.10.0,1.9.0,,,,,,,,1.9.0,,,,Table SQL / API,,,,,1,pull-request-available,,,,"Now the TableConfig has only interface to config the SqlParser config which is very broad and hard to use for user, we should at least supply an interface to config the sql conformance.",,danny0405,jark,twalthr,xuefuz,,,,,,,,,,"danny0405 commented on pull request #9212: [FLINK-13338] Sql conformance is hard to config in TableConfig
URL: https://github.com/apache/flink/pull/9212
 
 
   ## What is the purpose of the change
   
   This patch adds an interface `TableConfig#setSqlDialect(SqlDialect)` to make the sql dialect configuration more user friendly.
   
   
   ## Brief change log
   
   *(for example:)*
     - Add new class `SqlDialect` to enumerate the sql dialects Flink supports now
     - Add configuration interface for sql dialect in `TableConfig`
   
   
   ## Verifying this change
   
   See tests in SqlToOperationConverterTest, PartitionableSinkITCase.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jul/19 05:27;githubbot;600","asfgit commented on pull request #9212: [FLINK-13338][table-api] Sql conformance is hard to config in TableConfig
URL: https://github.com/apache/flink/pull/9212
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 10:32;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 11:15:13 UTC 2019,,,,,,,,,,"0|z04uco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/19 11:11;twalthr;[~danny0405] why do we need the conformance level in 1.9? This sounds like a new feature to me.;;;","25/Jul/19 02:58;ykt836;Could we just set default conformance to HIVE? Currently it seems unnecessary to config conformance. ;;;","25/Jul/19 12:52;twalthr;Flink SQL should be the default conformance. Hive is not even standard compliant.;;;","02/Aug/19 11:15;twalthr;Fixed in 1.10.0: 687425db49e332225bfdba631111325ca68bef53
Fixed in 1.9.0: fab290c4890ab8f18f00bd638989bc1cec9c5f24;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blink planner not compiling with Scala 2.12,FLINK-13327,13245823,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,chesnay,chesnay,18/Jul/19 16:04,18/Jul/19 19:12,13/Jul/23 08:10,18/Jul/19 19:12,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,,,,,"[https://travis-ci.org/apache/flink/jobs/560428262]

 
{code:java}
11:48:37.007 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/plan/nodes/resource/ExecNodeResourceTest.scala:183: error: overriding method isBounded in trait StreamTableSource of type ()Boolean;
11:48:37.007 [ERROR]  value isBounded needs `override' modifier
11:48:37.007 [ERROR] class MockTableSource(val isBounded: Boolean, schema: TableSchema)
11:48:37.007 [ERROR]                           ^
11:48:40.784 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/util/TableTestBase.scala:852: error: overriding method isBounded in trait StreamTableSource of type ()Boolean;
11:48:40.784 [ERROR]  value isBounded needs `override' modifier
11:48:40.784 [ERROR] class TestTableSource(val isBounded: Boolean, schema: TableSchema)
11:48:40.785 [ERROR]                           ^
11:48:40.855 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/util/testTableSources.scala:135: error: overriding method isBounded in trait StreamTableSource of type ()Boolean;
11:48:40.855 [ERROR]  value isBounded needs `override' modifier
11:48:40.855 [ERROR]     val isBounded: Boolean,
11:48:40.855 [ERROR]         ^
11:48:40.906 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/util/testTableSources.scala:345: error: overriding method isBounded in trait StreamTableSource of type ()Boolean;
11:48:40.906 [ERROR]  value isBounded needs `override' modifier
11:48:40.906 [ERROR]     val isBounded: Boolean,
11:48:40.906 [ERROR]         ^
11:48:40.982 [WARNING] 6 warnings found
11:48:40.987 [ERROR] four errors found{code}
 

[~godfreyhe]  [~dawidwys]",,dwysakowicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13168,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 19:12:06 UTC 2019,,,,,,,,,,"0|z04t6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/19 19:12;dwysakowicz;Fixed in
master via: abeb1f5cc73e33800d0af1367f345d1bf2f2822d
1.9: 0faa66747775bd75cc2f55c4d6b00560a8c41c05;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix serializer snapshot recovery in BaseArray and BaseMap serializers.,FLINK-13322,13245743,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,18/Jul/19 10:46,01/Aug/19 10:57,13/Jul/23 08:10,23/Jul/19 02:11,,,,,,,,,,1.9.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"In BaseArray and BaseMap serializers, their element (or key/value) serializers are not stored in the config snapshot. When restoring the BaseArray/BaseMap serializers from the snapshots, their element/key/value serializers might be incorrect. This situation will happen when user uses his custom kryo serializers as element/key/value serializers.",,jark,TsReaper,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 23 02:11:51 UTC 2019,,,,,,,,,,"0|z04sow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/19 11:14;twalthr;[~TsReaper] please set the component and improve the description. About which component are we talking about?;;;","22/Jul/19 08:53;jark;Fixed in 1.10.0: bc2fd3133cc2ce138e41c2cbb3471051d7fc327e;;;","23/Jul/19 02:11;jark;Fixed in 1.9.0: 56e45c8397bdaf3a8dec45164021c54db5b47e7e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove link to hive_compatibility from catalog docs,FLINK-13320,13245730,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,18/Jul/19 10:12,18/Jul/19 10:14,13/Jul/23 08:10,18/Jul/19 10:14,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Hive,Documentation,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 10:14:30 UTC 2019,,,,,,,,,,"0|z04sm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/19 10:14;chesnay;master: 33e84009ec71a7e120996d8400e0eb96143eed18

1.9: fc078d6f0a23c58bd3f930567e9bbcce59f2a408;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catalog documentation is missing chinese page,FLINK-13319,13245728,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,18/Jul/19 10:09,18/Jul/19 10:14,13/Jul/23 08:10,18/Jul/19 10:14,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Ecosystem,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 10:14:48 UTC 2019,,,,,,,,,,"0|z04sls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/19 10:14;chesnay;master: c52f14b55cfbec392db74fa894fd6131f268c21c 

1.9: cbf010f1485b10f101ad8ed56dd32665d3dfd7b7 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blink planner tests failing on Scala 2.12,FLINK-13318,13245725,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingzhang,chesnay,chesnay,18/Jul/19 10:07,27/Jan/22 17:43,13/Jul/23 08:10,23/Jul/19 11:49,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,Tests,,,,0,pull-request-available,,,,"[https://travis-ci.org/apache/flink/builds/559909681]
{code:java}
13:30:03.531 [INFO] Results:
13:30:03.531 [INFO] 
13:30:03.533 [ERROR] Failures: 
13:30:03.534 [ERROR]   CalcTest.testScalarFunctionAccess:64 planBefore expected:<...t$giveMeCaseClass$$f[e1bff2b06d8e0e495536102224cfe83().my], _c1=[org$apache$flink$table$plan$batch$table$CalcTest$giveMeCaseClass$$fe1bff2b06d8e0e495536102224cfe83().clazz], _c2=[org$apache$flink$table$plan$batch$table$CalcTest$giveMeCaseClass$$fe1bff2b06d8e0e495536102224cfe83().my], _c3=[org$apache$flink$table$plan$batch$table$CalcTest$giveMeCaseClass$$fe1bff2b06d8e0e495536102224cfe83]().clazz])
+- Logica...> but was:<...t$giveMeCaseClass$$f[4a420732fc04b1351889eb0e88eb891().my], _c1=[org$apache$flink$table$plan$batch$table$CalcTest$giveMeCaseClass$$f4a420732fc04b1351889eb0e88eb891().clazz], _c2=[org$apache$flink$table$plan$batch$table$CalcTest$giveMeCaseClass$$f4a420732fc04b1351889eb0e88eb891().my], _c3=[org$apache$flink$table$plan$batch$table$CalcTest$giveMeCaseClass$$f4a420732fc04b1351889eb0e88eb891]().clazz])
+- Logica...>
13:30:03.534 [ERROR]   CalcTest.testSelectFromGroupedTableWithFunctionKey:154 planBefore expected:<...alcTest$MyHashCode$$[d14b486109d9dd062ae7c60e0497797]5($2)])
      +- Log...> but was:<...alcTest$MyHashCode$$[3cd929923219fc59162b13a4941ead4]5($2)])
      +- Log...>
13:30:03.534 [ERROR]   CalcTest.testSelectFunction:109 planBefore expected:<...alcTest$MyHashCode$$[d14b486109d9dd062ae7c60e0497797]5($2)], b=[$1])
+- L...> but was:<...alcTest$MyHashCode$$[3cd929923219fc59162b13a4941ead4]5($2)], b=[$1])
+- L...>
13:30:03.534 [ERROR]   CorrelateTest.testCrossJoin:41 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6($2)], rowType=[Rec...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6($2)], rowType=[Rec...>
13:30:03.534 [ERROR]   CorrelateTest.testCrossJoin2:52 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6($2, _UTF-16LE'$')]...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6($2, _UTF-16LE'$')]...>
13:30:03.534 [ERROR]   CorrelateTest.testLeftOuterJoinWithLiteralTrue:74 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6($2)], rowType=[Rec...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6($2)], rowType=[Rec...>
13:30:03.534 [ERROR]   CorrelateTest.testLeftOuterJoinWithoutJoinPredicates:63 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6($2)], rowType=[Rec...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6($2)], rowType=[Rec...>
13:30:03.535 [ERROR]   JoinTest.testFilterJoinRule:143 planBefore expected:<...le$JoinTest$Merger$$[223b7380fec29c4077a893c60165d845($2, org$apache$flink$table$plan$batch$table$JoinTest$Merger$$223b7380fec29c4077a893c60165d845]($2, $5))])
   +- Lo...> but was:<...le$JoinTest$Merger$$[d18a3011491fab359eccb50f2d0d9a18($2, org$apache$flink$table$plan$batch$table$JoinTest$Merger$$d18a3011491fab359eccb50f2d0d9a18]($2, $5))])
   +- Lo...>
13:30:03.535 [ERROR]   CorrelateStringExpressionTest.testCorrelateJoins1:39 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6($2)], rowType=[Rec...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6($2)], rowType=[Rec...>
13:30:03.535 [ERROR]   CorrelateStringExpressionTest.testCorrelateJoins2:45 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6($2)], rowType=[Rec...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6($2)], rowType=[Rec...>
13:30:03.535 [ERROR]   CorrelateStringExpressionTest.testCorrelateJoins3:51 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6($2, _UTF-16LE'$')]...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6($2, _UTF-16LE'$')]...>
13:30:03.535 [ERROR]   CorrelateStringExpressionTest.testCorrelateJoins4:57 planBefore expected:<...ble$util$TableFunc2$[b3b1f988779be024ed9386bce5019112]($2)], rowType=[Reco...> but was:<...ble$util$TableFunc2$[c3732037e6eae881ddb6ed2a3eb8a6c5]($2)], rowType=[Reco...>
13:30:03.535 [ERROR]   CorrelateStringExpressionTest.testCorrelateJoins7:84 planBefore expected:<...ble$util$TableFunc2$[b3b1f988779be024ed9386bce5019112]($2)], rowType=[Reco...> but was:<...ble$util$TableFunc2$[c3732037e6eae881ddb6ed2a3eb8a6c5]($2)], rowType=[Reco...>
13:30:03.535 [ERROR]   CorrelateStringExpressionTest.testCorrelateJoins8:92 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6(SUBSTRING($2, 2))]...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6(SUBSTRING($2, 2))]...>
13:30:03.535 [ERROR]   PushFilterIntoTableSourceScanRuleTest.testWithUdf:94 planAfter expected:<...ssions$utils$Func1$$[a39386268ffec8461452460bcbe089ad]($2), 32)])
   +- Lo...> but was:<...ssions$utils$Func1$$[8a89e0d5a022a06a00c7734a25295ff4]($2), 32)])
   +- Lo...>
13:30:03.535 [ERROR]   CalcTest.testMultiMap:157 planBefore expected:<...sions$utils$Func24$$[4d71da721f8fba30223be1cd2b5af2ce(org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f0, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f1, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f2, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f3).f0], _c1=[org$apache$flink$table$expressions$utils$Func24$$4d71da721f8fba30223be1cd2b5af2ce(org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f0, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f1, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f2, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f3).f1], _c2=[org$apache$flink$table$expressions$utils$Func24$$4d71da721f8fba30223be1cd2b5af2ce(org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f0, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f1, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f2, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f3).f2], _c3=[org$apache$flink$table$expressions$utils$Func24$$4d71da721f8fba30223be1cd2b5af2ce(org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f0, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f1, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f2, org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387]($0, $1, $2).f3).f3]...> but was:<...sions$utils$Func24$$[a58474b2f957945aea81ebf1946ac35c(org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f0, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f1, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f2, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f3).f0], _c1=[org$apache$flink$table$expressions$utils$Func24$$a58474b2f957945aea81ebf1946ac35c(org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f0, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f1, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f2, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f3).f1], _c2=[org$apache$flink$table$expressions$utils$Func24$$a58474b2f957945aea81ebf1946ac35c(org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f0, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f1, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f2, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f3).f2], _c3=[org$apache$flink$table$expressions$utils$Func24$$a58474b2f957945aea81ebf1946ac35c(org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f0, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f1, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f2, org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f]($0, $1, $2).f3).f3]...>
13:30:03.535 [ERROR]   CalcTest.testScalarResult:145 planBefore expected:<...ssions$utils$Func1$$[a39386268ffec8461452460bcbe089ad]($0)])
+- LogicalTab...> but was:<...ssions$utils$Func1$$[8a89e0d5a022a06a00c7734a25295ff4]($0)])
+- LogicalTab...>
13:30:03.535 [ERROR]   CalcTest.testSimpleMap:135 planBefore expected:<...sions$utils$Func23$$[de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f0], _c1=[org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f1], _c2=[org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387($0, $1, $2).f2], _c3=[org$apache$flink$table$expressions$utils$Func23$$de6190eff5cfcd5dd1d5877a871e2387]($0, $1, $2).f3])
+-...> but was:<...sions$utils$Func23$$[a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f0], _c1=[org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f1], _c2=[org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f($0, $1, $2).f2], _c3=[org$apache$flink$table$expressions$utils$Func23$$a124414bedd7bf1ea9d54ab7ef606f4f]($0, $1, $2).f3])
+-...>
13:30:03.535 [ERROR]   ColumnFunctionsTest.testAddColumns:200 planBefore expected:<...eam$table$TestFunc$$[fd4dfa9e9ae53c7b8d0f13d2db94ac9b]($0, $1)])
+- Logica...> but was:<...eam$table$TestFunc$$[dade06dcc492e15f9ba187a01e7be727]($0, $1)])
+- Logica...>
13:30:03.535 [ERROR]   ColumnFunctionsTest.testStar:46 planBefore expected:<...eam$table$TestFunc$$[fd4dfa9e9ae53c7b8d0f13d2db94ac9b]($0, $1)])
+- Logica...> but was:<...eam$table$TestFunc$$[dade06dcc492e15f9ba187a01e7be727]($0, $1)])
+- Logica...>
13:30:03.535 [ERROR]   CorrelateTest.testCrossJoin:41 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6($2)], rowType=[Rec...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6($2)], rowType=[Rec...>
13:30:03.535 [ERROR]   CorrelateTest.testCrossJoin2:53 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6($2, _UTF-16LE'$')]...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6($2, _UTF-16LE'$')]...>
13:30:03.535 [ERROR]   CorrelateTest.testCustomType:78 planBefore expected:<...ble$util$TableFunc2$[b3b1f988779be024ed9386bce5019112](org$apache$flink$ta...> but was:<...ble$util$TableFunc2$[c3732037e6eae881ddb6ed2a3eb8a6c5](org$apache$flink$ta...>
13:30:03.535 [ERROR]   CorrelateTest.testFilter:114 planBefore expected:<...ble$util$TableFunc2$[b3b1f988779be024ed9386bce5019112]($2)], rowType=[Reco...> but was:<...ble$util$TableFunc2$[c3732037e6eae881ddb6ed2a3eb8a6c5]($2)], rowType=[Reco...>
13:30:03.535 [ERROR]   CorrelateTest.testFlatMap:179 planBefore expected:<...ble$util$TableFunc2$[b3b1f988779be024ed9386bce5019112]($2)], rowType=[Reco...> but was:<...ble$util$TableFunc2$[c3732037e6eae881ddb6ed2a3eb8a6c5]($2)], rowType=[Reco...>
13:30:03.535 [ERROR]   CorrelateTest.testLeftOuterJoinWithLiteralTrue:64 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6($2)], rowType=[Rec...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6($2)], rowType=[Rec...>
13:30:03.535 [ERROR]   CorrelateTest.testScalarFunction:125 planBefore expected:<...ble$util$TableFunc1$[ad38060966060e704b09fa4c9428769]6(SUBSTRING($2, 2))]...> but was:<...ble$util$TableFunc1$[e1a0c63ecf595c7329d87aae4f6f425]6(SUBSTRING($2, 2))]...>
13:30:03.535 [ERROR]   OverWindowTest.testScalarFunctionsOnOverWindow:47 planBefore expected:<...ssions$utils$Func1$$[a39386268ffec8461452460bcbe089ad](AS(SUM($0) OVER (PA...> but was:<...ssions$utils$Func1$$[8a89e0d5a022a06a00c7734a25295ff4](AS(SUM($0) OVER (PA...>
13:30:03.535 [INFO] 
13:30:03.535 [ERROR] Tests run: 3667, Failures: 28, Errors: 0, Skipped: 14{code}",,godfreyhe,jark,jinyu.zj,trohrmann,,,,,,,,,,"beyond1920 commented on pull request #9193: [FLINK-13318][table-planner-blink] Fix Blink planner tests failing on Scala 2.12
URL: https://github.com/apache/flink/pull/9193
 
 
   ## What is the purpose of the change
   
   Fix Blink planner tests failing on Scala 2.12. Some query plan in Scala 2.12 is different with that in Scala 2.11 if it contains udf without SerialVersionUID explicitly.
   
   ## Brief change log
   
   Add SerialVersionUID for all `UserDefinedFunction`s in blink planner which is Scala language.
   
   ## Verifying this change
   Existed UT, IT
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector:  no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 06:50;githubbot;600","asfgit commented on pull request #9193: [FLINK-13318][table-planner-blink] Fix Blink planner tests failing on Scala 2.12 by setting SerialVersionUID of UDF explicitly
URL: https://github.com/apache/flink/pull/9193
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jul/19 11:49;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 23 11:49:36 UTC 2019,,,,,,,,,,"0|z04slc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/19 03:40;jark;cc [~godfreyhe] [~qingru.zhang];;;","19/Jul/19 04:02;godfreyhe;i fix it now;;;","19/Jul/19 10:37;jingzhang;[~godfreyhe][~jark] The root cause is the following code: 
{code:java}
public abstract class UserDefinedFunction implements FunctionDefinition, Serializable {

	/**
	 * Returns a unique, serialized representation for this function.
	 */
	public final String functionIdentifier() {
		final String md5 = EncodingUtils.hex(EncodingUtils.md5(EncodingUtils.encodeObjectToString(this)));
		return getClass().getCanonicalName().replace('.', '$').concat(""$"").concat(md5);
	}
{code}

For a UDF which implements in Scala, scala2.12 and scala2.11 will generate different md5 value.
Maybe we should ignore `TableFunctionScan`'s digest when compare actual plan and expect plan.;;;","19/Jul/19 11:39;jark;As discussed with [~godfreyhe], how about we force using Java UDF in plan tests. It will still cover function digest in plan phase and we can cover Scala UDF E2E in IT cases. ;;;","20/Jul/19 11:24;jingzhang;[~jark][~godfreyhe] I'm sorry to miss an very important information before, all those scala udf which generates different serialized bytes in scala2.12 and scala2.11 are lack of SerialVersionUID explicitly, maybe JVM generate different default serialVersionUID for those class in scala2.12 and scala2.11. 
So how about fix those plan difference by add serialVersionUID in udfs?;;;","20/Jul/19 11:30;godfreyhe;(y) I think this is the root cause;;;","22/Jul/19 03:45;jark;+1 for this approach!;;;","23/Jul/19 08:11;trohrmann;What's the progress [~jark];;;","23/Jul/19 08:24;jark;[~till.rohrmann] We still need some discussion about the approach with [~twalthr] in the pull request. ;;;","23/Jul/19 11:49;jark;Fixed in 1.10.0: 898b190ab0c69617a6a0208dc553e0399761730d
Fixed in 1.9.0: 4698f3ce5b922b7f14bbcfd6489acf2f46736e7d;;;",,,,,,,,,,,,,,,,,,,,,,
Correct resultType of some PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner,FLINK-13314,13245649,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingzhang,jingzhang,jinyu.zj,18/Jul/19 02:03,27/Jan/22 17:43,13/Jul/23 08:10,23/Jul/19 10:43,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Correct resultType of the following PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner:

Minus/plus/Div/Mul/Ceil/Floor/Round

 ",,jark,jinyu.zj,trohrmann,,,,,,,,,,,"beyond1920 commented on pull request #9152: [FLINK-13314][table-planner-blink] Correct resultType of some PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner
URL: https://github.com/apache/flink/pull/9152
 
 
   ## What is the purpose of the change
   
   Correct resultType of some PlannerExpression(`Minus`/`plus`/`Div`/`Mul`/`Ceil`/`Floor`/`Round`..,) when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner.
   
   ## Brief change log
   
   *(for example:)*
     - Fix minor bug in RexNodeConverter when convert `between` and `notBetween` to RexNode.
     - Fix minor bug in PlannerExpressionConverter when convert DataType to TypeInformation.
     - Correct resultType of PlannerExpression(`Minus`/`plus`/`Div`/`Mul`/`Ceil`/`Floor`/`Round`..,)  in Blink planner when operands contains DecimalTypeInfo or BigDecimalTypeInfo
   
   
   ## Verifying this change
   
   ITCase
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 02:23;githubbot;600","wuchong commented on pull request #9152: [FLINK-13314][table-planner-blink] Correct resultType of some PlannerExpression when operands contains DecimalTypeInfo or BigDecimalTypeInfo in Blink planner
URL: https://github.com/apache/flink/pull/9152
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jul/19 10:44;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,FLINK-12810,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 23 10:43:36 UTC 2019,,,,,,,,,,"0|z04s4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/19 02:18;jingzhang;[~jark] Would you please assign the Jira to me? Thanks a lot.;;;","23/Jul/19 08:11;trohrmann;What's the progress [~jark]?;;;","23/Jul/19 08:25;jark;I think it is hopefully to be merged today. [~till.rohrmann];;;","23/Jul/19 10:43;jark;Fixed in 1.10.0: 0002032eae73d881d58feeb59294e8e15f354d5e 
Fixed in 1.9.0:  5c0521d846895598ef4b10a6a66c1b803a3504a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-python releases 2 jars,FLINK-13308,13245448,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dian.fu,chesnay,chesnay,17/Jul/19 11:13,02/Oct/19 17:49,13/Jul/23 08:10,19/Jul/19 09:58,1.9.0,,,,,,,,,1.9.0,,,,API / Python,Build System,,,,0,pull-request-available,,,,"{{flink-python}} uses a classifier to differentiate itseld from the old python API. turns out thsi doesn't work since it still tries to release a normal unshaded flink-python jar.

We should drop the classifier, and either stick to flink-python or rename it as proposed in FLINK-12776.",,aljoscha,dian.fu,,,,,,,,,,,,"zentol commented on pull request #9148: [FLINK-13308][python] Drop the classifier of the flink-python jar
URL: https://github.com/apache/flink/pull/9148
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jul/19 09:57;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 19 09:58:20 UTC 2019,,,,,,,,,,"0|z04r94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/19 11:53;dian.fu;Hi [~chesnay] Thanks a lot for opening this ticket. Either way makes sense to me and personally I'm in favor of dropping the classifier. Do you think it makes sense to you?;;;","17/Jul/19 11:57;chesnay;We should drop the classifier in any case, making it a reasonable first step.;;;","17/Jul/19 12:00;dian.fu;Sounds good. I'll take this issue and provide a PR ASAP.;;;","18/Jul/19 02:17;aljoscha;The older Python API was removed, so we don't have to worry about name clashes anymore, right?;;;","18/Jul/19 06:26;chesnay;Personally I would prefer a different name because it is very much an entirely different module.;;;","19/Jul/19 09:58;chesnay;master: b483e1db2618595b782fce5a336a7bd087abec8c

1.9: d8f544ba7fe6d9a7b784b285967d20a2edfc6d94 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix implementation of getString and getBinary method in NestedRow,FLINK-13304,13245397,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,TsReaper,TsReaper,TsReaper,17/Jul/19 08:12,01/Aug/19 10:57,13/Jul/23 08:10,23/Jul/19 02:11,,,,,,,,,,1.9.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,The `getString` and `getBinary` method in `NestedRow` are not implemented correctly. Also there is no tests guarding these complex data formats.,,jark,TsReaper,,,,,,,,,,,,"TsReaper commented on pull request #9139: [FLINK-13304][table-runtime-blink] Fix implementation of getString and getBinary method in NestedRow
URL: https://github.com/apache/flink/pull/9139
 
 
   ## What is the purpose of the change
   
   The `getString` and `getBinary` methods are not implemented correctly in `NestedRow`. This PR fixes the implementation and adds test cases for complex data formats.
   
   ## Brief change log
   
    - Fix implementation of `getString` and `getBinary` in `NestedRow`.
    - Add tests for complex data formats.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows: run the newly added `ComplexTest`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jul/19 08:42;githubbot;600","TsReaper commented on pull request #9154: [FLINK-13304][table-runtime-blink] Fix implementation of getString and getBinary method in NestedRow and add tests for complex data formats
URL: https://github.com/apache/flink/pull/9154
 
 
   ## What is the purpose of the change
   
   The `getString` and `getBinary` methods are not implemented correctly in `NestedRow`. This PR fixes the implementation and adds test cases for complex data formats.
   
   ## Brief change log
   
    - Fix implementation of `getString` and `getBinary` in `NestedRow`.
    - Add tests for complex data formats.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows: run the newly added `ComplexTest`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 03:21;githubbot;600","wuchong commented on pull request #9139: [FLINK-13304][FLINK-13322][FLINK-13323][table-runtime-blink] Fix implementation of getString and getBinary method in NestedRow, fix serializer restore in BaseArray/Map serializer and add tests for complex data formats
URL: https://github.com/apache/flink/pull/9139
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 08:52;githubbot;600","wuchong commented on pull request #9154: [FLINK-13304][FLINK-13322][FLINK-13323][table-runtime-blink] Fix implementation of getString and getBinary method in NestedRow, fix serializer restore in BaseArray/Map serializer and add tests for complex data formats
URL: https://github.com/apache/flink/pull/9154
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jul/19 02:12;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 23 02:11:31 UTC 2019,,,,,,,,,,"0|z04r08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/19 08:52;jark;Fixed in 1.10.0: e53a85d144e99c94533f6b6bb946e2d1125a758b;;;","23/Jul/19 02:11;jark;Fixed in 1.9.0: b62e22c40ea49936981ad3c8ccd0a2564bc88d86;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-python failed on Travis,FLINK-13299,13245363,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongwei,sunhaibotb,sunhaibotb,17/Jul/19 03:02,18/Jul/19 06:21,13/Jul/23 08:10,17/Jul/19 18:10,1.10.0,,,,,,,,,1.9.0,,,,,,,,,0,,,,,"Log: [https://api.travis-ci.com/v3/job/216620643/log.txt]

Error:

___________________________________ summary ____________________________________

ERROR: py27: InvocationError for command /home/travis/build/flink-ci/flink/flink-python/dev/.conda/bin/python3.7 -m virtualenv --no-download --python /home/travis/build/flink-ci/flink/flink-python/dev/.conda/envs/2.7/bin/python2.7 py27 (exited with code 1) py33: commands succeeded ERROR: py34: InvocationError for command /home/travis/build/flink-ci/flink/flink-python/dev/.conda/bin/python3.7 -m virtualenv --no-download --python /home/travis/build/flink-ci/flink/flink-python/dev/.conda/envs/3.4/bin/python3.4 py34 (exited with code 100) py35: commands succeeded py36: commands succeeded py37: commands succeeded ============tox checks... [FAILED]============ PYTHON exited with EXIT CODE: 1. Trying to KILL watchdog (12896). ./tools/travis_watchdog.sh: line 229: 12896 Terminated watchdog",,sunhaibotb,sunjincheng121,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 17 18:10:03 UTC 2019,,,,,,,,,,"0|z04qso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/19 18:10;sunjincheng121;Release-1.9: 64eb96901761930454f6d9058a31e0104c33dee7
Master: 200a5bf9dca9d398cf07879d4d1e407a2f41d839;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add state processor api to opt/ directory in flink-dist,FLINK-13293,13245269,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sjwiesman,sjwiesman,sjwiesman,16/Jul/19 16:42,22/Jul/19 04:54,13/Jul/23 08:10,22/Jul/19 04:54,,,,,,,,,,1.9.0,,,,Build System,,,,,0,pull-request-available,,,,,,sjwiesman,tzulitai,,,,,,,,,,,,"sjwiesman commented on pull request #9133: [FLINK-13293][state-processor-api][build] Add state processor api to opt/ directory in flink-dist
URL: https://github.com/apache/flink/pull/9133
 
 
   ## What is the purpose of the change
   
   Similarly to flinks other libraries (cep, gelly, etc) the state processor api should be bundled in the opt directory of flink dist
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage. Manually verified. 
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 17:11;githubbot;600","asfgit commented on pull request #9133: [FLINK-13293][state-processor-api][build] Add state processor api to opt/ directory in flink-dist
URL: https://github.com/apache/flink/pull/9133
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 04:38;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 22 04:54:22 UTC 2019,,,,,,,,,,"0|z04q7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/19 04:54;tzulitai;Merged.

master (1.10.0): c8af3c95dbac76458c01bd50e4d6f85aeca3e89a
1.9.0: 752e7ae6a43455234606a9a7da50f40182b2db51;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check connectors runnable in blink runner,FLINK-13285,13245144,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,16/Jul/19 09:01,09/Dec/19 06:19,13/Jul/23 08:10,09/Dec/19 06:19,,,,,,,,,,1.10.0,,,,Connectors / Common,,,,,0,pull-request-available,,,,"Now FLIP-32 is almost done, we should let connectors get rid of flink-table-planner dependence.

And there are still some planner class need to extract to table-common, just like SchemaValidator.",,aljoscha,jark,lzljs3620320,shenlang,TsReaper,,,,,,,,,"TsReaper commented on pull request #9236: [FLINK-13285][jdbc] Fix JDBC connectors with DataTypes.DATE/TIME/TIMESTAMP support
URL: https://github.com/apache/flink/pull/9236
 
 
   ## What is the purpose of the change
   
   Currently JDBC connector does not support DataTypes.DATE/TIME/TIMESTAMP with their default conversion classes to be LocalDate/LocalTime/LocalDateTime. This PR fixes the support of these data types for JDBC connector.
   
   ## Brief change log
   
    - Fix JDBC connector with DataTypes.DATE/TIME/TIMESTAMP support
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as the existing tests in the jdbc connector package. This PR updates these tests with some time data types.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive):no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 09:52;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,11400,,,,,,,,FLINK-13075,,,,,,,,,,,,,,,FLINK-13266,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-07-16 09:01:31.0,,,,,,,,,,"0|z04obo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct some builtin functions' return type inference in Blink planner,FLINK-13284,13245133,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/Jul/19 08:24,06/Feb/23 02:59,13/Jul/23 08:10,22/Jul/19 06:29,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Several builtin functions in Blink planner such as 
{code:java}
DATE_FORMAT{code}
declares 
{code:java}
ReturnTypes.cascade(ReturnTypes.explicit(SqlTypeName.VARCHAR), SqlTypeTransforms.TO_NULLABLE){code}
which should be 
{code:java}
ReturnTypes.cascade(ReturnTypes.explicit(SqlTypeName.VARCHAR), SqlTypeTransforms.FORCE_NULLABLE){code}
instead.",,godfreyhe,jark,lincoln.86xy,lzljs3620320,,,,,,,,,,"wuchong commented on pull request #9146: [FLINK-13284] [table-planner-blink] Correct some builtin functions' r…
URL: https://github.com/apache/flink/pull/9146
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 06:29;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 22 06:29:40 UTC 2019,,,,,,,,,,"0|z04pds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/19 02:51;lzljs3620320;Maybe Jira title should be Correct some builtin time functions' return type inference in Blink planner?;;;","22/Jul/19 06:29;jark;Fixed in 1.10.0: 6961d9bad804543a939be231dac36011d5b38c4a
Fixed in 1.9.0: 694b20874d54f76a86eb81eb7e1346833500f798;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalExecutorITCase#testUseCatalogAndUseDatabase failed on Travis,FLINK-13282,13245125,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,phoenixjiangnan,jark,jark,16/Jul/19 06:56,01/Aug/19 10:58,13/Jul/23 08:10,17/Jul/19 21:32,,,,,,,,,,1.9.0,,,,Table SQL / Client,,,,,0,,,,,"
{code}
06:06:00.442 [ERROR] Tests run: 14, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 42.026 s <<< FAILURE! - in org.apache.flink.table.client.gateway.local.LocalExecutorITCase
06:06:00.443 [ERROR] testUseCatalogAndUseDatabase(org.apache.flink.table.client.gateway.local.LocalExecutorITCase)  Time elapsed: 17.208 s  <<< ERROR!
org.apache.flink.table.client.gateway.SqlExecutionException: Could not create environment instance.
	at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testUseCatalogAndUseDatabase(LocalExecutorITCase.java:454)
Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: 
Invalid view 'TestView1' with query:
SELECT scalarUDF(IntegerField1) FROM default_catalog.default_database.TableNumber1
Cause: SQL validation failed. From line 1, column 8 to line 1, column 31: No match found for function signature scalarUDF(<NUMERIC>)
	at org.apache.flink.table.client.gateway.local.LocalExecutorITCase.testUseCatalogAndUseDatabase(LocalExecutorITCase.java:454)
{code}


Here is an instance: https://api.travis-ci.org/v3/job/559247230/log.txt


This maybe effected by FLINK-13176.",,jark,phoenixjiangnan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 17 21:32:00 UTC 2019,,,,,,,,,,"0|z04pc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/19 06:56;jark;cc [~phoenixjiangnan];;;","17/Jul/19 06:04;docete;The reason is that the FunctionCatalog would look up functions in catalog if the catalog could return a valid TableFactory(HiveCatalog in this case). And return a FunctionLookup.Result even if find nothing. I think we must change the function-discovery strategy(e.g., fallback to userFunctions in this case) or re-think the logic of FunctionCatalog. [~phoenixjiangnan] What do you think? ;;;","17/Jul/19 21:32;phoenixjiangnan;disabled test yesterday in FLINK-13294, fixed root cause in FLINK-13296;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the verify logic of AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest,FLINK-13281,13245119,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,16/Jul/19 06:29,18/Jul/19 10:00,13/Jul/23 08:10,18/Jul/19 10:00,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,,,,"FLINK-13060 ensure FailoverStrategies should respect restart constraints. However, this change did not cover the {{AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest}} which should also set as {{FixedDelayRestartStrategy}}. Currently, as the pending checkpoint would still be discarded due to [failUnacknowledgedPendingCheckpoints|https://github.com/apache/flink/blob/a7bdbb9aff734fc586e74c95984c7f956dbf576c/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L977] within {{CheckpointCoordinator}}, {{AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest}} could pass the test.

This is actually not what we want the test did, we should actually verify that when region failover, pending checkpoints would be discarded.",,yunta,,,,,,,,,,,,,"Myasuka commented on pull request #9123: [FLINK-13281] Fix the verify logic of AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest
URL: https://github.com/apache/flink/pull/9123
 
 
   ## What is the purpose of the change
   
   [FLINK-13060](https://issues.apache.org/jira/browse/FLINK-13060) ensure FailoverStrategies should respect restart constraints. However, this change did not cover the `AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest` which should also set as `FixedDelayRestartStrategy`. Currently, as the pending checkpoint would still be discarded due to `failUnacknowledgedPendingCheckpoints` within `CheckpointCoordinator`, `AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest` could pass the test.
   
   This is actually not what we want the test did, we should actually verify that when region failover, pending checkpoints would be discarded.
   
   ## Brief change log
   
   * Fix the verify logic of `AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest`
   
   ## Verifying this change
   
   This change modify previous test `AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 06:54;githubbot;600","zentol commented on pull request #9123: [FLINK-13281] Fix the verify logic of AdaptedRestartPipelinedRegionStrategyNGAbortPendingCheckpointsTest
URL: https://github.com/apache/flink/pull/9123
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 09:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 10:00:26 UTC 2019,,,,,,,,,,"0|z04pao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/19 10:00;chesnay;master: 59a4d2c1b52f8cceb5e4e287ee3a94e129d505e7

1.9: ca837bd8d467975df9101f6ea5c19cb48cef5ac5 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
not able to query table registered in catalogs in SQL CLI,FLINK-13279,13245063,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,phoenixjiangnan,phoenixjiangnan,15/Jul/19 22:03,01/Oct/19 15:39,13/Jul/23 08:10,30/Jul/19 09:45,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / API,Table SQL / Client,Table SQL / Legacy Planner,Table SQL / Planner,,0,pull-request-available,,,,"When querying a simple table in catalog, SQL CLI reports ""org.apache.flink.table.api.TableException: No table was registered under the name ArrayBuffer(default: select * from hivetable).""

[~ykt836] can you please help to triage this ticket to proper person?

Repro steps in SQL CLI (to set up dependencies of HiveCatalog, please refer to dev/table/catalog.md):

{code:java}
Flink SQL> show catalogs;
default_catalog
myhive

Flink SQL> use catalog myhive
> ;

Flink SQL> show databases;
default

Flink SQL> show tables;
hivetable
products
test

Flink SQL> describe hivetable;
root
 |-- name: STRING
 |-- score: DOUBLE

Flink SQL> select * from hivetable;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: No table was registered under the name ArrayBuffer(default: select * from hivetable).
{code}

Exception in log:


{code:java}
2019-07-15 14:59:12,273 WARN  org.apache.flink.table.client.cli.CliClient                   - Could not execute SQL statement.
org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL query.
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQueryInternal(LocalExecutor.java:485)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQuery(LocalExecutor.java:317)
	at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:469)
	at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:291)
	at java.util.Optional.ifPresent(Optional.java:159)
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:200)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:123)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:105)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:194)
Caused by: org.apache.flink.table.api.TableException: No table was registered under the name ArrayBuffer(default: select * from hivetable).
	at org.apache.flink.table.api.internal.TableEnvImpl.insertInto(TableEnvImpl.scala:529)
	at org.apache.flink.table.api.internal.TableEnvImpl.insertInto(TableEnvImpl.scala:507)
	at org.apache.flink.table.api.internal.BatchTableEnvImpl.insertInto(BatchTableEnvImpl.scala:58)
	at org.apache.flink.table.api.internal.TableImpl.insertInto(TableImpl.java:428)
	at org.apache.flink.table.api.internal.TableImpl.insertInto(TableImpl.java:416)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeQueryInternal$10(LocalExecutor.java:476)
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:202)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQueryInternal(LocalExecutor.java:474)
	... 8 more

{code}

However, {{select * from myhive.`default`.hivetable;}} seems to work well

Also note this is tested with changes in https://github.com/apache/flink/pull/9049",,aljoscha,danny0405,dwysakowicz,jark,lirui,lzljs3620320,phoenixjiangnan,twalthr,xuefuz,,,,,"dawidwys commented on pull request #9167: [FLINK-13279][table] Fallback to the builtin catalog when looking for registered tables.
URL: https://github.com/apache/flink/pull/9167
 
 
   ## What is the purpose of the change
   
   Makes the builtin catalog a fallback catalog. If the name is not found in the current catalog it will be looked up in the builtin catalog, before trying to resolve it as a full path.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
    - TableSinkItCase#testSinkWithNonDefaultCurrentCatalog
    - case in PathResolutionTest
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 13:02;githubbot;600","dawidwys commented on pull request #9167: [FLINK-13279][table] Fallback to the builtin catalog when looking for registered tables.
URL: https://github.com/apache/flink/pull/9167
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jul/19 11:19;githubbot;600","dawidwys commented on pull request #9229:  [FLINK-13279][table-sql-client] Fully qualify sink name in sql-client 
URL: https://github.com/apache/flink/pull/9229
 
 
   ## What is the purpose of the change
   
   This PR fixes the problem that sink registered automatically via sql-client is not accessible when the current catalog or database is changed to something different than the built-in ones. 
   
   
   ## Brief change log
   
   See commit messages
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   * testDefaultCatalogDifferentFromBuiltin
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (**yes** / no)
     - The serializers: (yes / b / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jul/19 15:10;githubbot;600","asfgit commented on pull request #9229:  [FLINK-13279][table-sql-client] Fully qualify sink name in sql-client 
URL: https://github.com/apache/flink/pull/9229
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 09:34;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,FLINK-13350,FLINK-13461,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 01 08:56:36 UTC 2019,,,,,,,,,,"0|i3ztta:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 22:11;phoenixjiangnan;we need some e2e tests to stop this from happening again  [~xuefuz] [~ruili] [~Terry1897];;;","16/Jul/19 08:53;danny0405;+1 for the end to end tests;;;","18/Jul/19 09:17;lzljs3620320;At present, the executeQuery of SQL Client is implemented by registerTableSink(into in memory sink) + insertInto + get the result from InMemorySink class. The register sink is through the interface of *TableEnv.registerTableSink*.

*But TableEnv.registerTableSink* is registered in builtin_catalog, not in current_catalog. So insertInto can't find the table in current_catalog.

There are others similar issues:

https://issues.apache.org/jira/browse/FLINK-13150

https://issues.apache.org/jira/browse/FLINK-12771

Why *TableEnv.registerTableSink* is registered in builtin_catalog? Because flink TableSource and TableSink can not be serializeable, So them can not be put into HiveCatalog.

IMO, this case is important for users because it is often the first step for users.

[~twalthr] [~dwysakowicz] What do you think about this?;;;","18/Jul/19 10:14;dwysakowicz;Hi, I think we should treat the builtin catalog as a fallback catalog, similarly as we do it for functions. I will take this task over and try to fix it, if that's ok.;;;","18/Jul/19 10:23;lzljs3620320;> Hi, I think we should treat the builtin catalog as a fallback catalog, similarly as we do it for functions.

Big Big +1, I used to think that function was fallback, and it was strange that source/sink could only find current Catalog.;;;","18/Jul/19 17:49;xuefuz;[~dwysakowicz] thanks for looking into this. After thinking about the proposal and reading the PR, I'm very concerned about the approach.

 

Table resolution is different from that for function and should be deterministic and unambiguous. In fact, I don't know any of DB product that has such a behavior. A table reference in user's query should be uniquely identified: either the reference itself is fully qualified, or the query engine qualifies it with the current database. None of database engine I know of would further resolve the table, if previous resolution fails, in the default database, for example. Instead, it just simple reports an error.

 

What's the consequence of this fallback resolution? A user query, which should fail, might not fail depending on if built-in catalog happens to have a table with the same name. The query may further succeed in execution and produce unexpected result. This subtle implication, however slim the chance is, introduces unpredictability in query behavior and can cause severe consequences for the user.

 

In summary, I think table reference and resolution should be deterministic and unambiguous and this proposal violates the principle.

 

The original problem, as I understand, is that the planner internally creates a table in built-in catalog and subsequently look up that table in the current catalog. Wouldn't the natural solution is to qualify the created table with the built-in catalog/database? This way, we don't have to change table resolution at the system level.

 ;;;","19/Jul/19 09:20;twalthr;Hi [~xuefuz],

thanks for your valuable feedback. Your concerns are valid. [~dwysakowicz]'s PR makes the API more user-friendly but we are also messing up the lookup design here.

I think the root cause of this discussion is that we don't have a proper separation between tables that need to be persisted in a catalog and temporary tables that are only available in the current session.

I did some research for future discussions:

MySQL: https://dev.mysql.com/doc/refman/8.0/en/create-temporary-table.html
{code}
TEMPORARY tables have a very loose relationship with databases (schemas). Dropping a database does not automatically drop any TEMPORARY tables created within that database. Also, you can create a TEMPORARY table in a nonexistent database if you qualify the table name with the database name in the CREATE TABLE statement. In this case, all subsequent references to the table must be qualified with the database name.
{code}

SQL Server: https://docs.microsoft.com/de-de/sql/relational-databases/databases/tempdb-database?view=sql-server-2017
{code}
Tempdb is used to hold: Temporary user objects that are explicitly created, such as: global or local temporary tables
{code}

We have two options: either we force users to fully specify the path or maintain a separate Map as MySQL does it. I think we can avoid the exception of this JIRA ticket easily by changing the SQL Client {{insertInto}} statement to a fully qualified one and stick to the current behavior for now.;;;","19/Jul/19 10:21;dwysakowicz;I also think [~xuefuz] raised a valid point there.

Just to add another point to the research I also checked how spark handles temporary tables. They actually introduced ambiguity similar as in the PR. Local temporary views in spark cannot be qualified with database. If a user uses a not qualified table name it is first checked if there is a temporary local view and only after that they check in the current database. (As far as I could tell they do not support multiple catalogs).

Also for a global temp view they are enforcing reserved database {{global_temp}}: https://spark.apache.org/docs/latest/sql-getting-started.html#global-temporary-view

As [~twalthr] mentioned we thought my PR would make the API a bit more user-friendly, but I agree this could result in surprising behavior. Therefore I agree we should not do it. The suggestion to fully qualify the name in sql-client should solve the reported issue and I think this will be the best solution as of now. I will close the current PR and prepare a new one.;;;","19/Jul/19 13:34;xuefuz;Thank [~twalthr] and [~dwysakowicz] for sharing the research results. Just wanted to clarify one point. The temp table that failed in name resolution is NOT created by the user, but by the planner. Thus to me the planner should be able to fully qualify that temp table with correct catalog and db. On the other hand, solving the problem at sql-client might not be enough since user may also do the same with tableEnv.sqlQuery().;;;","19/Jul/19 14:08;jark;I agree with [~xuefuz], we still have the problem because users can use a HIVE catalog, and in the meanwhile, registering TableSource/TableSink on tableEnv.;;;","30/Jul/19 09:45;twalthr;Fixed in 1.9.0: 9b1d78198cd25275b17e1d43ebd75a8a2f6cff7f
Fixed in 1.10.0: b1db13f5cfb3b81409d6f6fb079424f44ccc826e;;;","30/Jul/19 12:05;lirui;Does this ticket fix blink planner as well? I ran some test with blink planner and seems I hit the same issue.;;;","30/Jul/19 12:30;dwysakowicz;[~lirui] It should there is nothing planner specific in the bug fix.;;;","30/Jul/19 12:37;lirui;[~dwysakowicz] Thanks for clarifying. I'll double check.;;;","31/Jul/19 06:21;lirui;Hi [~dwysakowicz], I was trying something like the following and got exception that sink does not exist:
{code}
TableEnvironment tableEnv = ... // create table environment
tableEnv.useCatalog(""some_hive_catalog"");
Table table = tableEnv.sqlQuery(""select * from some_hive_table"");
CollectRowTableSink sink = new CollectRowTableSink();
tableEnv.registerTableSink(""sink"", sink.configure(table_names_and_typeinfos));
tableEnv.insertInto(table, ""sink"");
tableEnv.execute(null);
{code}
I understand this may be out of the scope of this ticket. Just wondering whether this is valid use case and do we plan to support it?;;;","31/Jul/19 06:54;dwysakowicz;As discussed in the ml thread: http://mail-archives.apache.org/mod_mbox/flink-dev/201907.mbox/%3ce22331ca-00f3-fbdb-e4cf-671959b9322c@apache.org%3e You have to fully qualify temporary sinks.

So it should be 
{code}
tableEnv.insertInto(table, ""default_catalog"", ""default_database"",  ""sink"");
{code};;;","31/Jul/19 07:12;lirui;[~dwysakowicz] Got it. Thanks for the pointers.;;;","01/Aug/19 04:33;xuefuz;[~dwysakowicz], I understand this is a conscious decision that we made in 1.9, but from the issue [~lirui] reported, the usability is very poor. There lacks a consistency: when user register a temp sink, he/she has no choice but registering it in built-in catalog; when he/she references the sink, he/she has to give the name of the catalog/db that he/she didn't choose to. I suppose we should revisit all topics around this post 1.9.

cc: [~twalthr] for future reference.;;;","01/Aug/19 08:56;twalthr;Hi [~xuefuz], I totally agree that the usability is not optimal here. This issue popped up too late to be fixed correctly, it should have been discussed as part of FLIP-30. We will definitely have to revisit the catalog handling again. ;;;",,,,,,,,,,,,,
Race condition in SourceStreamTaskTest.finishingIgnoresExceptions(),FLINK-13275,13245044,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kkl0u,kkl0u,kkl0u,15/Jul/19 19:00,17/Jul/19 11:58,13/Jul/23 08:10,17/Jul/19 11:58,1.10.0,,,,,,,,,1.10.0,,,,Tests,,,,,0,pull-request-available,test-stability,,,"Travis run https://api.travis-ci.org/v3/job/558897776/log.txt with the race condition.

The race condition consists of the following series of steps:

1) the {{finishTask}} of the source puts the {{POISON_PILL}} in the {{mailbox}}. This will be put as first letter in the queue because we call {{sendPriorityLetter()}} in the {{MailboxProcessor}}. 

2) then in the {{cancel()}} of the source (called by the {{cancelTask()}}) leads the source out of its run-loop which throws the exception in the test source. The latter, calls again {{sendPriorityLetter()}} for the exception, which means that this exception letter may override the previously sent {{POISON_PILL}} (because it jumps the line), 

3) if the {{POISON_PILL}} has already been executed, we are good, if not, then the test harness sets the exception in the {{StreamTaskTestHarness.TaskThread.error}}.

To fix that, I would suggest to follow a similar strategy for the root problem https://issues.apache.org/jira/browse/FLINK-13124 as in release-1.9.",,aljoscha,kkl0u,klion26,trohrmann,,,,,,,,,,"kl0u commented on pull request #9125: [FLINK-13275] Fix race condition in finishTask().
URL: https://github.com/apache/flink/pull/9125
 
 
   ## What is the purpose of the change
   
   This PR fixes the race condition that may manifest itself during `stop-with-savepoint`, as described in the related JIRA.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as `SourceStreamTaskTest.finishingIgnoresExceptions()`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 07:32;githubbot;600","kl0u commented on pull request #9125: [FLINK-13275] Fix race condition in finishTask().
URL: https://github.com/apache/flink/pull/9125
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jul/19 11:58;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 17 11:58:54 UTC 2019,,,,,,,,,,"0|z04ou0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 19:05;kkl0u;What do you think [~aljoscha]?;;;","15/Jul/19 23:09;aljoscha;I think your suggested fix is good, yes!;;;","16/Jul/19 01:48;trohrmann;Does this only affects the current master and not the {{release-1.9}} branch [~kkl0u]?;;;","16/Jul/19 07:24;kkl0u;Yes [~till.rohrmann], this affects only the {{master}} and not the release-1.9. I also a PR almost ready.;;;","17/Jul/19 11:58;kkl0u;Merged on master with e97c894;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
blink runner should avoid stream operator implementing BoundedOneInput,FLINK-13257,13244887,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,15/Jul/19 07:24,01/Aug/19 10:57,13/Jul/23 08:10,24/Jul/19 12:34,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"According to https://issues.apache.org/jira/browse/FLINK-11879 , BoundedOneInput should not coexist with checkpoint, so we can not use BoundedOneInput in streaming mode.",,aljoscha,jark,lzljs3620320,trohrmann,,,,,,,,,,"JingsongLi commented on pull request #9109: [FLINK-13257][table-planner-blink] blink runner should avoid stream operator implementing BoundedOneInput
URL: https://github.com/apache/flink/pull/9109
 
 
   
   ## What is the purpose of the change
   
   According to https://issues.apache.org/jira/browse/FLINK-11879 , BoundedOneInput should not coexist with checkpoint, so we can not use BoundedOneInput in streaming mode.
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 07:43;githubbot;600","asfgit commented on pull request #9109: [FLINK-13257][table-planner-blink] blink runner should avoid stream operator implementing BoundedOneInput
URL: https://github.com/apache/flink/pull/9109
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jul/19 12:32;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-11879,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 24 12:34:13 UTC 2019,,,,,,,,,,"0|z04nvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/19 02:38;trohrmann;Is this strictly needed for 1.9.0 [~lzljs3620320] [~twalthr] [~ykt836]?;;;","24/Jul/19 08:29;trohrmann;[~jark] will review the PR.;;;","24/Jul/19 12:34;jark;Fixed in 1.10.0: 98552d3e382ea094d965407a600d511b4cc7467a
Fixed in 1.9.0: a1f566d7541ed34d34f091b45165222cd983d999

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Periodical checkpointing is stopped after failovers,FLINK-13256,13244885,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,zhuzh,zhuzh,15/Jul/19 07:18,18/Jul/19 08:04,13/Jul/23 08:10,18/Jul/19 08:04,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"In this case, we observed that the job initially is triggering periodical checkpoints as expected.

But after 2 region failovers, no checkpoint is triggered any more, even after all the tasks are RUNNING again.

A sample log is attached along with the related topology desc pic.

This case may not be reproduced every time.

 ",,klion26,liyu,srichter,yunta,zhuzh,,,,,,,,,"Myasuka commented on pull request #9128: [FLINK-13256] Ensure periodical checkpoint still valid when region failover abort pending checkpoints
URL: https://github.com/apache/flink/pull/9128
 
 
   ##  What is the purpose of the change
   
   After we introduce region failover, pending checkpoints would be aborted when task failed. However, previous implementation would come across a bug leading to periodical checkpoint is invalid.
   Previously, when checkpoint coordinator trigger a checkpoint and the size of pending checkpoints is larger than `maxConcurrentCheckpointAttempts` (default 1), it would just assign the `currentPeriodicTrigger` to null. However, the pending checkpoints would then possibly be aborted due to region failover, which lead to no pending checkpoints and no `currentPeriodicTrigger`. In the end, the periodical checkpoint mechanism would become not valid anymore.
   
   ## Brief change log
   
     - Ensure when `CheckpointCoordinator` abort pending checkpoints, the `currentPeriodicTrigger` would must not be null.
     - Add test `FailoverStrategyCheckpointCoordinatorTest` to verify this case.
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Added test `FailoverStrategyCheckpointCoordinatorTest` to verify `currentPeriodicTrigger` is not null when `checkpointCoordinator#abortPendingCheckpointsWithTriggerValidation` is called.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 09:22;githubbot;600","StefanRRichter commented on pull request #9128: [FLINK-13256] Ensure periodical checkpoint still valid when region failover abort pending checkpoints
URL: https://github.com/apache/flink/pull/9128
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 07:58;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/19 07:15;zhuzh;15_15_20__07_15_2019.jpg;https://issues.apache.org/jira/secure/attachment/12974676/15_15_20__07_15_2019.jpg","15/Jul/19 07:14;zhuzh;jm_no_cp_after_failover.log;https://issues.apache.org/jira/secure/attachment/12974677/jm_no_cp_after_failover.log",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 08:04:25 UTC 2019,,,,,,,,,,"0|z04nv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 10:21;yunta;After digging into the heap dump of jobmanager provided by [~zhuzh], I figured out the root cause. To be short, this is due to {{currentPeriodicTrigger}} in {{CheckpointCoordinator}} had been assigned to {{null}} during triggering checkpoint as there still existed a pending {{checkpoint-387}}. However, the pending checkpoints would then be aborted due to the region failover and no active {{currentPeriodicTrigger}} anymore.;;;","18/Jul/19 08:04;srichter;Merged in:
master: 1ec34249a0
release-1.9: b7bfafca14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip hive connector test for JDK9 profile,FLINK-13255,13244882,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,lirui,lirui,15/Jul/19 07:13,02/Oct/19 17:49,13/Jul/23 08:10,19/Jul/19 10:01,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Hive,Travis,,,,0,pull-request-available,,,,"The Hive binary we depend upon has issues when running with JDK9, e.g. it assumes application class loader is URL class loader, which is no longer true in JDK9.",,jark,lirui,,,,,,,,,,,,"lirui-apache commented on pull request #9159: [FLINK-13255][hive] Skip hive connector test for JDK9 profile
URL: https://github.com/apache/flink/pull/9159
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Skip hive connector test for JDK9.
   
   
   ## Brief change log
   
     - Skip hive tests in JDK9 profile
   
   ## Verifying this change
   
   Existing tests
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 08:12;githubbot;600","zentol commented on pull request #9159: [FLINK-13255][hive] Skip hive connector test for JDK9 profile
URL: https://github.com/apache/flink/pull/9159
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jul/19 10:01;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 22 09:51:58 UTC 2019,,,,,,,,,,"0|z04nug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 07:26;lirui;I guess we can have a dedicated Travis job for hive connector, and exclude this job from the JDK9 profile.
[~chesnay] [~xuefuz] [~phoenixjiangnan] [~Terry1897] do you think it's the right way to go?;;;","15/Jul/19 09:40;chesnay;Let's first solve FLINK-13215; if the tests do indeed fail (which is likely) then we can disable the tests on java 9 quite easily as we do for cassandra.;;;","19/Jul/19 10:01;chesnay;master: 311dd15b0296945d4dcce80495bde2e9080f00bc 

1.9: 1e6578e96e1552e5ba9b7f12902c8e7035a0eac5 ;;;","22/Jul/19 09:51;jark;It seems that we still have this problem in the latest CRON jobs. https://travis-ci.org/apache/flink/builds/561664350

I just created an issue to track this. FLINK-13366;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock may occur in JDBCUpsertOutputFormat,FLINK-13253,13244855,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,15/Jul/19 02:01,01/Aug/19 10:57,13/Jul/23 08:10,15/Jul/19 06:36,1.9.0,,,,,,,,,1.9.0,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"In close, it await the flush scheduler terminal, but it hold the lock of JDBCUpsertOutputFormat instance, maybe the async thread is waiting for this lock in the flush method, so there might be a deadlock here.

First, it should not await scheduler terminal, because it has flushed all data to jdbc, what we should do is let async thread quit.

Second, we should add lock outside the closed check in the flusher, in this way, we can ensure async thread secure exiting.",,jark,lzljs3620320,shenlang,,,,,,,,,,,"JingsongLi commented on pull request #9107: [FLINK-13253][jdbc] Deadlock may occur in JDBCUpsertOutputFormat
URL: https://github.com/apache/flink/pull/9107
 
 
   
   ## What is the purpose of the change
   
   In close, it await the flush scheduler terminal, but it hold the lock of JDBCUpsertOutputFormat instance, maybe the async thread is waiting for this lock in the flush method, so there might be a deadlock here.
   
   First, it should not await scheduler terminal, because it has flushed all data to jdbc, what we should do is let async thread quit.
   Second, we should add lock outside the closed check in the flusher, in this way, we can ensure async thread secure exiting.
   
   ## Verifying this change
   
   Manual test
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 02:04;githubbot;600","JingsongLi commented on pull request #9108: [FLINK-13253][jdbc] Deadlock may occur in JDBCUpsertOutputFormat
URL: https://github.com/apache/flink/pull/9108
 
 
   
   ## What is the purpose of the change
   
   In close, it await the flush scheduler terminal, but it hold the lock of JDBCUpsertOutputFormat instance, maybe the async thread is waiting for this lock in the flush method, so there might be a deadlock here.
   
   First, it should not await scheduler terminal, because it has flushed all data to jdbc, what we should do is let async thread quit.
   Second, we should add lock outside the closed check in the flusher, in this way, we can ensure async thread secure exiting.
   
   ## Verifying this change
   
   ut
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 03:12;githubbot;600","wuchong commented on pull request #9108: [FLINK-13253][jdbc] Deadlock may occur in JDBCUpsertOutputFormat
URL: https://github.com/apache/flink/pull/9108
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 06:33;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 06:36:10 UTC 2019,,,,,,,,,,"0|z04nog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 06:36;jark;Fixed in 
- 1.10.0: https://github.com/apache/flink/commit/30587cbc68f881cc7cc43c83b3551902fd05c9c9
- 1.9.0: https://github.com/apache/flink/commit/98a4b6042769e21c33a8be6b63de9de0327d4e85
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distributed Jepsen test fails with blocked TaskExecutor,FLINK-13249,13244642,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,srichter,trohrmann,trohrmann,12/Jul/19 15:57,02/Aug/19 11:43,13/Jul/23 08:10,18/Jul/19 08:40,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,"The distributed Jepsen test which kills {{JobMasters}} started to fail recently. From a first glance, it looks as if the {{TaskExecutor's}} main thread is blocked by some operation. Further investigation is required.",,gaoyunhaii,gjy,kangzai,maguowei,pnowojski,srichter,trohrmann,zhuzh,zjwang,,,,,"StefanRRichter commented on pull request #9138: [FLINK-13249][runtime] Fix handling of partition producer responses b…
URL: https://github.com/apache/flink/pull/9138
 
 
   
   ## What is the purpose of the change
   
   This PR fixes the problem in FLINK-13249 by ensuring that processing the partition producer response is not blocking any netty thread, but is always executed by the task's executor.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (kind of)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jul/19 07:57;githubbot;600","StefanRRichter commented on pull request #9138: [FLINK-13249][runtime] Fix handling of partition producer responses b…
URL: https://github.com/apache/flink/pull/9138
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 08:37;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,2400,,,,,,,,,,,,,FLINK-13254,FLINK-13272,,,,,FLINK-12530,,,,,,,,,,,,,,,"12/Jul/19 16:39;trohrmann;jstack_25661_YarnTaskExecutorRunner;https://issues.apache.org/jira/secure/attachment/12974545/jstack_25661_YarnTaskExecutorRunner",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 08:40:49 UTC 2019,,,,,,,,,,"0|z04mdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/19 16:37;trohrmann;FLINK-13013 introduces a deadlock when setting up the {{SingleInputGate}}. The problem is that when we setup the {{SingleInputGate}} we try to request the partitions. Requesting the partitions acquires first the {{requestLock}}. Next we try to create the {{PartitionRequestClient}} in a blocking fashion. If the corresponding partition cannot be found, we ask the {{JobMaster}} about the state of the producer via the {{PartitionProducerStateProvider#requestPartitionProducerState}}. The response to this request arrives in a different thread which tries to call {{SingleInputGate#retriggerPartitionRequest}}. The problem is that this method also requires the {{SingleInputGate#requestLock}} which creates the deadlock.;;;","15/Jul/19 08:56;gaoyunhaii;Hi [~till.rohrmann], I think a little differently for the cause of this issue. I agree with that it is caused by the deadlock between _requestSubpartition -> waitForChannel_  and _retriggerPartitionRequest,_ but I think it might not be caused by FLINK-13013. Instead, I think it might be caused by FLINK-12530, since in this issue we move the retriggerPartitionRequest from the Task#executor to the Netty IO Thread. Since after the changing, the Netty IO thread is blocked when trying to acquire the request lock, then it cannot proceed to handle the connection of the other channels, and this cause the _waitForChannel_ to wait forever.;;;","15/Jul/19 09:02;srichter;[~gaoyunhaii] thanks for the input, I think what you wrote makes sense. So the point that we would have to address is ensuring that the callback is not run by the netty IO thread, because they are never allowed to run blocking op?;;;","15/Jul/19 09:24;gaoyunhaii;Hi [~srichter], I think in this case the key is the Netty thread  and the task thread is mutable blocked and both block states cannot be released. 

For the reasons cause this, I further read the code and found that the codes is similar to let the Netty thread to execute something like  _future.handleAsync(\{routine 1: PartitionProducerStateResponseHandle::new at Task.java#1090}, Task#executor).thenAccept(\{routine2 : retriggerPartitionRequest at SingleInputGate#605})_, I think we might intend to make both routine 1 and routine 2 to be executed with the Task#executor, however, if the routing 1 executes really fast and when the Netty thread get to the thenAccept, it found the previous stage has finished, then the Netty thread will execute routine 2 directly. ;;;","15/Jul/19 09:34;srichter;[~gaoyunhaii] That is also how I understood your comment, just wanted to clarify that we agree the approach should be to change the `thenAccept({routine2})` to `thenAcceptAsync({routine2}, taskExecutor)` to be on the safe side and guarantee it never runs from the netty thread. The biggest downside would be that we have to pass down the executor into the gate.;;;","15/Jul/19 09:49;trohrmann;I think the root cause is that the {{SingleInputGate#setup}} method holds the {{SingleInputGate#requestLock}} while it waits on the channel completion and the channel can only be established once the partition state update has arrived.;;;","15/Jul/19 10:25;trohrmann;[~gaoyunhaii] explained me the problem and now I finally got it. It looks indeed that we need to execute the call back from the partition state checker asynchronously https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java#L605.;;;","15/Jul/19 10:29;srichter;[~till.rohrmann] alright, I think then we have a consensus on how to approach this. I think the last remaining question is whether or not we actually require to pass in the task's executor or simply use the common pool. Option one obviously has the advantage that it integrates with how execution is supposed to happen and with with the task lifecycle, disadvantage is that it seems like we would need to pass down the executor from quiet far way. Option 2 would be simpler if we agree that there is no requirement that this has to run through the particular executor.;;;","16/Jul/19 08:17;trohrmann;This is a very good question. The down side of the global executor is that it's not really well defined who else is using it. Assume that there is another user (low level priority) who runs a long running task in it. Then, the retriggering of the partition request would again not happen. Such a usage could come even from a user function.

If we don't have a concrete and immediate plan how to refactor this code in the future, I would slightly be against using the global executor because I fear that it will remain for quite some time and could bite us later.;;;","16/Jul/19 08:52;pnowojski;I tend to agree with [~till.rohrmann] that it would be better to pass this executor somehow. I'm more worried about how invasive this change for the tests might be, not for the actual production code. We both create and setup input gates inside {{Task}} class, so {{Task#executor}} is just right besides them, or did I miss something [~srichter]?;;;","16/Jul/19 09:34;srichter;My tendency is also more towards having a well-specified executor, in particular as the code is not just some stateless request.

I would have another suggestion that might get around the problem, so how about changing:
{code}
	void requestPartitionProducerState(
		IntermediateDataSetID intermediateDataSetId,
		ResultPartitionID resultPartitionId,
		Consumer<? super ResponseHandle> responseConsumer);
{code}

and then do in `Task`
{code}
		futurePartitionState.whenCompleteAsync(
			(ExecutionState executionState, Throwable throwable) -> {
				responseConsumer.accept(new PartitionProducerStateResponseHandle(executionState, throwable));
			},
			executor);
{code}

This would keep the control where execution happens in the task and we don't need to pass down the executor. Wdyt?
;;;","17/Jul/19 02:11;trohrmann;I like this idea [~srichter]. It is not so invasive and should be easy to implement.;;;","18/Jul/19 08:40;srichter;Merged in:
master: 23bd23b325
release-1.9: 3eff6387b5;;;",,,,,,,,,,,,,,,,,,,
Network stack is leaking files,FLINK-13245,13244579,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zjwang,chesnay,chesnay,12/Jul/19 10:52,02/Oct/19 17:50,13/Jul/23 08:10,29/Jul/19 10:27,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"There's file leak in the network stack / shuffle service.

When running the {{SlotCountExceedingParallelismTest}} on Windows a large number of {{.channel}} files continue to reside in a {{flink-netty-shuffle-XXX}} directory.

From what I've gathered so far these files are still being used by a {{BoundedBlockingSubpartition}}. The cleanup logic in this class uses ref-counting to ensure we don't release data while a reader is still present. However, at the end of the job this count has not reached 0, and thus nothing is being released.

The same issue is also present on the {{ResultPartition}} level; the {{ReleaseOnConsumptionResultPartition}} also are being released while the ref-count is greater than 0.

Overall it appears like there's some issue with the notifications for partitions being consumed.

It is feasible that this issue has recently caused issues on Travis where the build were failing due to a lack of disk space.
",,azagrebin,maguowei,nkruber,pnowojski,sewen,shixg,zjwang,,,,,,,"zhijiangW commented on pull request #9132: [FLINK-13245][network] Fix the bug of file resource leak while canceling partition request
URL: https://github.com/apache/flink/pull/9132
 
 
   ## What is the purpose of the change
   
   *On producer side the netty handler receives the `CancelPartitionRequest` for releasing the `ResultSubpartitionView` resource. In previous implementation we try to find the corresponding view via available queue in `PartitionRequestQueue`. But in reality the view is not always available to stay in this queue, then the view would never be released.*
   
   *Furthermore the release of `ResultPartition/ResultSubpartitions` is based on the reference counter in `ReleaseOnConsumptionResultPartition`, but while handling the `CancelPartitionRequest` in `PartitionRequestQueue`, the `ReleaseOnConsum#ptionResultPartition` is never notified of consumed subpartition. That means the reference counter would never decrease to 0 to trigger partition release, which would bring file resource leak in the case of `BoundedBlockingSubpartition`.*
       
   *In order to fix above two issues, the corresponding view is released via all reader queue instead, and then it would call `ReleaseOnConsumptionResultPartition#onConsumedSubpartition` meanwhile to solve this bug.*
   
   ## Brief change log
   
     - *Fix the logic in handling `CancelPartitionRequest` in `PartitionRequestQueue`*
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - *Added unit tests for verifying the partition and view are both released while receiving `CancelPartitionRequest` for the case of available/unavailable view.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 15:40;githubbot;600","zhijiangW commented on pull request #9132: [FLINK-13245][network] Fix the bug of resource leak while releasing partition and view reader
URL: https://github.com/apache/flink/pull/9132
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 12:16;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,FLINK-12645,FLINK-12655,,,,FLINK-13371,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 29 10:27:13 UTC 2019,,,,,,,,,,"0|i31e3o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/19 13:50;chesnay;With the extensive help of [~azagrebin] we tracked this down to an issue in the {{PartitionRequestQueue}}, where readers are only properly released if they are marked as available (i.e., having credit). If a consumer attempted to close a connection for which the producer has no credit the actual release was skipped.

This issue only surfaced since the new {{BoundedBlockingSubpartition}} only releases data when all readers have been released, contrary to the previous implementation.

[~zjwang] [~pnowojski] [~NicoK] [~sewen]

Our proposal would be to modify {{PartitionRequestQueue#userEventTriggered}} as follows:

{code}
// Cancel the request for the input channel
int size = availableReaders.size();
final NetworkSequenceViewReader toRelease = allReaders.remove(toCancel);
// remove reader from queue of available readers
for (int i = 0; i < size; i++) {
	NetworkSequenceViewReader reader = pollAvailableReader();
	if (reader != toRelease) {
		registerAvailableReader(reader);
	}
}
toRelease.releaseAllResources();
markAsReleased(toRelease.getReceiverId());
{code};;;","12/Jul/19 16:35;pnowojski;Thanks for investigating this and this indeed seems wrong. I think the change make sense (at least after a quick look from my side). As I'm away next week, can you [~zjwang] take a look at this (either review [~azagrebin]'s/[~chesnay]'s fix or implement the fix/tests)?;;;","15/Jul/19 16:14;azagrebin;Here is some more clarification about our debugging with [~chesnay]. We basically found two problems:
 * In case of RemoteInputChannel, producer's PartitionRequestQueue#userEventTriggered always gets a CancelPartitionRequest from each consumer's RemoteInputChannel except one before CloseRequest comes and triggers PartitionRequestQueue#close. The problem is that PartitionRequestQueue#userEventTriggered releases the toCancel reader only if it is in availableReaders but always removes it from allReaders. This means that when PartitionRequestQueue#close is called only not canceled readers are released, others are leaked in PartitionRequestQueue#userEventTriggered. [~chesnay] has suggested a fix for it but the files are still not deleted when the test job shuts down. They are deleted only when the TE and network shut down at the end of the test. To fix this, the next point has to be resolved.
 * After our further investigation another problem was found. The final release of ReleaseOnConsumptionResultPartition happens only if pendingReferences is zero. This happens only if notifySubpartitionConsumed is called for all readers in PartitionRequestQueue. notifySubpartitionConsumed is currently called only in PartitionRequestQueue#close but not in case of CancelPartitionRequest, handleException and channelInactive. It means that the partition will linger in those cases and will be released only when the TE and network shut down where pendingReferences is not checked. It leads to the question of why we have 2 separate reader methods: notifySubpartitionConsumed and releaseAllResources if they both are basically parts of partition release. Why do we need notifySubpartitionConsumed, what does it mean and how is the fact of 'being Consumed' used except releasing? It seems both methods need to be called in all cases: CancelPartitionRequest, handleException, channelInactive and CloseRequest when the partition is not needed any more in PartitionRequestQueue.;;;","15/Jul/19 16:22;zjwang;Thanks for finding this potential issue and the investigation! [~chesnay]  [~azagrebin]

I think the idea of above modifications makes sense, because the `availableReader` is not always equivalent to `allReaders`, then it is proper to find the canceled view reader from `allReaders` instead.

This issue also exists in previous {{SpillableSubpartition}} which actually uses memory type in {{SlotCountExceedingParallelismTest,}} so we could not find this potential bug then.

In detail, we should also call `toRelease.notifySubpartitionConsumed` before calling `toRelease.releaseAllResources` in above modifications. Otherwise the reference counter in {{ReleaseOnConsumptionResultPartition}} would not decrease to zero and really release partition via {{ResultPartitionManager}}.

I would submit the PR and add some unite tests later tomorrow.;;;","16/Jul/19 04:15;zjwang;We have some previous assumptions that ResultSubpartitionView could be released individually, but all the subpartitions are released together via `ResultPartition/ResultPartitionManager`.

After thinking it through, it might be reasonable to have both methods as {{ResultSubpartitionView#notifySubpartitionConsumed}} and {{ResultSubpartitionView#releaseAllResources}}, because they describe the different semantics. 
 * `releaseAllResources` is used for releasing resources from ResultSubpartitionView aspect. The view is created by netty stack which is also responsible for triggering the release. In detail it has two scenarios to trigger release: One case is that netty channel inactive/exception as current {{PartitonRequestQueue#channelInactive}} and {{PartitionRequestQueue#exceptionCaught}} done. The other case is that when ResultSubpartitionView is actually consumed via {{NettyMessage#CannelPartitionRequest}}.

 * `notifySubpartitionConsumed` only indicates the ResultSubpartition/View consumed via {{CannelPartitionRequest}}, so we should call this method while handling the cancel message. For the case of channel exception/inactive, it does not always indicate the consumption semantic, so we should not call this method as current done in {{PartitionRequestQueue}}. It is up to {{JobMaster}} whether to release partition in the case of channel inactive/exception. For the streaming job if the consumer fails, the {{JobMaster}} would also cancel the producer task to release the whole {{ResultPartition}}. For the batch job of blocking partition, if the consumer TM exits to cause channel inactive, the {{ResultPartition}} might not need to be released.

Overall, these two methods seem to decouple the release between {{ResultPartition}} and {{ResultSubpartitionView}}. So it makes sense to keep them as now, as long as we could handle the {{CannelPartitionRequest}} message correctly based on above modifications. [~azagrebin]

 ;;;","16/Jul/19 13:38;azagrebin;Thanks for the clarification [~zjwang]! I have some more questions.

Firstly, some comments about the second point `notifySubpartitionConsumed`. At the moment, the semantics for the partition lifecycle is that there are two cases: release or not on consumption:
 * If release on consumption is set to shuffle service then job master never tries to reuse any partitions. This means that the shuffle service should just monitor the first consumption attempt for each subpartition and independently how it ends (consumed or failed), it should release all subpartitions after one attempt is done for each of them. Job Master will always restart everything and not released partitions from previous attempts will just linger around, even successfully produced ones.
 * If release is done outside then again independently from how any consumption attempt ends, Job Master will decide when to release the successfully produced partitions. The partition should be auto-released only if the production fails which happens in Task.

Basically, it looks like (sub)partition needs only some kind of `View is released/done for any reason notification` to clean up readers and count consumption attempts for auto release.

Secondly, some questions about the described difference between 'releaseAllResources\notifySubpartitionConsumed' in PartitionRequestQueue if it is still needed.

I think the confusion comes from the name of _CancelPartitionRequest_ which seems to be actually a confirmation of consumption from the consumer to producer at the same time (_NettyPartitionRequestClient#close_). Then we should notify it as an end of consumption to the whole partition and it is expected always to happen, right? This sounds more like _Acknowledge-_ or _ConfirmPartitionRequest_.

At the same time, it also serves as a cancelation of producer/consumer communication in case of consumer internal channel failure. At least as I see it in _PartitionRequestClientHandler#decodeMsg_ calling _cancelRequestFor_. This case sounds more like '_cancelation'_. But does it actually mean that we should notify the end of consumption to the whole partition on the producer side? Is it not the similar case as channel inactive/exception? The consumption might have been not successful but the end of consumption notification will lead to the subpartition full release. Could the job master reuse this sub-partition again for the recovered consumer if it tried?

Also looking more into `CloseRequest`, it will release and confirm consumption for all channels but does it actually mean that the consumption is done for all of them? Could it be that some of them failed internally in the mean time?;;;","19/Jul/19 11:01;zjwang;Thanks for these further thoughts and I think it would make things more clearly after discussion. [~azagrebin]

I could understand your above concerns. I agree that the current semantics of `CancelPartitionRequest/CloseRequest` are not very accurate, because they could indicate either successful consumption on consumer side or consumer task fails /any exceptions during consumption.

Considering the concern of when to call `notifySubpartitionConsumed`, I think the current implementation is based on whether the producer receives the confirmable notification(Cancel/CloseRequest) from consumer side. If it receives any messages then it would call `notifySubpartitionConsumed` no matter with consumer finishes/fails. In the case of handling channel exception, it only happens in consumer locally, so it would not call `notifySubpartitionConsumed`. Also for the case of channel inactive, the producer could not distinguish whether it is caused by initiative close connection on consumer side or TM lost exceptionally, so it would not call ` notifySubpartitionConsumed`.

For the first concern, we could provide the more definitely messages for clearly semantics of consumption successful/failed instead of current `Cancel/ClosePartition`. For the second concern we could also consider the proper way for handling messages/exception/inactive. But one precondition is that we should confirm the specific semantic of releasing partition based on consumption in partition management feature.

Currently there are three strategies which could release partition:
 * partition release based on consumers confirmation via network
 * partition release based on JM notification
 * partition release when disconnection between TM/JM

For the first strategy (partition release based on consumers confirmation): 
 * We could define the network message as `ReleasePartition` instead of current `Cancel/ClosePartition`. Then it might not care about whether the consumer finishes/fails during consumption. The precondition for this way is reliable network notification, but actually we have no ack mechanism for such message in application layer. Even if consumer task fails before establishing the connection with producer, we still need rely on JM notification of releasing partitions of producers.
 * We could not provide specific semantic as now, and the current strategy is only coupling with existing mechanism in network stack.
 * The semantic actually could be defined clearly as partition release based on one successful consumption. And considering the implementation it could be done by both consumer notification and JM notification.

In general I think we should consider how to define different release strategies which should provide specific semantics, and not caring about implementations when thinking about strategy. Actually any strategy could be implemented in multiple ways. E.g. the semantics might be divided into at-least once consumption, exactly-once consumption and at-most once consumption. After we confirm the specific semantics, then we would know how to refactor the current network stack considering implementation for certain strategy.

It might need worth further re-architecture the partition release strategy in release-1.10, because the feature of interactive queries is difficult to expand another strategy based on current architecture. In order not to block current release, the existing modifications could solve the file leak issue I think.;;;","19/Jul/19 15:42;nkruber;I agree with [~zjwang] - changing the semantics should be tackled separately, not necessarily as part of this bug fix. I'll see when I have time to look at the PR so we can get this merged;;;","19/Jul/19 16:31;zjwang;After confirming the comments from [~chesnay] in PR, I found that for the case of `SlotCountExceedingParallelismTest` it would not generate `ReleaseOnConsumptionResultPartition` because the partition is blocking type. So the reference counter would not be used in `ResultPartition`, and the files for bounded blocking partition could be released finally via calling `TaskExecutorGateway#releasePartitions` based on `RegionPartitionReleaseStrategy`.

The description of this jira ticket might not be accurate. In my local running this test in Mac system, it has no file leaks after finished. I am not sure why it has file leaks in windows system and I guess it might be relevant with mmap internal mechanism in different systems. I would double verify this test in windows system.

My PR modifications seems only for the case of pipelined partition which is using `ReleaseOnConsumptionResultPartition`, then the call of `notifySubpartitionConsumed` would make the reference counter become 0 finally to trigger release. But for the pipelined partition it is no issues for persistent file.;;;","22/Jul/19 13:00;azagrebin;[~zjwang] true, to reproduce the failure, JobManagerOptions.FORCE_PARTITION_RELEASE_ON_CONSUMPTION has to be true (it has been recently changed to false by default).;;;","22/Jul/19 15:42;azagrebin;[~zjwang] [~NicoK]
I agree that we should address the semantics of partition lifecycle separately. But the fine grained recovery is already implemented and planned for the release. I just wanted to make sure that network stack and this fix are in sync with the lifecycle semantics and the fine grained recovery effort.

`release on consumption` notion was an optimisation to save RPC release calls from JM and fallback to the previous behaviour, but as [~zjwang] pointed out this internal release is unreliable atm. If activated, it may always be done now, also in case of consumer failure (both notify/release), but only on a best effort. It means we have to adjust JM and send RPC release at least in case of consumer failure or just producer restart (FLINK-13371). When the fine-grained recovery is stable we might not need this option at all and can simplify network stack later if needed.;;;","23/Jul/19 02:50;zjwang;[~azagrebin] I totally agree with your above point. The first priority should make the current fine grained recovery work in release-1.9. As we confirmed before, the consumption notification via network is just best-effort atm, not always reliable especially when the network connection is not established during consumer failed.

I remembered that the JM would always release partitions while restarting producer tasks before, maybe I missed some parts while reviewing the relevant PRs of partition lifecycle feature. I am sorry for not giving this potential issue from network stack before.

We could solve the current issue in FLINK-13771 now, and further address the semantics of partition release and refactor the network behavior if necessary in release-1.10.;;;","23/Jul/19 11:54;azagrebin;[~zjwang] Thanks for the confirmation! I guess we all overlooked the potential issue.;;;","24/Jul/19 17:02;sewen;Thanks for this discussion. I commented on the PR suggestion to always call {{notifySubpartitionConsumed()}} when releasing a reader.

My suggestion for Flink 1.10 would be:
  - Drop {{notifySubpartitionConsumed()}} completely
  - Drop the {{ReleaseOnConsumptionResultPartition}}
  - For bounded blocking partitions, the release happens always from the scheduler (no {{JobManagerOptions.FORCE_PARTITION_RELEASE_ON_CONSUMPTION}} any more)
  - Pipelined subpartitions are released when the one and only reader/view is released. There can be no further reader, so might as well immediate release it. The result partition as whole is released when all subpartitions are released.
;;;","24/Jul/19 17:24;sewen;An observation on the current state of the implementation of {{ReleaseOnConsumptionResultPartition}}

We have a weird inconsistent situation where we
  1. have a count down on the ResultPartition level about the {{notifySubpartitionConsumed()}} calls from the subpartitions, to release the partition when all subpartitions are released.
  2. have results that can have multiple readers/views and can hence receive multiple release/consumed calls.

This can probably lead to counting two notifications from one subpartition, and then releasing too early, unless there is super careful accounting when to notify about consumption and when not to.
This careful accounting seems super fragile to me, given the state of the netty stack. I expect that we will have issues were we either notify too often (early release) or not often enough (lingering files).

I would suggest to do the following: 
  - Change {{ReleaseOnConsumptionResultPartition}} to have a flag per subpartition that tracks whether there was a {{notifySubpartitionConsumed()}} call or not, so that multiple calls are idempotent.
  - Always send {{notifySubpartitionConsumed()}} calls with every {{releaseAllResources()}} call.;;;","26/Jul/19 07:55;zjwang;Thanks for joining this discussion and giving helpful suggestions [~sewen].

I agree with the further refactoring of above relevant processes for partition release in release-1.10.

Considering the inconsistent situation mentioned above, actually it could work correctly by default now. Because the reference counter in `ReleaseOnConsumptionResultPartition` would only be used for the case of pipelined partitions by default. For the blocking partitions which would be consumed multiple times via creating multiple readers/views, the release should be controlled by the scheduler.

But I agree the above logic is fragile and might cause inconsistent easily if not setting correctly. We could further refactor this issue later.;;;","29/Jul/19 10:27;sewen;Fixed in
  - 1.9.0 as of 7116ab71edc183d34d128453e06a3efc15ad8905
  - master as of 71a53d49d1c6ce0e5f840e1b528cb75323dc2665;;;",,,,,,,,,,,,,,,
AbstractOperatorRestartTestBase fails on Windows,FLINK-13243,13244568,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Jul/19 09:51,24/Jul/19 14:56,13/Jul/23 08:10,15/Jul/19 09:17,1.9.0,,,,,,,,,1.9.0,,,,Tests,,,,,0,pull-request-available,,,,"The regular expressions contain new-lines, resulting in no matches being found.",,,,,,,,,,,,,,,"zentol commented on pull request #9103: [FLINK-13243][tests] Simplify exception matching
URL: https://github.com/apache/flink/pull/9103
 
 
   Reworks how the exception message matching is being done in the `AbstractOperatorRestoreTestBase` to not be subjected to OS-specific line-endings.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jul/19 09:56;githubbot;600","zentol commented on pull request #9103: [FLINK-13243][tests] Simplify exception matching
URL: https://github.com/apache/flink/pull/9103
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 09:16;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 09:17:19 UTC 2019,,,,,,,,,,"0|z04lx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 09:17;chesnay;master: f0e07043aa8531a5b88be21bda5bff075aedd4a7
1.9: 550eec39c8883ecafe5f44b60ab4fb180b16784f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StandaloneResourceManagerTest fails on travis,FLINK-13242,13244567,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,chesnay,chesnay,12/Jul/19 09:47,02/Oct/19 17:44,13/Jul/23 08:10,30/Jul/19 18:26,1.9.0,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"https://travis-ci.org/apache/flink/jobs/557696989

{code}
08:28:06.475 [ERROR] testStartupPeriod(org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerTest)  Time elapsed: 10.276 s  <<< FAILURE!
java.lang.AssertionError: condition was not fulfilled before the deadline
	at org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerTest.assertHappensUntil(StandaloneResourceManagerTest.java:114)
	at org.apache.flink.runtime.resourcemanager.StandaloneResourceManagerTest.testStartupPeriod(StandaloneResourceManagerTest.java:60)
{code}",,azagrebin,maguowei,shixg,SleePy,sunhaibotb,trohrmann,xtsong,zhuzh,,,,,,"azagrebin commented on pull request #9126: [FLINK-13242][tests][coordination] Get SlotManager.failUnfulfillableRequest in main thread of StandaloneResourceManager for verification in StandaloneResourceManagerTest
URL: https://github.com/apache/flink/pull/9126
 
 
   ## What is the purpose of the change
   
   In `StandaloneResourceManagerTest` we check `isFailingUnfulfillableRequest` in the test thread, although the underlying field `SlotManager.failUnfulfillableRequest` has no synchronisation and set in the main thread of `StandaloneResourceManager`. This can lead to a visibility problem of `SlotManager.failUnfulfillableRequest`: set in the main thread of `StandaloneResourceManager` but never updated in the test thread.
   
   ## Brief change log
   
   The idea for the fix is read `SlotManager.failUnfulfillableRequest` in the main thread of `StandaloneResourceManager`, get future result and check in the `StandaloneResourceManagerTest.assertHappensUntil`.
   
   ## Verifying this change
   
   run `StandaloneResourceManagerTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 08:11;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,3600,,,,,,,,,,,,,FLINK-13202,,,FLINK-13202,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 29 07:10:02 UTC 2019,,,,,,,,,,"0|z04lww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/19 17:00;azagrebin;There is at least a potential problem with the visibility of _SlotManager.failUnfulfillableRequest_ in _StandaloneResourceManagerTest#testStartupPeriod_.

The test checks _failUnfulfillableRequest_ directly in its thread but it is set in the main RPC thread of _StandaloneResourceManager#initialize_.

I reproduced it only once locally when I tried the test first time in IDE.

I can submit a PR with a fix where _StandaloneResourceManagerTest#testStartupPeriod_ queries _isFailingUnfulfillableRequest()_ in the main thread of _StandaloneResourceManager_.

Also as [~trohrmann@apache.org] noticed, we could use _getMainThreadExecutor()_ directly to schedule _setFailUnfulfillableRequest()_ in _StandaloneResourceManager#initialize_.

cc [~sewen] [~xintongsong];;;","18/Jul/19 03:08;sunhaibotb;Another instance: [https://api.travis-ci.com/v3/job/216807085/log.txt];;;","21/Jul/19 02:51;xtsong;Hi [~azagrebin], I think I found the problem.

In _StandaloneResourceManager#initialize()_ it uses _getMainThreadExecutor()_ to execute _setFailUnfulfillableRequest()_. However, before _setFailUnfulfillableRequest()_ is executed, the main thread executor of the resource manager might be replaced by a new one when it accepts the granted leader ship, leading to _setFailUnfulfillableRequest()_ never being executed. This only happens when the _StandaloneResourceManager#initialize()_ is invoked before _TestingLeaderElectionService#isLeader()_.

The problem can be re-produced and verified as follows:
 * Add logs in _StandaloneResourceManager#initialize()_, _TestingLeaderElectionService#isLeader()_ and _ResourceManager#setFailUnfulfillableRequest()_, and run the test. In most cases, you should see _TestingLeaderElectionService#isLeader()_ invoked before _StandaloneResourceManager#initialize()_, _setFailUnfulfillableRequest()_ is invoked, and the test should pass.
 * Add a short sleep time (in my case 100ms) in _MockResourceManagerRuntimeServices#grantLeadership()_ before _rmLeaderElectionService.isLeader()_, and run the test again. Now you should see _TestingLeaderElectionService#isLeader()_ invoked after _StandaloneResourceManager#initialize()_, _setFailUnfulfillableRequest()_ is never invoked, and the test should fail.
 * Add another short sleep time (also 100ms in my case) in _StandaloneResourceManager#initialize()_, inside the _getRpcService().getScheduledExecutor().schedule()_ block, right before _getMainThreadExecutor()_. This should change the order of invoking back and fix the failure.
 * If you invoke _getMainThreadExecutor()_ twice in _StandaloneResourceManager#initialize()_, once before the sleep and the other after it, and print out the fetched main thread executors, you should find that they are two different objects.
 * Now if you remove the sleep in _StandaloneResourceManager#initialize()_, you should see the printed two main thread executors are the same object, and the test is broken again.

I'm thinking that maybe _setFailUnfulfillableRequest(true)_ does not need to be invoked on the PRC main thread. Instead of calling on the main thread executor, I tried call _setFailUnfulfillableRequest(true)_ directly in the _getRpcService().getScheduledExecutor().schedule()_ block in _StandaloneResourceManager#initialize()_ and it fixes the problem.

I think we do not care whether the _setFailUnfulfillableRequest(true)_ happens on main thread or not in production, as long as it eventually get invoked. And for this test case, we may have a bit inconsistency that after _setFailUnfulfillableRequest(true)_ the _isFailingUnfulfillableRequest()_ may not get the correct result immediately, which I think is acceptable and the 10s timeout for _assertHappensUntil()_ should be long enough to catch the invoking of _setFailUnfulfillableRequest(true)_ eventually. What do you think?;;;","22/Jul/19 13:54;azagrebin;Hi [~xintongsong], I think this is a very gooding finding and the test failure makes sense now.

I do not think we can use _getRpcService().getScheduledExecutor()_ to set _failUnfulfillableRequest_ because it is again used from the main thread in _SlotManager#internalRequestSlot_ and ideally all state should be modified in the main thread.

Why do we schedule _setFailUnfulfillableRequest_ only once in _StandaloneResourceManager.initialize_ before RM gets leadership? If I understand correctly, this flag is to give some time to task executors to register the available slots before the slot requests can be checked whether they can be fulfilled or not. The task executors will register themselves every time this RM gets the leadership. It looks like _setFailUnfulfillableRequest_ should be scheduled after each leader election. Then the changing of the executor fencing token should not be a problem like for any other operation on a leader. cc [~sewen] [~till.rohrmann]

The same applies to the usage of 
 _getRpcService().getScheduledExecutor()_ to schedule _setFailUnfulfillableRequest._ We can use directly _getMainThreadExecutor().schedule()_. Now it does not work because of the same problem.;;;","23/Jul/19 01:35;xtsong;[~azagrebin]

I agree. The purpose of scheduling to invoke _setFailUnfulfillableRequest_ with a delay is to give time for task executors to register, and it makes sense to do this after each leader election. In that case, we should also make sure the flag is set back to _false_ when leadership is revoked.;;;","23/Jul/19 14:32;trohrmann;+1 for starting the timeout once the {{ResourceManager}} has gained leadership and resetting it once it loses leadership.;;;","29/Jul/19 07:10;SleePy;Another instance, [https://travis-ci.com/flink-ci/flink/jobs/220314470];;;",,,,,,,,,,,,,,,,,,,,,,,,,
YarnResourceManager does not handle slot allocations in certain cases,FLINK-13241,13244565,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,zhuzh,zhuzh,12/Jul/19 09:44,24/Oct/19 14:37,13/Jul/23 08:10,30/Jul/19 11:52,1.9.0,,,,,,,,,1.9.0,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"In the case that a job allocates a few slots first and after a period allocates some other slots. The YarnResourceManager seems to receive and ignore the latter slot requests.

To produce this issue, we can create a job with 2 vertices in different shared groups, as shown below:

!17_37_05__07_12_2019.jpg|width=433,height=127!

Slot allocation for map2 vertex happens after the source vertex acquires slots to decide its location, thus to meet the input constraints.

YarnResourceManager can receive slot requests for map2, but seems not to handle it and the job will hang there waiting for resources.

In my observation, this issue does not happen on Flink(Version: 1.9-SNAPSHOT, Rev:3bc322a, Date:26.06.2019 @ 17:28:51 CST). It should be a new issue after that.

 ",,aljoscha,maguowei,sewen,shixg,trohrmann,xtsong,zhuzh,,,,,,,"xintongsong commented on pull request #9105: [FLINK-13241][Yarn/Mesos] Fix Yarn/MesosResourceManager setting managed memory size into wrong configuration instance.
URL: https://github.com/apache/flink/pull/9105
 
 
   ## What is the purpose of the change
   
   This PR fix the bug that Yarn/MesosResourceManager set managed memory size into wrong configuration instance. As a result, such setting does not overwrite the config on TM side, causing RM/TM slot resource profile inconsistency. As a result, pending task manager slots on RM side cannot be completed once created, prevent RM from starting new TMs for subsequence slot requests.
   
   ## Brief change log
   
   - Yarn/MesosResourceManager set managed memory into the proper configuration instance.
   
   ## Verifying this change
   
     - Manually verified the change by running a streaming job with two vertices using lazy from source strategy on Yarn. The job requests slots in 2 rounds and we verified that RM does start new TM for the second round slot requests, which it doesn't without this change.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jul/19 00:45;githubbot;600","xintongsong commented on pull request #9246: [FLINK-13241][Yarn/Mesos] Duplication of PR#9105 on master branch.
URL: https://github.com/apache/flink/pull/9246
 
 
   This PR is a duplication of #9105 on master branch. The original PR #9105 was opened on release-1.9 branch.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jul/19 06:08;githubbot;600","tillrohrmann commented on pull request #9246: [FLINK-13241][Yarn/Mesos] Duplication of PR#9105 on master branch.
URL: https://github.com/apache/flink/pull/9246
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 11:49;githubbot;600","xintongsong commented on pull request #9105: [FLINK-13241][Yarn/Mesos] Fix Yarn/MesosResourceManager setting managed memory size into wrong configuration instance.
URL: https://github.com/apache/flink/pull/9105
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jul/19 21:26;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,FLINK-13579,,,,,FLINK-14074,,,,,,,,"12/Jul/19 09:38;zhuzh;17_37_05__07_12_2019.jpg;https://issues.apache.org/jira/secure/attachment/12974498/17_37_05__07_12_2019.jpg",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 30 11:52:17 UTC 2019,,,,,,,,,,"0|z04lwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/19 12:32;trohrmann;Thanks for reporting this issue. Could you please share the corresponding logs of the failed runs [~zhuzh]?;;;","12/Jul/19 13:44;xtsong;[~till.rohrmann], [~zhuzh]

Quick updates on discoveries so far.

I found that the resource manager failed to match registered slots to the pending task manager slots. The first round pending slots are not completed by the registered slots, however the pending requests still get allocated on the registered slots through the code path `handleFreeSlot`, which also unassigned the request from the pending task slot. As result, the free pending task slots are left in the slot manager while there is actually no pending slots. That prevent the resource manager from requesting new containers for the second round slot requests.

The registered slots are not matched with pending task manager slots because managed memory size in resource profiles do not exactly match. Could be small errors during megabytes and bytes conversions. I'll continue debugging on this.;;;","12/Jul/19 15:02;trohrmann;Thanks for looking into this [~xintongsong]. Keep us posted what you find.;;;","12/Jul/19 15:08;sewen;I would vote to calculate with bytes everywhere.
It would be simplest if we started using {{MemorySize}} instead of longs.
That has the advantage that we always keep bytes internally (no rounding error and loss) but can still give users a way to configure bigger units like megabytes.;;;","13/Jul/19 00:58;xtsong;Turns out it's not a bytes/megabytes converting issue.

We explicitly set the exact managed memory size into configuration on RM side, to avoid TM calculating managed memory from fraction and uncertain JVM free memory. However, we were setting the wrong configuration instance. In YarnResourceManager constructor, we copied the configuration instance because we are going to alter it. But we altered the original configuration instead of the copied one which is used by TM.

I've opened a PR to fix it, and same problem for Mesos.;;;","30/Jul/19 11:52;trohrmann;Fixed via

1.10.0: 01f8c3508140988dcff7e3a72bf1b483db8f79f5
1.9.0: b4697f7e41acfb6b76bb7a67d9d19bc577a023c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
TemporalTypesTest randomly failed on travis,FLINK-13234,13244486,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,ykt836,ykt836,12/Jul/19 01:20,12/Jul/19 06:08,13/Jul/23 08:10,12/Jul/19 06:08,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,test-stability,,,,"TemporalTypesTest>ExpressionTestBase.evaluateExprs:154 Wrong result for: [CURRENT_DATE] optimized to: [CURRENT_DATE] expected:<2019-07-1[2]> but was:<2019-07-1[1]>

 

more details in: [https://api.travis-ci.org/v3/job/557582157/log.txt]",,lzljs3620320,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 12 06:08:20 UTC 2019,,,,,,,,,,"0|z04lew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/19 02:21;lzljs3620320;Sorry for this, [https://github.com/apache/flink/pull/9095] is fixing to this.;;;","12/Jul/19 06:03;trohrmann;Another instance: https://api.travis-ci.org/v3/job/557572319/log.txt;;;","12/Jul/19 06:08;ykt836;fixed via: 53a044c987718d74db3a9b6102f1fa27324a7fab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TemporalTypesTest>ExpressionTestBase.evaluateExprs in blink planner fails on PST Timezone,FLINK-13232,13244456,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,phoenixjiangnan,phoenixjiangnan,11/Jul/19 20:56,12/Jul/19 06:07,13/Jul/23 08:10,12/Jul/19 06:07,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"TemporalTypesTest>ExpressionTestBase.evaluateExprs in blink planner seems to have a bug on timezone.

When I run it locally at PST timezone, it always fails as:

{code:java}
[ERROR] Failures:
[ERROR]   TemporalTypesTest>ExpressionTestBase.evaluateExprs:154 Wrong result for: [CURRENT_DATE] optimized to: [CURRENT_DATE] expected:<2019-07-1[2]> but was:<2019-07-1[1]>
{code}

I didn't find it fails on travis. Thus I suspect there's a bug w.r.t. timezone.",,phoenixjiangnan,,,,,,,,,,,,,"JingsongLi commented on pull request #9095: [FLINK-13232][table-planner-blink] Remove CURRENT_DATE test to avoid timezone error
URL: https://github.com/apache/flink/pull/9095
 
 
   
   ## What is the purpose of the change
   
   CURRENT_DATE return UTC date, if user want to return local date, user should use LOCALTIMESTAMP to get date.
   
   ## Verifying this change
   
   ut
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jul/19 01:37;githubbot;600","KurtYoung commented on pull request #9095: [FLINK-13232][table-planner-blink] Correct CURRENT_DATE test to avoid timezone error
URL: https://github.com/apache/flink/pull/9095
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jul/19 06:07;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 12 06:07:46 UTC 2019,,,,,,,,,,"0|z04l88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/19 06:07;ykt836;merged in 1.9.0: 53a044c987718d74db3a9b6102f1fa27324a7fab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExpressionReducer with udf bug in blink,FLINK-13229,13244334,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,11/Jul/19 13:45,15/Jul/19 01:57,13/Jul/23 08:10,15/Jul/19 01:57,,,,,,,,,,1.10.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When a udf is reduced by ExpressionReducer, there will be a code gen exception.",,jark,lzljs3620320,,,,,,,,,,,,"JingsongLi commented on pull request #9091: [FLINK-13229][table-planner-blink] ExpressionReducer with udf bug in blink
URL: https://github.com/apache/flink/pull/9091
 
 
   
   ## What is the purpose of the change
   
   When a udf is reduced by ExpressionReducer, there will be a code gen exception.
   
   ## Verifying this change
   
   ut
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jul/19 13:51;githubbot;600","wuchong commented on pull request #9091: [FLINK-13229][table-planner-blink] ExpressionReducer with udf bug in blink
URL: https://github.com/apache/flink/pull/9091
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 01:49;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 01:57:37 UTC 2019,,,,,,,,,,"0|z04khk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 01:57;jark;Fixed in 1.10: https://github.com/apache/flink/commit/a5812688d254c174153f5f46876311153795714e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopRecoverableWriterTest.testCommitAfterNormalClose fails on Travis,FLINK-13228,13244333,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,liyu,trohrmann,trohrmann,11/Jul/19 13:41,28/Sep/20 06:43,13/Jul/23 08:10,29/Jul/19 13:01,1.9.0,,,,,,,,,1.9.0,,,,FileSystems,,,,,0,pull-request-available,test-stability,,,"{{HadoopRecoverableWriterTest.testCommitAfterNormalClose}} failed on Travis with

{code}
HadoopRecoverableWriterTest.testCommitAfterNormalClose Â» IO The stream is closed
{code}

https://api.travis-ci.org/v3/job/557293706/log.txt",,gaoyunhaii,liyu,sewen,trohrmann,,,,,,,,,,"carp84 commented on pull request #9235: [FLINK-13228][tests][filesystems] Harden HadoopRecoverableWriterTest
URL: https://github.com/apache/flink/pull/9235
 
 
   ## What is the purpose of the change
   Currently test cases will fail when trying to close the output stream if all data written but `ClosedByInterruptException` occurs at the ending phase. This PR proposes to close the stream silently if the test case logic passed properly.
   
   ## Brief change log
   Close the output stream silently in `HadoopRecoverableWriterTest` if the test case logic passed properly.
   
   
   ## Verifying this change
   This change is on the test cases and verified by sanity test through modifying hadoop source code to simulate the rarely happening situation that triggers the issue.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 08:50;githubbot;600","asfgit commented on pull request #9235: [FLINK-13228][tests][filesystems] Harden HadoopRecoverableWriterTest
URL: https://github.com/apache/flink/pull/9235
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 12:58;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-12071,,,,,,,,,,,,,FLINK-18818,,,,,,,,"24/Jul/19 08:03;liyu;FLINK-13228.hadoop.debug.patch;https://issues.apache.org/jira/secure/attachment/12975604/FLINK-13228.hadoop.debug.patch","24/Jul/19 12:08;liyu;FLINK-13228.hadoop.debug.v2.patch;https://issues.apache.org/jira/secure/attachment/12975640/FLINK-13228.hadoop.debug.v2.patch","24/Jul/19 12:19;liyu;FLINK-13288.demo.patch;https://issues.apache.org/jira/secure/attachment/12975642/FLINK-13288.demo.patch",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 29 13:01:41 UTC 2019,,,,,,,,,,"0|z04khc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/19 11:36;chesnay;Another instance: https://travis-ci.org/apache/flink/jobs/560546002;;;","24/Jul/19 08:01;liyu;Cannot reproduce the issue until now and have tried the below methods:
* Try to run {{HadoopRecoverableWriterTest#testCommitAfterNormalClose}} in local Intellij environment with ""Repeat until failure"" mode, ran thousands of times but cannot reproduce.

* Try to run {{HadoopRecoverableWriterTest}} repeatedly on travis but cannot reproduce
** Modify the travis scripts to run {{HadoopRecoverableWriterTest}} 100 times per [test|https://travis-ci.org/carp84/flink/builds/562680839] with {{forkCount=1}}, never reproduced
** Modify the travis scripts to run [all tests|https://travis-ci.org/carp84/flink/builds/562715072] in {{flink-hadoop-fs}} module with {{forkCount=2}}, never reproduced

* Try to modify the {{org.apache.hadoop.net.SocketOutputStream}} class to produce {{ClosedByInterruptException}} more aggressively, but still haven't got a way to reproduce
** To do this, we need to apply the attached patch to hadoop 2.8.3 code base and do local install, then clone the [flink-shaded|https://github.com/apache/flink-shaded] repository, switch to the {{release-7.0}} branch, and run command {{mvn -DskipTests -Dhadoop.version=2.8.3 -pl flink-shaded-hadoop-2 install -am}};;;","24/Jul/19 08:21;liyu;From current investigation, the most weird part is that from travis log, there's an additional data flush in {{DataStreamer}} as indicated below:
{noformat}
23:31:07,549 INFO  org.apache.hadoop.hdfs.StateChange                            - DIR* completeFile: /tests/qoadqaraomqogyax/.part-0.inprogress.748c58f2-4db8-4b2f-a10a-e5ea09410b7e is closed by DFSClient_NONMAPREDUCE_-1957051875_1
23:31:07,552 WARN  org.apache.hadoop.hdfs.DataStreamer                           - DataStreamer Exception
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:478)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:766)
13:33:59,846 INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit     - allowed=true	ugi=travis (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tests/fsledeprirhvkkht/.part-0.inprogress.fd9bc50d-96aa-48bc-86e1-8662382b53c9	dst=null	perm=null	proto=rpc
13:33:59,848 INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit     - allowed=true	ugi=travis (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/tests/fsledeprirhvkkht/.part-0.inprogress.fd9bc50d-96aa-48bc-86e1-8662382b53c9	dst=/tests/fsledeprirhvkkht/part-0	perm=travis:supergroup:rw-r--r--	proto=rpc
13:33:59,849 INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit     - allowed=true	ugi=travis (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/tests/fsledeprirhvkkht	dst=null	perm=null	proto=rpc
13:33:59,850 INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit     - allowed=true	ugi=travis (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tests/fsledeprirhvkkht/part-0	dst=null	perm=null	proto=rpc
13:33:59,855 INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit     - allowed=true	ugi=travis (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/tests/fsledeprirhvkkht	dst=null	perm=null	proto=rpc

{noformat}

However, when debugging with the attached hadoop patch, we could confirm the data will be split into two packets in our test case, and if any of them failed to write, we will see below exception:
{noformat}
[DataStreamer for file /tests/yufomllbjxlhztth/.part-0.inprogress.fca749ce-ce9d-4fef-9f08-3937358b4c05 block BP-1698089270-127.0.0.1-1563951866468:blk_1073741825_1001] WARN  org.apache.hadoop.hdfs.DataStreamer  - DataStreamer Exception
java.nio.channels.ClosedByInterruptException
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:77)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:179)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:137)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
        at java.io.DataOutputStream.flush(DataOutputStream.java:123)
        at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:766)
[DataXceiver for client DFSClient_NONMAPREDUCE_-1981706570_1 at /127.0.0.1:51159 [Receiving block BP-1698089270-127.0.0.1-1563951866468:blk_1073741825_1001]] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Exception for BP-1698089270-127.0.0.1-1563951866468:blk_1073741825_1001
java.io.IOException: Premature EOF from inputStream
        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:208)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:521)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:923)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:854)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:166)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:103)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:288)
        at java.lang.Thread.run(Thread.java:745)
Test testCommitAfterNormalClose(org.apache.flink.runtime.fs.hdfs.HadoopRecoverableWriterTest) failed with:
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:51147,DS-8f7fb1af-ce7d-4547-b50b-a53f00c40408,DISK]] are bad. Aborting...
        at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1530)
        at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1465)
        at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
        at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
{noformat}

But if we throw {{ClosedByInterruptException}} after the two packets completes, we will see no additional flush happened to {{DataStreamer}}, as indicated by below log:
{noformat}
24794 [PacketResponder: BP-695740659-127.0.0.1-1563954445528:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - PacketResponder: BP-695740659-127.0.0.1-1563954445528:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
24795 [IPC Server handler 4 on 53181] INFO  org.apache.hadoop.hdfs.StateChange  - DIR* completeFile: /tests/ujoamxgnvmlapclr/.part-0.inprogress.e3bcef26-5811-495b-961b-1c65b20acccf is closed by DFSClient_NONMAPREDUCE_1297545335_1
24796 [IPC Server handler 5 on 53181] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=jueding (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tests/ujoamxgnvmlapclr/.part-0.inprogress.e3bcef26-5811-495b-961b-1c65b20acccf	dst=null	perm=null	proto=rpc
24797 [IPC Server handler 6 on 53181] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=jueding (auth:SIMPLE)	ip=/127.0.0.1	cmd=rename	src=/tests/ujoamxgnvmlapclr/.part-0.inprogress.e3bcef26-5811-495b-961b-1c65b20acccf	dst=/tests/ujoamxgnvmlapclr/part-0	perm=jueding:supergroup:rw-r--r--	proto=rpc
24798 [IPC Server handler 7 on 53181] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=jueding (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/tests/ujoamxgnvmlapclr	dst=null	perm=null	proto=rpc
24799 [IPC Server handler 8 on 53181] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=jueding (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/tests/ujoamxgnvmlapclr/part-0	dst=null	perm=null	proto=rpc
{noformat}

Will try to figure out where the additional flush comes from and I believe we could reproduce and resolve the issue once this is located.;;;","24/Jul/19 12:08;liyu;{noformat}
23:31:07,552 WARN org.apache.hadoop.hdfs.DataStreamer - DataStreamer Exception
java.nio.channels.ClosedByInterruptException at
java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:478)
{noformat}

After a closer check of the above log, I found the root cause is {{ClosedByInterruptException}} occurred after {{channel.write(buf)}} completes writing data in {{SocketOutputStream#performIO}}, and now I could stably reproduce the issue with the v2 hadoop patch as attached.

Since all data are already written successfully and file also marked as complete by NameNode, we should silently ignore the {{ClosedByInterruptedException}} instead of throwing it out as an error, which IMO is something hadoop should fix. Will file a JIRA for HDFS once find out a proper solution.

As per how to fix the issue here, since the issue is thrown at closing the {{RecoverableFsDataOutputStream}} (easily to confirm after flattening the try-with-resource to a normal try-catch), I think we could directly try-catch the exception and ignore it if failed to close the {{RecoverableFsDataOutputStream}}, because this is irrelative to the target of the test case (checking whether commit after normal close works). Wdyt? [~till.rohrmann] [~chesnay]

Will attach the draft patch here for a straight forward check.

 ;;;","24/Jul/19 12:26;liyu;Please check the demo patch, actually all other test cases will fail (except for {{testCloseWithNoData}} which has no try-with-resource) with the v2 hadoop patch, so the final PR will change all test methods in a similar way.

And the given patch could also fix FLINK-12071 (actually these two are duplicates with the same root cause). For example, below is the exception I observed for {{testExceptionWritingAfterCloseForCommit}} with v2 hadoop patch
{noformat}
Caused by: java.lang.IllegalArgumentException: Self-suppression not permitted
	at java.lang.Throwable.addSuppressed(Throwable.java:1043)
	at org.apache.flink.core.fs.AbstractRecoverableWriterTest.testExceptionWritingAfterCloseForCommit(AbstractRecoverableWriterTest.java:308)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	... 22 more
Caused by: java.io.IOException: The stream is closed
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:147)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.hadoop.hdfs.DataStreamer.closeStream(DataStreamer.java:987)
	at org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:839)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:834)
	Suppressed: java.io.IOException: The stream is closed
		at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:147)
		at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
		at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
		at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
		at java.io.FilterOutputStream.close(FilterOutputStream.java:159)
		... 3 more
{noformat};;;","24/Jul/19 15:28;sewen;Thanks for this diagnosis. I came to a similar conclusion:

The test runs fulls (you can see the rename command, list command, open command for verification) and only after that can be the failure. That only leaves the closing or deleting of the file.

In any case, the test passed properly.

[~carp84] can you open a PR with the fix? will merge it asap.;;;","26/Jul/19 09:06;liyu;Thanks [~sewen] for taking a look and sorry for the late response. Just submitted the PR, FYI.;;;","29/Jul/19 13:01;sewen;Fixed in
  - 1.9.0 via 0e9f463668378bd7469194ebf0af76e3c125f0d7
  - master via e373c4481e6a0ca0e1e73a6170b9e3da5cc9be5b;;;",,,,,,,,,,,,,,,,,,,,,,,,
KafkaProducerExactlyOnceITCase.testMultipleSinkOperators fails on Travis,FLINK-13226,13244329,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,becket_qin,trohrmann,trohrmann,11/Jul/19 13:38,01/Aug/19 07:28,13/Jul/23 08:10,01/Aug/19 07:28,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Kafka,,,,,0,pull-request-available,test-stability,,,"The {{KafkaProducerExactlyOnceITCase.testMultipleSinkOperators}} fails on Travis with not producing output for 300 s.

https://api.travis-ci.org/v3/job/557290235/log.txt",,becket_qin,shixg,trohrmann,,,,,,,,,,,"becketqin commented on pull request #9224: [FLINK-13226] [connectors / kafka] Fix race condition between transaction commit and produc…
URL: https://github.com/apache/flink/pull/9224
 
 
   …er closure.
   
   ## What is the purpose of the change
   This patch fixes a race condition between the checkpointing thread and main thread. The sequence causing the deadlock is the following:
   1. In `FlinkKafkaProducer`, the main thread encounters a problem and closes all the producer to start failover.
   2. The previous checkpoint has completed, so the checkpointing thread grabs the checkpoint lock and tries to commit the transaction on the producer that has been closed in step 1. This commit will never succeed due to [KAFKA-6635](https://issues.apache.org/jira/browse/KAFKA-6635). So the checkpoint thread blocks forever.
   3. In `StreamTask`, the main thread will eventually try to release all the record writer. To do that, it attempts to grab the checkpoint lock which is hold by checkpoint thread in step 2 and will never be released. So the main thread also blocks forever.
   
   KAFKA-6635 has been fixed in Kafka 2.3.0. But Flink 1.9 does not rely on that yet, and we also support Kafka 0.11. So we are just going to fix on the Flink side first. The solution is to make sure that in `FlinkKafkaProducer` any operation relying on the underlying sender thread to finish throws an exception if the producer is closed.
   
   This patch also fixes a minor issue of duplicated static inner class name in `KafkaConsumerTestBase`.
   
   ## Brief change log
   - Make `FlinkKafkaProducer` and `FlinkKafkaInternalProducer` thread safe.
   - Fix static inner class name collision in `KafkaConsumerTestBase`.
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - Added test to ensure exception will be thrown if a blocking method is called on `FlinkKafkaProducer` and `FlinkKafkaInternalProducer` after the producer is closed.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 08:53;githubbot;600","becketqin commented on pull request #9224: [FLINK-13226] [connectors / kafka] Fix race condition between transaction commit and produc…
URL: https://github.com/apache/flink/pull/9224
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 08:53;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 01 07:27:57 UTC 2019,,,,,,,,,,"0|z04kgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 16:00;becket_qin;I took a look at the stack trace here. We are hitting KAFKA-6635 here which is causing a deadlock.

The expected behavior of the test is the following:
 # produce some messages to Kafka 
 # flush and checkpoint some of the messages.
 # produce some more messages.
 # inject an artificial exception to cause the application to fail
 # failover, complete the producing and finish the job

The process stuck at step 5.

 

The task main thread was about to exit and tried to grab checkpoint lock to release the operator chain outputs.

{noformat}

""Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Unnamed) (1/1)"" #224 prio=5 os_prio=0 tid=0x00007f7360c28800 nid=0x36e4 waiting for monitor entry [0x00007f732d44d000]
 java.lang.Thread.State: BLOCKED (on object monitor)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:489)
 - waiting to lock <0x000000009914c9d8> (a java.lang.Object)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:685)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:515)
 at java.lang.Thread.run(Thread.java:748)

{noformat}

But the checkpointing thread was holding the checkpoint lock and blocked waiting for the Kafka transaction to be committed.

{noformat}

""Async calls on Source: Custom Source -> Map -> (Sink: Unnamed, Sink: Unnamed) (1/1)"" #227 daemon prio=5 os_prio=0 tid=0x00007f7343ccf800 nid=0x36e7 waiting on condition [0x00007f732cf4a000]
 java.lang.Thread.State: WAITING (parking)
 at sun.misc.Unsafe.park(Native Method)
 - parking to wait for <0x00000000ec805fb8> (a java.util.concurrent.CountDownLatch$Sync)
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
 at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
 at org.apache.kafka.clients.producer.internals.TransactionalRequestResult.await(TransactionalRequestResult.java:50)
 at org.apache.kafka.clients.producer.KafkaProducer.commitTransaction(KafkaProducer.java:698)
 at org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.commitTransaction(FlinkKafkaInternalProducer.java:86)
 at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.commit(FlinkKafkaProducer.java:906)
 at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.commit(FlinkKafkaProducer.java:98)
 at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:283)
 at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:827)
 - locked <0x000000009914c9d8> (a java.lang.Object)
 at org.apache.flink.runtime.taskmanager.Task$2.run(Task.java:1178)
 at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)

{noformat}

The KafkaProducer internal sender thread is supposed to complete the transaction commit and notify the thread committing transaction. However, the internal sender thread has already gone because the producers has been closed when the task main thread disposes the all the operators. Therefore the checkpointing thread will just block forever, so does the task main thread.

This fix should be on the KafkaProducer side. However, currently the fix is only in Kafka 2.3.0. And there is no chance this to be backported to 0.11. Given that we do not have Kafka 2.3.0 dependency in Flink 1.9. We probably still need to fix it by ourselves in Flink first.

I think the solution here is basically
 # Ensure the closing of the FlinkKafkaProducer is mutually exclusive to any other thread.
 # After the FlinkKafkaProducer is closed, no further action is made to the producer.

 ;;;","29/Jul/19 08:27;trohrmann;[~pnowojski] please help reviewing.;;;","01/Aug/19 07:27;becket_qin;Fixed via

1.9
4f2d5f1d41efefd5704261777245afccebd44ea7
3c2f2e7a1e7abaf88d5734906f26ef4c8aa0df83
bb858ab8df711212779881c8514bbbf91fbedaca

1.10
6d474988e3d71c9df8ee85912d5eed5a26869a50
cc3184a35f2430c94535a095c2f926e912f692bf
09f96b339f4890d7a44ae92c915ea8c0f6f244cb

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blink planner should set ScheduleMode to LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST for batch jobs,FLINK-13221,13244304,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fan_li_ya,ykt836,ykt836,11/Jul/19 12:19,11/Mar/20 11:12,13/Jul/23 08:10,23/Jul/19 01:34,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,fan_li_ya,,,,,,,,,,,,,"liyafan82 commented on pull request #9101: [FLINK-13221][Table SQL / Planner] Blink planner should set ScheduleMode to LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST for batch jobs
URL: https://github.com/apache/flink/pull/9101
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Make batch jobs use LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST as the default scheduling mode.
   
   
   ## Brief change log
   
     - Replace LAZY_FROM_SOURCES with LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST for batch jobs.
     
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jul/19 06:35;githubbot;600","liyafan82 commented on pull request #9190: [FLINK-13221][Table SQL / Planner] [Release-1.9] Blink planner should set ScheduleMode to LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST for batch jobs
URL: https://github.com/apache/flink/pull/9190
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Make batch jobs use LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST as the default scheduling mode.
   
   
   ## Brief change log
   
     - Replace LAZY_FROM_SOURCES with LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST for batch jobs.
     
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 03:12;githubbot;600","KurtYoung commented on pull request #9190: [FLINK-13221][Table SQL / Planner][Release-1.9] Blink planner should set ScheduleMode to LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST for batch jobs
URL: https://github.com/apache/flink/pull/9190
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 09:37;githubbot;600","KurtYoung commented on pull request #9101: [FLINK-13221][Table SQL / Planner] Blink planner should set ScheduleMode to LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST for batch jobs
URL: https://github.com/apache/flink/pull/9101
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jul/19 01:35;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16543,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 23 01:34:38 UTC 2019,,,,,,,,,,"0|z04kb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/19 12:34;ykt836;after [https://github.com/apache/flink/pull/9073] merges;;;","22/Jul/19 01:41;ykt836;I changed this to 1.9.0's blocker, since this is a crucial one to fully turn on runtime's fine grained recovery for batch jobs.

[~fan_li_ya] Could you also open a PR to 1.9 branch?;;;","22/Jul/19 03:12;fan_li_ya;[~ykt836]. Sure. Thanks a lot.;;;","23/Jul/19 01:34;ykt836;merged in 1.10.0: d5cb1b9e5aefdc73026985f67a991f099e8f4364

merged in 1.9.0: aceaca6970f15c377c116ce3fbb27b178ef4a615;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive connector fails hadoop 2.4.1 builds,FLINK-13219,13244297,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,11/Jul/19 11:43,24/Jul/19 14:55,13/Jul/23 08:10,12/Jul/19 09:16,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"The hive connector does not work with hadoop 2.4, but the tests are still run in the corresponding cron profile.

https://travis-ci.org/apache/flink/jobs/555723021

We should add a profile for skipping the hive tests that we enable for these profiles.",,,,,,,,,,,,,,,"zentol commented on pull request #9086: [FLINK-13219][hive] Disable tests for hadoop 2.4 profile
URL: https://github.com/apache/flink/pull/9086
 
 
   Disables hive tests for the hadoop 2.4.1 travis profiles.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jul/19 11:48;githubbot;600","zentol commented on pull request #9086: [FLINK-13219][hive] Disable tests for hadoop 2.4 profile
URL: https://github.com/apache/flink/pull/9086
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jul/19 09:15;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 12 09:16:26 UTC 2019,,,,,,,,,,"0|z04k9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/19 09:16;chesnay;master: b7f855a4af1cdf6ba6a77f723905ffcf2969a86c
1.9: 8e50e5d84a11e159436d888bd1c433d60b30229f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blink planner does not compile with Scala 2.12,FLINK-13217,13244295,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,chesnay,chesnay,11/Jul/19 11:35,01/Aug/19 10:58,13/Jul/23 08:10,15/Jul/19 05:21,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,,,,,"{code}
[ERROR] C:\Dev\Repos\flink\flink-table\flink-table-planner-blink\src\main\scala\org\apache\flink\table\calcite\FlinkLogicalRelFactories.scala:111: error: overriding method createSort in trait SortFactory of type (x$1: org.apache.calcite.plan.RelTraitSet, x$2: org.apache.calcite.rel.RelNode, x$3: org.apache.calcite.rel.RelCollation, x$4: org.apache.calcite.rex.RexNode, x$5: org.apache.calcite.rex.RexNode)org.apache.calcite.rel.RelNode;
[ERROR]  method createSort needs `override' modifier
[ERROR]     def createSort(
[ERROR]         ^
[ERROR] C:\Dev\Repos\flink\flink-table\flink-table-planner-blink\src\main\scala\org\apache\flink\table\calcite\FlinkLogicalRelFactories.scala:181: error: overriding method createJoin in trait JoinFactory of type (x$1: org.apache.calcite.rel.RelNode, x$2: org.apache.calcite.rel.RelNode, x$3: org.apache.calcite.rex.RexNode, x$4: org.apache.calcite.rel.core.JoinRelType, x$5: java.util.Set[String], x$6: Boolean)org.apache.calcite.rel.RelNode;
[ERROR]  method createJoin needs `override' modifier
[ERROR]     def createJoin(
[ERROR]         ^
{code}",,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 05:21:58 UTC 2019,,,,,,,,,,"0|z04k94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 05:21;jark;Fixed in 1.10.0: 2c4e83c4d9de47ff282e43c3c7b1447e2f9b96a8
1.9.0: 79552effc4b65f8a947bc7feedf54e45cd03437d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AggregateITCase.testNestedGroupByAgg fails on Travis,FLINK-13216,13244292,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jark,trohrmann,trohrmann,11/Jul/19 11:27,01/Aug/19 10:57,13/Jul/23 08:10,16/Jul/19 07:36,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,"The {{AggregateITCase.testNestedGroupByAgg}} fails on Travis with

{code}
AggregateITCase.testNestedGroupByAgg:472 expected:<List(1,1,1,1,1, 3,1,15,15,3, 4,1,34,34,4, 7,2,23,5,2)> but was:<List(1,1,7,1,1, 3,1,15,15,3, 4,1,34,34,4, 7,2,23,5,2)>
{code}

https://api.travis-ci.org/v3/job/557214216/log.txt",,jark,trohrmann,,,,,,,,,,,,"wuchong commented on pull request #9120: [FLINK-13216][FLINK-13153][table-planner-blink] Fix Max_Retract and Min_Retract may produce incorrect result
URL: https://github.com/apache/flink/pull/9120
 
 
   
   
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fix unstable tests failed on travis, including `AggregateITCase.testNestedGroupByAgg` and `SplitAggregateITCase.testMinMaxWithRetraction`.
   
   
   ## Brief change log
   
   
   The reason is we didn't set null to acc.max/min which may have an old value when we need to get a new max/min from an empty MapView. And the old value will be output to downstream instead of a null value. This influences the final result. 
   
   This causes many unstable cases, which all contains max/min with retract, including:
   - AggregateITCase.testNestedGroupByAgg
   - SplitAggregateITCase.testMinMaxWithRetraction
   
   
   ## Verifying this change
   
   The situation is not reproduced 100%. I tested in my local machine, for example, `AggregateITCase.testNestedGroupByAgg` can reproduce this problem every 100 times. After fix this problem, I didn't reproduce this problem after running 1000 times.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 14:13;githubbot;600","asfgit commented on pull request #9120: [FLINK-13216][FLINK-13153][table-planner-blink] Fix Max_Retract and Min_Retract may produce incorrect result
URL: https://github.com/apache/flink/pull/9120
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 07:29;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-13153,,,FLINK-13142,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 16 07:37:34 UTC 2019,,,,,,,,,,"0|z04k8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/19 07:37;jark;Fixed in 1.10.0: 1bede7ef01d4694e890f8602d19ce8198120e8d7
Fixed in 1.9.0: db38ed0e503ebd244a52144e3c320df571943dd9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive connector does not compile on Java 9,FLINK-13215,13244286,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,11/Jul/19 10:58,01/Aug/19 10:58,13/Jul/23 08:10,16/Jul/19 09:36,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-hive_2.11: Compilation failure
[ERROR] /C:/Dev/Repos/flink/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/batch/connectors/hive/FlinkStandaloneHiveRunner.java:[56,15] package sun.net.www is not visible
{code}",,lirui,xuefuz,,,,,,,,,,,,"zentol commented on pull request #9085: [FLINK-13215][hive] Mirate ParseUtil usages to URLEncoder
URL: https://github.com/apache/flink/pull/9085
 
 
   Fixes a Java 9 compilation problem; the `ParseUtil` class is no longer accessible by default. Migrate to public APIs instead.
   
   @bowenli86 Could you take a look at this one? What's a good way to test this change?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jul/19 11:09;githubbot;600","zentol commented on pull request #9085: [FLINK-13215][hive] Mirate ParseUtil usages to URLEncoder
URL: https://github.com/apache/flink/pull/9085
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jul/19 10:08;githubbot;600","zentol commented on pull request #9127: [FLINK-13215][hive] Simply copying of classpath
URL: https://github.com/apache/flink/pull/9127
 
 
   The `FlinkStandaloneHiveRunner` creates an external Hive process and for this purpose provides the classpath of the test.
   
   The existing code was processing the classpath in what seems to be unnecessary ways; the tests pass on Travis without any of this processing.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 08:49;githubbot;600","zentol commented on pull request #9127: [FLINK-13215][hive] Simply copying of classpath
URL: https://github.com/apache/flink/pull/9127
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 09:35;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 16 09:36:04 UTC 2019,,,,,,,,,,"0|z04k74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/19 11:03;chesnay;Potential alternative for {{ParseUtil.encodePath(path);}} might be {{URLEncoder.encode(path, StandardCharsets.UTF_8.name())}}; running this on Travis now.;;;","15/Jul/19 10:00;lirui;I was trying to encode white spaces in the path. And seems the following can serve the same purpose.
{code}
classpath.replaceAll(""\\s"", ""%20"")
{code};;;","16/Jul/19 09:36;chesnay;master: b63bd5dda6fa9881a2a3f373f273145ea70a5657
1.9: 7ec3bb8885a51ac2ce4eab6b5978c50f1ff4bf09;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive connector is missing jdk.tools exclusion for Java 9,FLINK-13214,13244284,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,11/Jul/19 10:47,11/Jul/19 10:57,13/Jul/23 08:10,11/Jul/19 10:57,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Hive,,,,,0,,,,,"{code}
[ERROR] Failed to execute goal on project flink-connector-hive_2.12: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.9-SNAPSHOT: Could not find artifact jdk.tools:jdk.tools:jar:1.7 at specified path C:\Dev\Java\9/../lib/tools.jar -> [Help 1]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 11 10:57:26 UTC 2019,,,,,,,,,,"0|z04k6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/19 10:57;chesnay;master: bdcadfa2df3a39dbd3ddc4d7390ac66d76057b5c ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Following FLINK-12951: Remove TableEnvironment#sql and add create table ddl support to sqlUpdate,FLINK-13209,13244261,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny0405,danny0405,danny0405,11/Jul/19 08:59,11/Jul/19 13:35,13/Jul/23 08:10,11/Jul/19 13:35,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,danny0405,xuefuz,,,,,,,,,,,,"danny0405 commented on pull request #9081: [FLINK-13209] Following FLINK-12951: Remove TableEnvironment#sql and …
URL: https://github.com/apache/flink/pull/9081
 
 
   ## What is the purpose of the change
   
   This patch move create table ddl support from `#sql` to `#sqlUpdate`, also remove the deprecation of `sqlUpdate` and `sqlQuery`
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jul/19 09:37;githubbot;600","KurtYoung commented on pull request #9081: [FLINK-13209] Following FLINK-12951: Remove TableEnvironment#sql and …
URL: https://github.com/apache/flink/pull/9081
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jul/19 13:33;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 11 13:35:28 UTC 2019,,,,,,,,,,"0|z04k1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/19 13:35;ykt836;merged in 1.9.0: b0bf8383330d9c5d8f824c438a4a4cf1044f25ba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoints/savepoints injection has loose ordering properties when a stop-with-savepoint is triggered,FLINK-13205,13244245,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,1u0,1u0,1u0,11/Jul/19 07:46,15/Jul/19 12:40,13/Jul/23 08:10,15/Jul/19 12:38,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"When a stop-with-savepoint is triggered at a source task, the task's dispatcher ({{Task.asyncCallDispatcher}})'s thread pool is extended (from single-threaded, it becomes multi-threaded).

This leads to a race of applying consequent checkpoints/savepoints from dispatcher's queue at the same time and checkpoints/savepoints would be not strictly ordered in the event stream.

As the result, checkpoints/savepoints that injected later than they should, may be ""silently subsumed"": potentially, they would be ignored and won't be reported to checkpoint coordinator.

*Proposed solution:*

Revert {{Task.asyncCallDispatcher}} behavior to be single-threaded.
For stop-with-savepoint feature, the dispatcher's thread that performs the synchronous savepoint doesn't need to be blocking and {{StreamTask.finishTask()}} invocation can be delegated to {{StreamTask.notifyCheckpointComplete()}}.

*Note:* imo, the issue described here is not critical, but the proposed change should simplify implementation. This ticket can be considered as enhancement.
",,1u0,kkl0u,,,,,,,,,,,,"kl0u commented on pull request #9021: [FLINK-13205][runtime] Make checkpoints injection ordered with stop-with-savepoint
URL: https://github.com/apache/flink/pull/9021
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 12:40;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 12:38:19 UTC 2019,,,,,,,,,,"0|z04jy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 12:38;kkl0u;Merged on master with 0c8c1d4b827700d788f5b4ddbd8a21192165d039
and on release-1.9 with b5aa6792bb04eaabb86bf9d73f12ec8441153f77;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Hive view row type mismatch when expanding in planner,FLINK-13197,13244184,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,phoenixjiangnan,phoenixjiangnan,10/Jul/19 22:19,18/Dec/19 13:30,13/Jul/23 08:10,18/Dec/19 13:30,,,,,,,,,,1.10.0,,,,Connectors / Hive,Table SQL / Planner,,,,0,,,,,"One goal of HiveCatalog and hive integration is to enable Flink-Hive interoperability, that is Flink should understand existing Hive meta-objects, and Hive meta-objects created thru Flink should be understood by Hive.

Taking an example of a Hive view v1 in HiveCatalog and database hc.db. Unlike an equivalent Flink view whose full path in expanded query should be hc.db.v1, the Hive view's full path in the expanded query should be db.v1 such that Hive can understand it, no matter it's created by Hive or Flink.

[~lirui] can you help to ensure that Flink can also query Hive's view in both Flink planner and Blink planner?

cc [~xuefuz]",,danny0405,dwysakowicz,jark,lirui,lzljs3620320,phoenixjiangnan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15186,,,,,,,,,,FLINK-12905,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 13:30:16 UTC 2019,,,,,,,,,,"0|z04jmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Aug/19 02:20;lirui;[~phoenixjiangnan] I suppose this has to wait for FLINK-12905 to get in first, is that correct?;;;","20/Aug/19 03:54;phoenixjiangnan;[~lirui] I think so.;;;","05/Dec/19 09:04;lirui;I hit some issue when trying to query views created in Hive. The view is created as:
{code}
create table src (key int, val string);
create view v as select key, count(*) from src group by key having count(*) > 1;
{code}
When querying this view from Flink, I hit the following exception:
{noformat}
org.apache.flink.table.api.TableException: Could not expand view. Types mismatch.
 Expected row type: RecordType(INTEGER key, BIGINT _c1)
 Expanded view type: RecordType(INTEGER key, BIGINT EXPR$1)


	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.expandView(FlinkPlannerImpl.scala:191)
	at org.apache.flink.table.planner.catalog.SqlCatalogViewTable.convertToRel(SqlCatalogViewTable.java:56)
	at org.apache.flink.table.planner.plan.schema.ExpandingPreparingTable.expand(ExpandingPreparingTable.java:59)
	at org.apache.flink.table.planner.plan.schema.ExpandingPreparingTable.toRel(ExpandingPreparingTable.java:55)
{noformat}

The expected row type is what we get from {{HiveCatalog}}, and the expanded view type is what we get after compiling the expanded query of the view.
I wonder whether it's necessary to check the equality of these two, given that we can't anticipate what we'll get from the catalog. I think we can at least be more lenient here, e.g. it doesn't make sense to compare the column names, perhaps nullability either.

[~dwysakowicz] could you please share your thoughts? BTW I'm using the blink planner.;;;","05/Dec/19 09:13;dwysakowicz;Hey [~danny0405] I think this is could be an argument for actually adding the cast we discussed here: https://github.com/apache/flink/pull/8859#discussion_r350518897 What do you think?

The problem here is that the automatic names differ between what calcite and what hive produces/
;;;","06/Dec/19 01:07;danny0405;Thanks [~dwysakowicz] and [~lirui] for reporting this ~

I thought a lot, and think we still should not add redundant casts/AS operators above the view relational expression.

- For column alias, i'm thinking about to support view alias from the create view statement, which should be finished in FLIP-71
- For nullability, we should fix it from the type inference

I'm always agains with the redundant nodes because it add a useless projection above the view thus would change the logical plan (make it more complicated and we may lose some plan promotion like project merge or transpose or reduction).;;;","06/Dec/19 02:36;lirui;[~dwysakowicz] [~danny0405] Thanks for your inputs. I'm not very familiar with the Calcite logics here, but just curious why we even care about the row type returned by the catalog? A view is essentially just a query, and the row type from the the catalog is what the external system ""thinks"" this query will produce. I don't think we consult external system for result types of ordinary queries. So why do we do that for views?;;;","06/Dec/19 08:25;dwysakowicz;[~lirui] View is not just a ""query"". It is closer to a Table than to a query. There can be a whole query built on top of a view that assumed correct information retrieved from the catalog. Moreover view can be even writable if certain requirements are met.

[~danny0405] I fully agree with you we should not introduce redundant cast. I am not sure though if the cast we are talking about is redundant. 
* It cannot be solved through DDL. The views are not always created from Flink. We also need to support views created in Hive externally.
* How does type inference relate in this case? If I am not mistaken Hive (at least in earlier versions) will always give you nullable types for literals when calcite will derive not null types which will never match.

BTW If I am not mistaken the utility method I suggest to use will not add additional cast nodes if the types and names match.;;;","06/Dec/19 11:59;lirui;[~dwysakowicz] I see. Thanks for the explanation. I agree we should make sure types match if we would support writing to views.;;;","06/Dec/19 13:03;danny0405;Let me have some study this weekend to see if we can have the solution without introducing the casts(from the Calcite community), i have no accurate answer at this moment.;;;","07/Dec/19 03:10;danny0405;Checking out the code of `RelOptUtil#createCastRel`, i'm wondering if we can rename the LogicalProject directly without add any above new projections with casts. After all, the code already rename the project with the constructor:

{code:java}
    if (rename) {
      // Use names and types from castRowType.
      return projectFactory.createProject(rel, castExps,
          castRowType.getFieldNames());
    }
{code}

So i thought if we can""
# If the view rel root node is a LogicalProject, get its input then construct the new project with the renamed one.
# If the view rel is not a LogicalProject, add a project above it with renamed input refs.
;;;","09/Dec/19 08:05;dwysakowicz;And what about null handling? E.g. in hive 1.x all columns will be nullable, even if they are backed by literal.;;;","11/Dec/19 04:18;lirui;[~dwysakowicz][~danny0405] I have just logged FLINK-15186 to track the issue.;;;","11/Dec/19 07:48;dwysakowicz;Good idea [~lirui] Thank you;;;","12/Dec/19 13:15;jark;{noformat}
Hi Dawid Wysakowicz Danny Chen, About add a cast:

> we may lose some plan promotion like project merge or transpose or reduction

 I don't think so, is our optimizer too weak? I think redundant cast will be removed easily by optimizer and should not affect optimizer.

And I think cast will enhance correctness, image:

- view sql will produce nullable fields
- view schema define fields not null.
- our code generator for cast consider null check, cast null to a not null field will fail.
- So add a cast is good to verify the data.

And maybe not only field name, nullable, but also precision, this kind of scene really needs a cast to transform.

So I am +1 for add a cast, What do you think? 
{noformat}

I repost [~lzljs3620320]'s comment under FLINK-15186 here, to make the discussion in a thread. 
From my point of view, if we only need to rename fields, it would be great to avoid a cast project. 
But if we want to handling nullable and precision, I think a project is required and it's fine to have a project on the view. 

If the cast project can'be avoided, why not simply introduce a cast project.
So I'm also +1 for cast project. 

;;;","16/Dec/19 06:28;danny0405;[~dwysakowicz]

> And what about null handling? E.g. in hive 1.x all columns will be nullable, even if they are backed by
> literal.

For hive (or other external storage) views, we may need to add the projection because it is hard to keep the nullability inference synced with Calcite.

If the view are create from Flink, the data type and nullability should always expect to be correct.
We can add casts to fix only the nullability but not the field name.

BTW, if all the assumption is correct, we should not expect to have any plan change with current code with the fix patch.;;;","16/Dec/19 08:37;lzljs3620320;We don't need distinguish hive from Flink.

I think we can just add a project and fix these field patterns:
 # Type and name totally same, do nothing.
 # Name not same, do rename.
 # Type not same, do cast.

If all fields are #1. Don't need add project. I think calcite can do this optimization to remove this project.;;;","17/Dec/19 06:27;danny0405;PR updated https://github.com/apache/flink/pull/10595/files , can someone helps to review ? Thanks in advance ~;;;","18/Dec/19 13:30;dwysakowicz;Fixed in:
master: 4e4dcf521748538aa164124fd613c9fe096ea604
1.10: 3d70c75477705b71acc83c010e7b89fce6c78c19;;;",,,,,,,,,,,,,,
Starting a TaskExecutor blocks the YarnResourceManager's main thread,FLINK-13184,13244006,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,xtsong,xtsong,10/Jul/19 03:30,28/Feb/20 10:46,13/Jul/23 08:10,20/Nov/19 09:03,1.10.0,1.8.1,1.9.0,,,,,,,1.10.0,1.8.3,1.9.2,,Deployment / YARN,,,,,2,pull-request-available,,,,"Currently, YarnResourceManager starts all task executors in main thread. This could cause RM to become unresponsive when launching a large number of TEs (e.g. > 1000) because it involves blocking I/O operations (writing files to HDFS, communicating with the node manager using a synchronous {{NMClient}}). As a consequence, TE registration/heartbeat timeouts can occur and Flink might allocate too many excessive containers (see FLINK-12342) because it cannot process the {{YarnResourceManager#onContainersAllocated}} calls.

There are different solution approaches but the end goal should be to not execute any blocking calls in the {{ResourceManager's}} main thread:

1. Start the TaskExecutors from a different thread (potentially thread pool) which is responsible for uploading the files and communicating with the NodeManager
2. Don't upload files (avoid blocking file system operations) and use the {{NMClientAsync}} for the communication with Yarn's {{NodeManager}}.
3. Upload files in a separate I/O thread and use the {{NMClientAsync}} for the communication with Yarn's {{NodeManager}}.",,elkhand,hequn8128,kisimple,moxian,Paul Lin,QiLuo,rongr,tison,trohrmann,wangyang0918,xtsong,ZhenqiuHuang,zhuqi,"xintongsong commented on pull request #9106: [FLINK-13184][yarn] Support launching task executors with multi-thread on YARN.
URL: https://github.com/apache/flink/pull/9106
 
 
   ## What is the purpose of the change
   
   This pull request support starting new TaskExecutors with multi-thread in YarnResourceManager, so when starting a large amount of TaskExecutors it won't the block main thread on RM from responding to other RPC messages (TE registration, heartbeats, etc.).
   
   ## Brief change log
   
   - b2ed97d0e6dd506602888aacf6146238fbcdfd48: Introduce a config option for the max number of threads on RM used for starting new TaskExecutors. 
   - 06c7209af04f49ca69a263f57fc713f3b9c55c87: Introduce a thread pool in YarnResourceManager and start new TaskExecutors with multi-thread. 
   - 25fc95f30720209e19bd010cdd517ff5e3c685d8: Update relevant test cases to wait for threads starting new TaskExecutors to finish.
   
   ## Verifying this change
   
     - Updated YarnResourceManagerTest to verify that YarnResourceManager starts new TaskExecutors properly.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jul/19 08:11;githubbot;600","tillrohrmann commented on pull request #9106: [FLINK-13184][yarn] Support launching task executors with multi-thread on YARN.
URL: https://github.com/apache/flink/pull/9106
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 09:25;githubbot;600","wangyang0918 commented on pull request #10143: [FLINK-13184]Starting a TaskExecutor blocks the YarnResourceManager's main thread
URL: https://github.com/apache/flink/pull/10143
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Currently, YarnResourceManager starts all task executors in main thread. This could cause RM to become unresponsive when launching a large number of TEs (e.g. > 1000) because it involves blocking I/O operations (writing files to HDFS, communicating with the node manager using a synchronous NMClient). 
   
   We could do the following two optimizations:
   * Use the NMClientAsync for the communication with Yarn's NodeManager
   * Don't upload files (avoid blocking file system operations), use dynamic properties instead
   
   
   ## Brief change log
   
   * Use NMClientAsync instead of NMClient to avoid starting TaskExecutor blocking call.
   * Use dynamic properties instead of uploading taskmanager-conf.yaml to hdfs.
   
   
   ## Verifying this change
   
   * The changes could be covered by the existing test. All the unit tests, integrated tests and e2e should pass.
   * Manually check with large scale, more than 5000 containers.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Nov/19 07:38;githubbot;600","TisonKun commented on pull request #10143: [FLINK-13184]Starting a TaskExecutor blocks the YarnResourceManager's main thread
URL: https://github.com/apache/flink/pull/10143
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 01:30;githubbot;600","wangyang0918 commented on pull request #10259: [FLINK-13184]Starting a TaskExecutor blocks the YarnResourceManager's main thread(Backport to 1.9)
URL: https://github.com/apache/flink/pull/10259
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Backport #10143 to release-1.9
   
   
   ## Brief change log
   
   none
   
   
   ## Verifying this change
   
   none
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 06:19;githubbot;600","wangyang0918 commented on pull request #10260: [FLINK-13184]Starting a TaskExecutor blocks the YarnResourceManager's main thread(Backport to 1.8)
URL: https://github.com/apache/flink/pull/10260
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Backport #10143 to release-1.8
   
   
   ## Brief change log
   
   none
   
   
   ## Verifying this change
   
   none
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 06:47;githubbot;600","TisonKun commented on pull request #10259: [FLINK-13184]Starting a TaskExecutor blocks the YarnResourceManager's main thread(Backport to 1.9)
URL: https://github.com/apache/flink/pull/10259
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 09:00;githubbot;600","TisonKun commented on pull request #10260: [FLINK-13184]Starting a TaskExecutor blocks the YarnResourceManager's main thread(Backport to 1.8)
URL: https://github.com/apache/flink/pull/10260
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 09:01;githubbot;600",,,,,,,,0,4800,,,0,4800,,,,,,,,,,,,,FLINK-14582,FLINK-13590,FLINK-9410,,,,,,FLINK-12342,,FLINK-15053,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 20 09:03:51 UTC 2019,,,,,,,,,,"0|z04ijk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/19 14:48;QiLuo;Below are the TM error log in this case:
———————————————————————— 
 
2019-07-09 13:56:59,110 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Connecting to ResourceManager akka.tcp://flink@xxx/user/resourcemanager(00000000000000000000000000000000). 2019-07-09 14:00:01,138 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor - Could not resolve ResourceManager address akka.tcp://flink@xxx/user/resourcemanager, retrying in 10000 ms: Ask timed out on [ActorSelection[Anchor(akka.tcp://flink@xxx/), Path(/user/resourcemanager)]] after [182000 ms]. Sender[null] sent message of type ""akka.actor.Identify"".. 2019-07-09 14:01:59,137 ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor - Fatal error occurred in TaskExecutor akka.tcp://flink@xxx/user/taskmanager_0. org.apache.flink.runtime.taskexecutor.exceptions.RegistrationTimeoutException: Could not register at the ResourceManager within the specified maximum registration duration 300000 ms. This indicates a problem with this instance. Terminating now. at org.apache.flink.runtime.taskexecutor.TaskExecutor.registrationTimeout(TaskExecutor.java:1023) at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$startRegistrationTimeout$3(TaskExecutor.java:1009) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:332) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:158) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142) at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165) at akka.actor.Actor$class.aroundReceive(Actor.scala:502) at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526) at akka.actor.ActorCell.invoke(ActorCell.scala:495) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) at akka.dispatch.Mailbox.run(Mailbox.scala:224) at akka.dispatch.Mailbox.exec(Mailbox.scala:234) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107);;;","07/Nov/19 09:35;trohrmann;cc [~fly_in_gis] let's continue our discussion of FLINK-14582 here.;;;","10/Nov/19 09:29;wangyang0918;[~trohrmann]

`NMClientAsync` use a internal thread pool to execute all the container launch event. When we call `startContainerAsync`, it just put a event to the blocking queue. So replacing the `NMClient` to `NMClientAsync` will not take many unknown implications. Also I will run more tests in our yarn cluster.

I think using dynamic properties instead of uploading a hdfs file is better. We have some big flink applications with more than 5000 containers. Even we use a thread pool to upload the config file to hdfs, it will still be very slow. Since the config for task manager and config uploaded by flink client have a small difference, so using dynamic properties is reasonable.;;;","12/Nov/19 04:06;hequn8128;Hi [~trohrmann] I'm preparing for the release of 1.8.3. Do you think it is a blocker for 1.8.3? Would be great to have your opinions here.;;;","20/Nov/19 01:31;tison;master via a90520cbe0958545d2d0e4cb552d3c763b4031f0
master via 404dede5e80a284112187cfe49bb6fc5f4eb2964

1.9 via 3c0849998d9f56a6321ba3e79c247336685196c0
1.9 via 1f1f9a3b5c09b9bf275bf26976569db84dff5e2a

1.8 via 15ca8f06bf083bb4869dcd121ca07d747c8fa4af
1.8 via 46e22ceaedb8e4d5750a7aca9e7d3b35b08fd19c;;;","20/Nov/19 09:03;tison;Thanks for your working [~fly_in_gis]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
fix spelling mistake in flink-table/flink-sql-client,FLINK-13182,13243999,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,lunge,lunge,10/Jul/19 02:37,16/Oct/19 13:43,13/Jul/23 08:10,16/Oct/19 13:43,1.9.0,,,,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"fix:

EXECUTION_CURRNET_DATABASE ->  EXECUTION_CURRENT_DATABASE

 

EXECUTION_CURRNET_CATALOG ->  EXECUTION_CURRENT_CATALOG

 

and rename the usage of code reference.

 

 ",,aljoscha,lunge,,,,,,,,,,,,"mrzhangboss commented on pull request #9053: [FLINK-13182][Table SQL / Client] Fix Spelling mistake in flink-table
URL: https://github.com/apache/flink/pull/9053
 
 
   
   
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   fix spelling and code reference in flink-table/flink-sql-client
   
   ## Brief change log
   
   
     - rename `EXECUTION_CURRNET_DATABASE` ->  `EXECUTION_CURRENT_DATABASE`
     - rename `EXECUTION_CURRNET_CATALOG` ->  `EXECUTION_CURRENT_CATALOG`
     - rename the code refernce in `flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/config/entries/ExecutionEntry.java`
   
   ## Verifying this change
   
   
   
   
     - *Already passed all the tests in `flink-sql-client`*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (JavaDocs)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 02:51;githubbot;600","aljoscha commented on pull request #9053: [FLINK-13182][Table SQL / Client] Fix Spelling mistake in flink-table
URL: https://github.com/apache/flink/pull/9053
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/19 13:43;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Wed Oct 16 13:43:05 UTC 2019,,,,,,,,,,"0|z04ii0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/19 13:43;aljoscha;These have been resolved as the side effect of another commit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLINK-12951 breaks SQL CLI's ExecutionContextTest,FLINK-13175,13243951,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,danny0405,phoenixjiangnan,phoenixjiangnan,09/Jul/19 19:22,10/Jul/19 14:18,13/Jul/23 08:10,10/Jul/19 13:56,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"https://github.com/apache/flink/pull/8844 breaks SQL CLI's ExecutionContextTest

Errors from it's CI in https://travis-ci.com/flink-ci/flink/jobs/214370966

{code:java}
14:23:25.985 [ERROR] Tests run: 6, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 19.518 s <<< FAILURE! - in org.apache.flink.table.client.gateway.local.ExecutionContextTest
14:23:25.985 [ERROR] testDatabases(org.apache.flink.table.client.gateway.local.ExecutionContextTest)  Time elapsed: 10.807 s  <<< ERROR!
org.apache.flink.table.client.gateway.SqlExecutionException: Could not create environment instance.
	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testDatabases(ExecutionContextTest.java:128)
Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: 
Invalid view 'TestView1' with query:
SELECT scalarUDF(IntegerField1) FROM default_catalog.default_database.TableNumber1
Cause: SQL validation failed. From line 1, column 38 to line 1, column 82: Object 'TableNumber1' not found within 'default_catalog.default_database'
	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testDatabases(ExecutionContextTest.java:128)
14:23:25.985 [ERROR] testCatalogs(org.apache.flink.table.client.gateway.local.ExecutionContextTest)  Time elapsed: 5.295 s  <<< ERROR!
org.apache.flink.table.client.gateway.SqlExecutionException: Could not create environment instance.
	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testCatalogs(ExecutionContextTest.java:87)
Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: 
Invalid view 'TestView1' with query:
SELECT scalarUDF(IntegerField1) FROM default_catalog.default_database.TableNumber1
Cause: SQL validation failed. From line 1, column 38 to line 1, column 82: Object 'TableNumber1' not found within 'default_catalog.default_database'
	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testCatalogs(ExecutionContextTest.java:87)
{code}
",,jark,phoenixjiangnan,trohrmann,,,,,,,,,,,"danny0405 commented on pull request #9052: [FLINK-13175] Fix ExecutionContextTest after FLINK-12951
URL: https://github.com/apache/flink/pull/9052
 
 
   ## What is the purpose of the change
   
   After FLINK-12951, table will always be registered into current catalog, this fix the behavior for ExecutionContextTest.
   
   
   ## Brief change log
   
     - Remove defaultCatalog and defaultDataBase totally from TableEnvironmentImpl
   
   
   ## Verifying this change
   
   This change is already covered by existing tests.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 02:43;githubbot;600","KurtYoung commented on pull request #9052: [FLINK-13175] Fix ExecutionContextTest after FLINK-12951
URL: https://github.com/apache/flink/pull/9052
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 07:55;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 10 14:18:40 UTC 2019,,,,,,,,,,"0|z04i7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/19 13:55;trohrmann;Another instance: https://api.travis-ci.org/v3/job/556668199/log.txt;;;","10/Jul/19 13:56;trohrmann;Fixed via https://github.com/apache/flink/commit/7dd5e29e006a35677e2314d1cc25135591d4de4e;;;","10/Jul/19 14:18;ykt836;Sorry for the inconvenience. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changes of HiveTableSinkTest in FLINK-13068 introduces compilation error,FLINK-13174,13243943,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,phoenixjiangnan,liyu,liyu,09/Jul/19 18:38,09/Jul/19 19:23,13/Jul/23 08:10,09/Jul/19 19:19,,,,,,,,,,,,,,Connectors / Hive,Tests,,,,0,,,,,"From the latest run of our [flink-benchmark|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks/4075/console], we could see error like below:
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-hive_2.11: Compilation failure: Compilation failure: 
[ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/batch/connectors/hive/HiveTableSinkTest.java:[230,17] cannot find symbol
[ERROR]   symbol:   class CatalogPartition
[ERROR]   location: class org.apache.flink.batch.connectors.hive.HiveTableSinkTest
[ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/batch/connectors/hive/HiveTableSinkTest.java:[232,81] cannot find symbol
[ERROR]   symbol:   variable HiveCatalogConfig
[ERROR]   location: class org.apache.flink.batch.connectors.hive.HiveTableSinkTest
[ERROR] /home/jenkins/workspace/flink-master-benchmarks/flink/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/batch/connectors/hive/HiveTableSinkTest.java:[233,39] cannot find symbol
[ERROR]   symbol:   class Path
[ERROR]   location: class org.apache.flink.batch.connectors.hive.HiveTableSinkTest
[ERROR] -> [Help 1]
{noformat}
And checking the commit history this issue is introduced by FLINK-13068",,liyu,phoenixjiangnan,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13068,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 09 19:23:01 UTC 2019,,,,,,,,,,"0|z04i5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/19 18:39;liyu;[~phoenixjiangnan] FYI.;;;","09/Jul/19 19:19;phoenixjiangnan;fixed in https://github.com/apache/flink/commit/fac54e945897c24c32bbfbd3ec0b7b6832309131;;;","09/Jul/19 19:23;liyu;Thanks for the quick fix [~phoenixjiangnan]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Planner should get table factory from catalog when creating sink for CatalogTable,FLINK-13170,13243879,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lirui,lirui,lirui,09/Jul/19 12:06,01/Aug/19 10:57,13/Jul/23 08:10,15/Jul/19 17:14,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,lirui,phoenixjiangnan,,,,,,,,,,,,"lirui-apache commented on pull request #9039: [FLINK-13170][table-planner] Planner should get table factory from ca…
URL: https://github.com/apache/flink/pull/9039
 
 
   …talog when creating sink for CatalogTable
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Planner should first try getting table factory from catalog when creating table sinks for `CatalogTable`.
   
   
   ## Brief change log
   
     - Make planner use `TableSinkFactory` returned by `Catalog::getTableFactory`.
   
   ## Verifying this change
   
   Existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? NA
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 12:13;githubbot;600","asfgit commented on pull request #9039: [FLINK-13170][table-planner] Planner should get table factory from ca…
URL: https://github.com/apache/flink/pull/9039
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 17:13;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13012,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 17:14:20 UTC 2019,,,,,,,,,,"0|z04hrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 17:14;phoenixjiangnan;merged in 1.10.0: 70239625748f078cf38784953984085583b6d9b8   1.9.0: 7b4f39d9482fcba2526d5fe1adef9850de3473fc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
numBuckets calculate wrong in BinaryHashBucketArea,FLINK-13161,13243812,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,LouisXu777,LouisXu777,LouisXu777,09/Jul/19 04:42,01/Aug/19 10:57,13/Jul/23 08:10,18/Jul/19 04:24,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"The original code is:

 
{code:java}
int minNumBuckets = (int) Math.ceil((estimatedRowCount / loadFactor / NUM_ENTRIES_PER_BUCKET));
int bucketNumSegs = Math.max(1, Math.min(maxSegs, (minNumBuckets >>> table.bucketsPerSegmentBits) +
      ((minNumBuckets & table.bucketsPerSegmentMask) == 0 ? 0 : 1)));
int numBuckets = MathUtils.roundDownToPowerOf2(bucketNumSegs << table.bucketsPerSegmentBits);
{code}
default value: loadFactor=0.75, NUM_ENTRIES_PER_BUCKET=15，maxSegs = 33(suppose, only need big than the number which calculated by minBunBuckets)

We suppose table.bucketsPerSegmentBits = 3, table.bucketsPerSegmentMask = 0b111. It means buckets in a segment is 8.

When set estimatedRowCount loop from 1 to 1000, we will see the result in attach file.

I will take an example:
{code:java}
estimatedRowCount: 200, minNumBuckets: 18, bucketNumSegs: 3, numBuckets: 16
{code}
We can see it request 3 segment, but only 2 segment needed(16 / 8), left one segment wasted. 

And consider the segment is preallocated, it means some segments will never used.

 

 ",,jark,LouisXu777,,,,,,,,,,,,"AjaxXu commented on pull request #9026: [FLINK-13161][table-blink-runtime]numBuckets calculate wrong in Binar…
URL: https://github.com/apache/flink/pull/9026
 
 
   …yHashBucketArea
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *fix numBuckets calculate in BinaryHashBucketArea*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 05:09;githubbot;600","AjaxXu commented on pull request #9026: [FLINK-13161][table-blink-runtime]numBuckets calculate wrong in Binar…
URL: https://github.com/apache/flink/pull/9026
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 05:14;githubbot;600","AjaxXu commented on pull request #9027: [FLINK-13161][table-blink-runtime]numBuckets calculate wrong in BinaryHashBucketArea
URL: https://github.com/apache/flink/pull/9027
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *fix numBuckets calculate in BinaryHashBucketArea*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 05:29;githubbot;600","asfgit commented on pull request #9027: [FLINK-13161][table-blink-runtime]numBuckets calculate wrong in BinaryHashBucketArea
URL: https://github.com/apache/flink/pull/9027
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 04:22;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/19 01:57;LouisXu777;result.log;https://issues.apache.org/jira/secure/attachment/12974118/result.log",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 18 04:24:51 UTC 2019,,,,,,,,,,"0|z04hcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/19 12:32;ykt836;cc [~lzljs3620320] [~TsReaper];;;","18/Jul/19 04:24;jark;Fixed in 1.10.0: 18ace00455aeba00ac459623c46f762900981aec
Fixed in 1.9.0: ca6016c7878245715301e5dfd1743b18106fd2bb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ClassNotFoundException when restore job,FLINK-13159,13243802,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,kring,kring,09/Jul/19 03:10,08/Aug/19 12:30,13/Jul/23 08:10,08/Aug/19 12:30,1.8.0,1.8.1,,,,,,,,1.8.2,1.9.0,,,API / Type Serialization System,,,,,0,pull-request-available,,,,"{code:java}
java.lang.Exception: Exception while creating StreamOperatorStateContext.
at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:195)
at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:250)
at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:738)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:289)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_b398b3dd4c544ddf2d47a0cc47d332f4_(1/6) from any of the 1 prov
ided restore options.
at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:307)
at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:135)
... 5 common frames omitted
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:130)
at org.apache.flink.runtime.state.filesystem.FsStateBackend.createKeyedStateBackend(FsStateBackend.java:489)
at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:291)
at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
... 7 common frames omitted
Caused by: java.lang.RuntimeException: Cannot instantiate class.
at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:384)
at org.apache.flink.runtime.state.heap.StateTableByKeyGroupReaders.lambda$createV2PlusReader$0(StateTableByKeyGroupReaders.java:74)
at org.apache.flink.runtime.state.KeyGroupPartitioner$PartitioningResultKeyGroupReader.readMappingsInKeyGroup(KeyGroupPartitioner.java:297)
at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readKeyGroupStateData(HeapRestoreOperation.java:290)
at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readStateHandleStateData(HeapRestoreOperation.java:251)
at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:153)
at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:127)
... 11 common frames omitted
Caused by: java.lang.ClassNotFoundException: xxx
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:382)
... 17 common frames omitted
{code}
A strange problem with Flink is that after a task has been running properly for a period of time, if any exception (such as ask timeout or ES request timeout) is thrown, the task restart will report the above error (xxx is a business model), and ten subsequent retries will not succeed, but the task will be resubmitted. Then it can run normally. In addition, there are three other tasks running at the same time, none of which has the problem.

My flink version is 1.8.0.",,aljoscha,diablox,kring,ssailappan,tzulitai,yunta,,,,,,,,"Myasuka commented on pull request #9375: [FLINK-13159] Prevent potential NPE of restored PojoSerializer when deserializing
URL: https://github.com/apache/flink/pull/9375
 
 
   ## What is the purpose of the change
   
   Prevent potential NPE when PojoSerializer restored.
   
   ## Brief change log
   
     - Ensure the `classloader` could be assigned when `PojoSerializer` restored.
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
     - Added test `testRestoreSerializerDeserialize` into `SerializerTestBase` to ensure all restored serializes could deserialize values as expected.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **yes**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/19 18:42;githubbot;600","asfgit commented on pull request #9375: [FLINK-13159] Prevent potential NPE of restored PojoSerializer when deserializing
URL: https://github.com/apache/flink/pull/9375
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Aug/19 07:58;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/19 09:34;kring;image-2019-08-05-17-29-40-351.png;https://issues.apache.org/jira/secure/attachment/12976679/image-2019-08-05-17-29-40-351.png","05/Aug/19 09:32;kring;image-2019-08-05-17-32-44-988.png;https://issues.apache.org/jira/secure/attachment/12976678/image-2019-08-05-17-32-44-988.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 08 12:30:25 UTC 2019,,,,,,,,,,"0|z04ha8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/19 06:31;yunta;[~kring] would you please give more details. From your description, you would meet the exception if your job failover. However, this could come to normal just after several times retries. Have you ever turned on [local recovery|https://ci.apache.org/projects/flink/flink-docs-stable/ops/state/large_state_tuning.html#task-local-recovery], and if the failling task would come to normal once it run on another worker node?;;;","09/Jul/19 06:53;kring;[~yunta]，thank you.My taskManager and JobManager are on the same machine, using local files as backends. Therefore, I did not configure local recovery. Just now, I created a savepoint for this task and tried to recover from the savepoint. Then I reproduced the exception 100 percent, so I'm going to debug flink.;;;","27/Jul/19 04:03;diablox;I'm using flink 1.8.1 on yarn session with rocksdb as state backend.

got the same problem when starting flink jobs with checkpoints: bin/flink run -s /flink/checkpoints/c2b8fa3c51393a2c6865ca13045eccad/chk-84 deploy/xxx.jar

job keep retrying and fail after about 8 seconds. 

Our job runs on flink 1.7.2 with no problem for about 4 months and could recover successfully. 

 

Full stack trace: 

java.lang.RuntimeException: Exception occurred while processing valve output watermark:
 at org.apache.flink.streaming.runtime.io.StreamInputProcessor$ForwardingValveOutputHandler.handleWatermark(StreamInputProcessor.java:265)
 at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:189)
 at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:111)
 at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:184)
 at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.run(OneInputStreamTask.java:105)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:300)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
 at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Cannot instantiate class.
 at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:384)
 at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:143)
 at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:37)
 at org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionSerializer.deserialize(CoGroupedStreams.java:581)
 at org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionSerializer.deserialize(CoGroupedStreams.java:506)
 at org.apache.flink.contrib.streaming.state.RocksDBListState.deserializeNextElement(RocksDBListState.java:144)
 at org.apache.flink.contrib.streaming.state.RocksDBListState.deserializeList(RocksDBListState.java:135)
 at org.apache.flink.contrib.streaming.state.RocksDBListState.getInternal(RocksDBListState.java:119)
 at org.apache.flink.contrib.streaming.state.RocksDBListState.get(RocksDBListState.java:111)
 at org.apache.flink.contrib.streaming.state.RocksDBListState.get(RocksDBListState.java:60)
 at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.onEventTime(WindowOperator.java:452)
 at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.advanceWatermark(InternalTimerServiceImpl.java:255)
 at org.apache.flink.streaming.api.operators.InternalTimeServiceManager.advanceWatermark(InternalTimeServiceManager.java:128)
 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:775)
 at org.apache.flink.streaming.runtime.io.StreamInputProcessor$ForwardingValveOutputHandler.handleWatermark(StreamInputProcessor.java:262)
 ... 7 more
Caused by: java.lang.ClassNotFoundException: com/xx/xx/xx
 at java.lang.Class.forName0(Native Method)
 at java.lang.Class.forName(Class.java:348)
 at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:382);;;","30/Jul/19 05:18;ssailappan;I am running into the same issue. I am running Flink 1.8.0 on EMR on a yarn cluster. The checkpoint is externalized and uses S3. The job runs fine for long times before it fails with the below exception and never recovers after that. It keeps trying to restore from the checkpoint and runs into the same error repeatedly.

 
java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:195)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:250)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:738)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:289)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_d28463815a9d818025b1ff96211a6dc9_(2/2) from any of the 1 provided restore options.
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:307)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:135)
	... 5 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:130)
	at org.apache.flink.runtime.state.filesystem.FsStateBackend.createKeyedStateBackend(FsStateBackend.java:489)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:291)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
	... 7 more
Caused by: java.lang.RuntimeException: Cannot instantiate class.
	at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:384)
	at org.apache.flink.api.common.typeutils.base.ListSerializer.deserialize(ListSerializer.java:133)
	at org.apache.flink.api.common.typeutils.base.ListSerializer.deserialize(ListSerializer.java:42)
	at org.apache.flink.runtime.state.heap.StateTableByKeyGroupReaders.lambda$createV2PlusReader$0(StateTableByKeyGroupReaders.java:74)
	at org.apache.flink.runtime.state.KeyGroupPartitioner$PartitioningResultKeyGroupReader.readMappingsInKeyGroup(KeyGroupPartitioner.java:297)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readKeyGroupStateData(HeapRestoreOperation.java:290)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.readStateHandleStateData(HeapRestoreOperation.java:251)
	at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:153)
	at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:127)
	... 11 more
Caused by: java.lang.ClassNotFoundException: xxx
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:382)
	... 19 more;;;","05/Aug/19 09:34;kring;My code uses a parent reference, which actually points to a subclass object. Then I tried debug Flink and found that CL was not passed in when this constructor !image-2019-08-05-17-29-40-351.png! was used to create PojoSerializer, resulting in no CL leading to Cannot instantiate class error when executing the org.apache.flink.api.java.typeutils.runtime.PojoSerializer#deserialize(org.apache.flink.core.memory.DataInputView) method.So I added  !image-2019-08-05-17-32-44-988.png! to the last line of the constructor, and then recompiled and packaged, which successfully solved the problem.[~ssailappan] I guess you have the same problem as me.;;;","06/Aug/19 07:57;yunta;Before Flink-1.8, {{PojoSerializer}} would always use the [1st constructor|https://github.com/apache/flink/blob/56c3e7cd653e4cb2ad0a76ca317aa9fa1d564dc2/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java#L119] which assign {{Thread.currentThread().getContextClassLoader()}} to private field {{ClassLoader cl}}. The [2nd constructor|https://github.com/apache/flink/blob/56c3e7cd653e4cb2ad0a76ca317aa9fa1d564dc2/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java#L147] without {{cl}} assigned would only be used to ensure the compatibility.

However, after Flink-1.8, we would use the [2nd constructor|https://github.com/apache/flink/blob/a0d236fba7c6abdabb461aa504b1e088a3982c31/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java#L142] to create a restore serializer or a reconfigured serializer from a {{PojoSerializerSnapshot}}. Unfortunately, the restored serializer does not contain the class loader.

I could use below code within {{PojoSubclassSerializerTest}} to reproduce this.
{code:java}
@Test
public void testRestorePojoSerializer() throws IOException {
   TypeSerializer<TestUserClassBase> serializer = createSerializer();
   TestUserClass2 bar = new TestUserClass2(
      ThreadLocalRandom.current().nextInt(), ""bar"", ThreadLocalRandom.current().nextFloat());
   ByteArrayOutputStream out = new ByteArrayOutputStream();
   try (DataOutputViewStreamWrapper outView = new DataOutputViewStreamWrapper(out)) {
      serializer.serialize(bar, outView);
      DataInputViewStreamWrapper inputViewStreamWrapper =
         new DataInputViewStreamWrapper(new ByteArrayInputStream(out.toByteArray()));
      serializer.deserialize(inputViewStreamWrapper);
   }
   TypeSerializerSnapshot<TestUserClassBase> snapshot = serializer.snapshotConfiguration();
   TypeSerializer<TestUserClassBase> restoreSerializer = snapshot.restoreSerializer();
   try (DataInputViewStreamWrapper inputView =
          new DataInputViewStreamWrapper(new ByteArrayInputStream(out.toByteArray()))) {
      restoreSerializer.deserialize(inputView);
   }
}
 {code}

[~tzulitai], what do you think of this? If this is really a bug, I am glad to help to resolve this issue.;;;","06/Aug/19 12:12;tzulitai;Thanks for the diagnosis [~yunta] [~kring].
The observations makes sense to me, and this indeed is a bug since the user classloader is not being used when deserializing.

Please ping me for a review [~yunta], I've assigned you to the JIRA ticket.;;;","06/Aug/19 12:14;tzulitai;I've made this a blocker for 1.8.2.
Ideally, it would be best if we can fix this for the upcoming 1.9.0 as well.;;;","08/Aug/19 12:30;tzulitai;Merged.

master: 268da6a03323b65d6297e4d2288ead39aba7d388
1.9.0: 5c09e1c29897e7c098ee55c0d7881c5d2016d94c
1.8.2: 0130b863d959de2bec5f0e9c6176a26ef85cb96d;;;",,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch (v1.7.1) sink end-to-end test failed on Travis,FLINK-13156,13243707,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,trohrmann,trohrmann,08/Jul/19 14:44,15/Jul/19 08:38,13/Jul/23 08:10,15/Jul/19 08:38,1.9.0,,,,,,,,,1.9.0,,,,Connectors / ElasticSearch,,,,,0,test-stability,,,,"The {{Elasticsearch (v1.7.1) sink end-to-end test}} failed on Travis because the test waited for Elasticsearch records indefinitely.

https://api.travis-ci.org/v3/job/554991871/log.txt",,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 08:38:23 UTC 2019,,,,,,,,,,"0|z04gp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/19 08:38;chesnay;master: 8375346d8de0c8b46e37983257f61443d4e80c05;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client end-to-end test fails on Travis,FLINK-13155,13243706,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,docete,trohrmann,trohrmann,08/Jul/19 14:41,25/Jul/19 13:50,13/Jul/23 08:10,25/Jul/19 13:50,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Client,,,,,0,pull-request-available,test-stability,,,"The {{SQL Client end-to-end test}} which executes {{test-scripts/test_sql_client.sh}} fails on Travis with non-empty out files.

https://api.travis-ci.org/v3/job/554991859/log.txt",,trohrmann,twalthr,,,,,,,,,,,,"docete commented on pull request #9031: [FLINK-13155][hotfix][e2e] fix SQL Client end-to-end test
URL: https://github.com/apache/flink/pull/9031
 
 
   ## What is the purpose of the change
   
   Fix SQL Client e2e test
   
   ## Brief change log
   
   Fix SQL Client e2e test
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 08:38;githubbot;600","docete commented on pull request #9031: [FLINK-13155][hotfix][e2e] fix SQL Client end-to-end test
URL: https://github.com/apache/flink/pull/9031
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 02:37;githubbot;600","docete commented on pull request #9051: [FLINK-13155][hotfix][e2e] fix SQL Client end-to-end test
URL: https://github.com/apache/flink/pull/9051
 
 
   ## What is the purpose of the change
   
   This PR fix SQL Client end-to-end test
   
   ## Brief change log
   
   * Exclude kafka json files in flink-sql-connector-kafka
   * packaging flink-sql-parser into flink sql client jar
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 02:40;githubbot;600","KurtYoung commented on pull request #9051: [FLINK-13155][hotfix][e2e] fix SQL Client end-to-end test
URL: https://github.com/apache/flink/pull/9051
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 23:43;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 25 13:50:06 UTC 2019,,,,,,,,,,"0|z04gow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/19 14:41;trohrmann;Another instance: https://api.travis-ci.org/v3/job/554991865/log.txt;;;","08/Jul/19 14:41;trohrmann;Might be related to FLINK-10107 which is still open.;;;","09/Jul/19 03:58;docete;FLINK-12024 intros kafka-clients 2.2.0 version which packaged some json files in JAR. This makes the  jar validation fails.

docete@Zhenghua-MacBook:~/Downloads/tmp$ jar -tf kafka-clients-2.2.0.jar | grep json | head
 common/message/DescribeDelegationTokenRequest.json
 common/message/DeleteRecordsRequest.json
 common/message/CreateDelegationTokenResponse.json
 common/message/MetadataRequest.json
 common/message/HeartbeatResponse.json
 common/message/StopReplicaRequest.json
 common/message/ProduceRequest.json
 common/message/DeleteGroupsRequest.json
 common/message/EndTxnRequest.json
 common/message/UpdateMetadataRequest.json

 

All these JSON files are used to generate KAFKA request classes on compile time(to build kafka jar) and useless at runtime. So we can exclude them safely in flink-sql-connector-kafka module.;;;","09/Jul/19 08:48;docete;Find another issue: flink-sql-client jar missed flink-sql-parser classes and throws ClassNotFoundException;;;","10/Jul/19 08:03;ykt836;Hi [~till.rohrmann], [~docete] found some issues and opened a PR to resolve it. I will merge it and see if this problem happens again in daily end to end tests. Does this issue happen occasionally or constantly? Where can we see the end to end test results so we can monitor it to see if the root cause get fixed? ;;;","11/Jul/19 00:34;ykt836;I pushed a fix: https://github.com/apache/flink/commit/475c30cd4064a7bc2e32c963b6ca58e7623251c6
;;;","25/Jul/19 13:50;twalthr;I will close this issue for now. Because it seemed that Kurt's commit fixed the issue. However, currently the test is broken again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Broken links in documentation,FLINK-13154,13243704,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,trohrmann,trohrmann,08/Jul/19 14:38,01/Aug/19 10:57,13/Jul/23 08:10,15/Jul/19 09:28,1.9.0,,,,,,,,,1.9.0,,,,Documentation,,,,,0,pull-request-available,,,,"Flink's broken link verification failed on Travis as it has discovered some broken links. We need to fix this for the upcoming release.

{code}
[2019-07-07 10:54:21] ERROR `/tutorials/datastream_api.html' not found.
[2019-07-07 10:54:21] ERROR `/tutorials/local_setup.html' not found.
[2019-07-07 10:54:21] ERROR `/tutorials/flink_on_windows.html' not found.
[2019-07-07 10:54:21] ERROR `/examples' not found.
[2019-07-07 10:54:23] ERROR `/zh/dev/connectors/pubsub.html' not found.
[2019-07-07 10:54:24] ERROR `/ops/filesystems.html' not found.
[2019-07-07 10:54:24] ERROR `/dev/table/catalog.html' not found.
[2019-07-07 10:54:28] ERROR `/zh/tutorials/datastream_api.html' not found.
[2019-07-07 10:54:28] ERROR `/zh/tutorials/local_setup.html' not found.
http://localhost:4000/tutorials/datastream_api.html:
Remote file does not exist -- broken link!!!
http://localhost:4000/tutorials/local_setup.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/tutorials/flink_on_windows.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/examples:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/dev/connectors/pubsub.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/ops/filesystems.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/dev/table/catalog.html:
Remote file does not exist -- broken link!!!
--
http://localhost:4000/zh/tutorials/datastream_api.html:
Remote file does not exist -- broken link!!!
http://localhost:4000/zh/tutorials/local_setup.html:
Remote file does not exist -- broken link!!!
{code}

https://api.travis-ci.org/v3/job/554991858/log.txt",,trohrmann,yunta,,,,,,,,,,,,"Myasuka commented on pull request #9023: [FLINK-13154][docs] Fix broken links of web docs
URL: https://github.com/apache/flink/pull/9023
 
 
   ## What is the purpose of the change
   
   Fix Flink's broken link before release-1.9
   
   ## Brief change log
   
   Fix Flink's broken link before release-1.9
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jul/19 18:21;githubbot;600","zentol commented on pull request #9023: [FLINK-13154][docs] Fix broken links of web docs
URL: https://github.com/apache/flink/pull/9023
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 09:27;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 09:28:02 UTC 2019,,,,,,,,,,"0|z04gog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/19 18:27;yunta;[~phoenixjiangnan] , please take a look at the PR [https://github.com/apache/flink/pull/9023]. I found you left a broken link of {{/dev/table/catalog.html}} within {{docs/dev/table/sqlClient.md}}. From what I could see, this might be related with [catalog.md in Blink|https://github.com/apache/flink/blob/blink/docs/dev/table/catalog.md]. To fix broken link problem, I just remove that line.;;;","15/Jul/19 09:28;chesnay;master: 4979f33a73a77ef0ca570d414a39d92320059918
1.9: 8310ad96cc5285ebacf4ba2fd55fa317ded3d6f5 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitAggregateITCase.testMinMaxWithRetraction failed on Travis,FLINK-13153,13243703,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,trohrmann,trohrmann,08/Jul/19 14:35,01/Aug/19 10:57,13/Jul/23 08:10,16/Jul/19 07:30,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,test-stability,,,,"{{SplitAggregateITCase.testMinMaxWithRetraction}} failed on Travis with

{code}
Failures: 
10:50:43.355 [ERROR]   SplitAggregateITCase.testMinMaxWithRetraction:195 expected:<List(2,2,2,1, 5,1,4,2, 6,2,2,1)> but was:<List(2,1,2,1, 5,1,4,2, 6,2,2,1)>
{code}

https://api.travis-ci.org/v3/job/554991853/log.txt",,banmoy,jark,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11493,,,,,,,,,FLINK-13216,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 16 07:30:59 UTC 2019,,,,,,,,,,"0|z04go8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/19 14:52;banmoy;Another instance: [https://api.travis-ci.org/v3/job/554872878/log.txt] .;;;","11/Jul/19 11:25;trohrmann;Another instance: https://api.travis-ci.org/v3/job/557214216/log.txt;;;","12/Jul/19 03:32;jark;I will look into this soon.;;;","16/Jul/19 07:30;jark;Fixed in 1.10.0: 1bede7ef01d4694e890f8602d19ce8198120e8d7
Fixed in 1.9.0: db38ed0e503ebd244a52144e3c320df571943dd9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""--yarnship"" doesn't support resource classloading",FLINK-13127,13243427,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,05/Jul/19 18:12,03/Dec/19 00:53,13/Jul/23 08:10,15/Aug/19 17:59,1.8.1,,,,,,,,,1.10.0,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"Currently yarnship works as follows:
 * user specifies directory to ship with the job
 * yarn ships it with the container
 * org.apache.flink.yarn.AbstractYarnClusterDescriptor#uploadAndRegisterFiles traverses directory recursively and adds each file to the classpath

This works well for shipping jars, but doesn't work correctly with shipping resources that we want to load using java.lang.ClassLoader#getResource method.

In order to make resource classloading work, we need to register it's directory instead of the file itself (java classpath expects directories or archives).

CLASSPATH=""shipped/custom.conf:${CLASSPATH}"" needs to become CLASSPATH=""shipped:${CLASSPATH}""",,dmvk,felixzheng,kisimple,trohrmann,xtsong,,,,,,,,,"dmvk commented on pull request #9022: [FLINK-13127] Fix --yarnship classpath
URL: https://github.com/apache/flink/pull/9022
 
 
   https://issues.apache.org/jira/browse/FLINK-13127
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jul/19 16:02;githubbot;600","tillrohrmann commented on pull request #9022: [FLINK-13127] Fix --yarnship classpath
URL: https://github.com/apache/flink/pull/9022
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 17:58;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-6949,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 15 17:59:05 UTC 2019,,,,,,,,,,"0|z04eyw:",9223372036854775807,This change orders the shipped files wrt to resources and jar files as well as alphabetically in these groups.,,,,,,,,,,,,,,,,,,,"08/Jul/19 09:48;wangyang0918;Hi [~dmvk],

You are correct. The shipped files and jars are included in CLASSPATH of job manager and task manager. However, the container workDir is not in CLASSPATH. So you could not use java.lang.ClassLoader#getResource to read/write your file. This feature should be supported in the future and maybe you could open your file by the following codes now.
{code:java}
Map<String, String> env = System.getenv(); 
final String workingDirectory = env.get(ApplicationConstants.Environment.PWD.key());
File myFile = new File(workingDirectory, ""myFileName"");
{code};;;","08/Jul/19 16:00;dmvk;Hello Yang, yes you can definitely do this. Problem is when you're using existing libraries that rely on classloading. Also yarnship behavior is now inconsistent, because classloading works for archives, but not for resources (*which are added to classpath too, but in incorrect format*).

 

I think proper behavior would be:
 * We traverse ship directory recursively
 ** If we find an archive, we add it to a archives list
 ** If we find a resource, we resolve its parent directory and add it to resourceDirectories set
 * We construct classpath for shipped files as follows: sorted(resourceDirectories) + sorted(archives)

 

I'll provide a patch for this today.

 ;;;","10/Jul/19 07:52;wangyang0918;Hi David,

Sounds great. It is exactly what we want. Just make ""–yarnship"" support jars, files and archives.;;;","15/Aug/19 17:59;trohrmann;Fixed via 17e2747e575cc4d9847be76b5ce9a75a7b6707f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stop fails with Universal Kafka Consumer,FLINK-13124,13243393,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,aljoscha,knaufk,knaufk,05/Jul/19 13:36,15/Jul/19 11:31,13/Jul/23 08:10,15/Jul/19 11:31,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,"When running the {{StateMachineExample}} (with the universal Kafka connector instead of 0.10) the Job always crashes with the following exception, when stopping it with {{flink stop <job-id>}}.
{noformat}
2019-07-05 13:16:49,809 INFO org.apache.flink.runtime.taskmanager.Task - Source: Custom Source (1/1) (1a22bba845872431e8695fc8f3793fcd) switched from RUNNING to FAILED.
java.lang.Exception: org.apache.flink.streaming.connectors.kafka.internal.Handover$ClosedException
taskmanager_1 | at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.checkThrowSourceExecutionException(SourceStreamTask.java:194)
taskmanager_1 | at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.performDefaultAction(SourceStreamTask.java:121)
taskmanager_1 | at org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:268)
taskmanager_1 | at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:376)
taskmanager_1 | at org.apache.flink.runtime.taskmanager.Task.run(Task.java:675)
taskmanager_1 | at java.lang.Thread.run(Thread.java:748)
taskmanager_1 | Caused by: org.apache.flink.streaming.connectors.kafka.internal.Handover$ClosedException
taskmanager_1 | at org.apache.flink.streaming.connectors.kafka.internal.Handover.close(Handover.java:182)
taskmanager_1 | at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.cancel(KafkaFetcher.java:175)
taskmanager_1 | at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.cancel(FlinkKafkaConsumerBase.java:817)
taskmanager_1 | at org.apache.flink.streaming.api.operators.StreamSource.cancel(StreamSource.java:124)
taskmanager_1 | at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.cancelTask(SourceStreamTask.java:144)
taskmanager_1 | at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.finishTask(SourceStreamTask.java:150)
taskmanager_1 | at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:781)
taskmanager_1 | at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:656)
taskmanager_1 | at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.triggerCheckpoint(SourceStreamTask.java:160)
taskmanager_1 | at org.apache.flink.runtime.taskmanager.Task$1.run(Task.java:1120)
taskmanager_1 | at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
taskmanager_1 | at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
taskmanager_1 | at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
taskmanager_1 | ... 1 more
{noformat}
Rebased on a `86bee8679112e76372a84083b1af18722644e1a0` the stacktrace looks like this:
{noformat}
taskmanager_1  | 2019-07-08 13:01:01,287 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Custom Source (1/1) (54504e053415cee82a2a7b4293d325e9) switched from RUNNING to FAILED.
taskmanager_1  | org.apache.flink.streaming.connectors.kafka.internal.Handover$ClosedException
taskmanager_1  | 	at org.apache.flink.streaming.connectors.kafka.internal.Handover.close(Handover.java:182)
taskmanager_1  | 	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.cancel(KafkaFetcher.java:175)
taskmanager_1  | 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.cancel(FlinkKafkaConsumerBase.java:814)
taskmanager_1  | 	at org.apache.flink.streaming.api.operators.StreamSource.cancel(StreamSource.java:124)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.cancelTask(SourceStreamTask.java:108)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.finishTask(SourceStreamTask.java:114)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:729)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:604)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.triggerCheckpoint(SourceStreamTask.java:124)
taskmanager_1  | 	at org.apache.flink.runtime.taskmanager.Task$1.run(Task.java:1151)
taskmanager_1  | 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
taskmanager_1  | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
taskmanager_1  | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
taskmanager_1  | 	at java.lang.Thread.run(Thread.java:748)
{noformat}
Rebased on {{86bee8679112e76372a84083b1af18722644e1a0}} without {{ExceptionUtils.rethrowException(error, error.getMessage());}} in {{Handover#pollNext()}}:
{noformat}
 2019-07-09 09:00:35,498 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Custom Source (1/1) (8568229c7efcddf75545a503bdb737f8) switched from RUNNING to FAILED.
taskmanager_1  | org.apache.flink.util.FlinkException: org.apache.flink.streaming.connectors.kafka.internal.Handover$ClosedException
taskmanager_1  | 	at org.apache.flink.streaming.connectors.kafka.internal.Handover.pollNext(Handover.java:85)
taskmanager_1  | 	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:131)
taskmanager_1  | 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:711)
taskmanager_1  | 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:95)
taskmanager_1  | 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:59)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.run(SourceStreamTask.java:102)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:335)
taskmanager_1  | 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:724)
taskmanager_1  | 	at java.lang.Thread.run(Thread.java:748)
taskmanager_1  | Caused by: org.apache.flink.streaming.connectors.kafka.internal.Handover$ClosedException
taskmanager_1  | 	at org.apache.flink.streaming.connectors.kafka.internal.Handover.close(Handover.java:184)
taskmanager_1  | 	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.cancel(KafkaFetcher.java:175)
taskmanager_1  | 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.cancel(FlinkKafkaConsumerBase.java:814)
taskmanager_1  | 	at org.apache.flink.streaming.api.operators.StreamSource.cancel(StreamSource.java:124)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.cancelTask(SourceStreamTask.java:108)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.finishTask(SourceStreamTask.java:114)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:729)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:604)
taskmanager_1  | 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.triggerCheckpoint(SourceStreamTask.java:124)
taskmanager_1  | 	at org.apache.flink.runtime.taskmanager.Task$1.run(Task.java:1151)
taskmanager_1  | 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
taskmanager_1  | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
taskmanager_1  | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
taskmanager_1  | 	... 1 more
{noformat}

Rebased on {{86bee8679112e76372a84083b1af18722644e1a0}} with the Kafka011 consumer. 
{noformat}
org.apache.flink.streaming.connectors.kafka.internal.Handover$ClosedException
	at org.apache.flink.streaming.connectors.kafka.internal.Handover.close(Handover.java:182)
	at org.apache.flink.streaming.connectors.kafka.internal.Kafka09Fetcher.cancel(Kafka09Fetcher.java:181)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.cancel(FlinkKafkaConsumerBase.java:814)
	at org.apache.flink.streaming.api.operators.StreamSource.cancel(StreamSource.java:124)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.cancelTask(SourceStreamTask.java:108)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.finishTask(SourceStreamTask.java:114)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:729)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:604)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.triggerCheckpoint(SourceStreamTask.java:124)
	at org.apache.flink.runtime.taskmanager.Task$1.run(Task.java:1151)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
",,aljoscha,becket_qin,highfei2011@126.com,kangzai,kkl0u,klion26,knaufk,liyu,pnowojski,sjwiesman,,,,"aljoscha commented on pull request #9090: [FLINK-13124] Don't forward exceptions when finishing SourceStreamTask
URL: https://github.com/apache/flink/pull/9090
 
 
   ## What is the purpose of the change
   
   Before, exceptions that occurred after cancelling a source (as the
   KafkaConsumer did, for example) would make a job fail when attempting a
   ""stop-with-savepoint"". Now we ignore those exceptions.
   
   ## Brief change log
   
   - add a `isFinished` flag in `SourceStreamTask`
   - check this flag before re-throwing exceptions
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - Added unit tests in `SourceStreamTaskTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Checkpointing, yes, because it fixes stop-with-savepoint
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jul/19 13:46;githubbot;600","kl0u commented on pull request #9090: [FLINK-13124] Don't forward exceptions when finishing SourceStreamTask
URL: https://github.com/apache/flink/pull/9090
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 11:03;githubbot;600","aljoscha commented on pull request #9115: [FLINK-13124] Don't forward exceptions when finishing SourceStreamTask
URL: https://github.com/apache/flink/pull/9115
 
 
   ## What is the purpose of the change
   
   Before, exceptions that occurred after cancelling a source (as the
   KafkaConsumer did, for example) would make a job fail when attempting a
   ""stop-with-savepoint"". Now we ignore those exceptions.
   
   ## Brief change log
   
   - add a `isFinished` flag in `SourceStreamTask`
   - check this flag before re-throwing exceptions
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - Added unit tests in `SourceStreamTaskTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Checkpointing, yes, because it fixes stop-with-savepoint
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 11:06;githubbot;600","kl0u commented on pull request #9115: [FLINK-13124] Don't forward exceptions when finishing SourceStreamTask
URL: https://github.com/apache/flink/pull/9115
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 11:28;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,FLINK-12749,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 15 11:31:57 UTC 2019,,,,,,,,,,"0|z04erc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/19 03:30;becket_qin;[~knaufk] Thanks for reporting the issue. I ran a few times the example locally but did not see the issue. And there was an IT case canceling a Kafka source connector, which did not produce the exception either.

Looking at the code, there was no obvious reason for the job to jump from Running to Failed due to canceling action. Can you maybe provide the step to reproduce the issue? Thanks.;;;","09/Jul/19 09:42;kkl0u;Thanks for all the runs [~knaufk]. I think I know now what the root cause is and I will open a PR as soon as I have an acceptable solution.;;;","15/Jul/19 11:31;kkl0u;Merged on master with 2e2f67ed348c334402a5d0af76b0fd47cedcf5a7
and on release-1.9 with 42a475d42ec136eecdc8eb972fc3d2d555e48a67;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catalog statistic is not bridged to blink planner ,FLINK-13116,13243333,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,godfreyhe,godfreyhe,05/Jul/19 10:31,02/Oct/19 17:49,13/Jul/23 08:10,25/Jul/19 13:58,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,This issue aims to let blink planner could use catalog statistic,,godfreyhe,jark,trohrmann,,,,,,,,,,,"godfreyhe commented on pull request #9083: [FLINK-13116] [table-planner-blink] Supports catalog statistics in blink planner
URL: https://github.com/apache/flink/pull/9083
 
 
   
   ## What is the purpose of the change
   
   *Supports catalog statistics in blink planner*
   
   
   ## Brief change log
   
     - *Supports catalog statistics in blink planner*
   
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
     - *Added CatalogStatsTest that validates the catalog statistics is used in optimizer*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / **not documented**)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jul/19 10:37;githubbot;600","asfgit commented on pull request #9083: [FLINK-13116] [table-planner-blink] Fix Catalog statistics is not bridged to blink planner
URL: https://github.com/apache/flink/pull/9083
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jul/19 13:58;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 25 13:58:42 UTC 2019,,,,,,,,,,"0|z04ee8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/19 08:18;chesnay;Why is a feature marked as a blocker? We're past the feature-freeze.;;;","18/Jul/19 07:47;trohrmann;[~ykt836], [~twalthr] please help to clarify whether this issue is really a 1.9.0 blocker.;;;","18/Jul/19 11:00;ykt836;[~chesnay] Sorry it's actually a bug about we didn't pass statistics from catalog to optimizer rightly. ;;;","25/Jul/19 13:58;jark;Fixed in 1.10.0:
 - 4817dd9df16c730e2e90f667f51f74ee40c3dd6c
 - 9cd95930bc89c2a4d4e8ee5b7e0c7394eca22b73
Fixed in 1.9.0: 
 - c6579ba1b1bc3a780b70791ab17ccf753260289b
 - 470e7cbcb98cd9ee4cd94ceb3424408475846b78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CatalogPartitionAPICompletenessTests in pyflink failed because of lacking ""getComment"" method",FLINK-13077,13242950,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhongwei,zhongwei,zhongwei,03/Jul/19 09:27,03/Jul/19 11:45,13/Jul/23 08:10,03/Jul/19 11:45,,,,,,,,,,1.9.0,,,,API / Python,Tests,,,,0,pull-request-available,,,,"The following exception will be thrown:
{code:java}
self = <pyflink.table.tests.test_catalog_completeness.CatalogPartitionAPICompletenessTests testMethod=test_completeness>

    def test_completeness(self):
>       self.check_methods()

pyflink/testing/test_case_utils.py:210: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'pyflink.table.tests.test_catalog_completeness.CatalogPartitionAPICompletenessTests'>

    @classmethod
    def check_methods(cls):
        java_primary_methods = {'getClass', 'notifyAll', 'equals', 'hashCode', 'toString',
                                'notify', 'wait'}
        java_methods = PythonAPICompletenessTestCase.get_java_class_methods(cls.java_class())
        python_methods = cls.get_python_class_methods(cls.python_class())
        missing_methods = java_methods - python_methods - cls.excluded_methods() \
            - java_primary_methods
        if len(missing_methods) > 0:
            raise Exception('Methods: %s in Java class %s have not been added in Python class %s.'
>                           % (missing_methods, cls.java_class(), cls.python_class()))
E           Exception: Methods: set([u'getComment']) in Java class org.apache.flink.table.catalog.CatalogPartition have not been added in Python class <class 'pyflink.table.catalog.CatalogPartition'>.
{code}
After a quick investigation of this issue, it seem that it is introduced in the commit and PR as follows:
 - [https://github.com/apache/flink/commit/59ff00d71d298fa61a92efa4fecd46f3cefc50f6]
 - [https://github.com/apache/flink/pull/8926]",,gjy,sunjincheng121,zhongwei,,,,,,,,,,,"WeiZhong94 commented on pull request #8968: [FLINK-13077][python] Fix the failed test in CatalogPartitionAPICompletenessTests caused by the lack of ""getComment"" method.
URL: https://github.com/apache/flink/pull/8968
 
 
   ## What is the purpose of the change
   
   *This pull request fixes the failed test in CatalogPartitionAPICompletenessTests caused by the lack of ""getComment"" method.*
   
   
   ## Brief change log
   
     - *Add `get_comment` method in python class `CatalogPartition`.*
   
   
   ## Verifying this change
   
   
   This change is already covered by existing tests `CatalogPartitionAPICompletenessTests`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (python docs)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 11:31;githubbot;600","sunjincheng121 commented on pull request #8968: [FLINK-13077][python] Fix the failed test in CatalogPartitionAPICompletenessTests caused by the lack of ""getComment"" method.
URL: https://github.com/apache/flink/pull/8968
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 11:40;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 03 11:45:54 UTC 2019,,,,,,,,,,"0|z04c20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/19 11:30;gjy;https://api.travis-ci.org/v3/job/553658404/log.txt;;;","03/Jul/19 11:33;sunjincheng121;[~zhongwei] Thanks for the report, And Thans [~gjy] add the new log, I will quickly fix this.;;;","03/Jul/19 11:35;sunjincheng121;Thanks for taking this ticket [~zhongwei];;;","03/Jul/19 11:45;sunjincheng121;Fixed in master: bd5ca420f752a16e9e81a4eda5cc4e23bfad1679;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Project pushdown rule shouldn't require the TableSource return a modified schema in blink planner,FLINK-13075,13242924,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,jark,jark,03/Jul/19 07:31,01/Aug/19 10:58,13/Jul/23 08:10,23/Jul/19 06:45,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"As the javadoc of {{org.apache.flink.table.sources.ProjectableTableSource#projectFields}} said, the table schema of the TableSource copy must not be modified by this method. However, {{PushProjectIntoTableSourceScanRule}} in blink planner requires the returning a table source with the modified table schema. This conflict with the javadoc and result in the TableSource works for flink planner can't work for blink planner.",,jark,wind_ljy,,,,,,,,,,,,"TsReaper commented on pull request #9197: [FLINK-13075][table-planner-blink] Project pushdown rule shouldn't require the TableSource return a modified schema in blink planner
URL: https://github.com/apache/flink/pull/9197
 
 
   ## What is the purpose of the change
   
   As the javadoc of `org.apache.flink.table.sources.ProjectableTableSource#projectFields` said, the table schema of the `TableSource` copy must not be modified by this method. However, `PushProjectIntoTableSourceScanRule` in blink planner requires the returning a table source with the modified table schema. This conflict with the javadoc and result in the `TableSource` works for flink planner can't work for blink planner.
   
   This PR fixes `PushProjectIntoTableSourceScanRule` and related class to make it work for blink planner.
   
   ## Brief change log
   
    - Fix `PushProjectIntoTableSourceScanRule` so that the schema of the newly created table source is the same with the original one.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as `TableSourceItCase`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 09:54;githubbot;600","wuchong commented on pull request #9197: [FLINK-13075][table-planner-blink] Project pushdown rule shouldn't require the TableSource return a modified schema in blink planner
URL: https://github.com/apache/flink/pull/9197
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jul/19 06:45;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,FLINK-13285,,,,,,,,,FLINK-13348,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 23 06:45:53 UTC 2019,,,,,,,,,,"0|z04bw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/19 09:52;jark;Please open test {{org.apache.flink.table.runtime.batch.sql.TableSourceITCase#testLookupJoinCsvTemporalTable}} after fix this problem. [~godfreyhe]
;;;","23/Jul/19 06:45;jark;Fixed in 1.10.0: bd351943ff93d3eb1324820dc858d3e9883427e8
Fixed in 1.9.0: e1e62e3629a8b5055dbab70709b9790cda33fa30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix PartitionableTableSink doesn't work in flink&blink planner,FLINK-13074,13242921,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,danny0405,danny0405,danny0405,03/Jul/19 07:18,01/Aug/19 10:57,13/Jul/23 08:10,23/Jul/19 13:19,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,danny0405,jark,,,,,,,,,,,,"danny0405 commented on pull request #8966: [FLINK-13074] Add PartitionableTableSink bridge logic to flink&blink …
URL: https://github.com/apache/flink/pull/8966
 
 
   ## What is the purpose of the change
   
   Add bridge logic of PartitionableTableSink for flink&blink planner, now we only add dynamic partitions for blink-planner and static partitions for flink-planner; after merging of blink and flink planner, these functions would also be merged.
   
   
   ## Brief change log
   
     - Add PartitionableTableSink bridge logic
   
   ## Verifying this change
   
   There are no tests for this patch now, would add test cases when both blink/flink planner upgrade Calcite version to 1.20.0
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? yes
     - If yes, how is the feature documented? not documented
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 10:30;githubbot;600","asfgit commented on pull request #8966: [FLINK-13074][table-planner-blink] PartitionableTableSink didn't work for  flink&blink planner
URL: https://github.com/apache/flink/pull/8966
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jul/19 13:18;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 23 13:19:17 UTC 2019,,,,,,,,,,"0|z04bvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/19 13:19;jark;Fixed in 1.10.0: e62ddb3033071afceaf3bfd2b44d9de037fcbb76
Fixed in 1.9.0: 2ee637352e7f6f91ee1b7cf0e2ca8c1491027702;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinaryRow in Blink runtime has wrong FIRST_BYTE_ZERO mask,FLINK-13073,13242913,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,LouisXu777,LouisXu777,LouisXu777,03/Jul/19 06:37,04/Jul/19 11:19,13/Jul/23 08:10,04/Jul/19 11:19,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"BinaryRow which in blink runtime module has static constant named FIRST_BYTE_ZERO, it's purpose is to filter first byte。

Use 0xFFF0 as a exsample:

its binary sequence is: 1111111111110000。It has 2 problems:
 # It can only skip 4 bits of header. If the header is big than 15, then any return long value from pos 0 intersect with FIRST_BYTE_ZERO will not equal to 0. Then will always return true.
 # It only can check 8 bit(0-7) of null fields, which left 8-55 bit of null field unchecked.",,jark,LouisXu777,,,,,,,,,,,,"AjaxXu commented on pull request #8961: [FLINK-13073][table-blink-runtime]BinaryRow in Blink runtime has wrong FIRST_BYTE_ZERO mask
URL: https://github.com/apache/flink/pull/8961
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: ( no )
     - The runtime per-record code paths (performance sensitive): ( no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no )
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 06:40;githubbot;600","asfgit commented on pull request #8961: [FLINK-13073][table-blink-runtime]BinaryRow in Blink runtime has wrong FIRST_BYTE_ZERO mask
URL: https://github.com/apache/flink/pull/8961
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Jul/19 11:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 04 11:19:30 UTC 2019,,,,,,,,,,"0|z04bts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/19 08:17;ykt836;Thanks for the reporting and fixing! [~LouisXu777];;;","03/Jul/19 08:33;LouisXu777;You are welcome. I am looking forward to contributing more to flink. [~ykt836];;;","03/Jul/19 10:36;LouisXu777;[~jark] [~lzljs3620320] [~ykt836] I find some problem when I review the code. It's because I'm not familiar with binary format when hex transform to binary. Sorry, I have fix the pr and pushed again. Welcome to review the new version.;;;","04/Jul/19 11:19;jark;Fixed in 1.9.0: 61119b84288e2707238d33469bcd7131d0da3514;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken links to contributing docs,FLINK-13067,13242878,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,03/Jul/19 02:47,10/Jul/19 11:14,13/Jul/23 08:10,10/Jul/19 11:14,,,,,,,,,,1.9.0,,,,Documentation,,,,,0,pull-request-available,,,,"As contributing links change on [https://github.com/apache/flink-web], all links to contributing related docs have become broken. We need to fix these broken links.",,sewen,yunta,,,,,,,,,,,,"Myasuka commented on pull request #8964: [FLINK-13067][docs] Fix broken links to contributing docs
URL: https://github.com/apache/flink/pull/8964
 
 
   ## What is the purpose of the change
   
   Fix broken links to contributing docs
   
   
   ## Brief change log
   
   Fix broken links to contributing websites in all docs
   
   ## Verifying this change
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 09:32;githubbot;600","asfgit commented on pull request #8964: [FLINK-13067][docs] Fix broken links to contributing docs
URL: https://github.com/apache/flink/pull/8964
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 09:39;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 10 11:14:00 UTC 2019,,,,,,,,,,"0|z04bm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/19 11:14;sewen;Fixed in 09445316bcc991ef262c74063e41af8eaa836ed8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncWaitOperator shouldn't be releasing checkpointingLock,FLINK-13063,13242792,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,srichter,pnowojski,pnowojski,02/Jul/19 16:17,27/Jun/22 09:21,13/Jul/23 08:10,09/Jul/19 12:09,1.6.4,1.7.2,1.8.1,1.9.0,,,,,,1.9.0,,,,API / DataStream,,,,,0,pull-request-available,,,,"1.

For the following setup of chained operators:
{noformat}
SourceOperator -> FlatMap -> AsyncOperator{noformat}
Lets assume that input buffer of {{AsyncOperator}} is full. We start processing a record from the {{SourceOperator}}, we pass it to the {{FlatMap}}, which fan it out (multiplies it 10 times). First multiplied record reaches {{AsyncOperator}} and is special treated (stored in {{AsyncWaitOperator#pendingStreamElementQueueEntry}} ) and then {{AsyncWaitOperator}} waits (and releases) on the checkpoint lock (in {{AsyncWaitOperator#addAsyncBufferEntry}} . If a checkpoint is triggered now, both {{SourceOperator}} and {{FlatMap}} will be checkpointed assumed that all of those 10 multiplied records were processed, which is not true. Only the first one is checkpointed by the {{AsyncWatiOperator}}. Remaining 9 are not. So if we ever restore state from this checkpoint, we have lost those 9 records.

2.

Similar issue (I think previously known) can happen if for example some upstream operator to the {{AsyncOperator}} fires a processing time timer, that emits some data. But in that case, {{AsyncWaitOperator#pendingStreamElementQueueEntry}} is being overwritten.

3.

If upstream operator has the following pseudo code:
{code:java}
stateA = true
output.collect(x)
stateB = true{code}
one would assume that stateA and stateB access/writes will be atomic from the perspective of the checkpoints. But again, because {{AsyncWaitOperator}} releases the checkpoint lock, they will not be.

CC [~aljoscha] [~StephanEwen] [~srichter]",,1u0,aitozi,aljoscha,elevy,jark,jingzhang,kisimple,lincoln.86xy,Paul Lin,pnowojski,sewen,srichter,,"StefanRRichter commented on pull request #9034: [FLINK-13063] Temporary fix for AsyncWaitOperator consistency problems.
URL: https://github.com/apache/flink/pull/9034
 
 
   ## What is the purpose of the change
   
   This PR is a temporary fix for the consistency problems with the `AsyncWaitOperator` that are described under FLINK-13063. This temporary fix works simply by deactivating all chaining for `AsyncWaitOperator`.
   
   
   ## Brief change log
   
   Set the chaining strategy in the constructor of `AsyncWaitOperator` to `NEVER` by default.
   
   
   ## Verifying this change
   
   This change is already covered by existing tests. I added an additional test that chaining is suppressed by default and also check that users could go back to the old (broken) behavior if they had to, by setting the chaining strategy manually on the operator.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (yes)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (change and backwards compatibility)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 09:55;githubbot;600","StefanRRichter commented on pull request #9034: [FLINK-13063] Temporary fix for AsyncWaitOperator consistency problems.
URL: https://github.com/apache/flink/pull/9034
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 12:07;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-16219,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 09 12:09:38 UTC 2019,,,,,,,,,,"0|z04b2w:",9223372036854775807,"This changes the default chaining behavior of the AsyncWaitOperator. By default, we now break chains so that the AsyncWaitOperator is never chained after another operator.",,,,,,,,,,,,,,,,,,,"02/Jul/19 16:22;pnowojski;Hot fix is to ensure that {{AsyncWaitOperator}} is always a head of an operator chain.

Proper solution might require a better/extended mailbox implementation (when {{AsyncWaitOperator}} is full, yield only to down the stream operators), that's beyond our reach for 1.9 release.;;;","03/Jul/19 19:57;sewen;+1 to fix this for now by forcing Async I/O operator to always be head-of-chain;;;","03/Jul/19 19:58;sewen;As discussed with [~pnowojski] and [~srichter] - a proper mailbox model can solve this consistently in the future without requiring to break the chain.;;;","04/Jul/19 09:39;1u0;I have encountered another issue with {{AsyncWaitOperator}} and how it interferes with checkpoints:

if the async function finishes some {{ResultFuture}} s with exception, some elements may be passed/emitted to downstream task(s) more than once.

An example test Flink job, may look like:
{code:java}
Source (1) -> AsyncOperator (1) -> Map (1) -> Sink (1)
{code}
I've created an example Flink job (hopefully properly implementing exactly-once source and sink, and state management) in [https://github.com/1u0/flink/blob/bug-async-function-more-than-once/flink-streaming-java/src/test/java/org/apache/flink/MyTestIT.java]. In this test job, the operator after the {{AsyncFunction}} just adds counter of seen events. In a test run, it's possible to see some committed events more than once with different counter values.;;;","04/Jul/19 09:55;pnowojski;[~1u0] could you write down why is it happening where is the bug?;;;","05/Jul/19 09:35;1u0;I think the root cause is the same, that the {{AsyncWaitOperator}} releases the lock in {{processElement|Watermark()}}. And can be addressed by not allowing the {{AsyncWaitOperator}} to be after some other operator in a chain.

The duplicates are happening due to:
 * the example source considers an element to be processed only after {{SourceContext.collect()}} returns;
 * the second (duplicate) copy of the element happens to be the snapshotted {{AsyncWaitOperator.pendingStreamElementQueueEntry}} that was passed in a ""pending"" {{processElement()}}.
;;;","09/Jul/19 12:09;srichter;Merged in:
master: c773ce5;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra Connector leaks Semaphore on Exception; hangs on close",FLINK-13059,13242725,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mchro,mchro,mchro,02/Jul/19 12:04,03/Sep/19 09:50,13/Jul/23 08:10,03/Sep/19 09:50,1.8.0,,,,,,,,,1.10.0,1.8.2,1.9.1,,Connectors / Cassandra,,,,,0,pull-request-available,,,,"In CassandraSinkBase the following code is present (comments are mine):

 
{code:java}
public void invoke(IN value) throws Exception {
   checkAsyncErrors();
   tryAcquire();
   //Semaphore held here

   final ListenableFuture<V> result = send(value);

   Futures.addCallback(result, callback); //Callback releases semaphore
}{code}
Any Exception happening inside send(value) will result in the semaphore not being released. Such exceptions are possible, e.g.
{code:java}
com.datastax.driver.core.exceptions.InvalidQueryException: Some partition key parts are missing: hest
at com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:50)
at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)
at com.datastax.driver.core.AbstractSession.prepare(AbstractSession.java:98)
at com.datastax.driver.mapping.Mapper.getPreparedQuery(Mapper.java:118)
at com.datastax.driver.mapping.Mapper.saveQuery(Mapper.java:201)
at com.datastax.driver.mapping.Mapper.saveQuery(Mapper.java:163)
at org.apache.flink.streaming.connectors.cassandra.CassandraPojoSink.send(CassandraPojoSink.java:128)
at org.apache.flink.streaming.connectors.cassandra.CassandraSinkBase.invoke(CassandraSinkBase.java:131)
at org.apache.flink.streaming.api.functions.sink.SinkFunction.invoke(SinkFunction.java:52)
{code}
The result of the semaphore not being released will be that when the exception bubbles out and causes the job to close, CassandraSinkBase.flush() will eventually be called. Flush will be deadlocked trying to acquire config.getMaxConcurrentRequests() from the semaphore, which has 1 less than that available.

The Flink job will thus be half-way closed, but marked as ""RUNNING"". Checkpointing will however fail with
{noformat}
INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Discarding checkpoint 201325 of job XXX. org.apache.flink.runtime.checkpoint.decline.CheckpointDeclineTaskNotReadyException: Task Source: XXX (3/4) was not running {noformat}
 ",,anmu,elevy,mchro,,,,,,,,,,,"mchro commented on pull request #8967: [FLINK-13059][Cassandra Connector] Release Semaphore correctly on Exception in send()
URL: https://github.com/apache/flink/pull/8967
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   Release the MaxConcurrentRequests Semaphore correctly, if an exception happens in send(). This should make flush() not deadlock, but just in case: make flush timeout controllable by MaxConcurrentRequestsTimeout.
   
   ## Brief change log
   
     - Release Semaphore correctly on Exception in send()
     - Use getMaxConcurrentRequestsTimeout() also for flush
   
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
     - Added unit test that validates semaphore released if Exception in send(); test fails before change.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 10:52;githubbot;600","zentol commented on pull request #8967: [FLINK-13059][Cassandra Connector] Release Semaphore correctly on Exception in send()
URL: https://github.com/apache/flink/pull/8967
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Sep/19 09:49;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 03 09:50:38 UTC 2019,,,,,,,,,,"0|z04ao0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Sep/19 09:50;chesnay;master: fbb4837a4d274d19eddbcc0a9ab96724ad8ef972
1.9: 22571aab57fc30450de8a850f1a8a6ea80fdba2c
1.8: b7ce7b8ff14807e4981591a7e26c99d5051d529f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct comments in ListState class,FLINK-13057,13242698,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hequn8128,hequn8128,hequn8128,02/Jul/19 09:36,10/Jul/19 11:14,13/Jul/23 08:10,10/Jul/19 11:14,,,,,,,,,,1.9.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"ListState can be a keyed state or an operator state, but the comment in ListState said it can only be a keyed state：
{code:java}
The state is only accessible by functions applied on a {@code KeyedStream}. 
{code}
We can change the comment from
{code:java}
* <p>The state is only accessible by functions applied on a {@code KeyedStream}. The key is
* automatically supplied by the system, so the function always sees the value mapped to the
* key of the current element. That way, the system can handle stream and state partitioning
* consistently together.{code}
to
{code:java}
 * <p>The state can be a keyed list state or an operator list state.
 * When it is a keyed list state, it is accessed by functions applied on a {@code KeyedStream}.
 * The key is automatically supplied by the system, so the function always sees the value mapped
 * to the key of the current element. That way, the system can handle stream and state
 * partitioning consistently together.
 * When it is an operator list state, the list is a collection of state items that are
 * independent from each other and eligible for redistribution across operator instances in case
 * of changed operator parallelism.
{code}
Appreciate any suggestions.",,hequn8128,sewen,,,,,,,,,,,,"hequn8128 commented on pull request #8994: [FLINK-13057][state] Correct comments in ListState class
URL: https://github.com/apache/flink/pull/8994
 
 
   
   
   ## What is the purpose of the change
   
   This pull request fixes the comments in `ListState`. 
   
   `ListState` can be a keyed state or an operator state, but the comment in `ListState` said it can only be a keyed state
   
   
   ## Brief change log
   
     - Correct comments in `ListState` class, i.e. `ListState` can be a keyed state or an operator state.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Jul/19 05:57;githubbot;600","asfgit commented on pull request #8994: [FLINK-13057][state] Correct comments in ListState class
URL: https://github.com/apache/flink/pull/8994
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 09:39;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 10 11:14:27 UTC 2019,,,,,,,,,,"0|z04ai0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/19 11:14;sewen;Fixed via 7f5d5838ec2d5c1bceb8f6acf76bb53e04aa3d8f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shading of AWS SDK in flink-s3-fs-hadoop results in ClassNotFoundExceptions,FLINK-13044,13242504,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,zeeman,zeeman,01/Jul/19 12:40,02/Oct/19 17:44,13/Jul/23 08:10,06/Aug/19 19:44,1.8.0,,,,,,,,,1.9.0,,,,FileSystems,,,,,0,pull-request-available,,,," The current build configuration shades the AWS SDK into {{flink-s3-fs-hadoop.jar}}

Due to [MSHAD-156|https://issues.apache.org/jira/browse/MSHADE-156] the shading itself does not only add the classes to the JAR but also alters the string literal {{PACKAGE_PREFIXES_TO_SHADE}} in {{org.apache.flink.fs.s3hadoop.S3FileSystemFactory}} from {{com.amazonaws.}} to {{org.apache.flink.fs.s3base.shaded.com.amazonaws}} - which causes settings like 
{noformat}fs.s3a.aws.credentials.provider: com.amazonaws.auth.DefaultAWSCredentialsProviderChain{noformat}
to not be remapped properly as the {{startsWith}} in {{org.apache.flink.fs.s3.common.HadoopConfigLoader.shadeClassConfig()}} doesn't match the beginning of the FQCN.

This is a known issue in the Maven shade plugin (see [MSHAD-156|https://issues.apache.org/jira/browse/MSHADE-156]) - a possible workaround is named in the ticket, too.

This issue is only visible looking into the compiled {{class}} file after the shading happened!

Fixing the {{PACKAGE_PREFIXES_TO_SHADE}} value alone doesn't help as based on {{FLINK_SHADING_PREFIX}} the FQCN is assembled as {noformat}org.apache.flink.fs.s3hadoop.shaded.com.amazonaws.auth.DefaultAWSCredentialsProviderChain{noformat} which can't be found either;
the class is located at {noformat}org.apache.flink.fs.s3base.shaded.com.amazonaws.auth.DefaultAWSCredentialsProviderChain{noformat} ",,sewen,trohrmann,zeeman,,,,,,,,,,,"derjust commented on pull request #9071: FLINK-13044 [BuildSystem / Shaded] Fix for wrong shading of AWS SDK in flink-s3-fs-hadoop
URL: https://github.com/apache/flink/pull/9071
 
 
   ## What is the purpose of the change
   
   Due to the bug MSHADE-156 in Maven's shading plugin [1] the merging of the AWS SDK also causes string literals to be changed in Flink classes:
   
   `PACKAGE_PREFIXES_TO_SHADE` in `org.apache.flink.fs.s3hadoop.S3FileSystemFactory` from `com.amazonaws.` to `org.apache.flink.fs.s3base.shaded.com.amazonaws` - which causes settings like `fs.s3a.aws.credentials.provider: com.amazonaws.auth.DefaultAWSCredentialsProviderChain`
   to not be remapped properly as the `startsWith` in `org.apache.flink.fs.s3.common.HadoopConfigLoader.shadeClassConfig()` doesn't match the beginning of the FQCN.
   
   Using `DefaultAWSCredentialsProviderChain` instead of Flink's hand-crafted chain can be required depending on the Flink deployment - i.e. as part of Fargate.
   
   This issue is only visible looking into the compiled class file after the shading happened!
   
   ## Brief change log
   
   * Fixing the `PACKAGE_PREFIXES_TO_SHADE` value alone doesn't help as based on `FLINK_SHADING_PREFIX` the FQCN is assembled as `org.apache.flink.fs.s3hadoop.shaded.com.amazonaws.auth.DefaultAWSCredentialsProviderChain`
   which can't be found either; the class is located at `org.apache.flink.fs.s3base.shaded.com.amazonaws.auth.DefaultAWSCredentialsProviderChain`
   
       * This commit follows the workaround shown in [1] to workaround the shading issue.
   * Also fixing the wrong package name in the Flink code for the shaded AWS SDK classes.
   
   ## Verifying this change
   
   This change can be verified as follows:
   - *Set `fs.s3a.aws.credentials.provider: com.amazonaws.auth.DefaultAWSCredentialsProviderChain` in `flink-conf.yaml`*
   - *Start JobManager and see it using the `DefaultAWSCredentialsProviderChain` instead of Flink's handcrafted chain*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: yes
   
   ## Documentation
   
     - Does this pull request introduce a new feature?   no
     - If yes, how is the feature documented? not applicable 
   
   
   [1] https://issues.apache.org/jira/browse/MSHADE-156
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 13:43;githubbot;600","zentol commented on pull request #9318: [FLINK-13044][s3][fs] Fix handling of relocated amazon classes
URL: https://github.com/apache/flink/pull/9318
 
 
   To provide credentials to S3 users may configure a credentials provider. For providers from amazon (which are relocated) we allow users to configure the original class name, and relocate it manually in the S3 filesystem factories.
   
   The factories however
   - used the wrong shading pattern
   - could not correctly detect providers with a ""com.amazon*"" prefix, as the String constant containing this prefix was also being relocated.
   
   This commit obfuscates the constant to prevent relocations, fixes the shading patterns, and sets up e2e tests to cover this case.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 10:43;githubbot;600","zentol commented on pull request #9071: FLINK-13044 [BuildSystem / Shaded] Fix for wrong shading of AWS SDK in flink-s3-fs-hadoop
URL: https://github.com/apache/flink/pull/9071
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 10:44;githubbot;600","zentol commented on pull request #9318: [FLINK-13044][s3][fs] Fix handling of relocated amazon classes
URL: https://github.com/apache/flink/pull/9318
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/19 19:42;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-8439,FLINK-13602,MSHADE-156,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,MSHADE-156,https://issues.apache.org/jira/browse/MSHADE-156,,,,,,,,,,9223372036854775807,,,Tue Aug 06 19:44:40 UTC 2019,,,,,,,,,,"0|yi0g1v:c",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/19 14:47;sewen;We can in theory drop shading of the filesystems all together now, and instead put them in the plugins folder of Flink.
That also gives class loader isolation and avoid class path pollution, and does not require an shading.

Because this is very new, we did not want to drop shading instantly, in case users still put the file system into ""/lib"".

As a workaround, you could comment out the class relocation part of the build, and put the resulting artifacts into ""/plugins"".
That should do the trick.

I hope that we will drop the evil shading magic completely from 1.10.;;;","11/Jul/19 13:29;zeeman;Glad to hear the plan for 1.10 [~sewen]!

May I ask what the best approach is for us to live with a vanilla Flink till then?
Any chance to get {{PR#9071}} in a (1.9.x?) patch release? 
TBH, having a non-code-modified version in production is our ultimate goal.;;;","11/Jul/19 13:45;sewen;This is clearly a bug and should be fixed in 1.9.0;;;","25/Jul/19 07:55;trohrmann;Could you please move this issue into ""In Progress"" [~zeeman].;;;","06/Aug/19 19:44;chesnay;master: 5e5f5bec72d30397253fc1f11105ca385ca8681f
1.9: 16f519a1e7efae9210725f741c27a647ac3f3271 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of parsing Dewey number from string,FLINK-13043,13242483,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fan_li_ya,fan_li_ya,fan_li_ya,01/Jul/19 10:58,13/Apr/21 20:39,13/Jul/23 08:10,05/Jul/19 07:02,,,,,,,,,,1.9.0,,,,Library / CEP,,,,,0,pull-request-available,,,,"There is a bug in the current implementation for parsing the Dewey number:

 

String[] splits = deweyNumberString.split(""\\."");

if (splits.length == 0) {
 return new DeweyNumber(Integer.parseInt(deweyNumberString));
 }

 

The length in the if condition should be 1 instead of 0.

 ",,fan_li_ya,sewen,,,,,,,,,,,,"liyafan82 commented on pull request #8936: [FLINK-13043][Library / CEP] Fix the bug of parsing Dewey number from string
URL: https://github.com/apache/flink/pull/8936
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   There is a bug in the current implementation for parsing the Dewey number:
   
    
   
   String[] splits = deweyNumberString.split(""\\."");
   
   if (splits.length == 0){
    return new DeweyNumber(Integer.parseInt(deweyNumberString)); 
   }
    
   
   The length in the if condition should be 1 instead of 0.
   
   
   ## Brief change log
   
     - Fix the problem by modifying the implementation of DeweyNumber#fromString
     
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as DeweyNumberTest.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Jul/19 11:42;githubbot;600","asfgit commented on pull request #8936: [FLINK-13043][Library / CEP] Fix the bug of parsing Dewey number from string
URL: https://github.com/apache/flink/pull/8936
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 22:36;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 05 07:02:19 UTC 2019,,,,,,,,,,"0|z0495s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/19 07:02;sewen;Fixed in 1.9.0 via 6624562982c9d57bebba8cb4b574b8ed28640a0d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken and irreproducible dockerized docs build,FLINK-13017,13241931,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,nkruber,nkruber,nkruber,27/Jun/19 12:24,05/Jul/19 07:00,13/Jul/23 08:10,05/Jul/19 07:00,1.6.4,1.7.2,1.8.0,1.9.0,,,,,,1.9.0,,,,Documentation,,,,,0,pull-request-available,,,,"The build tools around {{docs/docker}} seem broken and (on my machine) give errors like the following while it is working on a colleague's machine:
{code}
bash: /etc/bash_completion.d/git-prompt.sh: No such file or directory
bash: __git_ps1: command not found
{code}

{code}
/usr/bin/env: 'ruby.ruby2.5': No such file or directory
bash: __git_ps1: command not found
{code}

Reason seems to be that your whole user's $HOME is mounted (writable!) into the docker container. We should just mount the docs directory to get
# builds which are independent from the host system (making them reproducible)
# not have the commands in the container affect the host(!)",,nkruber,sewen,,,,,,,,,,,,"NicoK commented on pull request #8917: [FLINK-13017][docs] do not mount local $HOME into docker
URL: https://github.com/apache/flink/pull/8917
 
 
   ## What is the purpose of the change
   
   Remove the (writable!) mount of a user's $HOME into the dockerized documentation build container in order to
   - make the builds independent from the host system (making them reproducible)
   - not have the commands in the container affect the host
   
   ## Brief change log
   
   - remove mounting user $HOME
   
   ## Verifying this change
   
   I verified building the docs inside the new environment.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jun/19 12:43;githubbot;600","asfgit commented on pull request #8917: [FLINK-13017][docs] do not mount local $HOME into docker
URL: https://github.com/apache/flink/pull/8917
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 22:36;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,FLINK-9728,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 05 07:00:32 UTC 2019,,,,,,,,,,"0|z045rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/19 07:00;sewen;Fixed in 1.9.0 via 7a8333b18503cc2414bb405dced91d4b83260a6d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTaskNetworkInput can linger records for longer period of times,FLINK-13016,13241918,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,27/Jun/19 11:38,04/Jul/19 11:30,13/Jul/23 08:10,04/Jul/19 11:30,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,There is a bug in {{StreamTaskNetworkInput#isAvailable}}. It ignores the case if some data/records are currently buffered in \{{currentRecordDeserializer}} field.,,aitozi,pnowojski,,,,,,,,,,,,"pnowojski commented on pull request #8963: [FLINK-13016][network] Fix StreamTaskNetworkInput#isAvailable
URL: https://github.com/apache/flink/pull/8963
 
 
   This PR introduces two changes:
   
   [FLINK-13016][network] Fix StreamTaskNetworkInput#isAvailable
   [FLINK-13013][network] Request partitions during InputGate#setup
   
   plus some hotfixes. For details, please check individual commits and their commit messages.
   
   This PR either relays on the existing tests or adds new tests like `StreamTaskNetworkInputTest` to add missing test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (**yes** / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 09:30;githubbot;600","pnowojski commented on pull request #8963: [FLINK-13016][network] Fix StreamTaskNetworkInput#isAvailable
URL: https://github.com/apache/flink/pull/8963
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Jul/19 11:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 04 11:30:39 UTC 2019,,,,,,,,,,"0|z045oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/19 11:30;pnowojski;merged to master as 76eb11fdc4561e15c7f3ad614b416cd2a84bf50e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the logic of needToCleanupState in KeyedProcessFunctionWithCleanupState,FLINK-13004,13241767,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,26/Jun/19 18:05,29/Jun/19 13:39,13/Jul/23 08:10,29/Jun/19 13:39,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Current implementation of needToCleanupState in KeyedProcessFunctionWithCleanupState actually has potention bug:
{code:java}
protected Boolean needToCleanupState(Long timestamp) throws IOException {
   if (stateCleaningEnabled) {
      Long cleanupTime = cleanupTimeState.value();
      // check that the triggered timer is the last registered processing time timer.
      return null != cleanupTime && timestamp == cleanupTime;
   } else {
      return false;
   }
}
{code}
Please note that it directly use ""=="" to judge whether *Long* type timestamp and cleanupTime equals. However, if that value is larger than 127L, the result would actually return false instead of wanted true.",,jark,yunta,,,,,,,,,,,,"Myasuka commented on pull request #8909: [FLINK-13004][table-runtime-blink] Correct the logic of needToCleanupState in KeyedProcessFunctionWithCleanupState
URL: https://github.com/apache/flink/pull/8909
 
 
   
   ## What is the purpose of the change
   
   Current implementation of `#needToCleanupState` in `KeyedProcessFunctionWithCleanupState` used incorrect way to judge whether the time should cleaned up. We should sue `equals` instead of `==` to compare wrapper type of Long. 
   
   ## Brief change log
   
     - Correct the logic of `#needToCleanupState` in `KeyedProcessFunctionWithCleanupState`.
     - Correct the check logic of  whether there are still records which have not been processed yet in `AbstractRowTimeUnboundedPrecedingOver`.
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Updated test of `OverWindowHarnessTest`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jun/19 07:20;githubbot;600","asfgit commented on pull request #8909: [FLINK-13004][table-runtime-blink] Correct the logic of needToCleanupState in KeyedProcessFunctionWithCleanupState
URL: https://github.com/apache/flink/pull/8909
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jun/19 13:33;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 29 13:39:28 UTC 2019,,,,,,,,,,"0|z044r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/19 02:06;jark;I think the {{timestamp}} parameter should be primitive long. Thanks for reporting this.

Could your elaborate more about why larger than 127L, the result would return false?;;;","27/Jun/19 04:54;yunta;[~jark], actually, the {{timestamp}} passed in was wrapper {{Long.}} For the comparison of wrapper and primitive value problem, you could refer to [question here|https://stackoverflow.com/questions/19485818/what-are-not-2-long-variables-equal-with-operator-to-compare-in-java].

I am preparing a PR to fix this and found previous tests have been modified to cater for this bug.;;;","27/Jun/19 10:31;jark;[~yunta] (y);;;","29/Jun/19 13:39;jark;Fixed in 1.9.0: 0640f97755f49b3331e8d81ca15dca0566a1890d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Date type doesn't consider the local TimeZone,FLINK-12990,13241624,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,26/Jun/19 03:38,27/Jun/19 01:39,13/Jul/23 08:10,27/Jun/19 01:38,,,,,,,,,,1.9.0,,,,API / Python,,,,,0,pull-request-available,,,,"Currently, the python DateType is converted by an `int` which indicates the days passed since 1970-1-1 and then the Java side will create a Java Date by call `new Date(days * 86400)`. As we know that the Date constructor expected milliseconds since 1970-1-1 00:00:00 GMT and so we should convert `days * 86400` to GMT milliseconds.",,dian.fu,sunjincheng121,,,,,,,,,,,,"dianfu commented on pull request #8892: [FLINK-FLINK-12990][python] Date type doesn't consider the local TimeZone
URL: https://github.com/apache/flink/pull/8892
 
 
   ## What is the purpose of the change
   
   *Currently, the python DateType is converted by an `int` which indicates the days passed since 1970-1-1 and then the Java side will create a Java Date by call `new Date(days * 86400)`. As we know that the Date constructor expected milliseconds since 1970-1-1 00:00:00 GMT and so we should convert `days * 86400` to GMT milliseconds. This PR will fix this issue.*
   
   ## Brief change log
   
     - *Add method getOffsetFromLocalMillis to deal with the offset calculation of the local timezone and use the calculated offset when creating the Date object.*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jun/19 04:15;githubbot;600","asfgit commented on pull request #8892: [FLINK-12990][python] Date type doesn't consider the local TimeZone
URL: https://github.com/apache/flink/pull/8892
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jun/19 01:39;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 27 01:38:59 UTC 2019,,,,,,,,,,"0|z043vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/19 01:38;sunjincheng121;Fixed in master: 731c38baf1e72f963db1c067ada20dc23e9fc23c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DescriptiveStatisticsHistogram#getCount does not return the number of elements seen,FLINK-12987,13241581,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,25/Jun/19 20:28,15/Aug/19 23:03,13/Jul/23 08:10,15/Aug/19 23:03,1.6.4,1.7.2,1.8.0,1.9.0,,,,,,1.10.0,,,,Runtime / Metrics,,,,,0,pull-request-available,,,,"{{DescriptiveStatisticsHistogram#getCount()}} returns the number of elements in the current window and not the number of total elements seen over time. In contrast, {{DropwizardHistogramWrapper}} does this correctly.

We should unify the behaviour and add a unit test for it (there is no generic histogram test yet).",,nkruber,,,,,,,,,,,,,"NicoK commented on pull request #8886: [FLINK-12987][metrics] fix DescriptiveStatisticsHistogram#getCount() not returning the number of elements seen
URL: https://github.com/apache/flink/pull/8886
 
 
   ## What is the purpose of the change
   
   `DescriptiveStatisticsHistogram#getCount()` only returned the number of currently
   stored elements, not the total number seen. This is different from the docs in `Histogram` and the `DropwizardHistogramWrapper` implementation and fixed by this PR.
   
   ## Brief change log
   
   - extend `HistogramStatistics` with an additional `AtomicLong` to count the number of elements seen
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   - add an abstract unit test for verifying any `Histogram` implementation based on
   the previous test from `DropwizardFlinkHistogramWrapperTest`
   - create `DescriptiveStatisticsHistogramTest` to run this test with `DescriptiveStatisticsHistogram`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **yes** (test-jar, test-scope `flink-metrics` -> `flink-runtime`)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jun/19 22:47;githubbot;600","NicoK commented on pull request #8886: [FLINK-12987][metrics] fix DescriptiveStatisticsHistogram#getCount() not returning the number of elements seen
URL: https://github.com/apache/flink/pull/8886
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 23:02;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 15 23:03:05 UTC 2019,,,,,,,,,,"0|z043ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/19 23:03;nkruber;fixed on master via fd9ef60cc8448a5f4d1915973e168aad073d8e8d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Sending data to kafka with CsvRowSerializationSchema always adding a ""\n"", ""\r"",""\r\n"" at the end of the message",FLINK-12979,13241435,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hmclouro,chaiyq,chaiyq,25/Jun/19 09:41,16/Oct/19 14:46,13/Jul/23 08:10,16/Oct/19 14:46,1.8.0,1.8.1,1.8.2,1.9.0,2.0.0,,,,,1.10.0,,,,API / Type Serialization System,,,,,0,pull-request-available,,,,"When sending data to kafka using CsvRowSerializationSchema, the CsvRowSerializationSchema#serialize method helps generating value for KafkaRecord,  which will call  CsvEncoder#endRow and in which  a _cfgLineSeparator will be added at the end of KafkaRecord.value.

But For  CsvRowSerializationSchema#Builder , when you calling the mothod setLineDelimiter only ""\n"",""\r"",""\r\n"" could be used as the parameter.  

It's not friendly when you want to send a message ""123,pingpong,21:00"" to kafka but kafka receives a message  ""123,pingpong,21:00\r\n"".

I'm not sure about the reason for limitting the lineDelimiter to ""\n"",""\r"",""\r\n"" ？ In previous version and jackson-databind, there's no limits on lineDelimiter. 

But at least it should let the application developer to set LineDelimiter with """".
",,twalthr,,,,,,,,,,,,,"cyq89051127 commented on pull request #9529: [FLINK-12979]Allowing set an empty line delimiter at end of an message using CsvRowSerializationSchema
URL: https://github.com/apache/flink/pull/9529
 
 
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *This PR helps sending data to kafka  adding an emtpy delimiter at then end of the message when using CsvRowSerializationSchema*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The patch allows user to set an empty line delimiter at the end of the message *
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *This patch just allowing set the line delimiter with """"*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (**yes** / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Aug/19 07:14;githubbot;600","asfgit commented on pull request #9529: [FLINK-12979]Allowing set an empty line delimiter at end of an message using CsvRowSerializationSchema
URL: https://github.com/apache/flink/pull/9529
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/19 14:44;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 16 14:46:11 UTC 2019,,,,,,,,,,"0|z042pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/19 08:22;twalthr;The implementation is according to the RFC standard. But we could allow setting a custom property to disable the new line at the end.;;;","02/Jul/19 07:00;chaiyq;When i check  com.fasterxml.jackson.dataformat.csv.CsvSchema, there's no limit for setLineSeparator.  The default is '\n', So, For flink, we need set the LineSeparator to ''.


 Even we set a custom property, we also need set  the LineSeparator to '' when we handle the custom property.  So  just  make '' as an available charactor is a simple but effective way to solve this. 

Just like following check. we could allow user set  an ''.  It has the same result. 

 if (!delimiter.equals(""\n"") && !delimiter.equals(""\r"") && !delimiter.equals(""\r\n"")) {
				throw new IllegalArgumentException(
					""Unsupported new line delimiter. Only \\n, \\r, or \\r\\n are supported."");
			}

Do you think  the way of  opening the limit of setting '' is ok ? ;;;","19/Jul/19 07:31;twalthr;Yes, I think that is ok. Given that the tests pass. Feel free to open a PR that updates docs, descriptor, factory, and tests.;;;","25/Aug/19 07:15;chaiyq;#9529;;;","16/Oct/19 14:46;twalthr;Fixed in 1.10.0: ff249bce3dd4ef61518fb1e647e31e8f85640b61;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
UserDefinedFunctionUtils should distinguish overload any parameters methods in blink,FLINK-12975,13241425,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,25/Jun/19 09:02,25/Jun/19 15:08,13/Jul/23 08:10,25/Jun/19 15:08,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Some function like:
{code:java}
class TableFunc7 extends TableFunction[MyAnyObject3] {

  def eval(row: MyAnyObject1): Unit = {
  }

  def eval(row: jMyAnyObject2): Unit = {
  }
}{code}
UserDefinedFunctionUtils can't distinguish these two methods, and throw exception.",,lzljs3620320,,,,,,,,,,,,,"JingsongLi commented on pull request #8869: [FLINK-12975][table-planner-blink] UserDefinedFunctionUtils should distinguish overload any parameters methods in blink
URL: https://github.com/apache/flink/pull/8869
 
 
   
   ## What is the purpose of the change
   
   If user has a function overload two any parameters method, UserDefinedFunctionUtils can not distinguish them and throw an exception. We should fix it.
   
   ## Verifying this change
   
   ut
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jun/19 09:05;githubbot;600","KurtYoung commented on pull request #8869: [FLINK-12975][table-planner-blink] UserDefinedFunctionUtils should distinguish overload any parameters methods in blink
URL: https://github.com/apache/flink/pull/8869
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jun/19 15:07;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 25 15:08:34 UTC 2019,,,,,,,,,,"0|z042n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/19 15:08;ykt836;fixed in 1.9.0: 97d28761add07a1c3569254302a1705e8128f91c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure calling open/close in join condition of generated functions for blink,FLINK-12972,13241419,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,25/Jun/19 08:40,26/Jun/19 02:53,13/Jul/23 08:10,26/Jun/19 02:53,,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Generated functions' *open* method are not called in join operator. e.g. *AbstractStreamingJoinOperator*, *SortMergeJoinOperator*, *HashJoinOperator, LongHashJoinGenerator*.",,lzljs3620320,,,,,,,,,,,,,"JingsongLi commented on pull request #8868: [FLINK-12972][table-planner-blink] Ensure calling open/close in join condition of generated functions for blink
URL: https://github.com/apache/flink/pull/8868
 
 
   
   ## What is the purpose of the change
   
   Generated functions' open method are not called in join operator. e.g. AbstractStreamingJoinOperator, SortMergeJoinOperator, HashJoinOperator, LongHashJoinGenerator.
   
   ## Verifying this change
   
   ut
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jun/19 08:42;githubbot;600","KurtYoung commented on pull request #8868: [FLINK-12972][table-planner-blink] Ensure calling open/close in join condition of generated functions for blink
URL: https://github.com/apache/flink/pull/8868
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jun/19 02:52;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 26 02:53:07 UTC 2019,,,,,,,,,,"0|z042ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/19 02:53;ykt836;merged in 1.9.0: 57603790e7e5ed3b7530e26f28d8b6dfe20d6422;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix thrift and protobuf dependency examples in documentation,FLINK-12957,13241194,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nkruber,nkruber,nkruber,24/Jun/19 08:07,26/Jun/19 08:04,13/Jul/23 08:10,26/Jun/19 08:03,1.7.2,1.8.0,1.9.0,,,,,,,1.7.3,1.8.1,1.9.0,,Documentation,,,,,0,pull-request-available,,,,The examples in the docs are not up-to-date anymore and should be updated.,,nkruber,tzulitai,,,,,,,,,,,,"NicoK commented on pull request #8848: [FLINK-12957][docs] fix thrift and protobuf dependency examples
URL: https://github.com/apache/flink/pull/8848
 
 
   ## What is the purpose of the change
   
   This PR includes some fixes in the docs.
   
   ## Brief change log
   
   - ""data Artisans"" -> ""Ververica""
   - update latency metric name to cover for latency granularity
   - fix typo in metrics docs
   - fix thrift and protobuf dependency examples
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/19 08:10;githubbot;600","asfgit commented on pull request #8848: [FLINK-12957][docs] fix thrift and protobuf dependency examples
URL: https://github.com/apache/flink/pull/8848
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jun/19 03:35;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 26 08:03:52 UTC 2019,,,,,,,,,,"0|z04180:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/19 03:36;tzulitai;Merged for 1.9.0: 46f94c11945676814c3ecc66e32b4eb895c0404e;;;","26/Jun/19 08:03;nkruber;merged into:
- 1.8: 9ad7cda7a145537e2968416a361e0d22d85828e6
- 1.7: 5b8154c3c6c60ca8a850d82856db84d4334bc327;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lint-python.sh cannot find flake8,FLINK-12931,13240919,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,phoenixjiangnan,phoenixjiangnan,21/Jun/19 17:13,25/Jun/19 01:06,13/Jul/23 08:10,25/Jun/19 01:03,1.9.0,,,,,,,,,1.9.0,,,,API / Python,,,,,0,pull-request-available,,,,"Hi guys,

I tried to run tests for flink-python with {{./dev/lint-python.sh}} by following README. But it reported it couldn't find flake8, error as

{code:java}
./dev/lint-python.sh: line 490: /.../flink/flink-python/dev/.conda/bin/flake8: No such file or directory
{code}

I've tried {{./dev/lint-python.sh -f}}, also didn't work.

I suspect the reason may be that I already have an anaconda3 installed and it conflicts with the miniconda installed by flink-python somehow. I'm not fully sure about that.

If that's the reason, I think we need to try to resolve the conflict because anaconda is a pretty common package that developers install and use. We shouldn't require devs to uninstall their existing conda environment in order to develop flink-python and run its tests. It's better if flink-python can have a well isolated environment on machines.
",,dian.fu,phoenixjiangnan,sunjincheng121,,,,,,,,,,,"dianfu commented on pull request #8840: [FLINK-12931][python] Fix lint-python.sh cannot find flake8
URL: https://github.com/apache/flink/pull/8840
 
 
   ## What is the purpose of the change
   
   *The lint-python.sh will execute failed during installing flake8 when there is anaconda2/anaconda3 installed. We need to solve this issue because anaconda is a pretty common package that developers install and use. 
   The reason to this issue is that `conda install flake8` will try to install flake8 into the conda home which may be the conda installed by user, not the miniconda installed by lint-python.sh.*
   
   
   ## Brief change log
   
     - *Updates lint-python.sh to specify the conda home explicitly when executing `conda remove` and `conda install`*
   
   ## Verifying this change
   
   This change can be verified as follows:
   
     - *Executes ./dev/lint-python.sh and it will succeed*
     - *Installs anaconda2/anaconda3, starts a new shell and makes sure the newly installed anaconda2/anaconda3 is used.*
     - *Executes ./dev/lint-python.sh -f and it will fail without the changes of this PR*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jun/19 04:41;githubbot;600","sunjincheng121 commented on pull request #8840: [FLINK-12931][python] Fix lint-python.sh cannot find flake8
URL: https://github.com/apache/flink/pull/8840
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jun/19 01:06;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 25 01:03:52 UTC 2019,,,,,,,,,,"0|z03ziw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/19 01:03;sunjincheng121;Fixed in master:8d98223282e8479964f13d92c5e0a591c1a72c71;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scala.StreamExecutionEnvironment.addSource does not propagate TypeInformation,FLINK-12929,13240886,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,fabio.lombardelli@gmail.com,fabio.lombardelli@gmail.com,21/Jun/19 15:15,13/Nov/19 19:20,13/Jul/23 08:10,10/Jul/19 11:19,,,,,,,,,,1.9.0,,,,API / Scala,API / Type Serialization System,,,,0,pull-request-available,,,,"In {{scala.StreamExecutionEnvironment.addSource}} I would expect that {{typeInfo}} is also passed to the {{javaEnv.addSource}} as second parameter and not only passed to the {{returns}} method:
{code:java}
  def addSource[T: TypeInformation](function: SourceFunction[T]): DataStream[T] = {
    require(function != null, ""Function must not be null."")
    
    val cleanFun = scalaClean(function)
    val typeInfo = implicitly[TypeInformation[T]]
    asScalaStream(javaEnv.addSource(cleanFun, <missing typeInfo>).returns(typeInfo))
  }
{code}",,aljoscha,fabio.lombardelli@gmail.com,highfei2011@126.com,knaufk,twalthr,,,,,,,,,"lombarde commented on pull request #8914: [FLINK-12929] Pass TypeInformation in addSource
URL: https://github.com/apache/flink/pull/8914
 
 
   Co-authored-by: Georg Rollinger <georg.rollinger@posteo.net>
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   Fixes https://issues.apache.org/jira/browse/FLINK-12929:
   Changed scala.StreamExecutionEnvironment.addSource to pass the type info for a SourceFunction that does not extend ResultTypeQueryable.
   
   ## Brief change log
   
   see above
   
   ## Verifying this change
   
   No test added as scala/StreamExecutionEnvironment.scala seems to be not covered yet
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): no
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no
     - The serializers: no
     - The runtime per-record code paths (performance sensitive): no
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no
     - The S3 file system connector: no
   
   ## Documentation
   
     - Does this pull request introduce a new feature? no
     - If yes, how is the feature documented? not applicable
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Jun/19 11:31;githubbot;600","aljoscha commented on pull request #8914: [FLINK-12929] Pass TypeInformation in addSource
URL: https://github.com/apache/flink/pull/8914
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 11:19;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,FLINK-14757,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 10 11:19:20 UTC 2019,,,,,,,,,,"0|z03zbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/19 14:00;twalthr;This makes sense to me. However, we still need to check for the result type queryable interface in this method. Would you like to prepare a fix [~fabio.lombardelli@gmail.com]?;;;","10/Jul/19 11:19;aljoscha;Fixed on master in
7ea00ad9004f920e0c4c1b6ed906e14418951272;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docker embedded job end-to-end test fails,FLINK-12925,13240809,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,1u0,1u0,21/Jun/19 08:26,25/Jun/19 08:56,13/Jul/23 08:10,25/Jun/19 08:56,,,,,,,,,,,,,,Tests,,,,,0,,,,,"The test fails at the one of the Docker image build steps:
{code}
Step 16/22 : ADD $job_artifacts/* $FLINK_JOB_ARTIFACTS_DIR/
ADD failed: no source files were specified
{code}

Example nightly run that includes affected test:
https://travis-ci.org/apache/flink/jobs/548148440",,1u0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 25 08:56:51 UTC 2019,,,,,,,,,,"0|z03yuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/19 09:43;1u0;There is a fix embedded in [~knaufk]'s PR: https://github.com/apache/flink/pull/8741 (that also covers {{test_kubernetes_embedded_job.sh}}).;;;","25/Jun/19 08:56;1u0;Should be fixed with commit {{58c0cbc194ee673c4eef93cade63e992dd6ff089}} ( https://github.com/apache/flink/pull/8828);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop support for register_table_sink with field_names and field_types parameters,FLINK-12920,13240767,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,21/Jun/19 02:12,25/Jun/19 03:13,13/Jul/23 08:10,25/Jun/19 03:12,,,,,,,,,,1.9.0,,,,API / Python,,,,,0,pull-request-available,,,,"The following registerTableSink API in TableEnvironment is deprecated:
{code:java}
@Deprecated
void registerTableSink(String name, String[] fieldNames, TypeInformation<?>[] fieldTypes, TableSink<?> tableSink);
{code}
We can drop the support of it in Python Table API.",,dian.fu,sunjincheng121,,,,,,,,,,,,"dianfu commented on pull request #8817: [FLINK-12920][python] Drop support of register_table_sink with parameters field_names and field_types
URL: https://github.com/apache/flink/pull/8817
 
 
   ## What is the purpose of the change
   
   *The following API registerTableSink in TableEnvironment has been deprecated at Java side:*
   `@Deprecated
   void registerTableSink(String name, String[] fieldNames, TypeInformation<?>[] fieldTypes, TableSink<?> tableSink);`
   *We should drop support of it in Python Table API.*
   
   ## Brief change log
   
     - *Remove the parameters field_names and field_types in register_table_sink*
     - *Update TestTableSink to make it accept field_names and field_types as parameters and configure the Java TableSink during constructing the Java TableSink*
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as *StreamTableEnvironmentTests*.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jun/19 03:01;githubbot;600","asfgit commented on pull request #8817: [FLINK-12920][python] Drop support of register_table_sink with parameters field_names and field_types
URL: https://github.com/apache/flink/pull/8817
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jun/19 03:13;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 25 03:12:58 UTC 2019,,,,,,,,,,"0|z03ylk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/19 03:12;sunjincheng121;Fixed in master: 5d8054be418cafaff9753e85724aa8bff21574da;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-hive compilation error in travis,FLINK-12919,13240761,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,xleesf,xleesf,21/Jun/19 01:05,21/Jun/19 01:21,13/Jul/23 08:10,21/Jun/19 01:21,,,,,,,,,,,,,,Connectors / Hive,,,,,0,,,,,[https://api.travis-ci.org/v3/job/548270362/log.txt],,xleesf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 21 01:21:29 UTC 2019,,,,,,,,,,"0|z03yk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/19 01:21;xleesf;Need to rebased on master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyedComplexChainTest.testMigrationAndRestore failed on Travis,FLINK-12916,13240692,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,trohrmann,trohrmann,20/Jun/19 16:32,19/Jul/19 09:44,13/Jul/23 08:10,19/Jul/19 09:44,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Checkpointing,Tests,,,,0,pull-request-available,test-stability,,,"The test case {{KeyedComplexChainTest.testMigrationAndRestore}} failed on Travis because a Task received the cancellation from one of its inputs
{code}
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Task received cancellation from one of its inputs
	at org.apache.flink.streaming.runtime.io.BarrierBuffer.notifyAbortOnCancellationBarrier(BarrierBuffer.java:428)
	at org.apache.flink.streaming.runtime.io.BarrierBuffer.processCancellationBarrier(BarrierBuffer.java:327)
	at org.apache.flink.streaming.runtime.io.BarrierBuffer.pollNext(BarrierBuffer.java:208)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:102)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.pollNextNullable(StreamTaskNetworkInput.java:47)
	at org.apache.flink.streaming.runtime.io.StreamInputProcessor.processInput(StreamInputProcessor.java:128)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask.performDefaultAction(OneInputStreamTask.java:101)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:268)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:376)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:676)
	... 1 more
{code}

https://api.travis-ci.org/v3/job/548181384/log.txt",,gjy,klion26,liyu,sunhaibotb,trohrmann,yunta,,,,,,,,"Myasuka commented on pull request #8820: [FLINK-12916][tests] Retry cancelWithSavepoint on cancellation barrier in AbstractOperatorRestoreTestBase
URL: https://github.com/apache/flink/pull/8820
 
 
   ## What is the purpose of the change
   
   Add retry logical for checkpoint failure reason on cancellation barrier. 
   
   ## Brief change log
   
     - Add retry logical for checkpoint failure reason on cancellation barrier in `AbstractOperatorRestoreTestBase`.
   
   ## Verifying this change
   
   This change is already covered by existing tests, such as `AbstractOperatorRestoreTestBase`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jun/19 05:37;githubbot;600","zentol commented on pull request #8820: [FLINK-12916][tests] Retry cancelWithSavepoint on cancellation barrier in AbstractOperatorRestoreTestBase
URL: https://github.com/apache/flink/pull/8820
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jul/19 09:42;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-13014,FLINK-11440,FLINK-13212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 19 09:44:54 UTC 2019,,,,,,,,,,"0|z03y4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/19 21:15;trohrmann;The problem seems to be that we trigger a savepoint for the custom source at a point in time where the {{Task}} is in state {{RUNNING}} but the {{StreamTask}} is not yet running (the {{isRunning}} flag has not been set to {{true}}). If this is the case, then the {{StreamTask}} will send a {{CancelCheckpointMarker}} to its consumers which make the savepoint fail.;;;","03/Jul/19 12:39;trohrmann;Another instance: https://api.travis-ci.org/v3/job/553289256/log.txt;;;","07/Jul/19 14:38;gjy;Another instance? https://api.travis-ci.org/v3/job/555068837/log.txt;;;","07/Jul/19 17:23;yunta;Another instance [https://api.travis-ci.org/v3/job/554872881/log.txt];;;","09/Jul/19 18:11;trohrmann;Another instance: https://api.travis-ci.org/v3/job/556303448/log.txt;;;","10/Jul/19 07:52;pnowojski;Another instance: [https://api.travis-ci.com/v3/job/214409483/log.txt];;;","10/Jul/19 08:10;gjy;Another instance: https://api.travis-ci.org/v3/job/556308423/log.txt;;;","11/Jul/19 15:59;trohrmann;Another instance: https://api.travis-ci.com/v3/job/215094505/log.txt;;;","12/Jul/19 06:01;trohrmann;Another instance: https://api.travis-ci.org/v3/job/557572323/log.txt;;;","12/Jul/19 19:21;liyu;Marking as blocker since from comment history the case failed frequently thus will probably cause trouble for the testing of 1.9.0 release. And I could see the root cause already located and PR submitted with change not only in test classes.

[~tzulitai] [~ykt836] FYI.;;;","16/Jul/19 07:29;sunhaibotb;Another instance: [https://api.travis-ci.com/v3/job/216341547/log.txt];;;","19/Jul/19 09:44;chesnay;master: 3616f5afd0d686e549d0d8ff993d3cd2067b0c49 

1.9: b9c09d832333be925c2bc8da75a1efc48f804065 ;;;",,,,,,,,,,,,,,,,,,,,
AbstractOperatorRestoreTestBase can deadlock if one test fails,FLINK-12915,13240691,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunta,trohrmann,trohrmann,20/Jun/19 16:29,25/Jun/19 11:55,13/Jul/23 08:10,25/Jun/19 11:55,1.9.0,,,,,,,,,1.9.0,,,,Runtime / State Backends,Tests,,,,0,pull-request-available,test-stability,,,"The {{AbstractOperatorRestoreTestBase}} can deadlock in case of a failing test case. The problem is that if one test fails then the corresponding test job won't be canceled. Due to that a succeeding test case might not get enough slots to execute because the job of the failing test is still running. This leads to a deadlocked test as happened here: https://api.travis-ci.org/v3/job/548181384/log.txt

A side effect is that the original test failure won't be properly displayed.",,trohrmann,,,,,,,,,,,,,"Myasuka commented on pull request #8812: [FLINK-12915][tests] Fix AbstractOperatorRestoreTestBase deadlock of ont test fails
URL: https://github.com/apache/flink/pull/8812
 
 
   ## What is the purpose of the change
   
   Fix the deadlock problem when  one of `AbstractOperatorRestoreTestBase` test fails.
   
   ## Brief change log
     - Fix the deadlock problem when  one of `AbstractOperatorRestoreTestBase` test fails.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jun/19 18:44;githubbot;600","Myasuka commented on pull request #8812: [FLINK-12915][tests] Fix AbstractOperatorRestoreTestBase deadlock of ont test fails
URL: https://github.com/apache/flink/pull/8812
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jun/19 12:37;githubbot;600","Myasuka commented on pull request #8812: [FLINK-12915][tests] Fix AbstractOperatorRestoreTestBase deadlock of ont test fails
URL: https://github.com/apache/flink/pull/8812
 
 
   ## What is the purpose of the change
   
   Fix the deadlock problem when  one of `AbstractOperatorRestoreTestBase` test fails.
   
   ## Brief change log
     - Fix the deadlock problem when  one of `AbstractOperatorRestoreTestBase` test fails.
   
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jun/19 12:37;githubbot;600","zentol commented on pull request #8812: [FLINK-12915][tests] Fix AbstractOperatorRestoreTestBase deadlock of ont test fails
URL: https://github.com/apache/flink/pull/8812
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Jun/19 11:55;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 25 11:55:14 UTC 2019,,,,,,,,,,"0|z03y4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/19 11:55;chesnay;master: 08d8cf5b8dff8193c69c5dd3657112ce862054ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the Python catalog test issue,FLINK-12910,13240634,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dian.fu,dian.fu,dian.fu,20/Jun/19 11:58,20/Jun/19 13:22,13/Jul/23 08:10,20/Jun/19 13:21,,,,,,,,,,1.9.0,,,,API / Python,,,,,0,pull-request-available,,,,"self = <pyflink.table.tests.test_catalog.CatalogTestBase testMethod=test_table_exists>

 

   def test_table_exists(self):

     self.catalog.create_database(self.db1, self.create_db(), False)

 

pyflink/table/tests/test_catalog.py:491: 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

 

   @staticmethod

   def create_db():

       gateway = get_gateway()

     j_database = gateway.jvm.GenericCatalogDatabase(\{""k1"": ""v1""}, CatalogTestBase.test_comment)

E       TypeError: 'JavaPackage' object is not callable

 

 

pyflink/table/tests/test_catalog.py:78: TypeError",,dian.fu,sunjincheng121,,,,,,,,,,,,"dianfu commented on pull request #8807: [FLINK-12910][python][tests] Fix the Python catalog test failure
URL: https://github.com/apache/flink/pull/8807
 
 
   ## What is the purpose of the change
   
   *This pull request fixes the python catalog test failure.*
   
   
   ## Brief change log
   
     - *fixes the python catalog test failure*
   
   
   ## Verifying this change
   
   This change is a trivial rework without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jun/19 12:21;githubbot;600","asfgit commented on pull request #8807: [FLINK-12910][python][tests] Fix the Python catalog test failure
URL: https://github.com/apache/flink/pull/8807
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jun/19 13:22;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 20 13:21:43 UTC 2019,,,,,,,,,,"0|z03xs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/19 12:00;dian.fu;This issue is introduced in [https://github.com/apache/flink/pull/8786]. Will provide a fix ASAP.;;;","20/Jun/19 13:21;sunjincheng121;Fixed in master : 7d8869d7b1a0d08b3ac0a4e3e5bbe23b41b2cb57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-table-planner-blink does not compile on master,FLINK-12908,13240593,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,godfreyhe,dwysakowicz,dwysakowicz,20/Jun/19 07:14,20/Jun/19 07:39,13/Jul/23 08:10,20/Jun/19 07:39,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,,,,,"07:10:08.691 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/plan/util/RexNodeExtractor.scala:24: error: object BuiltInFunctionDefinitions is not a member of package org.apache.flink.table.expressions
07:10:08.691 [ERROR] import org.apache.flink.table.expressions.BuiltInFunctionDefinitions._
07:10:08.691 [ERROR]

https://travis-ci.org/apache/flink/jobs/548050227",,dwysakowicz,godfreyhe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 20 07:39:51 UTC 2019,,,,,,,,,,"0|z03xj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/19 07:14;dwysakowicz;cc [~godfreyhe] [~ykt836];;;","20/Jun/19 07:33;godfreyhe;fix: https://github.com/apache/flink/pull/8802;;;","20/Jun/19 07:39;ykt836;fixed by: 564b80e85237e6356611310470fd0ebd3a790442;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-table-planner-blink fails to compile with scala 2.12,FLINK-12907,13240434,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jark,dwysakowicz,dwysakowicz,19/Jun/19 14:21,20/Jun/19 01:32,13/Jul/23 08:10,20/Jun/19 01:32,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Planner,,,,,0,,,,,"{code}
14:03:15.204 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/plan/nodes/resource/ExecNodeResourceTest.scala:269: error: overriding method getOutputType in trait TableSink of type ()org.apache.flink.api.common.typeinfo.TypeInformation[org.apache.flink.table.dataformat.BaseRow];
14:03:15.204 [ERROR]  method getOutputType needs `override' modifier
14:03:15.204 [ERROR]   @deprecated def getOutputType: TypeInformation[BaseRow] = {
14:03:15.204 [ERROR]                   ^
14:03:15.217 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/plan/nodes/resource/ExecNodeResourceTest.scala:275: error: overriding method getFieldNames in trait TableSink of type ()Array[String];
14:03:15.217 [ERROR]  method getFieldNames needs `override' modifier
14:03:15.217 [ERROR]   @deprecated def getFieldNames: Array[String] = schema.getFieldNames
14:03:15.217 [ERROR]                   ^
14:03:15.219 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/plan/nodes/resource/ExecNodeResourceTest.scala:280: error: overriding method getFieldTypes in trait TableSink of type ()Array[org.apache.flink.api.common.typeinfo.TypeInformation[_]];
14:03:15.219 [ERROR]  method getFieldTypes needs `override' modifier
14:03:15.219 [ERROR]   @deprecated def getFieldTypes: Array[TypeInformation[_]] = schema.getFieldTypes
{code}

https://api.travis-ci.org/v3/job/547655787/log.txt",,dwysakowicz,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 20 01:32:52 UTC 2019,,,,,,,,,,"0|z03wjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/19 01:32;jark;Fixed in 1.9.0: 671ac182e514500c5f2b430877c6ac30b26e6ec7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskCheckpointStatisticDetailsHandler uses wrong value for JobID when archiving,FLINK-12896,13240363,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xymaqingxiang,xymaqingxiang,xymaqingxiang,19/Jun/19 07:43,02/Oct/19 17:48,13/Jul/23 08:10,20/Jun/19 08:38,1.7.2,1.8.0,1.9.0,,,,,,,1.7.3,1.8.1,1.9.0,,Runtime / REST,,,,,0,pull-request-available,,,,"There are 2 bugs, as follows:

1. could not found the checkpoint details for subtasks.

!image-2019-06-19-15-32-15-994.png!

2. The jobs directory has an exception: job directory, the ArchivedJson we get in FsJobArchivist is wrong.

!image-2019-06-19-15-40-08-481.png!

!image-2019-06-19-15-41-48-051.png!

 ",,xymaqingxiang,,,,,,,,,,,,,"maqingxiang commented on pull request #8801: [hotfix][FLINK-12896][HistoryServer] modify :jobId key in TaskCheckpointStatisticDetailsHandler
URL: https://github.com/apache/flink/pull/8801
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jun/19 04:17;githubbot;600","maqingxiang commented on pull request #8801: [hotfix][FLINK-12896][HistoryServer] modify :jobId key in TaskCheckpointStatisticDetailsHandler for History Server
URL: https://github.com/apache/flink/pull/8801
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jun/19 09:36;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/19 07:32;xymaqingxiang;image-2019-06-19-15-32-15-994.png;https://issues.apache.org/jira/secure/attachment/12972185/image-2019-06-19-15-32-15-994.png","19/Jun/19 07:39;xymaqingxiang;image-2019-06-19-15-40-08-481.png;https://issues.apache.org/jira/secure/attachment/12972183/image-2019-06-19-15-40-08-481.png","19/Jun/19 07:41;xymaqingxiang;image-2019-06-19-15-41-48-051.png;https://issues.apache.org/jira/secure/attachment/12972182/image-2019-06-19-15-41-48-051.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 20 08:38:00 UTC 2019,,,,,,,,,,"0|z03w40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/19 07:49;xymaqingxiang;[~chesnay] [~klion26] 
I'm sorry to bother you, please check it. Thank you very much.;;;","20/Jun/19 08:38;chesnay;master: 95a0cb7e93c98c5212c9b482f14fa4cea2656fe3
1.8: 9855ca8f04974af5089a8a5f4b2c8c898393ff5b
1.7: 0abc0b6e474eb60ebae8755270a8c04f48e98499 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerProcessFailureBatchRecoveryITCase.testTaskManagerProcessFailure failed on travis ,FLINK-12895,13240355,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,sunhaibotb,sunhaibotb,19/Jun/19 06:40,21/Jun/19 08:44,13/Jul/23 08:10,21/Jun/19 07:32,1.9.0,,,,,,,,,1.7.3,1.8.1,1.9.0,,Runtime / Coordination,Tests,,,,0,test-stability,,,,"Logs:  [https://api.travis-ci.org/v3/job/547509708/log.txt]

Build: [https://travis-ci.org/apache/flink/builds/547509701]",,sunhaibotb,trohrmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11059,,,,FLINK-12863,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 21 08:44:08 UTC 2019,,,,,,,,,,"0|z03w28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/19 16:45;trohrmann;Another instance https://api.travis-ci.org/v3/job/548180557/log.txt

This instance might have failed because of a different reason because it occurred with the fix for FLINK-12863.;;;","20/Jun/19 17:13;trohrmann;Another instance without the FLINK-12863 fix: https://api.travis-ci.org/v3/job/548102166/log.txt;;;","20/Jun/19 23:12;trohrmann;I looked into the problem and I think that the original problem has been caused by FLINK-11059 and should be fixed with FLINK-12863.

The reason why the test case also failed with FLINK-12863 was because I tried to harden the test case {{TaskExecutorTest#testSyncSlotsWithJobMasterByHeartbeat}} by directly executing the response from {{JobMasterGateway#offerSlots}} in the {{TaskExecutor}}. This was the wrong approach since it changed the order between heartbeats and slot offering response.;;;","21/Jun/19 07:32;trohrmann;Should hopefully been fixed with

1.9.0: a95dac57ef0e1949fd4751ca19350da96c3bf52f
1.8.1: 55c8a69cfa4d40ef2863987eb89adb08f0c45dda
1.7.3: 7333b619fdf3443e179b3f6e8d3147ab4946f91c

If not, then we have to reopen this issue.;;;","21/Jun/19 08:44;sunhaibotb;(y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Job keeps in FAILING state,FLINK-12889,13240208,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,xinpu,xinpu,18/Jun/19 13:37,05/Jul/19 08:42,13/Jul/23 08:10,05/Jul/19 08:42,1.7.2,1.8.1,1.9.0,,,,,,,1.7.3,1.8.2,1.9.0,,Runtime / Task,,,,,0,pull-request-available,,,,"There is a topology of 3 operator, such as, source, parser, and persist. Occasionally, 5 subtasks of the source encounters exception and turns to failed, at the same time, one subtask of the parser runs into exception and turns to failed too. The jobmaster gets a message of the parser's failed. The jobmaster then try to cancel all the subtask, most of the subtasks of the three operator turns to canceled except the 5 subtasks of the source, because the state of the 5 ones is already FAILED before jobmaster try to cancel it. Then the jobmaster can not reach a final state but keeps in  Failing state meanwhile the subtask of the source kees in canceling state. 
 
The job run on a flink 1.7 cluster on yarn, and there is only one tm with 10 slots.
 
The attached files contains a jm log , tm log and the ui picture.
 
The exception timestamp is about 2019-06-16 13:42:28.",,knaufk,limbo,trohrmann,xinpu,xtsong,xymaqingxiang,zjwang,,,,,,,"tillrohrmann commented on pull request #8948: [FLINK-12889] Set FatalExitExceptionHandler for StreamTask#asyncOperationsThreadPool
URL: https://github.com/apache/flink/pull/8948
 
 
   ## What is the purpose of the change
   
   In order to avoid the swallowing of uncaught exceptions in asynchronous checkpoint operations,
   this commit sets the FatalExitExceptionHandler for the StreamTask#asyncOperationsThreadPool.
   
   For testing purposes the uncaught exception handler was made configurable in the StreamTask.
   
   ## Verifying this change
   
   Added `StreamTaskTest#testUncaughtExceptionInAsynchronousCheckpointingOperation`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jul/19 13:14;githubbot;600","tillrohrmann commented on pull request #8950: [FLINK-12889] Set FatalExitExceptionHandler for StreamTask#asyncOperationsThreadPool
URL: https://github.com/apache/flink/pull/8950
 
 
   Backport of #8948 for `release-1.7`. This PR does not contain the test because it would have been too much work to backport it.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jul/19 13:24;githubbot;600","tillrohrmann commented on pull request #8948: [FLINK-12889] Set FatalExitExceptionHandler for StreamTask#asyncOperationsThreadPool
URL: https://github.com/apache/flink/pull/8948
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Jul/19 08:42;githubbot;600",,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/19 13:36;xinpu;20190618104945417.jpg;https://issues.apache.org/jira/secure/attachment/12972098/20190618104945417.jpg","18/Jun/19 13:36;xinpu;jobmanager.log.2019-06-16.0;https://issues.apache.org/jira/secure/attachment/12972099/jobmanager.log.2019-06-16.0","18/Jun/19 13:37;xinpu;taskmanager.log;https://issues.apache.org/jira/secure/attachment/12972100/taskmanager.log",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 05 08:42:25 UTC 2019,,,,,,,,,,"0|z03v5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/19 05:31;zjwang;My previous analysis in ML was as follows:
  
 Actually all the five ""Source: ServiceLog"" tasks are not in terminal state on JM view, the relevant processes shown in below: 
 * The checkpoint in task causes OOM issue which would call `Task#failExternally` as a result, we could see the log ""Attempting to fail task externally"" in TM.
 * The source task would transform state from RUNNING to FAILED and then starts a canceler thread for canceling task, we could see log ""Triggering cancellation of task"" in TM.
 * When JM starts to cancel the source tasks, the rpc call `Task#cancelExecution` would find the task was already in FAILED state as above step 2, we could see log ""Attempting to cancel task"" in TM.

 
 At last all the five source tasks are not in terminal states from jm log, I guess the step 2 might not create canceler thread successfully, because the root failover was caused by OOM during creating native thread in step1, so it might exist possibilities that createing canceler thread is not successful as well in OOM case which is unstable. If so, the source task would not been interrupted at all, then it would not report to JM as well, but the state is already changed to FAILED before. 
  
 For the other vertex tasks, it does not trigger `Task#failExternally` in step 1, and only receives the cancel rpc from JM in step 3. And I guess at this time later than the source period, the canceler thread could be created succesfully after some GCs, then these tasks could be canceled as reported to JM side.
  
 I think the key problem is under OOM case some behaviors are not within expectations, so it might bring problems. Maybe we should handle OOM error in extreme way like making TM exit to solve the potential issue.
  
 [~till.rohrmann] do you think it is worth fixing or have other concerns?;;;","02/Jul/19 09:03;trohrmann;Thanks for reporting this issue [~xinpu] and the analysis [~zjwang]. I think your analysis is correct. The problem is that we don't catch the last OOM when creating the task canceller thread. This should indeed kill the TM process because it is no longer guaranteed that this Flink process works correctly.

I would suggest to create the {{StreamTask#asyncOperationsThreadPool}} with a {{FatalExitExceptionHandler}} as an uncaught exception handler. This should cause the TM to exit in a situation you've described [~xinpu].;;;","05/Jul/19 08:42;trohrmann;Fixed via
1.9.0: 28ac5ef7f1769e9256b3089ed4853736771ae457
1.8.2: fbfe7d4db8242e4db3c779b3526f6eaa0c599e40
1.7.3: 7aad9649b83387f86d076014654f3af091640c05;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong SSL setup examples in docs,FLINK-12871,13239914,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nkruber,nkruber,nkruber,17/Jun/19 11:34,17/Jun/19 22:49,13/Jul/23 08:10,17/Jun/19 22:49,1.7.2,1.8.0,1.9.0,,,,,,,1.7.3,1.8.1,1.9.0,,Documentation,,,,,0,pull-request-available,,,,"The SSL setup examples [1] were updated to rely on PKCS12 format (instead of the old JKS keystore) but PKCS12 does not support separate passwords for the key store and the key itself.

{code}
> Warning:  Different store and key passwords not supported for PKCS12 KeyStores. Ignoring user-specified -keypass value.
{code}


Also, some of the examples still rely on the old JKS keystore and are not using PKCS12 yet.


[1] https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/security-ssl.html#example-ssl-setup-standalone-and-kubernetes",,nkruber,,,,,,,,,,,,,"NicoK commented on pull request #8767: [FLINK-12871][docs] fix separate keypass not compatible with PKCS12 stores
URL: https://github.com/apache/flink/pull/8767
 
 
   ## What is the purpose of the change
   
   The SSL setup examples were updated to rely on PKCS12 format in 6f9aa832f85447e38e265d413aff829daadcecc4 (instead of the old JKS keystore) but PKCS12 does not support separate passwords for the key store and the key itself.
   
   ```
   Warning:  Different store and key passwords not supported for PKCS12 KeyStores. Ignoring user-specified -keypass value.
   ```
   
   Also, some of the examples still rely on the old JKS keystore and are not using PKCS12 yet.
   
   Once approved, this PR should also make it into the 1.7 and 1.8 docs.
   
   ## Brief change log
   
   - remove separate `keypass` entries from the examples
   - add PKCS12 format to the internal key (store) generation
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage. I manually tested the following variants:
   - internal with PKCS12
   - internal with JKS keystore
   - self-signed REST, PKCS12
   - self-signed REST, JKS keystore
   - REST with CA, PKCS12
   - REST with CA, JKS keystore
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jun/19 13:50;githubbot;600","NicoK commented on pull request #8767: [FLINK-12871][docs] fix separate keypass not compatible with PKCS12 stores
URL: https://github.com/apache/flink/pull/8767
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jun/19 15:11;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 17 22:49:08 UTC 2019,,,,,,,,,,"0|z03tcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/19 22:49;nkruber;Fixed via:
 * master: 59b9a938862124050de56970a4b618a82c831943
 * release-1.8: 0b2525734ea920a5347f9dee915b684aee98a702
 * release-1.7: 272fafe66830a99e99dffdc42ea27c00a6cc8a5e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn and Mesos clusters can not be deployed if plugins dir does not exist,FLINK-12868,13239869,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,pnowojski,pnowojski,17/Jun/19 07:05,21/Jun/19 14:04,13/Jul/23 08:10,21/Jun/19 14:04,1.9.0,,,,,,,,,1.9.0,,,,Deployment / Mesos,Deployment / YARN,,,,0,pull-request-available,,,,"{noformat}
-----------------------------------------------------------
 The program finished with the following exception:\\n\\norg.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster\\n\\t
at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploySessionCluster(AbstractYarnClusterDescriptor.java:385)\\n\\t
at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:616)\\n\\t
at org.apache.flink.yarn.cli.FlinkYarnSessionCli.lambda$main$3(FlinkYarnSessionCli.java:844)\\n\\t
at java.security.AccessController.doPrivileged(Native Method)\\n\\tat javax.security.auth.Subject.doAs(Subject.java:422)\\n\\t
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)\\n\\t
at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)\\n\\t
at org.apache.flink.yarn.cli.FlinkYarnSessionCli.main(FlinkYarnSessionCli.java:844)
Caused by: org.apache.flink.yarn.AbstractYarnClusterDescriptor$YarnDeploymentException: The environment variable 'FLINK_PLUGINS_DIR' is set to '/opt/flink/plugins' but the directory doesn't exist. 
at org.apache.flink.yarn.AbstractYarnClusterDescriptor.addEnvFolderToShipFiles(AbstractYarnClusterDescriptor.java:1540)
at org.apache.flink.yarn.AbstractYarnClusterDescriptor.addEnvironmentFoldersToShipFiles(AbstractYarnClusterDescriptor.java:1531)
at org.apache.flink.yarn.AbstractYarnClusterDescriptor.startAppMaster(AbstractYarnClusterDescriptor.java:704)
at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deployInternal(AbstractYarnClusterDescriptor.java:507)
at 
{noformat}
",,pnowojski,,,,,,,,,,,,,"pnowojski commented on pull request #8759: [FLINK-12868][yarn] Fix yarn cluster can not be deployed if plugins dir does not exist
URL: https://github.com/apache/flink/pull/8759
 
 
   Plugins dir should be optional and cluster deployment should not fail if it doesn't exist.
   
   ## Verifying this change
   
   This PR adds a new unit test to add better coverage (apart of end to end/integration tests) for this bug.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no**)**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / no ****/ don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (**yes** / no / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jun/19 08:08;githubbot;600","pnowojski commented on pull request #8759: [FLINK-12868][yarn] Fix yarn cluster can not be deployed if plugins dir does not exist
URL: https://github.com/apache/flink/pull/8759
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jun/19 14:02;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-06-17 07:05:29.0,,,,,,,,,,"0|z03t2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State inconsistency between RM and TM on the slot status,FLINK-12865,13239849,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,trohrmann,gaoyunhaii,gaoyunhaii,17/Jun/19 03:49,21/Jun/19 07:31,13/Jul/23 08:10,21/Jun/19 07:31,,,,,,,,,,1.7.3,1.8.1,1.9.0,,Runtime / Coordination,,,,,0,,,,,"There may be state inconsistency between TM and RM due to race condition and message loss:
 # When TM sends heartbeat, it retrieve SlotReport in the main thread, but sends the heartbeat in another thread. There may be cases that the slot on TM is FREE initially and SlotReport read the FREE state, then RM requests slot and mark the slot as allocated, and the SlotReport finally override the allocated status at the RM side wrongly.
 # When RM requests slot, TM received the requests but the acknowledge message get lot. Then RM will think this slot is free. 

 Both the problems may cause RM marks an ALLOCATED slot as FREE. This may currently cause additional retries till the state is synchronized after the next heartbeat, and for the inaccurate resource statistics for the fine-grained resource management in the future.",,gaoyunhaii,klion26,liyu,shixg,sunjincheng121,tison,trohrmann,xleesf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-12863,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 21 07:31:45 UTC 2019,,,,,,,,,,"0|z03syg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/19 04:15;gaoyunhaii;I think adding a version to the slot may solves this problem.
 # Add a SYNCING status in the RM side. SYNCING means request is sent to TM but not knowing the result of the request. A slot with SYNCING status cannot be allocated to others.
 # RM and TM maintains a version for each slot, and the version starts from 0.
 # Whenever RM requests slot, it add the version by 1, and send the requests to TM. TM will only do allocation when RM's version > TM's version. 
 # TM will also attach the version in the HeartBeat and RM will only accept the slot status when the TM's version >= RM's version.
 # If the SYNCING status keeps too long time, the request will be resent.

The version method is a simplified solution of the full vector clock based state management. In the full vector-clock design, the version should be a vector represents (RM's version, TM's version). Whenever RM modify the slot's status (requestSlot) and TM modify the slot's status (freeSlot), It need to first increase the corresponding component and send the sync messages, and the messages can only be accepted when the vector version >= messages' vector version.

However, since for the status of slot TM will only modify its status when freeing slots, we can ignore the component of TM's side will only cause a freed slot be marked as allocated, this will not cause error, and the free status can be finally updated to RM with Heartbeat message.;;;","17/Jun/19 08:57;sunjincheng121;Hi [~gaoyunhaii], Thanks for report this issue and help to fix it! :)

I want to know is there any abnormal information? If I understand correctly that it should not happen frequently. right?

The reason I asked this question is that I want the evaluator to be a blocker released in 1.8.1.  If so, we need to fix it as soon as possible and mark it as Critical.

What do you think? [~gaoyunhaii]  [~till.rohrmann];;;","17/Jun/19 15:10;trohrmann;Thanks for opening this issue [~gaoyunhaii].

I can see how the first problem occurs:

1. Create {{SlotReport}} with {{SlotStatus(0, free)}}
2. Receive {{TaskExecutor#requestSlot(0, allocationId)}}
3. Acknowledge requestSlot and update slot state in {{TaskManagerSlot}} to {{ALLOCATED}}
4. Send {{SlotReport}} with free slot to RM because it is sent from different thread

However, I do not fully understand the second problem. If the message is lost (which should actually not happen with TCP as the underlying transport), then https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java#L801 should either time out and trigger a retry or the heartbeat report should complete the pending slot request.

Concerning [~xiaogang.shi] [comment|https://issues.apache.org/jira/browse/FLINK-12863?focusedCommentId=16865239&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16865239], I'm not quite sure whether I fully understand it. I think if you have a pending slot request assigned to a slot and you receive a {{SlotStatus}} which says that this slot is {{FREE}}, then you don't cancel an assigned pending slot request: https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java#L625. Maybe you can write me the exact sequence of steps to produce the problematic case.

Instead of adding the versioning as a solution, I was wondering whether a simple fix couldn't be removing the concurrency from the {{HeartbeatManager}}. I think this is currently causing the problems because we can generate a {{SlotReport}} for an earlier point of time and send it back at a later point in time when the state of the {{TaskExecutor}} has already changed. Executing all {{HeartbeatManager}} related calls in the actor's main thread should, thus, solve this problem.

There should also be no problem with the {{PromiseActorRef}} [~xiaogang.shi] because the heartbeat response will be directly sent back to the {{ResourceManager}}. Since we have [causal transitive ordering|https://doc.akka.io/docs/akka/2.5.4/scala/general/message-delivery-reliability.html] of messages the following sequence cannot happen:

1. Create & send {{SlotReport(SlotStatus(0, free))}}
2. Process {{requestSlot}} on TM
3. Update slot status in main thread after receiving response of {{requestSlot}} on RM
4. Process heartbeat response with {{SlotReport(SlotStatus(0, free))}} on RM

What still can happen is the following sequence because we receive the result from {{TaskExecutorGateway#requestSlot}} via a {{PromiseActorRef}}:

1. Process {{requestSlot}} on TM
2. Create & send {{SlotReport(SlotStatus(0, allocated))}} on TM
3. Process heartbeat response with {{SlotReport(SlotStatus(0, allocated))}} on RM
4. Update slot status in main thread after receiving response of {{requestSlot}} on RM

This should, however, not be a problem because this would simply complete the pending slot request earlier.;;;","18/Jun/19 02:10;shixg;[~till.rohrmann]You are right that there is no problem with the postponed handling of slot requests. I revisited the code and found that we do use ask to send heartbeat requests, but the responses are not sent back to {{PromiseActorRef}}. Instead, they are sent back directly to RM with a separate RPC method. So the handling of the heartbeat reponses will not be postponed. 

After revisiting the code, it seems sending heartbeats in the main thread will fix the problem.

Thanks a lot for your explanation and sorry for my misleading information.;;;","18/Jun/19 03:51;gaoyunhaii;[~till.rohrmann] Hi Till, thanks a lot for the explanation and sorry for not being accurate for the second problem. Although TCP handles retransmission, we may encounter  AkkaAskTimeoutException before the retransmission succeeds. Then during the processing of the exception in the SlotManager, we mark the slot as FREE. However, since TM has received the requests, it will mark the slot as ALLOCATED, thus cause the inconsistency. But I also think that it only happens in rare scenarios.

For both problems, we end up with RM marking a slot as FREE while TM marking a slot as allocated. Currently it will not cause serious problem. The next heartbeat will sync the right status to RM. During this period, although RM may also assign this slot to other requests, the requestSlot to TM will fail with SlotOccupiedException due to the slot has been allocated to the previous request. RM can handle this exception rightly. Since It will only cause some addition retries and its frequency is not high, I think this issue may not be a blocker of 1.8.1 [~sunjincheng121], and I agree that sending heartbeats in the main threads is enough since it addresses most cases of FLINK-12865 and FLINK-12863 uniformly.

However, if in the future if we have other functions that rely on the status of the slots (For example, if we use bookkeeping method to maintain the remaining resources on each TM with fine-grained resources and update the resources according to the slots' status, the inconsistency may cause temporarily overestimating the remaining resources of some TM, and scheduling some Tasks to TM without enough resources), I think this issue may requires more thoughts. Since it is not the case for now, I also think that we would not need to take these cases into consideration right now. ;;;","18/Jun/19 09:06;trohrmann;Thanks for the clarification [~gaoyunhaii]. I think you are right that we always need to handle the case of timeouts. In this case the assumption is that the request failed and that we need to retry. I'm not sure what else we can do against timeouts. The only thing I can think of is to assume the success case which should avoid that we overestimate the available set of cluster resources. But this would require some changes to the slot allocation protocol because we would also need to notify the allocation failure to the JM so that it can handle it.

At the moment I would suggest that we go for the simple solution to make the {{HeartbeatManagerImpls}} single threaded. Hence, this issue should be fixed with the same fix as for FLINK-12863. I'm currently working on it and hope to open a PR today or tomorrow.;;;","21/Jun/19 07:31;trohrmann;Fixed via

1.9.0: a95dac57ef0e1949fd4751ca19350da96c3bf52f
1.8.1: 55c8a69cfa4d40ef2863987eb89adb08f0c45dda
1.7.3: 7333b619fdf3443e179b3f6e8d3147ab4946f91c;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Race condition between slot offerings and AllocatedSlotReport,FLINK-12863,13239796,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,trohrmann,trohrmann,trohrmann,16/Jun/19 13:59,27/Mar/20 13:51,13/Jul/23 08:10,21/Jun/19 07:31,1.9.0,,,,,,,,,1.7.3,1.8.1,1.9.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,"With FLINK-11059 we introduced the {{AllocatedSlotReport}} which is used by the {{TaskExecutor}} to synchronize its internal view on slot allocations with the view of the {{JobMaster}}. It seems that there is a race condition between offering slots and receiving the report because the {{AllocatedSlotReport}} is sent by the {{HeartbeatManagerSenderImpl}} from a separate thread. 

Due to that it can happen that we generate an {{AllocatedSlotReport}} just before getting new slots offered. Since the report is sent from a different thread, it can then happen that the response to the slot offerings is sent earlier than the {{AllocatedSlotReport}}. Consequently, we might receive an outdated slot report on the {{TaskExecutor}} causing active slots to be released.

In order to solve the problem I propose to add a fencing token to the {{AllocatedSlotReport}} which is being updated whenever we offer new slots to the {{JobMaster}}. When we receive the {{AllocatedSlotReport}} on the {{TaskExecutor}} we compare the current slot report fencing token with the received one and only process the report if they are equal. Otherwise we wait for the next heartbeat to send us an up to date {{AllocatedSlotReport}}.",,dian.fu,gaoyunhaii,klion26,liyu,shixg,sunjincheng121,tiemsn,tison,trohrmann,xleesf,,,,"tillrohrmann commented on pull request #8783: [FLINK-12863][FLINK-12865] Remove concurrency from HeartbeatManager(Sender)Impl
URL: https://github.com/apache/flink/pull/8783
 
 
   ## What is the purpose of the change
   
   This commit makes the HeartbeatManager implementations to use the RpcEndpoint's
   main thread executor in order to remove concurrency. Furthermore, this commit changes the HeartbeatListener interface to directly return a payload instead of returning a future when
   HeartbeatListener#retrievePayload is called.
   
   Since the HeartbeatManager implementations now use the RpcEndpoint's main thread,
   we remove the splicing into the RpcEndpoint's main thread in the implementations
   of the HeartbeatListeners in the TaskExecutor, JobMaster and ResourceManager
   components.
   
   cc @azagrebin
   
   ## Brief change log
   
   * Remove future executor from HeartbeatManager implementations
   * Use the components main thread executor as the `ScheduledExecutor` for the `HeartbeatManagerImpl` in the `JobMaster`, `TaskExecutor` and `ResourceManager` component.
   * Change return type of `HeartbeatListener#retrievePayload` to synchronously return the payload. This avoids another source of concurrency.
   
   ## Verifying this change
   
   Exclude the last two commits and run `JobMasterTest#testAllocatedSlotReportDoesNotContainStaleInformation` as well as `TaskExecutorTest#testSlotReportDoesNotContainStaleInformation` in a loop. They should eventually fail.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jun/19 14:32;githubbot;600","tillrohrmann commented on pull request #8783: [FLINK-12863][FLINK-12865] Remove concurrency from HeartbeatManager(Sender)Impl
URL: https://github.com/apache/flink/pull/8783
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jun/19 07:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,FLINK-9417,FLINK-9827,,,,,FLINK-11059,,,,,,,FLINK-12865,FLINK-12895,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 21 07:31:20 UTC 2019,,,,,,,,,,"0|z03smo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/19 14:04;trohrmann;This race condition causes the {{YARNSessionCapacitySchedulerITCase.perJobYarnClusterWithParallelism}} to fail. An instance of this test failure can be found here https://api.travis-ci.org/v3/job/546108501/log.txt.;;;","17/Jun/19 02:24;shixg;I think a similar problem happens in the heartbeats between RM and TM. 

When a RM receives a slot request from JM, it will find an available slot, mark it as pending, and send a slot request to TM. In the cases where the slot request is following a heartbeat request, RM will receive the heartbeat response first and will remove the pending slot. RM may reuse the slot when it receives a new slot request from JM, leading to duplicated slot allocation. 

A solution proposed by [~yungao.gy] is using version numbers. Each slot is equipped with a version number, which is increased once a new pending request is generated. These version numbers then are attached to the heartbeats sent to TM. Once a heartbeat response is received, we don't need to remove those pending slot requests whose version numbers are greater than those of heartbeats.

I think the solution can also work here. What do you think? [~yungao.gy][~till.rohrmann];;;","17/Jun/19 02:55;shixg;Btw, i want to note that the race condition may not necessarily be caused by {{HeartbeatManagerSenderImpl}} sending heartbeats in a seperate thread. It can solve the problem in JM, but not the one in RM.

Even when RM send heartbeat requests in the main thread, right after a slot request, the heartbeart responses may be handled first by RM. It's because RM uses ask to send both heartbeat and slot requests. Temporary {{PromiseActor}}s will be created to receive responses from TM. Since there is no guarantee on the execution order of actors, the {{PromiseActor}} which receives response first may be executed later.

;;;","17/Jun/19 03:23;tiemsn;Hi [~till.rohrmann], as [~xiaogang.shi] said, we found the same race condition between RM and TM, and adding a version in each slot to solve it. I think adding fencing token to AllocatedSlotReport may be have some defects. Considering how would you update the fencing token? When offering slots succeeds or before offering slots? If when offering slots succeeds, it may happen that JM use the new fencing token while TM considering the offering slots fail, so TM may not update the token, and JM have no change to use the old token any more. If TM updates the token before offering slots, it may happen that JM doesn't receive the offering, so JM doesn't update the token. I think using a version may be more suitable, as we can compare two version, the bigger version will be correct always.;;;","17/Jun/19 05:47;gaoyunhaii;Hi all, I create a Jira for the state inconsistency between RM and TM: [FLINK-12865 | https://issues.apache.org/jira/browse/FLINK-12865]. I agree with that this two problems shares similarity. For the inconsistency between RM and TM, previously we solve it with the version method, but currently I has not found the cases when FenceToken is not available. I think more thoughts would be required to compare the two methods.;;;","17/Jun/19 14:43;trohrmann;Thanks for mentioning the heartbeat issue between TM and RM [~xiaogang.shi] and [~gaoyunhaii]. I would suggest to continue the discussion for this problem on FLINK-12865.

[~tiemsn] I agree with you that keeping the fencing token up to date can be a bit of a hassle. One would need to include the fencing token in the heartbeat response from the TM to the JM.

The problem with an increasing version is what do you do in case of an overflow?

Instead of going this way, I'm actually now in favor of removing the concurrency from the {{HeartbeatManager}}. This would execute all heartbeat calls in the actor's main thread. If the heartbeat response would be generated synchronously, the race condition should be solved. I actually believe that this should also solve FLINK-12865 without having to introduce the versions.;;;","18/Jun/19 08:22;sunjincheng121;Hi guys, Is there a consensus on the solution for FLINK-12863 and FLINK-12865? and when do you plan to resolve it? :) ;;;","18/Jun/19 08:59;trohrmann;I'm currently working on a fix. I hope that I can open a PR today or tomorrow which removes concurrency from the {{HeartbeatManagerImpls}}.;;;","18/Jun/19 10:06;sunjincheng121;Thanks [~till.rohrmann] and Sounds pretty good! :);;;","21/Jun/19 07:31;trohrmann;Fixed via

1.9.0: a95dac57ef0e1949fd4751ca19350da96c3bf52f
1.8.1: 55c8a69cfa4d40ef2863987eb89adb08f0c45dda
1.7.3: 7333b619fdf3443e179b3f6e8d3147ab4946f91c;;;",,,,,,,,,,,,,,,,,,,,,,
Potential distributed deadlock in case of synchronous savepoint failure,FLINK-12858,13239692,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,1u0,1u0,1u0,15/Jun/19 12:02,31/Jul/19 09:12,13/Jul/23 08:10,31/Jul/19 08:48,1.9.0,,,,,,,,,1.9.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"Current implementation of stop-with-savepoint (FLINK-11458) would lock the thread (on {{syncSavepointLatch}}) that carries {{StreamTask.performCheckpoint()}}. For non-source tasks, this thread is implied to be the task's main thread (stop-with-savepoint deliberately stops any activity in the task's main thread).

Unlocking happens either when the task is cancelled or when the corresponding checkpoint is acknowledged.

It's possible, that other downstream tasks of the same Flink job ""soft"" fail the checkpoint/savepoint due to various reasons (for example, due to max buffered bytes {{BarrierBuffer.checkSizeLimit()}}. In such case, the checkpoint abortion would be notified to JM . But it looks like, the checkpoint coordinator would handle such abortion as usual and assume that the Flink job continues running.",,1u0,guanghui,kkl0u,klion26,liyu,mooonzhang,pnowojski,shixg,trohrmann,,,,,"1u0 commented on pull request #9131: [FLINK-12858][checkpointing] Stop-with-savepoint, workaround: fail whole job when savepoint is declined by a task
URL: https://github.com/apache/flink/pull/9131
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 09:12;githubbot;600",,,,,,,,,,,,,,,0,600,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 31 08:48:55 UTC 2019,,,,,,,,,,"0|z03rzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/19 07:05;guanghui;Hi,[~1u0],can you give more description about this issue,such as ""non-source tasks""？thanks.;;;","11/Jul/19 10:23;1u0;Hi [~guanghui], by non-source tasks I mean tasks that are not start nodes in a Flink job execution graph, they are nodes that have some incoming edges.;;;","11/Jul/19 13:17;kkl0u;I think that the problem here is not limited to the {{stop-with-savepoint}}. This seems to be related to that the {{BarrierBuffer.checkSizeLimit()}} configuration parameter aborts a checkpoint and reports it to the checkpoint coordinator without respecting the user-specified parameter that specifies the policy of how a checkpoint failure should be handled (the  {{CheckpointExceptionHandler}}) and without somehow informing the task about it.

Another implication of that is that I am not sure how this also affects the already existing {{cancel-with-savepoint}} command from a user's perspective.;;;","16/Jul/19 15:20;1u0;Some recap of discussions around this issue:
the proposed workaround solution is to fail execution of the whole job.
In case of stop-with-savepoint and {{drain=true}}, we cannot unlock the tasks and continue job execution (as it may side effect on job results). Handling two cases differently may be a little involved.;;;","16/Jul/19 15:27;1u0;I'm not sure, but failing the task (that originates the discarded savepoint/checkpoint) may be not an option due to region recovery (which would not restart the tasks that are actually locked).;;;","23/Jul/19 14:28;trohrmann;[~kkl0u] had a comment whether to add a test or not. I would be in favour of guarding this fix with a test case.;;;","26/Jul/19 15:09;kkl0u;The test is ready and there is a PR for it here https://github.com/apache/flink/pull/9240;;;","31/Jul/19 08:48;kkl0u;Merged on master with 7b4d4b9114e6deff5d7c41925936665db53ecd8a
and on release-1.9 with 9407e1b4a56c853688d7395da448d7d107c6fb76;;;",,,,,,,,,,,,,,,,,,,,,,,,
[hotfix timeout] Deadlock occurs when requiring exclusive buffer for RemoteInputChannel,FLINK-12852,13239507,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,14/Jun/19 10:20,31/Aug/21 07:17,13/Jul/23 08:10,11/Jul/19 07:26,1.7.2,1.8.1,1.9.0,,,,,,,1.9.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"When running tests with an upstream vertex and downstream vertex, deadlock occurs when submitting the job:
{code:java}
""Sink: Unnamed (3/500)"" #136 prio=5 os_prio=0 tid=0x00007f2cca81b000 nid=0x38845 waiting on condition [0x00007f2cbe9fe000]
java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for <0x000000073ed6b6f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:233)
at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestMemorySegments(NetworkBufferPool.java:180)
at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestMemorySegments(NetworkBufferPool.java:54)
at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.assignExclusiveSegments(RemoteInputChannel.java:139)
at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.assignExclusiveSegments(SingleInputGate.java:312)
- locked <0x000000073fbc81f0> (a java.lang.Object)
at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:220)
at org.apache.flink.runtime.taskmanager.Task.setupPartionsAndGates(Task.java:836)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:598)
at java.lang.Thread.run(Thread.java:834)
{code}
This is due to the required and max of local buffer pool is not the same and there may be over-allocation, when assignExclusiveSegments there are no available memory.

 

The detail of the scenarios is as follows: The parallelism of both upstream vertex and downstream vertex are 1500 and 500 respectively. There are 200 TM and each TM has 10696 buffers( in total and has 10 slots. For a TM that runs 9 upstream tasks and 1 downstream task, the 9 upstream tasks start first with local buffer pool \{required = 500, max = 2 * 500 + 8 = 1008}, it produces data quickly and each occupy about 990 buffers. Then the DownStream task starts and try to assigning exclusive buffers for 1500 -9 = 1491 InputChannels. It requires 2981 buffers but only 1786 left. Since not all downstream tasks can start, the job will be blocked finally and no buffer can be released, and the deadlock finally occurred.

 

I think although increasing the network memory solves the problem, the deadlock may not be acceptable.  Fined grained resource management  Flink-12761 can solve this problem, but AFAIK in 1.9 it will not include the network memory into the ResourceProfile.",,aitozi,gaoyunhaii,jinxing6042@126.com,maguowei,nkruber,pnowojski,sewen,trohrmann,wind_ljy,zjwang,,,,"gaoyunhaii commented on pull request #8925: [FLINK-12852][network] Fix the deadlock occured when requesting exclusive buffers
URL: https://github.com/apache/flink/pull/8925
 
 
   ## What is the purpose of the change
   
   This pull request tries to fix the deadlock problem during requesting exclusive buffers. Since currently the number of maximum buffers and the number of required buffers are not the same for local buffer pools, there may be cases that the local buffer pools of the upstream tasks occupy all the buffers while the downstream tasks fail to acquire exclusive buffers to make progress. Although this problem can be fixed by increasing the number of total buffers, the deadlock may not be acceptable. Therefore, this PR tries to failover the current execution when the deadlock occurs and tips users to increase the number of buffers in the exceptional message.
   
   ## Brief change log
   
   The main changes include
   
   1. Add an option for the timeout of `requestMemorySegment` for each channel. The default timeout is 30s. This option is marked as undocumented since it may be removed within the future implementation.
   2. Transfer the timeout to `NetworkBufferPool`.
   3. `requestMemorySegments` will throw `IOException(""Insufficient buffer"")`  if not all segments acquired after timeout.
   
   ## Verifying this change
   
   1. Added test that validates `requestMemorySegments` end exceptionally if not all segments acquired after timeout.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): **no**
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: **no**
     - The serializers: **no**
     - The runtime per-record code paths (performance sensitive): **no**
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: **no**
     - The S3 file system connector: **no**
   
   ## Documentation
   
     - Does this pull request introduce a new feature? **no**
     - If yes, how is the feature documented? **not applicable**
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Jun/19 11:01;githubbot;600","pnowojski commented on pull request #8925: [FLINK-12852][network] Fix the deadlock occured when requesting exclusive buffers
URL: https://github.com/apache/flink/pull/8925
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jul/19 06:59;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-15031,,,FLINK-13203,FLINK-24035,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 11 07:26:16 UTC 2019,,,,,,,,,,"0|z03qug:",9223372036854775807,"To avoid a potential deadlock, this adds a timeout (default value of 30 seconds, configurable via {{taskmanager.network.memory.exclusive-buffers-request-timeout-ms}}) for how long Task will be waiting for assignment of exclusive memory segments. However it is possible that for some previously working deployments this default timeout value is too low and might has to be increased.",,,,,,,,,,,,,,,,,,,"28/Jun/19 04:38;gaoyunhaii;For all the possible methods to solve this problem come to me: 
 # Reserver buffers for exclusive buffers: It is hard to know how much required to reserve for the exclusive buffers, especially it is hard to know how many tasks will be scheduled to this TM.
 # Make the required buffers and the max buffers the same for all the local buffer pools: It may cause previous running jobs unable to run due to the total buffers is less than the sum of the updated required buffers.
 # Postpone the acquirement of exclusive buffers: It does not solves the deadlock problem, since the downstream still may not acquired any buffers to make progress.
 # Add a timeout for the _requestMemorySegments._

Therefore, I think currently we need to use the last method. I'd like to 
 # Add an option for the timeout of requestMemorySegment for each channel. The default timeout is 30s. This option will be marked as undocumented since it may be removed within the future implementation.
 # Transfer the timeout to NetworkBufferPool.
 # RequestMemorySegments will throw IOException(""Insufficient buffer"")  if not all segments acquired after timeout.;;;","02/Jul/19 10:14;sewen;Thanks for reporting this. This is a pretty critical bug!
Definitely a blocker for the 1.9 release, if we want to make that the ""batch works well"" release.

;;;","02/Jul/19 10:37;sewen;The fix suggested here is a quick band aid to not get a deadlock and rather fail.
That is an improvement, but arguable not the best user experience.

In your option (2) - I think I do not fully understand. Why would that break existing jobs?

I cannot think of a very good short term solution for this at the moment.
A long-term solution could be:
  - We make memory buffers a resource per slot - slots can not eat into each others' network buffers.
  - The new batch scheduler sets slot sharing so that it can only share slots within a pipelined region (also helps with managed memory fragmentation). We thus know how many tasks will be in a slot at most (and it should not be too many).
  - we compute the minimum number of buffers per gate (""per-channel x num-channels + floating"") and derive the total number of needed buffers for all tasks that might run concurrently.
  - we parameterize the output gates to know the total number of needed buffers and allow them request more only from the buffers beyond that number.
  - input gates take only the ""per-channel * num-channels + floating"" and never more (I think that even is the case right now)
;;;","02/Jul/19 11:37;sewen;Quick comment to understand the implications of this:

  - The blocking subpartition implementation should not be affected, because it eagerly releases resources.
  - If bounded shuffles and rebalancing were always blocking, then this could only happen in FORWARD channels.
  - At least in the DataSet API, the FORWARD channels are almost always slot-local, so realized via local channels, which are not affected

That means a workaround for DataSet / Table Flink runner could be to have all non-FORWARD strategies as blocking shuffles

For the Table Blink runner, we need to think a bit more...;;;","02/Jul/19 11:50;gaoyunhaii;Hi [~sewen], very thanks for the explanation!

For the option 2, I thought it might break the existing jobs since it increases the _required buffers_ if we increase the number of required buffers to the number of max buffers. For example, if we have a TM with 10 buffers, and we have two local buffer pools on this TM with [required, max) equals to [2, 8). Before the increasing, the total required buffers is 4 and both the two local buffer pools can be created. But after the increasing, the total required buffers become 16, and creating the second local buffer pool will fail with InsufficientNumberOfBuffers.

On the other side, if we want to totally avoid the problems brought by increasing the number of required buffers, we can only decrease the number of max buffers to the number of required buffers, but this might decrease the performance because the local buffer pool could not apply as many buffers as before.

I'm also a bit worry that the above problem might also exist if we want to limit the network memory per slot for the long run: increasing the current required buffers of local buffer pool might be not compatible with existing jobs and reduces the number of tasks that can be scheduled to the same TaskManager, while decreasing the max buffer of local buffer pool might decrease the performance of the network layer.

 ;;;","02/Jul/19 12:35;gaoyunhaii;Hi [~sewen], I think except for the batch scenarios, this issue also affects the stream jobs. In fact, the issue was also detected in a stream job. For stream jobs, the upstream tasks may start first, and they then start produce data. This piece of data might occupied all the buffers of the TM. Then the downstream tasks start and they cannot acquired the exclusive buffers required. Since the downstream tasks can not consume buffers from the upstream tasks, the occupied buffers of the upstream task cannot be recycled, then the deadlock occurs.;;;","02/Jul/19 15:40;zjwang;[~sewen] thanks for concerning this issue and sharing the suggestions.

The root reason of this issue is the mechanism of distributing global buffers among LocalBufferPools. As we know the local pool has core size and max size for partition/gate. The core size must be satisfied otherwise an exception would be thrown. Every local pool could use max size at most if there are enough buffers in global pool to make better use of resource. But the precondition is the extra buffers beyond core size could be returned finally after redistribution.  This precondition/assumption is not always satisfied especially in credit-based mode for the following reasons:
 * In non-credit mode, the producer could always push data to network until backpressure, so the extra used buffers in partition's local pool could be recycled in time. But now the producer could not send buffer to network until the consumer requested the exclusive buffers as initial credits.
 * In non-credit mode, the gate's local pool only needs the core size equal to numChannels, but now we need 2*numChannels as exclusive core size by default in credit-based mode. So the probability is higher than before.
 * In non-credit mode, the buffer request from global pool is lazy by data driven, that means after consumer receives data from network, then the local pool would request buffer from global pool. But now the exclusive buffer request from global pool is eager during startup. So the probability is also higher than before. If we make exclusive request also lazy, then it might also relieve the deadlock issue.

I think the previous non-credit mode could not avoid deadlock completely in theory, not very sure, especially for some corner cases. But in credit-based mode, the above factors would make the probability of deadlock much higher than before. 

I agree with the implications mentioned by Stephan, the current blocking partition would not exist this issue. For streaming job it still exists this probability, but i think it should not be a blocker for release-1.9.

The current proposed PR makes the deadlock to fail instead. It seems a bit better than now to tell users what happens, but it does not solve this issue in root. And users might still have a bad experience. We ever avoided this issue in alibaba via network resource matching in ResourceProfile and fixed size in local pool.

I also agree with the general ideas Stephan proposed for solving this issue. The slot resource isolation (including shuffle resource) might be a right way to go in future. ATM we could make some improvements to decrease this probability, such as making exclusive size as 1 by default which Jira was already created before and making exclusive request lazy as floating buffers.

 ;;;","03/Jul/19 06:44;maguowei;In long run there may be another way to avoid the dead lock. Task could spill some buffer to the disk and return the buffer to the local buffer pool. I think this could increase the utilization of memory in most time. What do you guys think?;;;","03/Jul/19 07:17;gaoyunhaii;Hi all, very thanks for the discussion and all the valuable suggestions. I'd like to have a short summarization of the possible solutions to this issue based on the above discussion:

For the short term, we may also
 # Use blocking shuffle instead of the the pipeline shuffle for the affected partitioner in batch jobs.
 # Decrease the exclusive buffers per channel to 1, and postpone the request of exclusive buffers till data is received. They decreases the probability of this issue.

For the long run, this issue may be also related with the fine-grained resource management, we may 
 # Make memory buffers a resource per slot and disallow slots to eat into each others' network buffers. 
 # Keep the required/max mechanism, but to fix the deadlock problem, the local buffer pool need be able to return the overused buffers when there are not enough required buffers for the other channels/local buffer pool, like spill to disk.

If there are misunderstands and missing points, let me know and I would update this comment. :) ;;;","03/Jul/19 07:52;zjwang;The spill way could solve the deadlock issue in function, which is very similar with previous SpillableSubpartition's  behavior. If we only want to spill the buffers between core size and max size after redistribution, it still could not solve another existing exception of  `insufficient number of network buffers' which was always experienced in Flink for large-scale job. If we also spill some buffers within core size to avoid that `IOException`, the performance regression might be serious but users are not aware of it. Users might prefer to increase buffer options to avoid performance regression if they know. Especially for streaming job, it is better not to touch disk unless necessary.

In contrast, if we agree that the slot resource matching would be the final way in future, then it could resolve both deadlock and `insufficient number of network buffers' issues. And users could decide to adjust the relevant buffer configs to make a tradeoff between performance and total resource usage. And we might further improve the internal mechanism for decreasing the requirements of core buffer sizes(including exclusive and core size in local buffer) to make job still run when given limited resource.;;;","03/Jul/19 08:09;zjwang;Hey [~gaoyunhaii], considering the second point for long run, I have not thought it throughly.

I am not very sure whether it is still worth keeping dynamic local pool between core and max size based on slot resource matching. If this dynamic would bring more troubles in practice, another option is adjusting it to a fix-size local pool instead. The system could calculate a reasonable default size for the local pool as now, and users could also tune it to any size they want. E.g. only 2 total buffers in local pool could also work for 100 subpartitions if users are not caring about the performance.;;;","03/Jul/19 09:34;gaoyunhaii;Hi [~zjwang] Thanks a lot for the explanation. I'm also still thinking on it currently, but one point is that compared with the previous required/max mechanism, we may not easily find a suitable fixed buffer pool size. If we use a size that is less than the number of max buffers before, we may not use as many buffers as before, which may decrease the performance. If we instead use a size equals to the number of max buffers before, then with the same number of buffers we may run less number of tasks compared with before, and the buffers may not be fully utilized if the task is not busy (Previously these buffers may be shared by others). On the other side, I totally agree with that the fixed buffer size solves the deadlock problem and should be easier to implement.;;;","03/Jul/19 10:47;zjwang;It is hardly to say whether we could get performance benefits via overusing the extra buffers beyond core size temporarily.

In general the tasks would be deployed into one TM by sequence in very short internal time. If the first task occupies some buffers actually belong to other following tasks, it would also bring some cost/overhead to recycle these extra buffers afterwards. Especially this greedy mechanism might only have limited benefits in special cases, such as backpressure. Overall it is hard to evaluate the final benefits in different scenarios.;;;","03/Jul/19 11:53;pnowojski;Couple of comments from my side. 

How does the resource allocation per slot solves the problem if we take into the account slot sharing?

Keeping resource sharing might give us some benefits in the future, although I think it doesn't help us now - at least not in streaming.

Maybe a solution could be that task initially allocates bare minimum of the resources for him to progress, and is only allowed to grab extra resources once it knows that it can make a progress (for broadcast if there is at least one down stream reader, for others if each partition/sub-partition has at least one reader).;;;","03/Jul/19 19:50;sewen;I think Piotr's suggestion goes in a good direction.

Even simpler, we could say that a pipelined partition cannot accept records (and thus allocate more than the minimum buffers) before it's reader (SubpartitionView) has not been created. By the time that connection is made, the receiver will have allocated its buffers.

This is effectively one way to make sure that the allocated buffers in a pipelined result partition can can only increase beyond the minimum once a partition knows that it can make progress through sending.

The price might me a slightly higher time to ramp up to full throughput after recovery.
We would have to investigate what the practical impact of that would be (hopefully not really anything in practice).;;;","04/Jul/19 01:13;gaoyunhaii;Hi all, very thanks [~pnowojski] and [~sewen] for the valuable suggestions, I also agree with that postponing allocation till the readers have been created makes it more easier to solve this issue. On the other side, to complement, I think we may also need to limit the orders of _requestPartitions_ among tasks while allow the ResultPartition only to grab the (extra) buffers after the readers have been created. If not, for jobs like A -> B -> C and A/B/C are mixed on TaskManagers, B may first starts and requests result partitions to A, then A starts sending records and grabbing extra buffers, then C may fail to start and finally cause a deadlock. To solve this issue, we may also need to require that the _requestPartition_ happens from Sinks towards Sources.

With postponing the allocation of (extra) buffers and requesting partitions in order, I think the deadlock could also be avoided for clusters with multiple jobs, since now  it ensures all the over-allocated buffers can be recycled finally. I have not found other counter-examples currently.

 ;;;","04/Jul/19 03:25;zjwang;Network resource matching in slot has many unsetting issues which would be further discussed future, so we could not make it effect in short time.

Lazy allocation buffers on producer side seems a feasible way atm. It could still retain the current core and maximum mechanism in local pool. But it brings another two effects:
 * Higher time to ramp up to full throughput as Stephan mentioned, especially for some very short-time jobs (several seconds finish) and I remembered there exists such cases in Kurt's benchmark before. We change the previous concurrent production and consumption to sequential way. For short-time job, before the consumer requests partition, all the data set might already be emitted and cached in partition pool on producer side before.
 * We rely on another assumption that produced buffers could be recycled finally once subpartition view is established. This assumption might limit our new features/improvements future. ATM we need to adjust the action to trigger partition request, that means RemoteInputChannel could only send partition request if the correspond task has no result partition or the partition's view has already been established. In future the InputSelection might also destroy the above assumption. Although the partition was requested, but the OP could select not to consumer that partition long time.;;;","04/Jul/19 09:20;pnowojski;Just for the record & FYI, I'm currently working in parallel on changing when {{requestPartitions}} call is happening (I'm trying to move it from {{SingleInputGate#get/poll}} to {{SingleInputGate#setup()}} ) https://issues.apache.org/jira/browse/FLINK-13013 .

 

>  is only allowed to grab extra resources once it knows that it can make a progress 

When writing this I forgot that it indeed requires not only the immediate successor operator to be ready, but for all of the downstream operators be ready as well as [~gaoyunhaii] wrote. So we would have to indeed initialize/grab resources in the order from the sinks up to the sources.

Do I understand it correctly, that we can have three solutions:
 # Grab only required resources from sinks up to the sources and once that completes grab additional resources.
 # Make buffers completely not shared between tasks - for example disable slot sharing and make buffer pools per slot
 # Make optional buffers ""revocable"", for example by spilling.

[~zjwang] which one of those three were you referring to when writing ""Network resource matching in slot"" and ""Lazy allocation buffers on producer side""?;;;","04/Jul/19 10:08;zjwang;[~pnowojski] ""Network resource matching in slot"" refers to the second one you mentioned, and  ""Lazy allocation buffers on producer side"" refers to the first one.;;;","04/Jul/19 10:19;zjwang;We have two directions in general: buffers sharing between tasks or not.
 * The sharing direction seems more flexible and has more possibilities, but we need to confirm the buffers revocable mechanism to not bring downsides. ATM we have above the first and third ways for it.

 * The not-sharing direction could avoid potential issues completely, but it might bring resource waste in some cases.;;;","04/Jul/19 12:26;aitozi;Hi， [~gaoyunhaii] [~zjwang]

I check the allocation logic again, I found that during createBufferPool for inputGate/resultPartition it will increase the variable  _numTotalRequiredBuffers_ .  If the _numTotalRequiredBuffers + numRequiredBuffers > totalNumberOfMemorySegments_ it will throw the network buffer not  enough exception. And the inputGate assignExclusiveBuffers will do the same thing. But during the localBufferPool#requestMemorySegment it will ask for buffer from the global buffer pool but this do not increase the _numTotalRequiredBuffers_ (this method will request extra buffers larger than the core size). So although the downstream have not enough buffer to assign, but it can also pass the _numTotalRequiredBuffers + numRequiredBuffers > totalNumberOfMemorySegments_ check in the _NetworkBufferPool#requestMemorySegments_ method. 

Although this can not indeed solve the deadlock problem, but i think this can partly hide this situation, otherwise it will fail with the check.

;;;","04/Jul/19 14:10;zjwang;Thanks for concerning this issue [~aitozi]

We already have the check `numTotalRequiredBuffers + numberOfSegmentsToRequest > totalNumberOfMemorySegments` during `NetworkBufferPool#requestMemorySegments` and this check is mainly used for judging whether the global size could satisfy the total core size. If we replace the `numberOfSegmentsToRequest` with `numRequiredBuffers` as you suggested above, what is the value for `numRequiredBuffers` here?

During `NetworkBufferPool#createBufferPool`, the `numRequiredBuffers` in check is for the core size in local pool. I guess your suggestion is also throwing the IOException(""Insufficient buffers..."") after allocating the exclusive buffers failed. There are two concerns for this solution:
 * How long do you want to wait before throwing the exception. The extra buffers recycle from other local pool might take some time after redistribution, but we could not estimate the rough time to throw this exception properly.
 * This seems a bit tricky/hacky to reuse the check here. If the exception tells users the message of insufficient buffers, users might be confused because the total buffers are actually enough. If the exception tells users the message of deadlock, users might still do not know what to do. The only benefit is identifying the deadlock easily, not via jstack.;;;","04/Jul/19 14:57;aitozi;[~zjwang] 
I'am sorry for confusing you that the variable name _numRequiredBuffers_ is wrong because I copy from another code branch. 
My mean is that _numTotalRequiredBuffers_ is not calculated correctly because it do not increase 1 when invoke _requestMemorySegment_.

{code:java}
@Nullable
public MemorySegment requestMemorySegment() {
	return availableMemorySegments.poll();
}
{code}

 So the check _numTotalRequiredBuffers + numberOfSegmentsToRequest > totalNumberOfMemorySegments_ may pass because the _numTotalRequiredBuffers_ is less than the real required buffer . 

If it is correctly calculated, the check in requestMemorySegments method will fail directly and will not need the timeout mechanism to prevent the deadlock.;;;","05/Jul/19 04:11;gaoyunhaii;Hi [~aitozi], very thanks for concerning this issue. In my opinion, currently local buffer pool is configured by [required buffers, max buffers). The required buffers indicates how many buffers that ensures to be allocated and the max buffers indicates how many buffers to occupy at most. The requested buffers may be larger than the required buffers and it cannot exceed the max buffers.

I think if we increase the number of required buffers when _requestMemorySegment_, the effect is equivalent to judge whether a local buffer pool can be created via requested buffers. Furthermore, I think it would be also equivalent to the timeout method in the original PR with _timeout = 0_: when we create a new local buffer pool, we see how many buffers are left and if it is not sufficient, the creation will fail. Since in many cases the over-allocated buffers of other local buffers can be recycled later, I think judging according to current remaining buffers might cause additional unnecessary failures.;;;","05/Jul/19 06:29;aitozi;Thanks [~gaoyunhaii] for your explanation, I think over this today, it's indeed equivalent to your original PR with _timeout = 0_ and it's not a good solution. ;;;","05/Jul/19 11:56;sewen;So, in summary we have these options.

  1. Only allocate the minimum per producer, *which is one buffer per channel*. This would be needed to keep the requirement similar to what we have at the moment, but it is much less than we recommend for the credit-based network data exchange (2* channels + floating)
    ==> *Not a good option in my opinion*

  2. Coordinate the deployment sink-to-source such that receivers always have their buffers first. This will be complex to implement and coordinate and break with many assumptions about tasks being independent (coordination wise) on the TaskManagers. Giving that assumption up will be a pretty big step and cause lot's of complexity in the future.
      It will also increase deployment delays. Low deployment delays should be a design goal in my opinion, as it will enable other features more easily, like low-disruption upgrades, etc.
   ==> *Not a good option either, in my opinion*

  3. Make buffers always revokable, by spilling.
      This is tricky to implement very efficiently, especially because
        - there is the logic that slices buffers for early sends for the low-latency streaming stuff
        - the spilling request will come from an asynchronous call.  That will probably stay like that even with the mailbox, because the main thread will be frequently blocked on buffer allocation when this request comes.
   ==> I think this would explode in complexity and bugs, unless we rewrite other parts significantly. *Not a good option either*

  4. We allocate the recommended number for good throughput (2*numChannels + floating) per consumer and per producer.
      No dynamic rebalancing any more. This would increase the number of required network buffers in certain high-parallelism scenarios quite a bit with the default config. Users can down-configure this by setting the per-channel buffers lower. But it would break user setups and require them to adjust the config when upgrading.
    ==> *Not a great option, in my opinion*

  5. We make the network resource per slot and ask the scheduler to attach information about how many producers and how many consumers will be in the slot, worst case. We use that to pre-compute how many excess buffers the producers may take.
     This will also break with some assumptions and lead us to the point that we have to pre-compute network buffers in the same way as managed memory. Seeing how much pain it is with the managed memory, this seems not so great.
   ==> *Not a great option either*

This is tricky. At this point it looks like option (4) is the only one that is feasible without severe performance issues or an explosion of complexity.;;;","05/Jul/19 13:42;pnowojski;For option number #2, we would need a new RPC call from JobManager to TaskManager, ""task X is unblocked"". Initial version could just send it once ""all tasks are running"", which later can be refined to ""once all downstream tasks are running"". Handling of ""task X is unblocked"" on the task managers maybe is not that difficult, considering that we already have the buffers rebalancing code. I think in the long term this would be the best solution if we want to keep min/optimal buffers range. 

[~sewen] how is the yours number #4 different to what is proposed in the current PR? It sounds like the current proposal is a superset of functionality that you described. If buffer requirements are unable to be met, fail the task with a timeout while we try to keep min required buffers vs optimal number of buffers constraint. If that's annoyance for the user, he should adjust the network config, without any changes in the behaviour (except of failure instead of deadlock).;;;","05/Jul/19 14:32;sewen;[~pnowojski] I think in the current PR, senders can still exceed that number of buffers (2*numChannels + floating) and thus cause deadlocks even all task could get that number of buffers (guarded by a timeout/fail). The suggestion would be to avoid dynamic rebalancing and fail eagerly if you cannot get that exact number.;;;","05/Jul/19 15:58;zjwang;Thanks for above detail conclusions [~sewen]!

I agree that the option 4 is the most feasible way ATM. The logic of fixed-size pool is easy and direct to avoid sharing buffers among tasks compared with dynamic size as now, so tasks are independent with each other, not relying on any assumptions. We ever realized the fixed-size local pool to avoid the buffers redistribution in early Blink version.

I understand the option 1 has the same direction with option 4, both with fixed-size local pool. The difference is that option 4 takes the (2*numChannels + floating) as the fixed size of pool for better performance, and the option 1 takes the numChannels as the fixed size of pool. The option 1 is compatible with users when upgrading, but it might bring performance regression. The option 4 could keep the performance but might need user to adjust the config when upgrade failure.

The other three options are indeed complex to implement and might break/rely on some assumptions to bring potential risks.;;;","06/Jul/19 06:13;gaoyunhaii;Very thanks [~sewen] for the detailed comparison of the different options. I also agree with that ATM option (4) should be the one method with the least modification. For option (4), we might also need to give users explicit guideline on the semantic changes of the buffer size. 

On the other side, I still think that the core/max semantics of the local buffer pool might be more flexible, and might have performance gain for cases like the ability of the downstream tasks is not stable. The core/max semantics might still be an option if other conditions are satisfied in the more longer future. 

At last, in considering of the advantages and disadvantages of the current options, and the publish timeline of 1.9, now do we still plan to have this issue fixed (to some extend) with some method in 1.9.0 ?;;;","10/Jul/19 13:41;sewen;All the options above are very invasive or may break user setups. At the same time, the bug seems to occur rarely.

I would suggest to actually not rush anything for 1.9 but go with the initially proposed solution to catch this situation (via a timeout) and trigger a recovery.

It's not the perfect solution, but the least invasive for now and we can think about how to properly fix this in the future.

So I am giving +1 for the original solution in the PR from Yun.;;;","11/Jul/19 07:26;pnowojski;I'm closing this ticket as a timeout hotfix and I'm setting a proper release note. I've created a new ticket https://issues.apache.org/jira/browse/FLINK-13203 for a proper fix. 

Merged to master as cf884899c72410e8365c2e5637374b82d846eb4e;;;"
Problems with ipv6 addresses,FLINK-12840,13239381,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sewen,potseluev,potseluev,13/Jun/19 22:00,05/Jul/19 06:59,13/Jul/23 08:10,05/Jul/19 06:59,1.8.0,,,,,,,,,1.9.0,,,,Runtime / Configuration,Runtime / Network,,,,0,pull-request-available,,,,"I found some problems when using flink with ipv6. The easiest way to reproduce is add 
{noformat}
env.java.opts: -Djava.net.preferIPv6Addresses=true{noformat}
to the flink-conf.yaml and run start-cluster.sh script.

 Jobmanager log:
{noformat}
2019-06-13 19:16:01,088 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils         - Trying to start actor system at localhost:6123

2019-06-13 19:16:01,147 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint         - Shutting StandaloneSessionClusterEntrypoint down with application status FAILED. Diagnostics java.lang.Exception: Could not create actor system

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:267)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:153)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:112)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:87)

at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.createRpcService(AkkaRpcServiceUtils.java:84)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createRpcService(ClusterEntrypoint.java:296)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:264)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:216)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$0(ClusterEntrypoint.java:172)

at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:171)

at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:535)

at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:65)

Caused by: com.typesafe.config.ConfigException$Parse: String: 56: Expecting close brace } or a comma, got ':' (if you intended ':' to be part of a key or string value, try enclosing the key or value in double quotes, or you may be able to rename the file .properties rather than .conf)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:475)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:595)

at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14)

at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260)

at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248)

at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180)

at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174)

at com.typesafe.config.impl.Parseable.parse(Parseable.java:299)

at com.typesafe.config.ConfigFactory.parseString(ConfigFactory.java:1046)

at com.typesafe.config.ConfigFactory.parseString(ConfigFactory.java:1056)

at org.apache.flink.runtime.akka.AkkaUtils$.getRemoteAkkaConfig(AkkaUtils.scala:601)

at org.apache.flink.runtime.akka.AkkaUtils$.getAkkaConfig(AkkaUtils.scala:218)

at org.apache.flink.runtime.akka.AkkaUtils.getAkkaConfig(AkkaUtils.scala)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:247)

... 12 more{noformat}
Taskmanager log:
{noformat}
2019-06-13 19:16:12,260 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils         - Trying to start actor system at [2a02:6b8:c02:7e8:0:1459:44c2:764a]:0

2019-06-13 19:16:12,310 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner       - TaskManager initialization failed.

java.lang.Exception: Could not create actor system

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:267)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:153)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:112)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:87)

at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils.createRpcService(AkkaRpcServiceUtils.java:84)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.createRpcService(TaskManagerRunner.java:412)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.<init>(TaskManagerRunner.java:134)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:332)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:302)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:299)

at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.main(TaskManagerRunner.java:299)

Caused by: com.typesafe.config.ConfigException$Parse: String: 55: List should have ended with ] or had a comma, instead had token: ':' (if you want ':' to be part of a string value, then double-quote it)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseArray(ConfigDocumentParser.java:533)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:249)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.consolidateValues(ConfigDocumentParser.java:152)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:420)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:405)

at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parse(ConfigDocumentParser.java:595)

at com.typesafe.config.impl.ConfigDocumentParser.parse(ConfigDocumentParser.java:14)

at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:260)

at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:248)

at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180)

at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174)

at com.typesafe.config.impl.Parseable.parse(Parseable.java:299)

at com.typesafe.config.ConfigFactory.parseString(ConfigFactory.java:1046)

at com.typesafe.config.ConfigFactory.parseString(ConfigFactory.java:1056)

at org.apache.flink.runtime.akka.AkkaUtils$.getRemoteAkkaConfig(AkkaUtils.scala:601)

at org.apache.flink.runtime.akka.AkkaUtils$.getAkkaConfig(AkkaUtils.scala:218)

at org.apache.flink.runtime.akka.AkkaUtils.getAkkaConfig(AkkaUtils.scala)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:247)

... 11 more{noformat}
The problem is the following code in the AkkaUtils:

 
{code:java}
val hostnameConfigString =
  s""""""
     |akka {
     |  remote {
     |    netty {
     |      tcp {
     |        hostname = $effectiveHostname
     |        bind-hostname = $bindAddress
     |      }
     |    }
     |  }
     |}
   """""".stripMargin
{code}
I fixed it by adding double quotes. I ran flink with this fix and jobmanager worked fine but there was a new problem with taskmanager:
{noformat}
2019-06-13 21:12:37,451 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils         - Actor system started at akka.tcp://flink@[2a02:6b8:c02:7e8:0:1459:44c2:764a]:14803

2019-06-13 21:12:37,470 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner       - TaskManager initialization failed.

org.apache.flink.configuration.IllegalConfigurationException: The configured hostname is not valid

at org.apache.flink.util.NetUtils.unresolvedHostToNormalizedString(NetUtils.java:150)

at org.apache.flink.util.NetUtils.unresolvedHostAndPortToNormalizedString(NetUtils.java:168)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:243)

at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:153)

at org.apache.flink.runtime.metrics.util.MetricUtils.startMetricsActorSystem(MetricUtils.java:132)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.<init>(TaskManagerRunner.java:135)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:332)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:302)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$1.call(TaskManagerRunner.java:299)

at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)

at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.main(TaskManagerRunner.java:299)

Caused by: java.lang.IllegalArgumentException

at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:123)

at org.apache.flink.util.NetUtils.unresolvedHostToNormalizedString(NetUtils.java:148)

... 10 more{noformat}
 

This problem caused by NetUtils.unresolvedHostToNormalizedString fails when gets an ipv6 address in brackets like [2a02:6b8:c02:7e8:0:1459:44c2:764a]. 

It works fine with fixed AkkaUtils and NetUtils.",,potseluev,sewen,,,,,,,,,,,,"PotseluevPasha commented on pull request #8734: [FLINK-12840] fix network utils to work with ipv6 correctly
URL: https://github.com/apache/flink/pull/8734
 
 
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   It fixes problems when using flink with ipv6.
   
   
   ## Brief change log
   
     - Fixed `AkkaUtils.getRemoteAkkaConfig` to work with ipv6 addresses in typesafe config.
     - Fixed `NetUtils.unresolvedHostToNormalizedString` to work with already normalized ipv6 addresses.
   
   
   ## Verifying this change
   
   Added test cases to `AkkaUtilsTest` and `NetUtilsTest`.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jun/19 22:18;githubbot;600","asfgit commented on pull request #8734: [FLINK-12840] fix network utils to work with ipv6 correctly
URL: https://github.com/apache/flink/pull/8734
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 22:36;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 05 06:59:45 UTC 2019,,,,,,,,,,"0|z03q2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jul/19 08:28;sewen;Thanks, that is a good diagnosis and the right fix!

I will try to get your patch in...;;;","05/Jul/19 06:59;sewen;Fixed via 3854552ceefd2b2b9c0e2a9b6152a7fcb69153fe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Time conversion is wrong in ManualClock,FLINK-12835,13239277,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,eaglewatcher,eaglewatcher,eaglewatcher,13/Jun/19 12:52,14/Jun/19 08:01,13/Jul/23 08:10,14/Jun/19 07:34,1.9.0,,,,,,,,,1.7.3,1.8.1,1.9.0,,Tests,,,,,1,pull-request-available,,,,"`currentTime` stored in ManualClock is nanoseconds, when converted to milliseconds is devided by 1_000L rather thant 1_000_000L.

The test logic of `SlotPoolTest` using `ManualClock` should also be refined.",,eaglewatcher,fan_li_ya,trohrmann,,,,,,,,,,,"eaglewatcherwb commented on pull request #8733: [FLINK-12835][tests] Fix wrong conversion of ManualClock bug
URL: https://github.com/apache/flink/pull/8733
 
 
   Change-Id: I4684c733c79634177cb7354ced1f4d8118617ae1
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *Fix wrong conversion of ManualClock bug.*
   
   ## Brief change log
   
     - *Fix wrong conversion of ManualClock bug*
   
   ## Verifying this change
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no )
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jun/19 12:56;githubbot;600","tillrohrmann commented on pull request #8733: [FLINK-12835][tests] Fix wrong conversion of ManualClock bug
URL: https://github.com/apache/flink/pull/8733
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jun/19 07:30;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 14 07:34:41 UTC 2019,,,,,,,,,,"0|z03pfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/19 07:34;trohrmann;Fixed via

1.9.0: e3c68859dd31ee2d651cbba5f31474652c9553dd
1.8.1: 685d49b1cf898a75722ad4c379b974ba597eb063
1.7.3: 36713880e106c3d13e00576d9eb49091b9c2a47d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableSinkITCase#testBoundedTableSink fails in IDE if run with other tests,FLINK-12830,13239222,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,dwysakowicz,dwysakowicz,13/Jun/19 09:36,01/Oct/19 15:38,13/Jul/23 08:10,13/Jun/19 10:31,1.9.0,,,,,,,,,1.9.0,,,,Table SQL / Legacy Planner,Tests,,,,0,,,,,"If I run all the tests in flink-table-planner/scala in IDE then the test {{TableSinkITCase#testBoundedTableSink}} fails consistently. It passes if I run this single test on its own.

This test was introduced in 851d27bb54596d1044a831160b09aad9f3b32316

{code}
java.lang.AssertionError: Different elements in arrays: expected 8 elements and received 11
 expected: [Comment#12,6, Comment#13,6, Comment#14,6, Comment#15,6, Hello world, how are you?,3, Hello world,2, Hello,2, Hi,1]
 received: [1,1,Hi, 2,2,Hello, 3,2,Hello world, Comment#12,6, Comment#13,6, Comment#14,6, Comment#15,6, Hello world, how are you?,3, Hello world,2, Hello,2, Hi,1] 
Expected :8
Actual   :11
{code}",,dwysakowicz,jark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 13 10:31:48 UTC 2019,,,,,,,,,,"0|z03p3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/19 09:37;dwysakowicz;cc [~jark];;;","13/Jun/19 10:24;jark;Thanks [~dwysakowicz], I will check this.;;;","13/Jun/19 10:31;jark;Fixed in 1.9.0: 180fe43e9edea9b38e4022af81e7fe9080ff9c9a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup some interfaces of BinaryString,FLINK-12802,13238723,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,11/Jun/19 10:03,05/Jul/19 01:47,13/Jul/23 08:10,05/Jul/19 01:47,,,,,,,,,,1.9.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,,,lzljs3620320,,,,,,,,,,,,,"JingsongLi commented on pull request #8689: [FLINK-12802][table-runtime-blink] Reducing the Code of BinaryString
URL: https://github.com/apache/flink/pull/8689
 
 
   
   ## What is the purpose of the change
   
   Clean BinaryString code.
   
   ## Verifying this change
   
   ut
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jun/19 10:18;githubbot;600","KurtYoung commented on pull request #8689: [FLINK-12802][table-runtime-blink] Reducing the Code of BinaryString
URL: https://github.com/apache/flink/pull/8689
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Jul/19 01:46;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,FLINK-11701,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 05 01:47:51 UTC 2019,,,,,,,,,,"0|z03m14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/19 01:47;ykt836;merged in 1.9.0: 18903fe9090ef71a668fc4befc2bded6c3787337;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Harden Tests when availableProcessors is 1,FLINK-12800,13238708,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xleesf,xleesf,xleesf,11/Jun/19 08:51,02/Oct/19 17:43,13/Jul/23 08:10,14/Jun/19 06:46,1.8.0,,,,,,,,,1.9.0,,,,Tests,,,,,0,pull-request-available,,,,"Some tests failed when Runtime.getRuntime().availableProcessors() is 1. The source vertex and other vertex may chained, thus make test failed. such as 

AsyncWaitOperatorTest#testOperatorChainWithProcessingTime

StreamingJobGraphGeneratorTest#testChainStartEndSetting

StreamOperatorChainingTest#testMultiChainingWithObjectReuse

StreamOperatorChainingTest#testMultiChainingWithoutObjectReuse

DataExchangeModeClosedBranchingTest

 

So we would specify parallelism of vertex directly to let its not chained with source .",,sewen,trohrmann,xleesf,,,,,,,,,,,"leesf commented on pull request #8716: [FLINK-12800] Harden Tests when availableProcessors is 1
URL: https://github.com/apache/flink/pull/8716
 
 
   ## What is the purpose of the change
   
   Harden Tests when availableProcessors is 1.
   
   
   ## Brief change log
   
   Set parallelism to 4 in the following failed tests.
   
   _AsyncWaitOperatorTest#testOperatorChainWithProcessingTime
   StreamingJobGraphGeneratorTest#testChainStartEndSetting
   StreamOperatorChainingTest#testMultiChainingWithObjectReuse
   StreamOperatorChainingTest#testMultiChainingWithoutObjectReuse_
   
   
   ## Verifying this change
   
   This change is already covered by existing tests.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (o)
     - If yes, how is the feature documented? (not documented)
   
   cc @StephanEwen @tillrohrmann 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jun/19 13:32;githubbot;600","tillrohrmann commented on pull request #8716: [FLINK-12800] Harden Tests when availableProcessors is 1
URL: https://github.com/apache/flink/pull/8716
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jun/19 06:48;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 14 06:46:39 UTC 2019,,,,,,,,,,"0|z03lxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/19 09:25;sewen;+1

Are you working on a pull request for this?;;;","12/Jun/19 09:39;xleesf;[~sewen] yes. Will open a PR soon.;;;","14/Jun/19 06:46;trohrmann;Fixed via 3fa062a8ad9cfbbef23adbfb3ad1406a68afb3eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix java docs in UserDefinedAggregateFunction,FLINK-12789,13238451,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hequn8128,hequn8128,hequn8128,10/Jun/19 05:52,10/Jun/19 09:01,13/Jul/23 08:10,10/Jun/19 09:00,,,,,,,,,,1.9.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"We use \{{UserDefinedAggregateFunction}} as the base class for \{{TableAggregateFunction}} and \{{AggregateFunction}}. However, the java docs in \{{UserDefinedAggregateFunction}} are only dedicated for \{{AggregateFunction}}. ",,hequn8128,sunjincheng121,,,,,,,,,,,,"hequn8128 commented on pull request #8674: [FLINK-12789][table][fix] Fix java docs in UserDefinedAggregateFunction
URL: https://github.com/apache/flink/pull/8674
 
 
   
   ## What is the purpose of the change
   
   This pull request fix java docs in `UserDefinedAggregateFunction`.
   
   We use UserDefinedAggregateFunction as the base class for TableAggregateFunction and AggregateFunction. However, the java docs in UserDefinedAggregateFunction are only dedicated for AggregateFunction. 
   
   
   ## Brief change log
   
     - Change all java docs about `AggregateFunction` to `UserDefinedAggregateFunction` in `UserDefinedAggregateFunction`.
   
   
   ## Verifying this change
   
   This change is a hotfix for java docs.
   
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jun/19 06:14;githubbot;600","asfgit commented on pull request #8674: [FLINK-12789][table][fix] Fix java docs in UserDefinedAggregateFunction
URL: https://github.com/apache/flink/pull/8674
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jun/19 09:01;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 10 09:00:08 UTC 2019,,,,,,,,,,"0|z03kcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/19 09:00;sunjincheng121;Fixed in master: dca346ff89f34e22b63620a0994787782e72a2ba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB savepoint recovery can use a lot of unmanaged memory,FLINK-12785,13238441,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,klion26,mikekap,mikekap,10/Jun/19 01:46,10/Jan/20 12:23,13/Jul/23 08:10,10/Jan/20 12:23,,,,,,,,,,1.10.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"I'm running an application that's backfilling data from Kafka. There's approximately 3 years worth of data, with a lot of watermark skew (i.e. new partitions were created over time) and I'm using daily windows. This makes a lot of the windows buffer their contents before the watermark catches up to ""release"" them. In turn, this gives me a lot of in-flight windows (200-300) with very large state keys in rocksdb (on the order of 40-50mb per key).

Running the pipeline tends to be mostly fine - it's not terribly fast when appends happen but everything works. The problem comes when doing a savepoint restore - specifically, the taskmanagers eat ram until the kernel kills it due to being out of memory. The extra memory isn't JVM heap since the memory usage of the process is ~4x the -Xmx value and there aren't any {{OutOfMemoryError}} exceptions.

I traced the culprit of the memory growth to [RocksDBFullRestoreOperation.java#L212|https://github.com/apache/flink/blob/68910fa5381c8804ddbde3087a2481911ebd6d85/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBFullRestoreOperation.java#L212] . Specifically, while the keys/values are deserialized on the Java heap, {{RocksDBWriteBatchWrapper}} forwards it to RocksDB's {{WriteBatch}} which buffers in unmanaged memory. That's not in itself an issue, but {{RocksDBWriteBatchWrapper}} flushes only based on a number of records - not a number of bytes in-flight. Specifically, {{RocksDBWriteBatchWrapper}} will flush only once it has 500 records, and at 40mb per key, that's at least 20Gb of unmanaged memory before a flush.

My suggestion would be to add an additional flush criteria to {{RocksDBWriteBatchWrapper}} - one based on {{batch.getDataSize()}} (e.g. 500 records or 5mb buffered). This way large key writes would be immediately flushed to RocksDB on recovery or even writes. I applied this approach and I was able to complete a savepoint restore for my job. That said, I'm not entirely sure what else this change would impact since I'm not very familiar with Flink.",,bslim,elevy,klion26,liyu,mikekap,Paul Lin,stevenz3wu,yuqi,yuyang08,,,,,"klion26 commented on pull request #10329: [FLINK-12785][StateBackend] RocksDB savepoint recovery can use a lot of unmanaged memory
URL: https://github.com/apache/flink/pull/10329
 
 
   
   ## What is the purpose of the change
   
   Currently `RocksDBWriteBatchWrapper` will flush the batch just based on item count, will consume too much memory for big kv.
   This PR will add consumed memory sized based control policy, will flush the batch either item count or consumed memory size exceeds the configured value.
   
   ## Brief change log
   
   - Add a configuration `state.backend.rocksdb.write-batch-size` in `ROcksDBCOnfigurableOptions` to set the batchSize, default 2mb
   - Add logic to flush write batch after consumed memory size exceeds the preconfiged value.
   
   
   ## Verifying this change
   
   
   This change added tests and can be verified as follows:
   
   - `RocksDBWriteBatchWrapperTest#testWriteBatchWrapperFlushAfterMemorySizeExceed` to test that the batch will flush after consumed memory exceeds the preconfigured value
    - `RocksDBWriteBatchWrapperTest#testWriteBatchWrapperFlushAfterCountExceed` to test that the batch will flush after item count exceeds the preconfigured value
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (docs)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Nov/19 07:05;githubbot;600","carp84 commented on pull request #10329: [FLINK-12785][StateBackend] RocksDB savepoint recovery can use a lot of unmanaged memory
URL: https://github.com/apache/flink/pull/10329
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 07:40;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-7289,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 10 12:23:23 UTC 2020,,,,,,,,,,"0|z03kao:",9223372036854775807,"Before FLINK-12785, user may encounter OOM if there are huge KV pairs when restoring from savepoint of RocksDB state backend. In FLINK-12785 we introduce a size limit in RocksDBWriteBatchWrapper with default value 2MB, and RocksDB's WriteBatch will flush if the consumed memory exceeds it. User could tune the limit through the state.backend.rocksdb.write-batch-size property in flink-conf.yaml if needed.
",,,,,,,,,,,,,,,,,,,"10/Jun/19 02:12;klion26;Thanks for filing the issue, I will have a look at it.;;;","11/Jun/19 02:27;klion26;I think your analysis is correct, we should better add another flush strategy based on the in-flight byte size in RocksDBWriteBatchWrapper. Do you want to fix it, or If you don't mind I'll give a patch for this [~mikekap]

cc [~srichter];;;","11/Jun/19 18:46;mikekap;[~klion26] you can go ahead. I know that the RocksDBWriteBatchWrapper is used in several places and I'm not sure if the same limit should apply to all of them. I set the limit at 5mb & 1000 messages, but I don't have a good sense of what's optimal.

Separately, from a performance perspective, it would probably make sense to use the settings from [https://github.com/facebook/rocksdb/blob/master/options/options.cc#L374] to speed up recovery, but that might be good to do as a follow-up change since it requires re-opening the db afterwards.;;;","12/Jun/19 02:02;klion26;[~mikekap] thanks for the input, I'll keep that in mind.;;;","23/Nov/19 08:02;liyu;I think fix of this one is necessary to control the total memory consumption of RocksDB (as described, during checkpoint restore), along with FLINK-7289. Let's try best to fix this in 1.10.0 release [~klion26].

CC [~sewen] [~yunta];;;","23/Nov/19 08:03;liyu;Add link to FLINK-7289 since this is also related to RocksDB memory consumption.;;;","10/Jan/20 11:09;liyu;Fixed via
master: 6df2e0e6db6d58819fcec9a9dd1e9cf1c632af0a
release-1.10: 6e4385497451fef25e2c0adef99cc32496ca9381;;;","10/Jan/20 11:12;liyu;[~klion26] Please add some release notes about the newly introduced property and how to use it.

Will close this issue after release note is completed.;;;","10/Jan/20 12:23;klion26;Already added the release not, closing this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix deriveTableAggRowType bug for non-composite types,FLINK-12778,13238219,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hequn8128,hequn8128,hequn8128,07/Jun/19 14:29,21/Jun/19 10:12,13/Jul/23 08:10,21/Jun/19 10:11,,,,,,,,,,1.9.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Currently, we call {{aggCalls.get(0).`type`.getFieldList.foreach(builder.add)}} when derive row type for table aggregate. However, for types which are not composite types, the field list would be null. Table Aggregate should, of course, support non-composite types.

To solve the problem, we should judge whether types are structured. This is because a composite type will be converted to a RelDataType which contains field list and is structured.",,hequn8128,sunjincheng121,,,,,,,,,,,,"hequn8128 commented on pull request #8771: [FLINK-12778][table] Fix derive row type bug for table aggregate
URL: https://github.com/apache/flink/pull/8771
 
 
   
   ## What is the purpose of the change
   
   This pull request fix derives row type bug for table aggregate.
   
   Currently, we call `aggCalls.get(0).type.getFieldList.foreach(builder.add)` when derive row type for table aggregate. However, for types which are not composite types, the field list would be null. Table Aggregate should, of course, support non-composite types.
   
   To solve the problem, we should judge whether types are structured. This is because a composite type will be converted to a RelDataType which contains field list and is structured.
   
   
   ## Brief change log
   
     - Fix `deriveTableAggRowType` method in `CommonTableAggregate`. When deriving row type, judge whether the result type of the table aggregate function is structured. Use `FieldInfoUtils.getFieldNames()` to get the field name when the result type is not structured.
     - In `AggregateOperationFactory.createAggregate()`, also use `FieldInfoUtils.getFieldNames()` to get the field name. 
     - Add tests to verify the results.
   
   
   ## Verifying this change
   
   This change added tests and can be verified as follows:
   
     - Add plan test for table aggregate(both for window and non-window table aggregate).
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jun/19 02:34;githubbot;600","asfgit commented on pull request #8771: [FLINK-12778][table] Fix derive row type bug for table aggregate
URL: https://github.com/apache/flink/pull/8771
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jun/19 10:12;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,FLINK-12779,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 21 10:11:34 UTC 2019,,,,,,,,,,"0|z03ixc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/19 10:11;sunjincheng121;Fixed in master: 18dedb7749b0c8672d28dadb290d66ca3d12a87d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKinesisConsumerTest.testSourceSynchronization unstable on Travis,FLINK-12768,13237920,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,thw,trohrmann,trohrmann,06/Jun/19 09:38,02/Aug/19 17:52,13/Jul/23 08:10,02/Aug/19 17:39,1.9.0,,,,,,,,,1.9.0,,,,Connectors / Kinesis,,,,,0,pull-request-available,test-stability,,,"The {{FlinkKinesisConsumerTest.testSourceSynchronization}} seems to be unstable on Travis. It fails with 

{code}
[ERROR] testSourceSynchronization(org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest)  Time elapsed: 10.031 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: iterable containing [""1"", <Watermark @ -4>]
     but: No item matched: <Watermark @ -4>
	at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.testSourceSynchronization(FlinkKinesisConsumerTest.java:950)
{code}

https://api.travis-ci.org/v3/job/541845510/log.txt

While looking into the problem, I noticed that the test case takes 1 second to execute on my machine. I'm wondering whether this really needs to take this long. Moreover, the test code contains {{Thread.sleeps}} and uses {{Whiteboxing}} which we should avoid.",,thw,trohrmann,,,,,,,,,,,,"tweise commented on pull request #9136: [FLINK-12768][tests] FlinkKinesisConsumerTest.testSourceSynchronization flakiness
URL: https://github.com/apache/flink/pull/9136
 
 
   PR for investigation only.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 19:43;githubbot;600","tweise commented on pull request #9183: [FLINK-12768][tests] FlinkKinesisConsumerTest.testSourceSynchronization flakiness
URL: https://github.com/apache/flink/pull/9183
 
 
   These are the net changes from https://github.com/apache/flink/pull/9136
   
   I propose we merge these and observe if it removes the flakiness in Travis.
   
   <!--
   *Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*
   
   *Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*
   
   ## Contribution Checklist
   
     - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.
     
     - Name the pull request in the form ""[FLINK-XXXX] [component] Title of the pull request"", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.
     Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.
   
     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
     
     - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](https://flink.apache.org/contributing/contribute-code.html#open-a-pull-request).
   
     - Each pull request should address only one issue, not mix up code from multiple issues.
     
     - Each commit in the pull request has a meaningful commit message (including the JIRA id)
   
     - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
   
   
   **(The sections below can be removed for hotfixes of typos)**
   -->
   
   ## What is the purpose of the change
   
   *(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*
   
   
   ## Brief change log
   
   *(for example:)*
     - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*
     - *Deployments RPC transmits only the blob storage reference*
     - *TaskManagers retrieve the TaskInfo from the blob cache*
   
   
   ## Verifying this change
   
   *(Please pick either of the following options)*
   
   This change is a trivial rework / code cleanup without any test coverage.
   
   *(or)*
   
   This change is already covered by existing tests, such as *(please describe tests)*.
   
   *(or)*
   
   This change added tests and can be verified as follows:
   
   *(example:)*
     - *Added integration tests for end-to-end deployment with large payloads (100MB)*
     - *Extended integration test for recovery after master (JobManager) failure*
     - *Added test that validates that TaskInfo is transferred only once across recoveries*
     - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)
     - The serializers: (yes / no / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / no / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)
     - The S3 file system connector: (yes / no / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / no)
     - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jul/19 17:06;githubbot;600","tillrohrmann commented on pull request #9183: [FLINK-12768][tests] FlinkKinesisConsumerTest.testSourceSynchronization flakiness
URL: https://github.com/apache/flink/pull/9183
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 17:41;githubbot;600","tweise commented on pull request #9136: [FLINK-12768][tests] FlinkKinesisConsumerTest.testSourceSynchronization flakiness
URL: https://github.com/apache/flink/pull/9136
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 17:52;githubbot;600",,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 17:39:45 UTC 2019,,,,,,,,,,"0|z03h34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/19 19:45;thw;I opened a PR hoping to be able to reproduce this on Travis with additional information. Unable to do so locally.;;;","02/Aug/19 17:39;trohrmann;Fixed via

1.10.0: 8da696535d0c39323f480cae8f4e9c66e866bec4
1.9.0: 66d0e31294b2588e1aabccb952fdbf2bcfabe878;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpillableSubpartitionTest deadlocks on Travis,FLINK-12740,13237700,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjwang,chesnay,chesnay,05/Jun/19 10:27,06/Jun/19 09:14,13/Jul/23 08:10,06/Jun/19 09:14,1.8.1,,,,,,,,,1.8.1,,,,Runtime / Network,,,,,0,pull-request-available,,,,https://travis-ci.org/apache/flink/jobs/541225542,,pnowojski,zjwang,,,,,,,,,,,,"zhijiangW commented on pull request #8635: [FLINK-12740][tests] Fix unstable SpillableSubpartitionTest
URL: https://github.com/apache/flink/pull/8635
 
 
   ## What is the purpose of the change
   
   *`SpillableSubpartitionTest#testConcurrentRequestAndReleaseMemory` is for verifying that `ResultPartition#releaseMemory` and `LocalBufferPool#requestBufferBuilderBlocking` are concurrent in two threads. But the `Future#thenRun` is used to trigger another action which might be executed by the same previous thread to cause stuck. We could use `Future#thenRunAsync` instead to solve this unstable issue.*
   
   ## Brief change log
   
     - *Fix `SpillableSubpartitionTest#testConcurrentRequestAndReleaseMemory`*
   
   ## Verifying this change
   
   Covered by itself.
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (yes / **no**)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)
     - The serializers: (yes / **no** / don't know)
     - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)
     - The S3 file system connector: (yes / **no** / don't know)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (yes / **no**)
     - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Jun/19 15:42;githubbot;600","pnowojski commented on pull request #8635: [FLINK-12740][tests] Fix unstable SpillableSubpartitionTest
URL: https://github.com/apache/flink/pull/8635
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jun/19 09:13;githubbot;600",,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,FLINK-12544,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 06 09:14:41 UTC 2019,,,,,,,,,,"0|z03fqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/19 11:21;pnowojski;I guess this is probably caused by: https://issues.apache.org/jira/browse/FLINK-12544 ?;;;","05/Jun/19 12:10;zjwang;Yes, the new added uint test for FLINK-12544 might be not stable. It runs always successful in the first time on my side. But if it runs repeatedly in a loop, it would stuck at the third time. I would double check this test and might submit a fix later today.;;;","06/Jun/19 09:14;pnowojski;Fixed as  c7bf460 in release-1.8 branch.;;;","06/Jun/19 09:14;pnowojski;Was caused by new test added in the above mentioned ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink does not build with scala 2.12,FLINK-12739,13237696,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Jun/19 10:18,07/Jun/19 08:06,13/Jul/23 08:10,07/Jun/19 08:06,1.9.0,,,,,,,,,1.9.0,,,,Build System,Table SQL / API,,,,0,,,,,"https://travis-ci.org/apache/flink/jobs/541175138

Bit of a weird one:

{code}
12:02:31.766 [INFO] flink-sql-parser ................................... FAILURE [  0.005 s]
12:02:31.766 [INFO] flink-table-planner-blink .......................... SKIPPED
12:02:31.766 [INFO] flink-contrib ...................................... SKIPPED
12:02:31.766 [INFO] flink-connector-wikiedits .......................... SKIPPED
12:02:31.766 [INFO] flink-yarn-tests ................................... SKIPPED
12:02:31.766 [INFO] flink-fs-tests ..................................... SKIPPED
12:02:31.766 [INFO] flink-docs ......................................... SKIPPED
12:02:31.766 [INFO] flink-ml-parent .................................... SKIPPED
12:02:31.766 [INFO] flink-ml-api ....................................... SKIPPED
12:02:31.766 [INFO] ------------------------------------------------------------------------
12:02:31.766 [INFO] BUILD FAILURE
12:02:31.766 [INFO] ------------------------------------------------------------------------
12:02:31.766 [INFO] Total time: 25:06 min
12:02:31.766 [INFO] Finished at: 2019-06-04T12:02:31+00:00
12:02:34.382 [INFO] Final Memory: 460M/1232M
12:02:34.382 [INFO] ------------------------------------------------------------------------
12:02:34.384 [ERROR] Could not find goal 'regex-property' in plugin org.codehaus.mojo:build-helper-maven-plugin:1.5 among available goals reserve-network-port, parse-version, maven-version, add-test-resource, add-test-source, add-resource, attach-artifact, add-source, remove-project-artifact, help -> [Help 1]
{code}",,aljoscha,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 07 08:06:20 UTC 2019,,,,,,,,,,"0|z03fpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/19 10:22;tzulitai;Could this be just a hiccup? From the master branch the build failure doesn't seem to be deterministic.;;;","05/Jun/19 10:23;chesnay;we only build scala 2.12 in the cron branches, and this has been failing for several days now. This could be just an issue with the plugin version, this module uses 1.5 while all other scala modules use 1.7 .;;;","07/Jun/19 08:06;chesnay;master: 8c3e94c3daf820050bd392ca52f5a195dfb1e7f8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceManager may release TM with allocated slots,FLINK-12736,13237676,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,azagrebin,chesnay,chesnay,05/Jun/19 08:33,10/Jul/19 20:54,13/Jul/23 08:10,10/Jul/19 20:53,1.7.2,1.8.1,1.9.0,,,,,,,1.7.3,1.8.2,1.9.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The {{ResourceManager}} looks out for TaskManagers that have not had any slots allocated on them for a while, as these could be released to safe resources. If such a TM is found the RM checks via an RPC call whether the TM still holds any partitions. If no partition is held then the TM is released.

However, in the RPC callback no check is made whether the TM is actually _still_ idle. In the meantime a slot could've been allocated on the TM.

",,azagrebin,kisimple,trohrmann,,,,,,,,,,,"azagrebin commented on pull request #8988: [FLINK-12736][coordination] Release TaskExecutor in SlotManager if no slot allocations after partition check
URL: https://github.com/apache/flink/pull/8988
 
 
   ## What is the purpose of the change
   
   The ResourceManager looks out for TaskManagers that have not had any slots allocated on them for a while, as these could be released to safe resources. If such a TM is found, the RM checks via an RPC call whether the TM still holds any partitions. If no partition is held then the TM is released. However, in the RPC callback no check is made whether the TM is actually still idle. In the meantime a slot could have been allocated on the TM. Even if the slot has been freed, there can be newly allocated partitions not included in check result.
   
   To make sure there was no resource allocation in between, we can mark the `taskManagerRegistration.getIdleSince()` time before starting the async 'no partition' check. The TM can be released only if the idle time after the check matches the previously marked one. Otherwise we discard the release and start over after the next timeout.
   
   ## Brief change log
   
     - mark the TM idle time before `canBeReleased` check in `SlotManager.checkTaskManagerTimeouts`
     - check it after the `canBeReleased` check and release only if it coincides
     - modify `SlotManagerTest.testTaskManagerNotReleasedBeforeItCanBe` to check that TM is not released if there was allocation between triggering of `canBeReleased` check and completing it
   
   ## Verifying this change
   
   run `SlotManagerTest.testTaskManagerNotReleasedBeforeItCanBe`
   
   ## Does this pull request potentially affect one of the following parts:
   
     - Dependencies (does it add or upgrade a dependency): (no)
     - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)
     - The serializers: (no)
     - The runtime per-record code paths (performance sensitive): (no)
     - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)
     - The S3 file system connector: (no)
   
   ## Documentation
   
     - Does this pull request introduce a new feature? (no)
     - If yes, how is the feature documented? (not applicable)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Jul/19 14:25;githubbot;600","tillrohrmann commented on pull request #8988: [FLINK-12736][coordination] Release TaskExecutor in SlotManager if no slot allocations after partition check
URL: https://github.com/apache/flink/pull/8988
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 12:20;githubbot;600","azagrebin commented on pull request #9041: [FLINK-12736][coordination] Release TaskExecutor in SlotManager if no slot allocations after partition check (1.8)
URL: https://github.com/apache/flink/pull/9041
 
 
   1.8 backport of #8988
   cc @tillrohrmann 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 12:44;githubbot;600","azagrebin commented on pull request #9042: [FLINK-12736][coordination] Release TaskExecutor in SlotManager if no slot allocations after partition check (1.7)
URL: https://github.com/apache/flink/pull/9042
 
 
   1.7 backport of #8988
   cc @tillrohrmann 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 12:44;githubbot;600","tillrohrmann commented on pull request #9042: [FLINK-12736][coordination] Release TaskExecutor in SlotManager if no slot allocations after partition check (1.7)
URL: https://github.com/apache/flink/pull/9042
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 20:53;githubbot;600","tillrohrmann commented on pull request #9041: [FLINK-12736][coordination] Release TaskExecutor in SlotManager if no slot allocations after partition check (1.8)
URL: https://github.com/apache/flink/pull/9041
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 20:54;githubbot;600",,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,FLINK-10941,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 10 20:53:19 UTC 2019,,,,,,,,,,"0|z03fl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/19 12:40;trohrmann;As a corollary, it could also happen that new partitions are stored on the TM if it can have allocated slots when the callback is being processed. I guess in order to properly solve this problem we would need something like a message counter between the RM and the TM. Only if the message counter is the same as before sending the partition check message, we can be sure that nothing has changed on the TM.;;;","04/Jul/19 08:55;azagrebin;FLINK-12736 is a followup for FLINK-10941;;;","04/Jul/19 09:26;azagrebin;Thanks for the corollary, [~till.rohrmann], I think it is a valid concern.

Alternatively to counters, we could use a simpler approach. We can mark the _taskManagerRegistration.getIdleSince()_ time before starting the async 'no partition' check. The TM can be released only if the idle time after the check matches the previously marked one. Otherwise we discard the release and start over after the next timeout. This way we could also guarantee that there was no resource allocation in between.;;;","05/Jul/19 08:53;trohrmann;This sounds like a very good idea [~azagrebin].;;;","10/Jul/19 20:53;trohrmann;Fixed via
1.9.0: 5c762cec46
1.8.2: 48f5c78177abb322ba6a7d60f375973f22831146
1.7.3: 9069f4dd82df715bf50a2bc45b968283c161f98b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
